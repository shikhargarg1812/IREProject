[
    {
        "video_id": "P0fMwA3X5KI",
        "video_title": "NVIDIA's Image Restoration AI: Almost Perfect",
        "position_in_playlist": 0,
        "description": "The paper \"Noise2Noise: Learning Image Restoration without Clean Data\" and its source code are available here:\n1. https://arxiv.org/abs/1803.04189\n2. https://github.com/NVlabs/noise2noise\n3. https://news.developer.nvidia.com/ai-can-now-fix-your-grainy-photos-by-only-looking-at-grainy-photos/\n\nHave a look at this too, some materials are now available for download! - https://developer.nvidia.com/rtx/ngx\n\nUnofficial implementation: https://github.com/yu4u/noise2noise\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-226279/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Image denoising is an area where we have a noisy image as an input, and we wish to get a clear, noise-free image. Neural network-based solutions are amazing at this, because we can feed them a large amount of training data with noisy inputs and clear outputs. And if we do that, during the training process, the neural network will be able to learn the concept of noise, and when presented with a new, previously unseen noisy image, it will be able to clear it up. However, with light transport simulations, creating a noisy image means following the path of millions and millions of light rays, which can take up to hours per training sample. And we need thousands, or potentially hundreds of thousands of these! There are also other cases where creating the clean images for the training set is not just expensive, but flat out impossible. Low-light photography, astronomical imaging, or magnetic resonance imaging, MRI in short are great examples of this. In these cases, we cannot use our neural networks simply because we cannot build such a training set as we don't have access to the clear images. In this collaboration between NVIDIA, Aalto University and MIT, scientists came up with an insane idea: let's try to train a neural network without clear images and use only noisy data. Normally, we would say that this is clearly impossible and end this research project. However, they show that under a suitable set of constraints, for instance, one reasonable assumption about the distribution of the noise opens up the possibility of restoring noisy signals without seeing clean ones. This is an insane idea that actually works, and can help us restore images with significant outlier content. Not only that, but it is also shown that this technique can do close to or just as well as other previously known techniques that have access to clean images. You can look at these images, many of which have many different kinds of noise, like camera noise, noise from light transport simulations, MRI imaging, and images severely corrupted with a ton of random text. The usual limitations apply, in short, it of course cannot possibly recover content if we cut out a bigger region from our images. This severely hamstrung training process can be compared to a regular neural denoiser that has access to the clean images, and the differences are negligible most of the time. So how about that, we can teach a neural network to denoise without ever showing it the concept of denoising. Just the thought of this boggles my mind so much it keeps me up at night. This is such a remarkable concept - I hope there will soon be followup papers that extend this idea to other problems as well. If you enjoyed this episode and you feel, that about 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. We use these funds to make better videos for you, and a small portion is also used to fund research conferences. You can find us at patreon.com/TwoMinutePapers and there is also a link to it in the video description. You know the drill, one dollar is almost nothing, but it keeps the papers coming. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=P0fMwA3X5KI",
        "paper_link": "https://arxiv.org/abs/1803.04189",
        "paper_title": "Noise2Noise: Learning Image Restoration without Clean Data"
    },
    {
        "video_id": "LBezOcnNJ68",
        "video_title": "NVIDIA's AI Makes Amazing Slow-Mo Videos",
        "position_in_playlist": 1,
        "description": "The paper \"Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation\" is available here:\nhttps://people.cs.umass.edu/~hzjiang//projects/superslomo/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nHave a look at this too, some materials are now available for download! - https://developer.nvidia.com/rtx/ngx\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-848903/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. How about some slow-motion videos! If we would like to create a slow-motion video and we don't own an expensive slow-mo camera, we can try to shoot a normal video and simply slow it down. This sounds good on paper, however, the more we slow it down, the more space we have between our individual frames, and at some point, our video will feel more like a slideshow. To get around this problem, in a previous video, we discussed two basic techniques to fill in these missing frames: one was a naive technique called frame blending that basically computes the average of two images. In most cases, this doesn't help all that much because it doesn't have an understanding of the motion that takes place in the video. The other one was optical flow. Now this one is much smarter as it tries to estimate the kind of translation and rotational motions that take place in the video and they typically do much better. However, the disadvantage of this is that it usually takes forever to compute, and, it often introduces visual artifacts. So now, we are going to have a look at NVIDIA's results, and the main points of interest are always around the silhouettes of moving objects, especially around regions where the foreground and the background meet. Keep an eye out for these regions throughout this video. For instance, here is one example I've found. Let me know in the comments section if you have found more. This technique builds on U-Net, a superfast convolutional neural network architecture that was originally used to segment biomedical images from limited training data. This neural network was trained on a bit over a 1.000 videos and computes multiple approximate optical flows and combines them in a way that tries to minimize artifacts. As you see in these side by side comparisons, it works amazingly well. Some artifacts still remain, but are often hard to catch. And, this architecture blazing fast. Not real-time yet, but creating a few tens of these additional frames takes only a few seconds. The quality of the results is also evaluated and compared to other works in the paper, so make sure to have a look! As the current, commercially available tools are super slow and take forever, I cannot wait to be able to use this technique to make some more amazing slow motion footage for you Fellow Scholars! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=LBezOcnNJ68",
        "paper_link": "https://people.cs.umass.edu/~hzjiang//projects/superslomo/",
        "paper_title": "Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"
    },
    {
        "video_id": "eSaShQbUJTQ",
        "video_title": "DeepMind's AI Takes An IQ Test",
        "position_in_playlist": 2,
        "description": "The paper \"Measuring abstract reasoning in neural networks\" is available here:\nhttp://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-1867751/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Throughout this series, we have seen many impressive applications of artificial intelligence. These techniques are capable of learning the piano from the masters of the past, beat formidable teams in complex games like DOTA 2, perform well in the game Sonic, The Hedgehog, or help us revive and impersonate famous actors who are not with us anymore. However, what is often not spoken about is how narrow, or how general these AI programs are. A narrow AI means an agent that can perform one task really well, but cannot perform other, potentially easier tasks. The holy grail of machine learning research is a general AI that is capable of obtaining new knowledge by itself through abstract reasoning. This is similar to how humans learn and is a critical step in obtaining a general AI. And to tackle this problem, scientists at DeepMind created a program that is able to generate a large amount of problems that test abstract reasoning capabilities. They are inspired by human IQ-tests with all the these questions about sizes, colors and progressions. They designed the training process in a way that the algorithm is given training data on the progression of colors, but it is never shown similar progression examples that involve object sizes. The concept is the same, but the visual expression of the progression is different. A human easily understands the difference, but teaching abstract reasoning like this to a computer sounds almost impossible. However, now we have a tool that can create many of these questions and the correct answers to them. And I will note that some of these are not as easy as many people would expect. For instance, a vertical number progression is relatively easy to spot, but have a good look at these ones...not so immediately apparent, right? Going back to being able to generate lots and lots of data...the black-belt Fellow Scholars know exactly what this means. This means that we can train a neural network to perform this task! Unfortunately, existing techniques and architectures perform quite poorly. Despite the fact that we have a ton of training data, they could only get 22-42% of the answers right. However, these networks are amazing at doing other things, like writing novels or image classification, therefore this means that their generalization capabilities are not too great when we go outside their core domain. This new technique goes by the name \"Wild Relation Network\" and is trained in a way that encourages reasoning. It is also designed in a way that it not only outputs a guess for the results, but also tries to provide a reason for it, which interestingly, further improved the accuracy of the network. And what is this accuracy we are talking about? It finds the correct solution 62.6% of the time. But it gets better, because this result was measured in the presence of distractor objects like these annoying lines and circles. This is quite confusing, even for humans, so a result above 60% is quite remarkable. And it gets even better, because if we don't use these distractions, it is correct 78% of the time. Wow! This is indeed a step towards teaching an AI how to reason, and as the authors made this dataset publicly available for everyone, I expect a reasonable amount of research works appearing in this area in the near future - who knows, perhaps even in the next few months. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=eSaShQbUJTQ",
        "paper_link": "http://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf",
        "paper_title": "Measuring abstract reasoning in neural networks"
    },
    {
        "video_id": "MvFABFWPBrw",
        "video_title": "DeepMind Has A Superhuman Level Quake 3 AI Team! \ud83d\ude80",
        "position_in_playlist": 3,
        "description": "Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nThe paper \"Human-level performance in first-person multiplayer games with population-based deep reinforcement learning\" and its corresponding blog post are available here:\n1. https://arxiv.org/abs/1807.01281\n2. https://deepmind.com/blog/capture-the-flag/\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepMind #Quake #Quake3",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. After having a look at OpenAI's effort to master the DOTA 2 game, of course, we all know that scientists at DeepMind are also hard at work on an AI that beats the Capture The Flag game mode in Quake 3. Quake III Arena is an iconic first person shooter game and Capture the Flag is a fun game mode where each team tries to take the other team's flag and carry it to their own base while protecting their own. This game mode requires good aiming skills, map presence, reading the opponents well, and tons of strategy. A nightmare situation for any kind of AI. Not only that, but in this version, the map changes from game to game, therefore the AI has to learn general concepts and be able to pull them off in a variety of different previously unseen conditions. This doesn't seem to be within the realm of possibilities to pull off. The minimaps here always show the location of the players, each are color coded to blue or red to indicate their teams. Much like humans, these AI agents learned by looking at the video output of the game and have never been told anything about the game or what the rules are. These scientists at DeepMind ran a tournament with 40 human players who were matched up against these agents randomly, both as opponents and teammates. In this tournament, a team of average human players had a win probability of 43%, where a team of strong players won slightly more than half, 52% of their games. And now hold on to your papers, because the agents were able to win 74% of their games. So the difference between the average and strong human player's winrate is 9%, and the difference between the strongest humans and the AI is more than twice that margin, 22%. This is insanity. And as you see, it barely matters what the size or the layout of the map is or how many teammates there are, the AI's winrate is always remarkably high. These agents showcase many humanlike behaviors such as staying at their own base to defend it, camping within the opponent's base, or following teammates. This builds on a new architecture by the name For The Win, FTW in short, good work folks. Instead of training one agent, it uses a population of agents that train and evolve from each other to make sure that a diverse set of playstyles are discovered. This uses recurrent neural networks, these are neural network variants that are able to learn and produce sequences of data. Here, two of these are used, a fast and a slow one that operate on different timescales but share a memory module. This means that one of them has a very accurate look at the near past, and the other one has a more coarse look, but can look back more into the past in return. If these two work together correctly, decisions can be made that are both good locally, at this point in time, and globally, to maximize the probability of winning the whole game. This is really huge because this algorithm can perform long-term planning, which is one of the key reasons why many difficult games and tasks still remain unsolved. Well, as it seems now, not for long. An additional challenge is that the game score is not necessarily subject to maximization like in most games, but there is a mapping from the scores into an internal reward, which means that the algorithm has to be able to predict its own progress towards winning. And note that even though Quake 3 and Capture The Flag is an excellent way to demonstrate the capabilities of this algorithm, this architecture can be generalized to other problems. I am going to give you a few more tidbits that I have found super interesting, but before, if you are enjoying this episode and would like to pick up some cool perks like early access, deciding the topic of future episodes or getting your name listed in the video description as a key supporter, why not support the show on Patreon? With this, you also help us make better videos in the future. You can find us at Patreon.com/TwoMinutePapers and we also support Bitcoin and other cryptocurrencies, the addresses are available in the video description. And now, onwards to the cool tidbits: - a human+agent team has been able to defeat an agent+agent team 5% of the time, indicating that these AIs are able to coordinate and play together with anyone they are given. I get goosebumps from this. Love it. - The reaction time and accuracy of the agents is better than that of humans, but not nearly perfect as many people would think. However, they outclass humans even if we artificially reduce their accuracy and reaction times. - In another experiment, two agents were paired up against two professional game tester humans who could freely communicate and train against the same agents for 12 hours to see if they can learn their patterns and force them to make mistakes. Even with this, humans had only won 25% of these games. Given the other numbers we have, it is very likely that this unfair advantage made no difference whatsoever. How about that. If there are any more questions, make sure to have a look at the paper that describes every possible tidbit you can possibly imagine. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MvFABFWPBrw",
        "paper_link": "https://arxiv.org/abs/1807.01281",
        "paper_title": "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning"
    },
    {
        "video_id": "xHpwLiTieu4",
        "video_title": "This is How You Hack A Neural Network",
        "position_in_playlist": 4,
        "description": "The paper \"Adversarial Reprogramming of Neural Networks\" is available here:\nhttps://arxiv.org/abs/1806.11146\n\nAndrej Karpathy's image classifier: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-1839406/\nCaptcha image source: https://en.wikipedia.org/wiki/CAPTCHA#/media/File:Captchacat.png\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a mind-boggling new piece of work from scientists at Google Brain on how to hack and reprogram neural networks to make them perform any task we want. A neural network is given by a prescribed number of layers, neurons within these layers and weights, or in other words, the list of conditions under which these neurons will fire. By choosing the weights appropriately, we can make the neural network perform a large variety of tasks, for instance, to tell us what an input image depicts, or predict new camera viewpoints when looking at a virtual scene. So this means that by changing the weights of a neural network, we can reprogram it to perform something completely different, for instance, solve a captcha for us. That is a really cool feature. This work reveals a new kind of vulnerability by performing this kind of reprogramming of neural networks in an adversarial manner, forcing them to perform tasks that they were originally not intended to do. It can perform new tasks that it has never done before, and these tasks are chosen by the adversary. So how do adversarial attacks work in general? What does this mean? Let's have a look at a classifier. These neural networks are trained on a given, already existing dataset. This means that they look at a lot of images of buses, and from these, they learn the most important features that are common across buses. Then, when we give them a new, previously unseen image of a bus, they will now be able to identify whether we are seeing a bus or an ostrich. A good example of an adversarial attack is when we present such a classifier with not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. And in this new work, we are not only interested in forcing the neural network to make a mistake, but we want it to make exactly the kind of mistake we want! That sounds awesome, but also, quite nebulous, so let's have a look at an example. Here, we are trying to reprogram an image classifier to count the number of squares in our images. Step number one, we create a mapping between the classifier's original labels to our desired labels. Initially, this network was made to identify animals like sharks, hens, and ostriches. Now, we seek to get this network to count the number of squares in our images, so we make an appropriate mapping between their domain and our domain. And then, we present the neural network with our images, these images are basically noise and blocks, where the goal is to create these in a way that they coerce the neurons within the neural network to perform our desired task. The neural network then says tiger shark and ostrich, which when mapped to our domain, means 4 and 10 squares respectively, which is exactly the answer we were looking for. Now as you see, the attack is not subtle at all, but it doesn't need to be. Quoting the paper: \"The attack does not need to be imperceptible to humans, or even subtle, in order to be considered a success. Potential consequences of adversarial reprogramming include theft of computational resources from public facing services, and repurposing of AI-driven assistants into spies or spam bots.\". As you see, it is of paramount importance that we talk about AI safety within this series and my quest is to make sure that everyone is vigilant that now, tools like this exist. Thank you so much for coming along on this journey, and if you're enjoying it, make sure to subscribe and hit the bell icon to never miss a future episode, some of which will be on followup papers on this super interesting topic. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=xHpwLiTieu4",
        "paper_link": "https://arxiv.org/abs/1806.11146",
        "paper_title": "Adversarial Reprogramming of Neural Networks"
    },
    {
        "video_id": "8GUYAVXmhsI",
        "video_title": "DeepMind's AI Learns The Piano From The Masters of The Past",
        "position_in_playlist": 5,
        "description": "The paper \"The challenge of realistic music generation: modelling raw audio at scale\" is available here:\nhttps://arxiv.org/abs/1806.10474\nhttps://drive.google.com/drive/folders/1fvS-DU8AcK078-5k6WGudiBn0XSeE0_D\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-1839406/\nScore image credit: https://pixabay.com/en/piano-music-score-music-sheet-1655558/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we will listen to a new AI from DeepMind that is capable of creating beautiful piano music. Because there are many other algorithms that do that, to put things into perspective, let's talk about the two key differentiating factors that set this method apart from previously existing techniques. One, music is typically learned from high-level representations, such as the score or MIDI data. This is a precise representation of what needs to be played, but they don't tell us how to play them. These small nuances are what makes the music come alive, and this is exactly what is missing from most of the synthesis techniques. This new method is able to learn these structures and generates not midi signals but raw audio waveforms. And two, it is better at retaining stylistic consistency. Most previous techniques create music that is consistent on a shorter time-scale, but do not take into consideration what was played 30 seconds ago, and therefore they lack the high-level structure that is the hallmark of quality songwriting. However, this new method shows stylistic consistency over longer time periods. Let's give it a quick listen and talk about the architecture of this learning algorithm after that. While we listen, I'll show you the composers it has learned from to produce this. I have  never heard any AI-generated music before with such articulation and the harmonies are also absolutely amazing. Truly stunning results. It uses an architecture that goes by the name autoregressive discrete autoencoder. This contains an encoder module that takes a raw audio waveform and compresses it down into an internal representation, where the decoder part is responsible for reconstructing the raw audio from this internal representation. Both of them are neural networks. The autoregressive part means that the algorithm looks at previous time steps in the learned audio signals when producing new notes, and is implemented in the encoder module. Essentially, this is what gives the algorithm longer-term memory to remember what it played earlier. As you have seen the dataset the algorithm learned from as the music was playing, I am also really curious how we can exert artistic control over the output by changing the dataset. Essentially, you can likely change what the student learns by changing the textbooks used to teach them. For now, let's marvel at one more sound sample. This is already incredible, and I can only imagine what we will be able to do not ten years from now, just a year from now. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=8GUYAVXmhsI",
        "paper_link": "https://arxiv.org/abs/1806.10474",
        "paper_title": "The challenge of realistic music generation: modelling raw audio at scale"
    },
    {
        "video_id": "yEOEqaEgu94",
        "video_title": "OpenAI + DOTA2: 180 Years of Learning Per Day",
        "position_in_playlist": 6,
        "description": "The blog post on OpenAI Five is available here:\n1. https://blog.openai.com/openai-five/\n2. https://blog.openai.com/openai-five-benchmark/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nDay9's Learning DOTA2 series. Note - sometimes explicit: https://www.youtube.com/watch?v=8AyrC5Ki31c&list=PLgmCLtUkEutILNA9EM0BON6ShoQGZhd3P\n\nI also recommend GameLeap's channel:\nhttps://www.youtube.com/channel/UCy0-ftAwxMHzZc74OhYT3PA/videos\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: OpenAI\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. You know that I am always excited to tell you about news where AI players manage to beat humans at more and more complex games. Today we are going to talk about DOTA 2 which is a multiplayer online battle arena game with a huge cult following and world championship events with a prize pool of over 40 million dollars. This is not just some game, and just to demonstrate how competitive it is and how quickly it is growing, last time we talked about this in Two Minute Papers episode 180 where an AI beat some of the best players of the game in a limited 1 versus 1 setting, and the prize pool was 20 million dollars back then. This was a huge milestone as this game requires long-term strategic planning, has incomplete information and a high-dimensional continuous action space, which is a classical nightmare situation for any AI. Then, the next milestone was set to defeat a human team in the full 5 versus 5 game, and I promised to report back when there is something new on this project. So here we go. If you look through the forums and our YouTube comments, it is generally believed that this is so complex that it would never ever happen. I would agree that the search space is indeed stupendously large and the problem is notoriously difficult, but whoever thinks this will never be solved has clearly not been watching enough Two Minute Papers. Now, you better hold on to your papers right away, because this video dropped 10 months ago, in August 2017, and since then, the AI has played 180 years worth of gameplay against itself every single day.  80% of these games it played against itself, and 20% against its past self, and even though five of these bots are supposed to work together as a team, there is no explicit communication channel between them. And now, it is ready to play 5 versus 5 matches. Some limitations still apply, but since then, the AI was able get a firm understanding of the importance of teamfighting, predicting the outcome of future actions and encounters, ganking, or in other words, ambushing unsuspecting opponents and many other important pieces of the game. The May 15th version of the AI was evenly matched against OpenAI's in-house team, which is a formidable result, and I find it really amusing that these scientists were beaten by their own algorithm. This is, however, not a world class DOTA 2 team. And the crazy part is that the next version of the AI was tested three weeks later, and it not only beat the in-house team easily, but also defeated several other teams and a semi-professional team as well. As it is often incorrectly said on several forums that these algorithms defeat humans because they can click faster, so I will note that that these bots perform about 150-170 actions per minute, which is approximately in line with an intermediate human player, and it is also to be noted that DOTA2 is not that sensitive to this metric. More clicking does really not mean more winning here at all. The human players were also able to train with an earlier version of this AI. There will be an upcoming event on July 28th where these bots will challenge a team of top players, so stay tuned for some more updates on this! There is no paper yet, but I've put a link to a blog post and the full video in the description, and it is a gold mine of information and was such a joy to read through. So, what do you think? Who will win? And is a 5 versus 5 game in DOTA 2 more complex than playing Starcraft 2? If you wish to hear more about this, please consider helping us tell this story to more people and convert them into Fellow Scholars by supporting the series through Patreon, and as always, we also accept Bitcoin, Ethereum, and Litecoin, the addresses are in the video description. And if you are now in the mood to learn some more about DOTA 2, I recommend taking a look at Day9's channel, I've put a link to a relevant series in the video description. Highly recommended. So there you go, a fresh Two Minute Papers episode, that's not two minutes and it's not about a paper. Yet. Love it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=yEOEqaEgu94"
    },
    {
        "video_id": "2FHHuRTkr_Y",
        "video_title": "OpenAI's Gaming AI Contest: Results | Two Minute Papers #265",
        "position_in_playlist": 7,
        "description": "Two Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThe blog post \"Retro Contest: Results\" and the corresponding paper is available here:\n1. https://blog.openai.com/first-retro-contest-retrospective/\n2. https://arxiv.org/abs/1804.03720\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-933427/\nMusic credit: https://opengameart.org/content/nes-shooter-music-5-tracks-3-jingles\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a contest by OpenAI where a bunch of AIs compete to decide who has the best transfer learning capabilities. Transfer learning means that the training and the testing environments differ significantly, therefore only the AIs that learn general concepts prevail, and the ones that try to get by with memorizing things will quickly fall. In this experiment, these programs start playing Sonic the Hedgehog, and are given a bunch of levels to train on. However, like in a good test at school, the levels for the final evaluation are kept secret! So, the goal is that only high-quality general algorithms prevail and we can't cheat through the program as we don't know what the final exam will entail. We only know that we have to make the most of the training materials to pass. Sonic is a legendary platform game where we have to blaze through levels by avoiding obstacles and traps often while traveling with the speed of sound. Here you can see the winning submission taking the exam on a previously unseen level. After one minute of training, as expected, the AI started to explore the controls, but is still quite inept and does not make any meaningful progress on the level. After 30 minutes, things look significantly better as the AI now understands the basics of the game. And look here, almost got up there, and, got it. It is clearly making progress as it collects some coins, defeats enemies, goes through the loop, and gets stuck seemingly because it doesn't yet know how being under water changes how high it can jump. This is quite a bit of a special case, so, we are getting there. After only 60-120 minutes, it became a competent player and was able to finish this challenging map with only a few mistakes. Really impressive transfer learning in just about an hour. And note that the algorithm has never seen this level before. Here you see a really cool visualization of three different AI's progress on the map, where the red dots indicate the movement of the character for earlier episodes, and the bluer colors show the progress at later stages of the training. I could spend all day staring at these. Videos are available for many many submissions, some of which even opened up their source code and there are a few high-quality write-ups as well, so make sure to have a look! There's gonna be lots of fun to be had there. This competition gives us something that is scientifically interesting, practical, and super fun at the same time. What more could you possibly want? Huge thumbs up for the OpenAI team for organizing this, and of course, congratulations to the participants. And now you see that we have a job where we train computers to play video games, and we are even paid for it. What a time to be alive. By the way, if you wish to unleash the inner scholar in you, Two Minute Papers shirts are available in many sizes and colors. We have mugs too! The links are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2FHHuRTkr_Y"
    },
    {
        "video_id": "lCoR-4OlIZI",
        "video_title": "Style Transfer...For Smoke and Fluids! | Two Minute Papers #264",
        "position_in_playlist": 8,
        "description": "The paper \"Example-based Turbulence Style Transfer\" is available here: http://nishitalab.org/user/syuhei/TurbuStyleTrans/turbu_styletrans.html\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-984175/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Fluid and smoke simulations are widely used in computer games and in the movie industry, and are capable of creating absolutely stunning video footage. We can very quickly put together a coarse simulation and run it cheaply, however, the more turbulent motion we're trying to simulate, the more resources and time it will take. If we wish to create some footage with the amount of visual quality that you see here, well, if you think the several hour computation time for light transport algorithms was too much, better hold on to your papers because it will take not hours, but often from days to weeks to compute. And to ease the computation time of such simulations, this is a technique that performs style transfer, but this time, not for paintings, but for fluid and smoke simulations! How cool is that! It takes the low-resolution source and detailed target footage, dices them up into small patches and borrows from image and texture synthesis techniques to create a higher resolution version of our input simulation. The challenge of this technique is that we cannot just put more swirly motion on top of our velocity fields because this piece of fluid has to obey to the laws of physics to look natural. Also, we have to make sure that there is not too much variation from patch to patch, so we have to perform some sort of smoothing on the boundaries of these patches. Also, our smoke plumes also have to interact with obstacles, which is anything but trivial to do well. Have a look at the ground truth results from the high resolution simulation - this is the one that would take a long time to compute. There are clearly deviations, but given how coarse the input footage was, I'll take this any day of the week. We can now look forward to seeing even higher quality smoke and fluids in the animation movies of the near future. There was a similar technique by the name Wavelet Turbulence, which is one of my all-time favorite papers that has been showcased in the very first Two Minute Papers episode. This is what it looked like and we are now celebrating its tenth anniversary. Imagine what a bomb this was ten years ago, and you know what, it is still going strong. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lCoR-4OlIZI",
        "paper_link": "http://nishitalab.org/user/syuhei/TurbuStyleTrans/turbu_styletrans.html",
        "paper_title": "Example-based Turbulence Style Transfer"
    },
    {
        "video_id": "gnctSz2ofU4",
        "video_title": "DeepMind's AI Learns To See | Two Minute Papers #263",
        "position_in_playlist": 9,
        "description": "Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe papers \"Neural scene representation and rendering\" and \"Gaussian Material Synthesis\" are available here:\n1. https://deepmind.com/documents/211/Neural_Scene_Representation_and_Rendering_preprint.pdf\n2. https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-2035427/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a recent DeepMind paper on neural rendering where they taught a learning-based technique to see things the way humans do. What's more, it has an understanding of geometry, viewpoints, shadows, occlusion, even self-shadowing and self-occlusion, and many other difficult concepts. So what does this do and how does it work exactly? It contains a representation and a generation network. The representation network takes a bunch of observations, a few screenshots if you will, and encodes this visual sensory data into a concise description that contains the underlying information in the scene. These observations are made from only a handful of camera positions and viewpoints. The neural rendering or seeing part means that we choose a position and viewpoint that the algorithm hasn't seen yet, and ask the generation network to create an appropriate image that matches reality. Now, we have to hold on to our papers for a moment and understand why this is such a crazy idea. Computer graphics researchers work so hard on creating similar rendering and light simulation programs that take tons of computational power to compute all aspects of light transport and in return, give us a beautiful image. If we slightly change the camera angles, we have to redo most the same computations, whereas a learning-based algorithm may just say \"don't worry, I got this\", and from previous experience, guesses the remainder of the information perfectly. I love it. And what's more, by leaning on what these two networks learned, it generalizes so well that it can even deal with previously unobserved scenes. If you remember, I have also worked on a neural renderer for about 3000 hours and created an AI that predicts photorealistic images perfectly. The difference was that this one took a fixed camera viewpoint, and predicted what the object would look like if we started changing its material properties. I'd love to see a possible combination of these two works, oh my! Super excited for this. There is a link in the video description to both of these works. Can you think of other possible uses for these techniques? Let me know in the comments section! And, if you wish to decide the order of future episodes or get your name listed as a key supporter for the series, hop over to our Patreon page and pick up some cool perks. We use these funds to improve the series and empower other research projects and conferences. As this video series is on the cutting edge of technology, of course, we also support cryptocurrencies like Bitcoin, Ethereum, and Litecoin. The addresses are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=gnctSz2ofU4",
        "paper_link": "https://deepmind.com/documents/211/Neural_Scene_Representation_and_Rendering_preprint.pdf",
        "paper_title": "Neural scene representation and rendering"
    },
    {
        "video_id": "KEdrBMZx53w",
        "video_title": "Infinite Walking in Virtual Reality | Two Minute Papers #262",
        "position_in_playlist": 10,
        "description": "The paper \"Towards Virtual Reality Infinite Walking: Dynamic Saccadic Redirection \" is available here:\nhttp://research.nvidia.com/publication/2018-08_Towards-Virtual-Reality\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-2561233/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "We are over 260 episodes into the series, but believe it or not, we haven't had a single episode on virtual reality. So at this point, you probably know that this paper has to be really good. The promise of virtual reality is indeed truly incredible. Doctors could be trained to perform surgery in a virtual environment, or even perform surgery from afar, we could enhance military training by putting soldiers into better flight simulators, expose astronauts to virtual zero-gravity simulations, you name it. And of course, games. As you see, virtual reality or VR in short, is on the rise these days and there is a lot of research going on on how to make more killer applications for it. The basics are simple - we put on a VR headset and walk around in our room and perform gestures, and these will be performed in a virtual would by our avatar. Sounds super fun, right? Well, yes, however, we have this headset on, and we don't really see our surroundings within the room, which makes it easy to bump into objects, or smash the controller into the wall, which is exactly what I did in the NVIDIA lab in Switzerland not so long ago. My greetings to all the kind people there, and sorry folks! So, what could be a possible solution? Creating virtual worlds with smaller scales? That kind of defeats the purpose, doesn't it? There has to be a better solution. So, how about redirection? Redirection is a simple concept that changes our movement in the virtual world so it deviates from our real path in the room in a way that both lets us explore the virtual world well, and not bump into walls and objects in the meantime. Most existing techniques out there either don't do redirection and make us bump into objects and walls within our room, or they do redirection, at the cost of introducing distortions and other disturbing changes into the virtual environment. This is not easy to perform well because it has to feel natural, but the changes we apply to the path deviates from what is natural. Here you can see how the blue and orange lines deviate, which means that the algorithm is at work. With this, we can wander about in a huge and majestic virtual landscape or a cramped bar, even when being confined to a small physical room. Loving the idea. This technique takes into consideration even other moving players in the room and dynamically remap our virtual paths to make sure we don't bump into them. There is a lot more in the paper that describes how the whole method adapts to human perception. Papers like this make me really happy because there are thousands of papers in the domain of human perception within computer graphics, many of which will now see quite a bit of practical use. VR is going to be a huge enabler for this area. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=KEdrBMZx53w",
        "paper_link": "http://research.nvidia.com/publication/2018-08_Towards-Virtual-Reality",
        "paper_title": "Towards Virtual Reality Infinite Walking: Dynamic Saccadic Redirection "
    },
    {
        "video_id": "WMr9ljLomUI",
        "video_title": "An AI For Image Manipulation Detection | Two Minute Papers #261",
        "position_in_playlist": 11,
        "description": "Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nPayPal and crypto links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe paper \"Learning Rich Features for Image Manipulation Detection\" is available here:\nhttps://arxiv.org/abs/1805.04953\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1440055/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As facial reenactment videos are improving at a rapid pace, it is getting easier and easier to create video impersonations of other people by transferring our gestures onto their faces. We have recently discussed a technique that is able to localize the modified regions within these videos, however, this technique was limited to human facial reenactment. That is great, but what about the more general case with manipulated photos? Well, do not worry for second, because this new learning-based algorithm can look at any image and highlight the regions that were tampered with. It can detect image splicing, which means that we take part of a different image and add it to this one. Or, copying an object and pasting it into the image elsewhere. Or, removing an object from a photo and filling in the hole with meaningful information harvested from the image. This we also refer to as image inpainting, and this is something that we also use often to edit our thumbnail images that you see here on YouTube. Believe it or not, it can detect all of these cases. And it uses a two-stream convolutional neural network to accomplish this. So what does this mean exactly? This means a learning algorithm that looks at one, the color data of the image to try to find unnatural contrast changes along edges and silhouettes, and two, the noise information within the image as well and see how they relate to each other. Typically, if the image has been tampered with, either the noise, or the color data is disturbed, or, it may also be that they look good one by one, but the relation of the two has changed. The algorithm is able to detect these anomalies too. As many of the images we see on the internet are either resized or compressed, or both, it is of utmost importance that the algorithm does not look at compression artifacts and thinks that the image has been tampered with. This is something that even humans struggle with on a regular basis, and this is luckily not the case with this algorithm. This is great because a smart attacker may try to conceal their mistakes by recompressing an image and thereby adding more artifacts to it. It's not going to fool this algorithm. However, as you Fellow Scholars pointed out in the comments of a previous episode, if we have a neural network that is able to distinguish forged images, with a little modification, we can perhaps turn it around and use it as a discriminator to help training a neural network that produces better forgeries. What do you think about that? It is of utmost importance that we inform the public that these tools exist. If you wish to hear more about this topic, and if you think that a bunch of videos like this a month is worth a dollar, please consider supporting us on Patreon. You know the drill, a dollar a month is almost nothing, but it keeps the papers coming. Also, for the price of a coffee, you get exclusive early access to every new episode we release, and there are even more perks on our Patreon page, patreon.com/TwoMinutePapers. We also support cryptocurrencies like Bitcoin, Ethereum and Litecoin, the addresses are available in the video description. With your help, we can make better videos in the future. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=WMr9ljLomUI",
        "paper_link": "https://arxiv.org/abs/1805.04953",
        "paper_title": "Learning Rich Features for Image Manipulation Detection"
    },
    {
        "video_id": "YTup-cvELK0",
        "video_title": "Beautiful Layered Materials, Instantly | Two Minute Papers #260",
        "position_in_playlist": 12,
        "description": "The paper \"Efficient Rendering of Layered Materials using an Atomic Decomposition with Statistical Operators\" is available here:\nhttps://belcour.github.io/blog/research/2018/05/05/brdf-realtime-layered.html\n\nMy course on photorealistic rendering at the Technical University of Vienna: https://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time and crypto payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As animation movies and computer game graphics become more and more realistic, they draw us more and more into their own worlds. However, when we see cars, mugs, minerals and paintings, and similar materials, we often feel that something is not right there, and the illusion quickly crumbles. This is such a peculiar collection of materials...so what is the common denominator between them? Normally, to create these beautiful images, we use programs that create millions and millions of rays of light and simulate how they bounce off of the objects within the scene. However, most of these programs bounce these rays off of the surface of these objects, where in reality, there are many sophisticated multi-layered materials with all kinds of coatings and varnishes. Such a simple surface model is not adequate to model these multiple layers. This new technique is able to simulate not only these surface interactions, but how light is scattered, transferred and absorbed within these layers, enabling us to create even more beautiful images with more sophisticated materials. We can envision new material models with any number of layers and it will be able to handle it. However, I left the best part for last. What is even cooler is that it takes advantage of the regularity of the data and builds a statistical model that approximates what typically happens with our light rays within these layers. What this results in is a real-time technique that still remains accurate. This is not normal. This used to take hours! This is insanity! And the whole paper was written by only one author, Laurent Belcour and was accepted to the most prestigious research venue in computer graphics. So huge congrats to Laurent for accomplishing this. If you would like to learn more about light transport, I am holding an Master-level course on it at the Technical University of Vienna. This course used to take place behind closed doors, but I feel that the teachings shouldn't only be available for the 20-30 people who can afford a University education, but they should be available for everyone. So, we recorded the entirety of the course and it is now available for everyone, free of charge. If you are interested, have a look at the video description to watch them. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=YTup-cvELK0",
        "paper_link": "https://belcour.github.io/blog/research/2018/05/05/brdf-realtime-layered.html",
        "paper_title": "Efficient Rendering of Layered Materials using an Atomic Decomposition with Statistical Operators"
    },
    {
        "video_id": "Te0L5_u_wIg",
        "video_title": "Faceforensics: This AI Detects DeepFakes",
        "position_in_playlist": 13,
        "description": "The paper \"FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces \" is available here:\nhttp://niessnerlab.org/projects/roessler2018faceforensics.html\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time and crypto payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-593358/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#Faceforensics #Deepfake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. With the recent ascendancy of several new AI-based techniques for human facial reenactment, we are now able to create videos where we transfer our gestures onto famous actors or politicians and impersonate them. Clearly, as this only needs a few minutes of video as training data from the target, this could be super useful for animating photorealistic characters for video games and movies, reviving legendary actors who are not with us anymore, and much more. And understandably, some are worried about the social implications of such a powerful tool. In other words, if there are tools to create forgery, there should be tools to detect forgery, right? If we can train an AI to impersonate, why not train an other AI to detect impersonation? This has to be an arms race. However, this is no easy task to say the least. As an example, look here. Some of these faces are real, some are fake. What do you think, which is which? I will have to admit, my guesses weren't all that great. But what about you? Let me know in the comments section. Compression is also an issue. Since all videos you see here on YouTube are compressed in some way to reduce filesize, some of the artifacts that appear may easily throw off not only an AI, but a human as well. I bet there will be many completely authentic videos that will be thought of as fakes by humans in the near future. So how do we solve these problems? First, to obtain a neural network-based solution, we need a large dataset to train it on. This paper contains a useful dataset with over a 1000 videos that we can use to train such a neural network. These records contain pairs of original and manipulated videos, along with the input footage of the gestures that were transferred. After the training step, the algorithm will be able to pick up on the smallest changes around the face and tell a forged footage from a real one, even in cases where we humans are unable to do that. This is really amazing. These green to red colors showcase regions that the AI thinks were tampered with. And it is correct. Interestingly, this can not only identify regions that are forgeries, but it can also improve these forgeries too. I wonder if it can detect footage that is has improved itself? What do you think? Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Te0L5_u_wIg",
        "paper_link": "http://niessnerlab.org/projects/roessler2018faceforensics.html",
        "paper_title": "FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces "
    },
    {
        "video_id": "Nq2xvsVojVo",
        "video_title": "Better Video Impersonations with AI | Two Minute Papers #258",
        "position_in_playlist": 14,
        "description": "The paper \"Deep Video Portraits\" is available here:\nhttp://gvv.mpi-inf.mpg.de/projects/DeepVideoPortraits/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nOne-time and crypto payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-2119595/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Earlier, we talked about an amazing technique where the inputs were a source video of ourselves, and a target actor. And the output was a reenactment, in other words, a video of this target actor with our facial gestures. This requires only a few minutes of video from the target which is usually already available on the internet. Essentially, we can impersonate other people, at least for one video. A key part of this new technique is that it extracts additional data such as pose and eye positions both from the source and target videos, and uses this data for the reconstruction. As opposed to this original Face2face technique from 2 years ago, which was already mind-blowing, you see here that this results in a new learning-based method that supports the reenactment of eyebrows and blinking, changing the background, plus head and gaze positioning as well. So far, this would still be similar to a non learning-based technique we've seen a few episodes ago. And now, hold on to your papers, because this algorithm enables us to not only impersonate, but also control the characters in the output video. The results are truly mesmerizing, I almost fell out of the chair when I've first seen them. And what's more, we can create really rich reenactments by editing the expressions, pose, and blinking separately by hand. What also needs to be emphasized here is that we see and talk to other human beings all the time, so we have a remarkably keen eye for these kinds of gestures. If something is off by just a few millimeters or is not animated in a way that is close to perfect, the illusion immediately falls apart. And the magical thing about these techniques is that every single iteration we get something that is way beyond the capabilities of the previous methods. And they come in quick succession. There are plenty of more comparisons in the paper as well, so make sure to have a look. It also contains a great idea that opens up the possibility of creating quantitative evaluations against ground truth footage. Turns out that we can have such a thing as ground truth footage. I wonder when we will see the first movie with this kind of reenactment of an actor who passed away. Do you have some other cool applications in mind? Let me know in the comments section. And, if you enjoyed this episode, make sure to pick up some cool perks on our Patreon page where you can manage your paper addiction by getting early access to these episodes and more. We also support cryptocurrencies, the addresses are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Nq2xvsVojVo",
        "paper_link": "http://gvv.mpi-inf.mpg.de/projects/DeepVideoPortraits/",
        "paper_title": "Deep Video Portraits"
    },
    {
        "video_id": "9S2g7iixB9c",
        "video_title": "Curiosity-Driven AI: How Effective Is It? | Two Minute Papers #257",
        "position_in_playlist": 15,
        "description": "The paper \"Curiosity-driven Exploration by Self-supervised Prediction\" and its source code is available here:\nhttps://pathak22.github.io/noreward-rl/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://flic.kr/p/M843Kp\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. There are many research projects about teaching an AI to play video games well. We have seen some amazing results from DeepMind's Deep Q Learning algorithm that performed on a superhuman level on many games, but faltered on others. What really made the difference is the sparsity of rewards and the lack of longer-term planning. What this means is that the more often we see the score change on our screen, the faster we know how well we are doing and change our strategy if needed. For instance, if we make a mistake in Atari Breakout, we lose a life almost immediately, but in a strategy game, a bad decision may come back to haunt us up to an hour after committing it. So, what can we do to build an AI that can deal with these cases? So far when we have talked about extrinsic rewards that come from the environment, for instance, our score in a video game, and most existing AIs are for all intents and purposes, extrinsic score-maximizing machines. And this work is about introducing an intrinsic reward by endowing an AI with one of the most humanlike attributes: curiosity. But hold on right there, how can a machine possibly become curious? Well, curiosity is defined by whatever mathematical definition we attach to it. In this work, curiosity is defined as the AI's ability to predict the results of its own actions. This is big, because it gives the AI tools to pre-emptively start learning skills that don't seem useful now, but might be useful in the future. In short, this AI is driven to explore, even if it hasn't been told how well it is doing. It will naturally start exploring levels in Super Mario, even without seeing the score. And now comes the great part: this curiosity really teaches the AI to learn new skills, and when we drop it into a new, previously unseen level, it will perform much better than a non-curious one. When playing Doom, the legendary first-person shooter game, it will also start exploring the level and is able to rapidly solve hard exploration tasks. The comparisons reveal that an AI infused with curiosity performs significantly better on easier tasks, but the even cooler part is that with curiosity, we can further increase the difficulty of the games and the sparsity of external rewards and can expect the agent to do well, even when previous algorithms failed. This will be able to play much harder games than previous works. And remember, games are only used to demonstrate the concept here, this will be able to do so much more. Love it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9S2g7iixB9c",
        "paper_link": "https://pathak22.github.io/noreward-rl/",
        "paper_title": "Curiosity-driven Exploration by Self-supervised Prediction"
    },
    {
        "video_id": "SWW0nVQNm2w",
        "video_title": "Neural Image Stitching And Morphing | Two Minute Papers #256",
        "position_in_playlist": 16,
        "description": "The paper \"Neural Best-Buddies: Sparse Cross-Domain Correspondence\" is available here:\nhttps://arxiv.org/abs/1805.04140\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-1580869/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Consider this problem: we have a pair of images that are visually quite different, but have similar semantic meanings, and we wish to map points between them. Now this might sound a bit weird, so bear with me for a moment. For instance, geese and airplanes look quite different, but both have wings, and front, and back regions. The paw of a lion looks quite different from a cat's foot, but they share the same function and are semantically similar. This is an AI-based technique that is able to find these corresponding points between our pair of images, in fact, the point pairs you've seen so far have been found by this AI. The main difference between this and previous non-learning-based techniques is that instead of pairing up regions based on pixel color similarities, it measures how similar they are in terms of the neural network's internal representation. This makes all the difference. So far this is pretty cool, but is that it? Mapping points? Well if we can map points effectively, we can map regions as a collection of points. This enables two killer applications. One, this can augment already existing artistic tools so that we can create a hybrid between two images. And the cool thing is that we don't even need to have any drawing skills because we only have to add these colored masks and the algorithm finds and stitches together the corresponding images. And two, it can also perform cross-domain image morphing. That's an amazing term, but what does this mean? This means that we have our pair of images from earlier, and we are not interested in stitching together a new image from their parts, but we want an animation where the starting point is one image, the ending point is the other, and we get a smooth and meaningful transition between the two. There are some really cool use-cases for this. For example, we can start out from a cartoon drawing, set our photo as an endpoint, and witness this beautiful morphing between the two. Kind of like in style transfer, but we have more fine-grained control over the output. Really cool. And note that many images inbetween are usable as-is. No artistic skills needed. And of course, there is a mandatory animation that makes a cat from a dog. As usual, there are lots of comparisons to other similar techniques in the paper. This tool is going to be invaluable for, I was about to say artists, but this doesn't require any technical expertise, just good taste and a little bit of imagination. What an incredible time to be alive. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SWW0nVQNm2w",
        "paper_link": "https://arxiv.org/abs/1805.04140",
        "paper_title": "Neural Best-Buddies: Sparse Cross-Domain Correspondence"
    },
    {
        "video_id": "tU484zM3pDY",
        "video_title": "NVIDIA's AI Removes Objects From Your Photos | Two Minute Papers #255",
        "position_in_playlist": 17,
        "description": "Our Patreon page with the details: https://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe paper \"Image Inpainting for Irregular Holes Using Partial Convolutions\" is available here:\nhttps://arxiv.org/abs/1804.07723\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Ever had an experience when you shot an almost perfect photograph of for instance, an amazing landscape but unfortunately it was littered with unwanted objects. If only we had an algorithm that could perform image inpainting, in other words, delete a small part of an image and have it automatically filled in! So let's have a look at NVIDIA's AI-based solution. On the left, you see the white regions that are given to the algorithm to correct, and on the right, you see the corrected images. So it works amazingly well, but the question is why? This is an established research field, so what new can an AI-based approach bring to the table? Well, traditional non-learning approaches either try to fill these holes in with other pixels from the same image that have similar neighborhoods, copy-paste something similar if you will, or they try to record the distribution of pixel colors and try to fill in something using that knowledge. And here comes the important part - none of these traditional approaches have an intuitive understanding of the contents of the image. And that is the main value proposition of the neural network-based learning techniques! This work also borrows from earlier artistic style transfer methods to make sure that not only the content, but the style of the inpainted regions also match the original image. It is also remarkable that this new method works with images that are devoid of symmetries and can also deal with cases where we cut out really crazy, irregularly shaped holes. Of course, like every good piece of research work, it has to be compared to previous algorithms. As you can see here, the quality of different techniques is measured against a reference output, and it is quite clear that this new method produces more convincing results than its competitors. For reference, PatchMatch is a landmark paper from almost 10 years ago that still represents the state of the art for non learning-based techniques. The paper contains a ton more of these comparisons, so make sure to have a look. Without doubt, this is going to be an invaluable tool for artists in the future. In fact, in this very series, we use Photoshop's built-in image inpainting tool on a daily basis, so this will make our lives much easier. Loving it. Also, did you know that you can get early access to each of these videos? If you are addicted to the series, have a look at our Patreon page, patreon.com/TwoMinutePapers, or just click the link in the video description. There are also other really cool perks like getting your name as a key supporter in the video description, or deciding the order of the next few episodes. We also support cryptocurrencies, the addresses are in the video description. And with this, you also help us make better videos in the future. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=tU484zM3pDY",
        "paper_link": "https://arxiv.org/abs/1804.07723",
        "paper_title": "Image Inpainting for Irregular Holes Using Partial Convolutions"
    },
    {
        "video_id": "EQX1wsL2TSs",
        "video_title": "This Technique Impersonates People | Two Minute Papers #254",
        "position_in_playlist": 18,
        "description": "The paper \"HeadOn: Real-time Reenactment of Human Portrait Videos\" is available here:\nhttp://niessnerlab.org/projects/thies2018headon.html\n\nMore on Apple's Memoji: https://www.youtube.com/watch?v=CjqERCCD4iM\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-1867320/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Deepfake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Two years ago, in 2016, we talked about a paper that enabled us to sit in front of a camera, and transfer our gestures onto a virtual actor. This work went by the name Face2face and showcased a bunch of mesmerizing results containing reenactments of famous political figureheads. It was quite amazing. But it is nothing compared to this one. And the reason for this is that the original Face2face paper only transferred expressions, but this new work is capable of transferring head and torso movements as well. Not only that, but mouth interiors also appear more realistic and more gaze directions are also supported. You see in the comparisons here that the original method disregarded many of these features and how much more convincing this new one is. This extended technique opens up the door to several, really cool new applications. For instance, consider this self-reenactment application. This means that you can re-enact yourself. Now, what would that be useful for, you may ask? Well, of course, you can appear to be the most professional person during a virtual meeting, even when sitting at home in your undergarment. Or, you can quickly switch teams based on who is winning the game. Avatar digitization is also possible, this basically means that we can create a stylized version of our likeness to be used in a video game. Somewhat similar to the Memoji presented in Apple's latest keynote with the iPhone X. And the entire process takes place in real time without using neural networks. This is as good as it gets. What a time to be alive! Of course, like every other technique, this one also has its own set of limitations. For instance, illumination changes in the environment are not always taken into account, and long-haired subjects with extreme motion may cause artifacts to appear. In short, don't use this for rock concerts. And with this, we are also one step closer to full character re-enactment for movies, video games and telepresence applications. This is still a new piece of technology, and may offer many more applications that we haven't thought of yet. After all, when the internet was invented, who thought that it could be used to order pizza or transfer Bitcoin? Or order pizza and pay with bitcoin. Anyway, if you have some more applications in mind, let me know in the comments section. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=EQX1wsL2TSs",
        "paper_link": "http://niessnerlab.org/projects/thies2018headon.html",
        "paper_title": "HeadOn: Real-time Reenactment of Human Portrait Videos"
    },
    {
        "video_id": "bcZFQ3f26pA",
        "video_title": "This AI Learned To See In The Dark",
        "position_in_playlist": 19,
        "description": "The paper \"Learning to See in the Dark\" and its source code is available here:\nhttp://cchen156.web.engr.illinois.edu/paper/18CVPR_SID.pdf\nhttps://github.com/cchen156/Learning-to-See-in-the-Dark\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-2618462/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#NightSight #NightMode",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If you start watching reviews of some of the more recent smartphones, you will almost always see a dedicated section to low-light photography. The result is almost always that cameras that work remarkably well in well-lit scenes produce almost unusable results in dim environments. So unless we have access to a super expensive camera, what can we really do to obtain more usable low-light images? Well, of course, we could try brightening the image up by increasing the exposure. This would help maybe a tiny bit, but would also mess up our white balance and also amplify the noise within the image. I hope that by now you are getting the feeling that there must be a better AI-based solution. Let's have a look! This is an image of a dark indoor environment, I am sure you have noticed. This was taken with a relatively high light sensitivity that can be achieved with a consumer camera. This footage is unusable. And this image was taken by an expensive camera with extremely high light sensitivity settings. This footage is kinda usable, but is quite dim and is highly contaminated by noise. And now, hold on to your papers, because this AI-based technique takes sensor data from the first, unusable image, and produces this. Holy smokes! And you know what the best part is? It produced this output image in less than a second. Let's have a look at some more results. These look almost too good to be true, but luckily, we have a paper at our disposal so we can have a look at some of the details of the technique! It reveals that we have to use a Convolutional Neural Network to learn the concept of this kind of image translation, but that also means that we require some training data. The input should contain a bunch of dark images, these are the before images, this can hardly be a problem, but the output should always be the corresponding image with better visibility. These are the after images. So how do we obtain them? The key idea is to use different exposure times for the input and output images. A short exposure time means that when taking a photograph, the camera aperture is only open for a short amount of time. This means that less light is let in, therefore the photo will be darker. This is perfect for the input images as these will be the ones to be improved. And the improved versions are going to be the images with a much longer exposure time. This is because more light is let in, and we'll get brighter and clearer images. This is exactly what we're looking for! So now that we have the before and after images that we referred to as input and output, we can start training the network to learn how to perform low-light photography well. And as you see here, the results are remarkable. Machine learning research at its finest. I really hope we get a software implementation of something like this in the smartphones of the near future, that would be quite amazing. And as we have only scratched the surface, please make sure to look at the paper as it contains a lot more details. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bcZFQ3f26pA",
        "paper_link": "http://cchen156.web.engr.illinois.edu/paper/18CVPR_SID.pdf",
        "paper_title": "Learning to See in the Dark"
    },
    {
        "video_id": "KL6U6iasUxs",
        "video_title": "AI-Based Large-Scale Texture Synthesis | Two Minute Papers #252",
        "position_in_playlist": 20,
        "description": "The paper \"Non-stationary Texture Synthesis by Adversarial Expansion\" and its source code is available here:\nhttp://vcc.szu.edu.cn/research/2018/TexSyn\nhttps://github.com/jessemelpolio/non-stationary_texture_syn\n\nErrata: please note that the image at the start of the video is of a wrong paper. Apologies! \n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nOne-time payment links and crypto addresses are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-3013486/\nTexture tiling image credit: https://commons.wikimedia.org/wiki/File:In-game-view-doom.png\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When an artist is in the process of creating digital media, such as populating a virtual world for an animation movie or a video game, or even in graphic design, the artist often requires a large number of textures for these kinds of works. Concrete walls, leaves, fabrics are materials that we know well from the real world and sometimes the process of obtaining textures is as simple as paying for a texture package and using it. But the problem quite often occurs that we wish to fill an entire road with a concrete texture, but we only have a small patch at our disposal. In this case, the easiest and worst solution is to copy-paste this texture over and over, creating really unpleasant results that are quite repetitive and suffer from seams. So what about an AI-based technique that looks at a small patch, and automatically continues it in a way that looks natural and seamless. This is an area within computer graphics and AI that we call texture synthesis. Periodic texture synthesis is simple, but textures with structure are super difficult. The selling point of this particular work is that it is highly efficient at taking into consideration the content and symmetries of the image. For instance, it knows that it has to take into consideration the concentric nature of the wood rings when synthesizing this texture, and it can also adapt to the regularities of this water texture and create a beautiful, high-resolution result. This is a neural-network based technique, so first, the question is, what should the training data be? Let's take a database of high-resolution images. Let's cut out a small part and pretend that we don't have access to the bigger image and ask a neural network to try to expand this small cutout. This sounds a little silly, so what is this trickery good for? Well, this is super useful because after the neural network has expanded the results, we now have a reference result in our hands that we can compare to and this way, teach the network to do better. Note that this architecture is a generative adversarial network, where two neural networks battle each other. The generator network is the creator that expands the small texture snippets, and the discriminator network takes a look and tries to tell it from the real deal. Over time, the generator network learns to be better at texture synthesis, and the discriminator network becomes better at telling synthesized results from real ones. Over time, this rivalry leads to results that are of extremely high quality. And as you can see in this comparison, this new technique smokes the competition. The paper contains a ton of more results and comparisons, and one of the most exhaustive evaluation sections I've seen in texture synthesis so far. I highly recommend reading it. If you would like to see more episodes like this, make sure to pick up one of the cool perks we offer through Patreon, such as deciding the order of future episodes, or getting your name in the video description of every episode as a key supporter. We also support cryptocurrencies like Bitcoin, Ethereum and Litecoin. We had a few really generous pledges in the last few weeks. I am quite stunned to be honest, and I regret that I cannot come in contact with these Fellow Scholars. If you can contact me, that would be great, if not, thank you so much everyone for your unwavering support. This is just incredible. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=KL6U6iasUxs",
        "paper_link": "http://vcc.szu.edu.cn/research/2018/TexSyn",
        "paper_title": "Non-stationary Texture Synthesis by Adversarial Expansion"
    },
    {
        "video_id": "cnquEovq1I4",
        "video_title": "We Taught an AI To Synthesize Materials \ud83d\udd2e",
        "position_in_playlist": 21,
        "description": "The paper \"Gaussian Material Synthesis\" and its source code is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nCredits:\nWe would like to thank Robin Marin for the material test scene and\nVlad Miller for his help with geometry modeling. Scene and geometry credits: Gold Bars \u2013 JohnsonMartin, Christmas Ornaments \u2013 oenvoyage, Banana \u2013 sgamusse, Bowl \u2013 metalix, Grapes \u2013 PickleJones, Glass Fruits \u2013 BobReed64, Ice cream \u2013 b2przemo, Vases \u2013 Technausea, Break Time \u2013 Jay\u2013Artist, Wrecking Ball \u2013 floydkids, Italian Still Life \u2013 aXel, Microplanet \u2013 marekv, Microplanet vegetation \u2013 macio.\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#neuralrendering",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Due to popular request, here's a more intuitive explanation of our latest work. Believe it or not, when I have started working on this, Two Minute Papers didn't even exist. In several research areas, there are cases where we can't talk about our work until it is published. I knew that the paper would not see the light of the day for quite a while, if ever, so I started Two Minute Papers to be able to keep my sanity and deliver a hopefully nice piece of work on a regular basis. In the end, this took more than 3000 work hours to complete, but it is finally here, and I am so happy to finally be able to present it to you. This work is in the intersection of computer graphics and AI, which you know is among my favorites. So what do we see here? This beautiful scene contains more than a 100 different materials, each of which has been learned and synthesized by an AI. None of these daisies and dandelions are alike, each of them have a different material model. The goal is to teach an AI the concept of material models such as metals, minerals and translucent materials. Traditionally, when we are looking to create a new material model with a light simulation program, we have to fiddle with quite a few parameters, and whenever we change something, we have to wait from 40 to 60 seconds until a noise-free result appears. In our solution, we don't need to play with these parameters. Instead, our goal is to grab a gallery of random materials, assign a score to each of them saying that I liked this one, I didn't like that one, and get an AI to learn our preferences and recommend new materials for us. This is quite useful when we\u2019re looking to synthesize not only one, but many materials. So this is learning algorithm number one, and it works really well for a variety of materials. However, these recommendations still have to be rendered with a light simulation program, which takes several hours for a gallery like the one you see here. Here comes learning algorithm number two to the rescue, a neural network that replaces this light simulation program and creates photorealistic visualizations. It is so fast, it not only does this in real time, but it is more than 10 times faster than real time. We call this a neural renderer. So we have a lot of material recommendations, and they are all photorealistic that we can visualize in real time. However, it is always a possibility that we have a recommendation that is almost exactly what we had in mind, but need a few adjustments. That\u2019s an issue, because to do that, we would have to go back to the parameter fiddling, which we really wanted to avoid in the first place. No worries, because the third learning algorithm is coming to the rescue. What this can do is take our favorite material models from the gallery, and map them onto a nice 2D plane where we can explore similar materials. If we combine this with the neural renderer, we can explore these photorealistic visualizations and everything is appears not in a few hours, but in real time. However, without a little further guidance, we get a bit lost because we still don\u2019t know which regions in this 2D space are going to give us materials that are similar to the one we wish to fine-tune. We can further improve this by exploring different combinations of the three learning algorithms. In the end, we can assign these colors to the background that describe either whether the AI expects us to like the output, or how similar the output will be. A nice use-case of this is where we have this glassy still life scene, but the color of the grapes is a bit too vivid for us. Now, we can go to this 2D latent space, and adjust it to our liking in real time. Much better. No material modeling expertise is required. So I hope you have found this explanation intuitive. We tried really hard to create something that is both scientifically novel and also useful for the computer game and motion picture industry. We had to throw away hundreds of other ideas until this final system materialized. Make sure to have a look at the paper in the description where every single element and learning algorithm is tested and evaluated one by one. If you are a journalist and you would like to write about this work, I would be most grateful, and I am also more than happy to answer questions in an interview format as well. Please reach out if you\u2019re interested. We also try to give back to the community, so for the fellow tinkerers out there, the entirety of the paper is under the permissive Creative Commons license, and the full source code and pre-trained neural networks are also available under the even more permissive MIT license. Everyone is welcome to reuse it or build something cool on top of it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cnquEovq1I4",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/",
        "paper_title": "Gaussian Material Synthesis"
    },
    {
        "video_id": "wm8tK91k37U",
        "video_title": "This Evolving AI Finds Bugs in Games | Two Minute Papers #250",
        "position_in_playlist": 22,
        "description": "Our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe paper \"Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari\" by Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter is available here:\nhttps://arxiv.org/abs/1802.08842\n\nThe bug has been reproduced by a human here: Reproduction: https://www.youtube.com/watch?v=VGyeUuysyqg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-2619483/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a learning algorithm that chooses a set of actions in an environment to maximize a score. This class of techniques enables us to train an AI to master a large variety of video games and has many more cool applications. For instance, in the game of Q*bert, at every time step, the AI has to choose the appropriate actions to control this orange character and light up all the cubes without hitting the purple enemy. This work proposes an interesting alternative to reinforcement learning and is named evolution strategies and it aims to train not one agent, but an entire population of agents in parallel. The efficiency of this population is assessed, much like how evolution works in nature, and new offsprings are created from the best performing candidates. Note that this is not the first paper using evolution strategies - this is a family of techniques that dates back to the 70s. However, an advantage of this variant is that it doesn't require long trial and error sessions to find an appropriate discount factor. But wait, what does this discount factor mean exactly? This is a number that describes whether the AI should focus only on immediate rewards at all costs, or whether it should be willing to temporarily make worse decisions for a better payoff in the future. This optimal number is different for every game, and depends on how much long-term planning it requires. With this evolutionary algorithm, we can skip this step entirely. And the really cool thing about this is that it is not only able to master many games, but after only 5 hours of training, it was able to find a way to abuse game mechanics in Q*bert in the most creative ways. It has found a glitch where it sacrifices itself to lure the purple blob into dropping down after it. And much to our surprise, it found that there is a bug - if it drops down from this position, it should lose a life for doing it, but due to a bug, it doesn't. It also learned another cool technique where it waits for the adversary to make a move and immediately goes the other way. Here's the same scene slowed down. It had also found and exploited another serious bug which was to the best of my knowledge, previously unknown - after completing the first level, it starts jumping around in a seemingly random manner. A moment later, we see that the game does not advance to the next level, but cubes start blinking and the AI is free to score as many points as it wishes. After this video, a human player was able to reproduce this, I've put a link to it in the video description. It also found out the age-old trick in breakout, where we dig a tunnel through the bricks, lean back, start reading a paper, and let physics solve the rest of the level. One of the greatest advantages of this technique is that instead of training only one agent, it works on an entire population. These agents can be trained independently, making the algorithm more parallelizable, which means that it is fast and maps really well to modern processors and graphics cards with many cores. And these algorithms are not only winning the game, they are breaking the game. Loving it. What a time to be alive! I think this is an incredible story that everyone needs to hear about. If you wish help us with our quest and get exclusive perks for this series, please consider supporting us on Patreon. We are available through patreon.com/TwoMinutePapers, and a link with the details is available in the video description. We also use part of these funds to give back to the community and empower research projects and conferences. For instance, we recently sponsored a conference aimed to teach young scientists to write and present their papers at international venues. We are hoping to invest some more into upgrading our video editing rig in the near future. We also support cryptocurrencies, such as Bitcoin, Ethereum and Litecoin. I am really grateful for your support. And this is why every video ends with... Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wm8tK91k37U",
        "paper_link": "https://arxiv.org/abs/1802.08842",
        "paper_title": "Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari"
    },
    {
        "video_id": "fklY2nH7AJo",
        "video_title": "AI Learns Painterly Harmonization | Two Minute Papers #249",
        "position_in_playlist": 23,
        "description": "The paper \"Deep Painterly Harmonization\" and its source code is available here:\nhttps://arxiv.org/abs/1804.03189\nhttps://github.com/luanfujun/deep-painterly-harmonization\n\nPick up cool perks on Patreon: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-3129429/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When we show a photograph to someone, most of the time we are interested in sharing our memories. Graduation, family festivities beautiful landscapes are common examples of this. With the recent ascendancy of these amazing neural style transfer techniques, we can take a painting, or any other source image, and transfer the style of this image to our contents. The style is transfered, but the contents remains unchanged. This takes place by running the images through a deep neural network, which, in its deeper layers, learns about high level concepts such as artistic style. This work has sparked a large body of followup research works. Feedforward real-time style transfer, temporally coherent style transfer for videos, you name it. However, these techniques are always about taking one image for content, and one for style. How about a new problem formulation where we paste in a part of a foreign image with a completely different style? For instance, if you feel that this ancient artwork is sorely missing a Captain America shield, or if Picasso's self-portrait is just not cool enough without shades, then this algorithm is for you. However, if we just drop in this part of a foreign image, anyone can immediately tell because of the differences in color and style. A previous, non-AI-based technique does way better, but it is still apparent that the image has been tampered with. But as you can see here, this new technique is able to do it seamlessly. It works by first performing style transfer from the painting to the new region, and then, in the second step, additional refinements are made to it to make sure that the response of our neural network is similar across the entirety of the painting. It is conjectured that if the neural network is stimulated the same way by every part of the image, then there shouldn't be outlier regions that look vastly different. And as you can see here, it works remarkably well on a range of inputs. I hope these scroll animations come out really smooth and creamy. This video took a long time to render in 4K resolution with 60 frames per second, and was only possible because of your support on Patreon. If you wish to help us create better videos in the future, please click the Patreon link in the video description and support the series. To validate this work, a user study was done that revealed that the users preferred the new technique over the older ones in 15 out 16 images. I think it is fair to say that this work smokes the competition. But what about comparisons to real paintings? A different user study was also created to answer this question. And the answer is that users were mostly unable to identify whether the painting was tampered with. Excellent work. The source code is also available, so let the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fklY2nH7AJo",
        "paper_link": "https://arxiv.org/abs/1804.03189",
        "paper_title": "Deep Painterly Harmonization"
    },
    {
        "video_id": "DglrYx9F3UU",
        "video_title": "This AI Reproduces Human Perception | Two Minute Papers #248",
        "position_in_playlist": 24,
        "description": "The paper \"The Unreasonable Effectiveness of Deep Networks as a Perceptual Metric\" is available here:\nhttps://richzhang.github.io/PerceptualSimilarity/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nOther papers showcased in the video:\nAutomatic Parameter Control for Metropolis Light Transport - https://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\nGaussian Material Synthesis - https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1285294/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Assessing how similar two images are has been a long standing problem in computer graphics. For instance, if we write a new light simulation program, we have to compare our results against the output of other algorithms and a noise-free reference image. However, this often means that we have many noisy images where the structure of the noise is different. This leads to endless arguments on which algorithm is favorable to the others, since who really gets to decide what kind of noise is favorable and what is not? These are important and long-standing questions that we need to find answers to. In an other application, we took a photorealistic material model and wanted to visualize other materials that look similar to it. However, in order to do this, we need to explain to the computer what it means that two images are similar. This is what we call a similarity metric. Have a look at this reference image, and these two variants of it. Which one is more similar to it, the blurred or the warped version? Well, according to most humans, warping is considered a less intrusive operation. However, some of the most ubiquitous similarity metrics, like computing a simple per-pixel difference thinks otherwise. Not good. What about this comparison? Which image is closer to the reference? The noisy or the blurry one? Most humans say that the noisy image is more similar, perhaps because with enough patience, one could remove all the noise pixel by pixel and get back the reference image, but in the blurry image, lots of features are permanently lost. Again, the classical error metrics think otherwise. Not good. And now comes the twist, if we build a database from many of these human decisions, feed it into a deep neural network, we'll find that this network will be able to learn and predict how humans see differences in images. This is exactly what we are looking for! You can see the agreement between this new similarity metric and these example differences. However, this shows the agreement on only three images. That could easily happen by chance. So, this chart shows how different techniques correlate with how humans see differences in images. The higher the number, the higher the chance that it thinks similarly to humans. The ones labeled with LPIPS denote the new proposed technique used on several different classical neural network architectures. This is really great news for all kinds of research works that include working with images. I can't wait to start experimenting with it! The paper also contains a more elaborate discussion on failure cases as well, so make sure to have a look. Also, if you would like to help us do more to spread the word about these incredible works and pick up cool perks, please consider supporting us on Patreon. Each dollar you contribute is worth more than a thousand views, which is a ton of help for the channel. We also accept cryptocurrencies, such as Bitcoin, Ethereum and Litecoin. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=DglrYx9F3UU",
        "paper_link": "https://richzhang.github.io/PerceptualSimilarity/",
        "paper_title": "The Unreasonable Effectiveness of Deep Networks as a Perceptual Metric"
    },
    {
        "video_id": "gvjCu7zszbQ",
        "video_title": "This AI Learns From Its Dreams  | Two Minute Papers #247",
        "position_in_playlist": 25,
        "description": "The paper \"World Models\" is available here:\nhttps://arxiv.org/abs/1803.10122\nhttps://worldmodels.github.io/\n\nSupport the series and pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-3077928/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to talk about an AI that not only plays video games really well, but can also dream up new, unseen scenarios, and more. This is an interesting new framework that contains a vision model that compresses what it has seen in the game into an internal code. As you see here, these latent variables are responsible to capture different level designs, and this variable simulates time and shows how the fireballs move towards us over time. This is a highly compressed internal representation that captures the most important aspects of the game. We also have a memory unit that not only stores previous experiences, but similarly to how an earlier work predicted the next pen strokes of a drawing, this can also dream up new gameplay. Finally, it is also endowed with a controller unit that is responsible for making decisions as to how to play the game. Here, you see the algorithm in action: on the left, there is the actual gameplay, and on the right you can see its compressed internal representation. This is how the AI thinks about the game. The point is that it is lossy, therefore some information is lost, but the essence of the game is retained. So, this sounds great, the novelty is clear, but how well does it play the game? Well, in this racing game, on a selection of a 100 random tracks, its average score is almost three times that of DeepMind's groundbreaking Deep Q-Learning algorithm. This was the AI that took the world by storm when DeepMind demonstrated how it learned to play Atari Breakout and many other games on a superhuman level. This is almost three times better than that on the racetrack game, though it is to be noted that DeepMind has also made great strides since their original DQN work. And now comes the even more exciting part! Because it can create an internal dream representation of the game, and this representation really captures the essence of the game, then it means that it is also be able to play and train within these dreams. Essentially, it makes up dream scenarios and learns how to deal with them without playing the actual game. It is a bit like how a we prepare for a first date, imagining what to say, and how to say it, or, imagining how we would incapacitate an attacker with our karate chops if someone were to attack us. And the cool thing is that with this AI, this dream training actually works, which means that the newly learned dream strategies translate really well to the real game. We really have only scratched the surface, so make sure to read the paper in the description. This is a really new and fresh idea, and I think it will give birth to a number of followup papers. Cannot wait to report on these back to you, so stay tuned and make sure to subscribe and hit the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=gvjCu7zszbQ",
        "paper_link": "https://arxiv.org/abs/1803.10122",
        "paper_title": "World Models"
    },
    {
        "video_id": "UMSNBLAfC7o",
        "video_title": "This Robot Adapts Like Animals | Two Minute Papers #246",
        "position_in_playlist": 26,
        "description": "The paper \"Robots that can adapt like animals\" and its source code is available here:\nhttps://members.loria.fr/jbmouret/nature_press.html\nhttps://members.loria.fr/code/ite_limbo_nature.zip\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about building a robot that works even when being damaged, and you will see that the results are just unreal. There are many important applications for such a robot where sending out humans may be too risky, such as putting out forest fires, finding earthquake survivors under rubble, or shutting down a malfunctioning nuclear plant. Since these are all dangerous use cases, it is a requirement that such a robot works even when damaged. The key idea to accomplish this is that we allow the robot to perform tasks such as walking not only in one, optimal way, but to explore and build a map of many alternative motions relying on different body parts. Some of these limping motions are clearly not optimal, but, whenever damage happens to the robot, it will immediately be able to choose at least one alternative way to move around, even with broken or missing legs. After building the map, it can be used as additional knowledge to lean on when the damage occurs, and the robot doesn't have to re-learn everything from scratch. This is great, especially given that damage usually happens in the presence of danger, and in these cases, reacting quickly can be a matter of life and death. However, creating such a map takes a ton of trial and error, potentially more than what we can realistically get the robot to perform. And now comes my favorite part, which is, starting a project in a computer simulation, and then, in the next step, deploying the trained AI to a real robot. This previously mentioned map of movements contains over 13,000 different kinds of gaits, and since we are in a simulation, it can be computed efficiently and conveniently. In software, we can also simulate all kinds of damage for free without dismembering our real robot. And since no simulation is perfect, after this step, the AI is deployed to the real robot that evaluates and adjusts to the differences. By the way, this is the same robot that surprised us in the previous episode when it showed that it can walk around just fine without any foot contact with the ground by jumping on its back and using its elbows. I can only imagine how much work this project took, and the results speak for themselves. It is also very easy to see the immediate utility of such a project. Bravo. I also recommend looking at the press materials. For instance, in the frequently asked questions, many common misunderstandings are addressed. For instance, it is noted that the robot doesn't understand the kind of damage that occurred, and doesn't repair itself in the strict sense but it tries to find alternative ways to function. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UMSNBLAfC7o",
        "paper_link": "https://members.loria.fr/jbmouret/nature_press.html",
        "paper_title": "Robots that can adapt like animals"
    },
    {
        "video_id": "m9XyXiL6n8w",
        "video_title": "AI Learns Real-Time 3D Face Reconstruction | Two Minute Papers #245",
        "position_in_playlist": 27,
        "description": "The paper \"Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network\" and its source code is available here:\nhttps://arxiv.org/abs/1803.07835\nhttps://github.com/YadiraF/PRNet\n\nAddicted? Pick up cool perks on our Patreon page! - https://www.patreon.com/TwoMinutePapers\n\nA few comments with some of the best applications:\nLowell Camp - \"This technology could be used for consumer-budget markerless facial motion capture, and if a follow-up paper enhances it with audio analysis for tongue posing, then it would require very little touch-up beyond a little temporal filtering.\" \nMilleoiseau - \"VOIP in game but with face tracking.\"\nEvan - \"Could this be used for some kind of automatic lip-reading system for deaf viewers to view live events?\"\nMatan - \"Monitor emotions for product improvement.\"\nIdjles Erle - \"Reconstructing ancestors faces from photos that are 150 years old. Working out from old photos who is more likely rested to whom.\"\nMorph Verse - \"Maybe create a toolsets for artists to support easy correct anatomy tools in characters with facial and body features, for faster workflow in apps like Blender or 3ds.\"\nBernard van Tonder - \"Encourage others to watch educational content:\nLet celebrities/sport idols teach important subjects by mapping their faces and voices onto people's faces in educational videos.\"\nAdam de Anda - \"Online shopping could get much more personalized. Send a selfie and be able to see sunglasses, hats, jewelry etc on your own face and able to rotate the image. Damn this actually pretty solid\"\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-1722556/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we have two extremely hard problems on the menu. One is facial alignment and the other is 3D facial reconstruction. For both problems, we have an image as an input, and the output should be either a few lines that mark the orientation of the jawline, mouth and eyes, and in the other case, we are looking for a full 3D computer model of the face. And all this should happen automatically, without any user intervention. This is extremely difficult, because this means that we need an algorithm that takes a 2D image, and somehow captures 3D information from this 2D projection, much like a human would. This all sounds great and would be super useful in creating 3D avatars for Skype calls, or scanning real humans to place them in digital media such as feature movies and games. That would be amazing, but, is this really possible? This work uses a convolutional neural network to accomplish this, and it not only provides high-quality outputs, but it creates them in less than 10 milliseconds per image, which means that it can process a hundred of them every second. That is great news indeed, because it also means that doing this for video in real time is also a possibility! But not so fast, because if we are talking about video, new requirements arise. For instance, it is important that such a technique is resilient against changes in lighting. This means that if we have different lighting conditions, the output geometry the algorithm gives us shouldn't be wildly different. The same applies to camera and pose as well. This algorithm is resilient against all three, and it has some additional goodies. For instance, it finds the eyes properly through glasses, and can deal with cases where the jawline is occluded by the hair, or infer its shape when one side is not visible at all. One of the key ideas is to give additional instruction to the convolutional neural network to focus more of its efforts to reconstruct the central region of the face because that region contains more discriminative features. The paper also contains a study that details the performance of this algorithm. It reveals that it is not only five to eight times faster than the competition, but also provides higher quality solutions. Since these are likely to be deployed in real-world applications very soon, it is a good time to start brainstorming about possible applications for this. If you have ideas beyond the animation movies and games line, let me know in the comments section. I will put the best ones in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=m9XyXiL6n8w",
        "paper_link": "https://arxiv.org/abs/1803.07835",
        "paper_title": "Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network"
    },
    {
        "video_id": "XcxzKLrCpyk",
        "video_title": "AI Photo Translation | Two Minute Papers #243",
        "position_in_playlist": 28,
        "description": "The paper \"Toward Multimodal Image-to-Image Translation\" and its source code is available here:\nhttps://junyanz.github.io/BicycleGAN/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-2985977/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Recently, a new breed of AI techniques surfaced that were capable of this new thing called image translation. And by image translation I mean that they can translate a drawn map to a satellite image, take a set of color labels and make a photorealistic facade, or take a sketch and create a photo out of it. This is done through a Generative Adversarial Network. This is an architecture where we have a pair of neural networks, one that learns to generate new images, and the other learns to tell a fake image from a real one. As they compete against each other, they get better and better without any human interaction. In these earlier applications, unfortunately, the output is typically one image, and since there are many possible shoes that could satisfy our initial sketch, it is highly unlikely that the one we are offered is exactly what we envisioned. This improved version enhanced this algorithm to be able to produce not one, but an entire set of outputs. And as you can see here, we have a night image, and a set of potential daytime translations on the right that are quite diverse. I really like how it has an intuitive understanding of the illumination differences of the building during night and daytime. It really seems to know how to add lighting to the building. It also models the atmospheric scattering during daytime, creates multiple kinds of pretty convincing clouds, or puts hills in the background. The results are both realistic, and the additional selling point is that this technique offers an entire selection of outputs. What I found to be really cool about the next comparisons is that ground truth images are also attached for reference. If we can take a photograph of a city at nighttime, we have access to the same view during the daytime too, or we can take a photograph of a shoe and draw the outline of it by hand. And as you can see here, there are not only lots of high-quality outputs, but in some cases, the ground truth image is really well approximated by the algorithm. This means that we give it a crude drawing, and it could translate this drawing into a photorealistic image that is very close to reality. I think that is mind blowing! The validation section of the paper reveals that this technique provides a great tradeoff between diversity and quality. There are previous methods that perform well if we need one high-quality solution, or many not so great ones, but overall this one provides a great package for artists working in the industry, and this will be a godsend for any kind of content generation scenario. The source code of this project is also available, and make sure to read the license before starting your experiments. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XcxzKLrCpyk",
        "paper_link": "https://junyanz.github.io/BicycleGAN/",
        "paper_title": "Toward Multimodal Image-to-Image Translation"
    },
    {
        "video_id": "GdTBqBnqhaQ",
        "video_title": "4 Experiments Where the AI Outsmarted Its Creators \ud83e\udd16",
        "position_in_playlist": 29,
        "description": "The paper \"The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities\" is available here:\nhttps://arxiv.org/abs/1803.03453\n\n\u2764\ufe0f Support the show on Patreon: https://www.patreon.com/TwoMinutePapers\n\nOther video resources:\nEvolving AI Lab - https://www.youtube.com/watch?v=_5Y1hSLhYdY&feature=youtu.be\nCooperative footage - https://infoscience.epfl.ch/record/99661/files/florenoetal_preprint.pdf\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-3010727/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, I am really excited to show you four experiments where AI researchers were baffled by the creativity and unexpected actions of their own creations. You better hold on to your papers. In the first experiment, robots were asked to walk around while minimizing the amount of foot contact with the ground. Much to the scientists' surprise, the robots answered that this can be done with 0% contact, meaning that they never ever touch the ground with the feet. The scientists wondered how that is even possible, and pulled up a video of the proof. This proof showed a robot flipping over and walking using its elbows. Talk about thinking outside the box! Wow. A different robot arm experiment also came to a surprising conclusion. At first, the robot arm had to use its grippers to grab a cube, which it successfully learned to perform. However, in a later experiment, the gripper was crippled, making the robot unable to open its fingers. Scientists expected a pathetic video with the robot to push the box around and always failing to pick up the cube. Instead, they have found this. You see it right, instead of using the fingers, the robot finds the perfect angle to smash the hand against the box to force the gripper to open and pick up the box. That is some serious dedication to solving the task at hand. Bravo! In the next experiment, a group of robots were tasked to find food and avoid poisonous objects in an environment, and were equipped with a light and no further instructions. First, they learned to use the lights to communicate the presence of food and poison to each other and cooperate. This demonstrates that when trying to maximize the probability of the survival of an entire colony, the concept of communication and cooperation can emerge even from simple neural networks. Absolutely beautiful. And what is even more incredible, is that later, when a new reward system was created that fosters self-preservation, the robots learned to deceive each other by lighting up the the food signal near the poison to take out their competitors and increase their chances. And these behaviors emerge from a reward system and a few simple neural networks. Mind blowing. A different AI was asked to fix a faulty sorting computer program. Soon, it achieved a perfect score without changing anything because it noticed that by short circuiting the program itself, it always provides an empty output. And of course, you know, if there are no numbers, there is nothing to sort. Problem solved. Make sure to have a look at the paper, there are many more experiments that went similarly, including a case where the AI found a bug in a physics simulation program to get an edge. With AI research improving at such a rapid pace, it is clearly capable of things that surpasses our wildest imagination, but we have to make sure to formulate our problems with proper caution, because the AI will try to use loopholes instead of common sense to solve them. When in a car chase, don't ask the car AI to unload all unnecessary weights to go faster, or if you do, prepare to be promptly ejected from the car. If you have enjoyed this episode, please make sure to have a look at our Patreon page in the video description where you can pick up really cool perks, like early access to these videos or getting your name shown in the video description, and more. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=GdTBqBnqhaQ",
        "paper_link": "https://arxiv.org/abs/1803.03453",
        "paper_title": "The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities"
    },
    {
        "video_id": "6FzVhIV_t3s",
        "video_title": "Gaussian Material Synthesis (SIGGRAPH 2018)",
        "position_in_playlist": 30,
        "description": "In this work, we teach an AI the concept of metallic, translucent materials and more. The paper \"Gaussian Material Synthesis\" and its source code is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/ \n\nAcknowledgments:\nWe would like to thank Robin Marin for the material test scene and Vlad Miller for his help with geometry modeling, Fel\u00edcia Zsolnai-Feh\u00e9r for improving the design of many figures, Hiroyuki Sakai, Christian Freude, Johannes Unterguggenberger, Pranav Shyam and Minh Dang for their useful comments, and Silvana Podaras for her help with a previous version of this work. We also thank NVIDIA for providing the GPU used to train our neural networks. This work was partially funded by Austrian Science Fund (FWF), project number P27974. Scene and geometry credits: Gold Bars \u2013 JohnsonMartin, Christmas Ornaments \u2013 oenvoyage, Banana \u2013 sgamusse, Bowl \u2013 metalix, Grapes \u2013 PickleJones, Glass Fruits \u2013 BobReed64, Ice cream \u2013 b2przemo, Vases \u2013 Technausea, Break Time \u2013 Jay-Artist, Wrecking Ball \u2013 floydkids, Italian Still Life \u2013 aXel, Microplanet \u2013 marekv, Microplanet vegetation \u2013 macio.\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#neuralrendering",
        "transcript": "Creating high-quality photorealistic materials for light transport simulations typically includes direct hands-on interaction with a principled shader. This means that the user has to tweak a large number of material properties by hand, and has to wait for a new image of it to be rendered after each interaction. This requires a fair bit expertise and the best setups are often obtained through a lengthy trial and error process. To enhance this workflow, we present a learning-based system for rapid mass-scale material synthesis. First, the user is presented with a gallery of materials and the assigned scores are shown in the upper left. Here, we learn the concept of glassy and transparent materials. By leaning on only a few tens of high-scoring samples, our system is able to recommend many new materials from the learned distributions. The learning step typically takes a few seconds, where the recommendations take negligible time and can be done on a mass scale. Then, these recommendations can be used to populate a scene with materials. Typically, each recommendation takes 40-60 seconds to render with global illumination, which is clearly unacceptable for real-world workflows for even mid-sized galleries. In the next step, we propose a convolutional neural network that is able to predict images of these materials that are close to the ones generated via global illumination, and takes less than 3 milliseconds per image. Sometimes a recommended material is close the one envisioned by the user, but requires a bit of fine-tuning. To this end, we embed our high-dimensional shader descriptors into an intuitive 2D latent space where exploration and adjustments can take place without any domain expertise. However, this isn't very useful without additional information because the user does not know which regions offer useful material models that are in line with their scores. One of our key observations is that this latent space technique can be combined with Gaussian Process Regression to provide an intuitive color coding of the expected preferences to help highlighting the regions that may be of interest. Furthermore, our convolutional neural network can also provide real-time predictions of these images. These predictions are close to indistinguishable from the real rendered images and are generated in real time. Beyond the preference map, this neural network also opens up the possibility of visualizing the expected similarity of these new materials to the one we seek to fine-tune. By combining the preference and similarity maps, we obtain a color coding that guides the user in this latent space towards materials that are both similar and have a high expected score. To accentuate the utility of our real-time variant generation technique, we show a practical case where one of the grape materials is almost done, but requires a slight reduction in vividity. This adjustment doesn't require any domain expertise or direct interaction with a material modeling system and can be done in real time. In this example, we learn the concept of translucent materials from only a handful of high-scoring samples and generate a large amount of recommendations from the learned distribution. These recommendations can then be used to populate a scene with relevant materials. Here, we show the preference and similarity maps of the learned translucent material space and explore possible variants of an input material. These recommendations can be used for mass-scale material synthesis, and the amount of variation can be tweaked to suit the user's artistic vision. After assigning the appropriate materials, displacements and other advanced effects can be easily added to these materials. We have also experimented with an extended, more expressive version of our shader that also includes procedural textured albedos and displacements. The following scenes were populated using the material learning and recommendation and latent space embedding steps. We have proposed a system for mass-scale material synthesis that is able to rapidly recommend a broad range of new material models after learning the user preferences from a modest number of samples. Beyond this pipeline, we also explored powerful combinations of the three used learning algorithms, thereby opening up the possibility of real-time photorealistic material visualization, exploration and fine-tuning in a 2D latent space. We believe this feature set offers a useful solution for rapid mass-scale material synthesis for novice and expert users alike and hope to see more exploratory works combining the advantages of multiple state of the art learning algorithms in the future.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6FzVhIV_t3s",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/",
        "paper_title": "Gaussian Material Synthesis"
    },
    {
        "video_id": "ni6P5KU3SDU",
        "video_title": "Evolving Generative Adversarial Networks | Two Minute Papers #242",
        "position_in_playlist": 31,
        "description": "The paper \"Evolutionary Generative Adversarial Networks\" is available here:\nhttps://arxiv.org/abs/1803.00657\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nRecommended for you:\nVideo game to reality conversion: https://www.youtube.com/watch?v=dqxqbvyOnMY\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-3100786/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. With the recent ascendancy of neural network-based techniques, we have witnessed amazing algorithms that are able to take an image from a video game and translate it into reality and the other way around. Or, they can also translate daytime images to their nighttime versions, or change summer to winter and back. Some AI-based algorithms can also create near photorealistic images from our sketches. So the first question is, how is this wizardry even possible? These techniques are implemented by using Generative Adversarial Networks, GANs in short. This is an architecture where two neural networks battle each other: the generator network is the artist who tries to create convincing, real-looking images. The discriminator network is the critic that tries to tell a fake image from a real one. The artist learns from the feedback of the critic and will improve itself to come up with better quality images, and in the meantime, the critic also develops a sharper eye for fake images. These two adversaries push each other until they both become adept at their tasks. However, the training of these GANs is fraught with difficulties. For instance, it is not guaranteed that this process converges to a point, and therefore it matters a great deal when we stop training the networks. This makes reproducing some works very challenging and is generally not a desirable property of GANs. It is also possible that the generator starts focusing on a select set of inputs and refuses to generate anything else, a phenomenon we refer to as mode collapse. So how could we possibly defeat these issues? This work presents a technique that mimics the steps of evolution in nature: evaluation and selection and variation. First, this means that not one, but many generator networks are trained, and only the ones that provide sufficient quality and diversity in their images will be preserved. We start with an initial population of generator networks, and evaluate the fitness of each of them. The better and more diverse images they produce more fit they are, the more fit they are, the more likely they are to survive the selection step where we eliminate the most unfit candidates. Okay, so we now see how a subset these networks become the victim of evolution. This is how networks get eaten, if you will. But, how do we produce new ones? And this is how we arrive to the variation step, where new generator networks are created by introducing variations to the networks that are still alive in this environment. This simulates the creation of an offspring, and will provide the next set of candidates for the next selection step, and we hope that if we play this game over a long time, we get more and more resilient offsprings. The resulting algorithm can be trained in a more stable way, and it can create new bedroom images when being shown a database of bedrooms. When compared to the state of the art, we see that this evolutionary approach offers higher quality images and more diversity in the outputs. It can also generate new human faces that are quite decent. They are clearly not perfect, but a technique that can pull this off consistently will be an excellent baseline for newer and better research works in the near future. We are also getting very close to an era where we can generate thousands of convincing digital characters from scratch to name just one application. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ni6P5KU3SDU",
        "paper_link": "https://arxiv.org/abs/1803.00657",
        "paper_title": "Evolutionary Generative Adversarial Networks"
    },
    {
        "video_id": "AbxPbfODGcs",
        "video_title": "This Fools Your Vision | Two Minute Papers #241",
        "position_in_playlist": 32,
        "description": "The paper \"Adversarial Examples that Fool both Human and Computer Vision\" is available here:\nhttps://arxiv.org/abs/1802.08195\n\nOur Patreon page with the details: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Lorin Atzberger, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-2479948/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Neural networks are amazing at recognizing objects when being shown an image, and in some cases, like traffic sign recognition, their performance can reach superhuman levels. But as we discussed in the previous episode, most of these networks have an interesting property where we can add small changes to an input photo and have the network misclassify it to something completely different. This is called an adversarial attack. A super effective neural network can be reduced to something that is less accurate than a coinflip with a properly crafted adversarial attack. So, of course, we may think that neural networks are much smaller and simpler than the human brain, and because of that, of course, we cannot perform such an adversarial attack on the human vision system...right? Or is it possible that some of the properties of machine vision systems can be altered to fool the human vision? And now, hold on to your papers, I think you know what's coming. This algorithm performs an adversarial attack on you. This image depicts a cat. And this image depicts, uh... a dog? Surely it's a dog, right? Well, no. This is an image of a previous cat plus some carefully crafted noise that makes it look like a dog. This is such a peculiar effect - I am staring at it, and I know for a fact that this is not a dog, this is cat plus noise, but I cannot not see it as a dog. Wow, this is certainly something that you don't see every day. So let's look at what changes were made to the image. Clearly, the nose appears to be longer and thicker, so that's a dog-like feature. But, it is of utmost importance that we don't overlook the fact that several cat-specific features still remain in the image, for instance, the whiskers are very cat-like. And despite that, we still see it as a dog. This is insanity. The technique works by performing an adversarial attack against an AI model, and modifying the noise generator model to better match the human visual system. Of course, the noise we have to add depends on the architecture of the neural network, and by this I mean the number of layers and the number of neurons within these layers, and many other parameters. However, a key insight of the paper is that there are still features that are shared between most architectures. This means that if we create an attack that works against 5 different neural network architectures, it is highly likely that it will also work on an arbitrary sixth network that we haven't seen yet. And it turns out that some of these noise distributions are also useful against the human visual system. Make sure to have a look at the paper, I have found it to be an easy read, and quite frankly I am stunned by the result. It is clear that machine learning research is progressing at a staggering pace, but I haven't expected this. I haven't expected this at all. If you are enjoying the series, please make sure to have a look at our Patreon page to pick up cool perks, like watching these episodes in early access, or getting your name displayed in the video description as a key supporter. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=AbxPbfODGcs",
        "paper_link": "https://arxiv.org/abs/1802.08195",
        "paper_title": "Adversarial Examples that Fool both Human and Computer Vision"
    },
    {
        "video_id": "SA4YEAWVpbk",
        "video_title": "One Pixel Attack Defeats Neural Networks | Two Minute Papers #240",
        "position_in_playlist": 33,
        "description": "The paper \"One pixel attack for fooling deep neural networks\" is available here:\nhttps://arxiv.org/abs/1710.08864\n\nThis seems like an unofficial implementation:\nhttps://github.com/Hyperparticle/one-pixel-attack-keras\n\nDifferential evolution animation credit: https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Malek Cellier, Frank Goertzen, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-3010129/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We had many episodes about new wondrous AI-related algorithms, but today, we are going to talk about an AI safety which is an increasingly important field of AI research. Deep neural networks are excellent classifiers, which means that after we train them on a large amount of data, they will be remarkably accurate at image recognition. So generally, accuracy is subject to maximization. But, no one has said a word about robustness. And here is where these new neural-network defeating techniques come into play. Earlier we have shown that we can fool neural networks by adding carefully crafted noise to an image. If done well, this noise is barely perceptible and can fool the classifier into looking at a bus and thinking that it is an ostrich. We often refer to this as an adversarial attack on a neural network. This is one way of doing it, but note that we have to change many-many pixels of the image to perform such an attack. So the next question is clear. What is the lowest number of pixel changes that we have to perform to fool a neural network? What is the magic number? One would think that a reasonable number would at least be a hundred. Hold on to your papers because this paper shows that many neural networks can be defeated by only changing one pixel. By changing only one pixel in an image that depicts a horse, the AI will be 99.9% sure that we are seeing a frog. A ship can also be disguised as a car, or, amusingly, almost anything can be seen as an airplane. So how can we perform such an attack? As you can see here, these neural networks typically don't provide a class directly, but a bunch of confidence values. What does this mean exactly? The confidence values denote how sure the network is that we see a labrador or a tiger cat. To come to a decision, we usually look at all of these confidence values and choose the object type that has the highest confidence. Now clearly, we have to know which pixel position to choose and what color it should be to perform a successful attack. We can do this by performing a bunch of random changes to the image and checking how each of these changes performed in decreasing the confidence of the network in the appropriate class. After this, we filter out the bad ones and continue our search around the most promising candidates. This process we refer to as differential evolution, and if we perform it properly, in the end, the confidence value for the correct class will be so low that a different class will take over. If this happens, the network has been defeated. Now, note that this also means that we have to be able to look into the neural network and have access to the confidence values. There is also plenty of research works on training more robust neural networks that can withstand as many adversarial changes to the inputs as possible. I cannot wait to report on these works as well in the future! Also, our next episode is going to be on adversarial attacks on the human vision system. Can you believe that? That paper is absolutely insane, so make sure to subscribe and hit the bell icon to get notified. You don't want to miss that one! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SA4YEAWVpbk",
        "paper_link": "https://arxiv.org/abs/1710.08864",
        "paper_title": "One pixel attack for fooling deep neural networks"
    },
    {
        "video_id": "veWkBsK0nwU",
        "video_title": "DeepMind's AI Learns Complex Behaviors From Scratch | Two Minute Papers #239",
        "position_in_playlist": 34,
        "description": "The paper \"Learning by Playing - Solving Sparse Reward Tasks from Scratch\" is available here:\nhttps://arxiv.org/abs/1802.10567\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-2009819/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a learning algorithm that chooses a set of actions in an environment to maximize a score. This class of techniques enables us to train an AI to master a large variety of video games and has many more cool applications. Reinforcement learning typically works well when the rewards are dense. What does this mean exactly? This means that if we play a game, and after making a mistake, we immediately die, it is easy to identify which action of ours was the mistake. However, if the rewards are sparse, we are likely playing something that is akin to a long-term strategy planner game. If we lost, it is possible that we were outmaneuvered in the final battle, but it is also possible that we lost the game way earlier due to building the wrong kind of economy. There are a million other possible reasons, because we get feedback on how well we have done only once, and much much after we have chosen our actions. Learning from sparse rewards is very challenging, even for humans. And it gets even worse. In this problem formulation, we don't have any teachers that guide the learning of the algorithm, and no prior knowledge of the environment. So this problem sounds almost impossible to solve. So what did DeepMind's scientists come up with to at least have a chance at approaching it? And now, hold on to your papers, because this algorithm learns like a baby learns about its environment. This means that before we start solving problems, the algorithm would be unleashed into the environment to experiment and master basic tasks. In this case, our final goal would be to tidy up the table. First, the algorithm learns to activate its haptic sensors, control the joints and fingers, then, it learns to grab an object, and then to stack objects on top of each other. And in the end, the robot will learn that tidying up is nothing else but a sequence of these elementary actions that it had already mastered. The algorithm also has an internal scheduler that decides which should be the next action to master, while keeping in mind that the goal is to maximize progress on the main task, which is, tidying up the table in this case. And now, onto validation. When we are talking about software projects, the question of real-life viability often emerges. So, the question is, how would this technique work in reality, and what else would be the ultimate test than running it on a real robot arm! Let's look here and marvel at the fact that it easily finds and moves the green block to the appropriate spot. And note that it had learned how to do it from scratch, much like a baby would learn to perform such tasks. And also note that this was a software project that was deployed on this robot arm, which means that the algorithm generalizes well for different control mechanisms. A property that is highly sought after when talking about intelligence. And if earlier progress in machine learning research is indicative of the future, this may learn how to perform backflips and play video games on a superhuman level within two followup papers. I cannot wait to see that, and I'll be here to report on that for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=veWkBsK0nwU",
        "paper_link": "https://arxiv.org/abs/1802.10567",
        "paper_title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch"
    },
    {
        "video_id": "oWpp1YYcCsU",
        "video_title": "DeepMind's AI Masters Even More Atari Games | Two Minute Papers #238",
        "position_in_playlist": 35,
        "description": "The paper \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\" is available here:\nhttps://arxiv.org/abs/1802.01561\n\nUpdate: Its source code is now available here: https://github.com/deepmind/scalable_agent\n\nDeepMind Lab: https://arxiv.org/abs/1612.03801\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-1548365/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a learning algorithm that we can use to choose a set of actions in an environment to maximize a score. There are many applications of such learners, but we typically cite video games because of the diverse set of challenges they can present the player with. And in reinforcement learning, we typically have one task, like learning backflips, and one agent that we wish to train to perform it well. This work is DeepMind's attempt to supercharge reinforcement learning by training one agent that can do a much wider variety of tasks. Now, this clearly means that we have to acquire more training data and also be prepared to process all this data as effectively as possible. By the way, the test suite you see here is also new where typical tasks in this environment involve pathfinding through mazes, collecting objects, finding keys to open their matching doors, and more. And every Fellow Scholar knows that the paper describing its details is of course, available in the description. This new technique builds upon an earlier architecture that was also published by DeepMind. This earlier architecture, A3C unleashes a bunch of actors into the wilderness, each of which gets a copy of the playbook that contains the current strategy. These actors then play the game independently, and periodically stop and share what worked and what didn't to this playbook. With this new IMPALA architecture, there are two key changes to this. One, in the middle, we have a learner, and the actors don't share what worked and what didn't to this learner, but they share their experiences instead. And later, the centralized learner will come up with the proper conclusions with all this data. Imagine if each football player in a team tries to tell the coach the things they tried on the field and what worked. That is surely going to work at least okay, but instead of these conclusions, we could aggregate all the experience of the players into some sort of centralized hive mind, and get access to a lot more, and higher quality information. Maybe we will see that a strategy only works well if executed by the players who are known to be faster than their opponents on the field. The other key difference is that with traditional reinforcement learning, we play for a given number of steps, then stop and perform learning. With this technique, we have decoupled the playing and learning, therefore it is possible to create an algorithm that performs both of them continuously. This also raises new questions, make sure to have a look at the paper, specifically the part with the new off-policy correction method by the name V-Trace. When tested on 30 of these levels and a bunch of Atari games, the new technique was typically able to double the score of the previous A3C architecture, which was also really good. And at the same time, this is at least 10 times more data-efficient, and its knowledge generalizes better to other tasks. We have had many episodes on neural network-based techniques, but as you can see research on the reinforcement learning side is also progressing at a remarkable pace. If you have enjoyed this episode, and you feel that 8 science videos a month is worth a dollar, please consider supporting us on Patreon. You can also pick up cool perks like early access to these episodes. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=oWpp1YYcCsU",
        "paper_link": "https://arxiv.org/abs/1802.01561",
        "paper_title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
    },
    {
        "video_id": "dxOHmvTaCN4",
        "video_title": "AI Learns Human Pose Estimation From Videos | Two Minute Papers #237",
        "position_in_playlist": 36,
        "description": "The paper \"DensePose: Dense Human Pose Estimation In The Wild\" is available here:\nhttps://arxiv.org/abs/1802.00434\nhttp://densepose.org/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-3178198/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This project is a collaboration between INRIA and Facebook AI research and is about pose estimation. Pose estimation means that we take an input photo or in the cooler case, video of people, and the output should be a description of their postures. This is kind of like motion capture for those amazing movie and computer game animations, but without the studio and the markers. This work goes even further and tries to offer a full 3D reconstruction of the geometry of the bodies, and it is in fact doing way more than that as you will see in a minute. Neural networks are usually great at these tasks, provided that we have a large number of training samples to train them. So, the first step is gathering a large amount of annotated data. This means an input photograph of someone, which is paired up with the correct description of their posture. This is what we call one training sample. This new proposed dataset contains 50 thousand of these training samples, and using that, we can proceed to step number two, training the neural network to perform pose estimation. But, there is more to this particular work. Normally, this pose estimation takes place with a 2D skeleton, which means that most techniques output a stick figure. But not in this case, because the dataset contains segmentations and dense correspondances between 2D images and 3D models, therefore, the network is also able to output fully 3D models. There are plenty of interesting details shown in the paper, for instance, since the annotated ground truth footage in the training set is created by humans, there is plenty of missing data that is filled in by using a separate neural network that is specialized for this task. Make sure to have a look at the paper for more cool details like this. This all sounds good in theory, but a practical application has to be robust against occlusions and rapid changes in posture, and the good thing is that the authors published plenty of examples with these that you can see here. Also, it has to be able to deal with smaller and bigger scales when people are closer or further away from the camera. This is also a challenge. The algorithm does a really good job at this, and remember, no markers and studio setup is required, and everything that you see here is performed interactively. The dataset will appear soon and it will be possible to reuse it for future research works, so I expect plenty of more collaboration and followup works for this problem. We are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dxOHmvTaCN4",
        "paper_link": "https://arxiv.org/abs/1802.00434",
        "paper_title": "DensePose: Dense Human Pose Estimation In The Wild"
    },
    {
        "video_id": "UPcR7S8ue1A",
        "video_title": "AI-Based Animoji Without The iPhone X | Two Minute Papers #236",
        "position_in_playlist": 37,
        "description": "The paper \"Avatar Digitization From a Single Image For Real-Time Rendering\" is available here:\nhttp://www.hao-li.com/publications/papers/siggraphAsia2017ADFSIFRTR.pdf\nhttp://www.hao-li.com/Hao_Li/Hao_Li_-_publications.html\n\nDemo for iOS:\nhttp://pinscreen.com/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Many of you have surely heard the word Animoji, which refers to these emoji figures that are animated in real time and react to our facial gestures. This is implemented in the new iPhone X phones, however, to accomplish this, it uses a dot projector get a good enough understanding of the geometry of the human face. So, how about a technique that doesn't need any specialized gear, takes not even a video of you, but one photograph as an input, and creates a digital avatar of us that can be animated in real time. Well, sign me up! Have a look at these incredible results. As you can see, the final result also includes secondary components like eyes, teeth, tongue, and gum. Now, these avatars don't have to be fully photorealistic, but have to capture the appearance and gestures of the user well enough so they can be used in video games or any telepresence application where a set of users interact in a virtual world. As opposed to many prior works, the hair is not reconstructed strand by strand, because doing this in real time is not feasible. Also, note that the information we are given is highly incomplete, because the backside of the head is not captured, but these characters also have a quite appropriate looking hairstyle there. So, how is this even possible? Well, first, the input image is segmented into the face part and the hair part. Then, the hair part is run through a neural network that tries to extract attributes like length, spikiness, are there hair buns, is there a ponytail, where the hairline is, and more. This is an extremely deep neural network with over 50 layers and it took 40 thousand images of different hairstyles to train. Now, since it is highly unlikely that the input photo shows someone with a hairstyle that was never ever worn by anyone else, we can look into a big dataset of already existing hairstyles and choose the closest one that fits the attributes extracted by the neural network. Such a smart idea, loving it. You can see how well this works in practice, and in the next step, the movement and appearance of the final hair geometry can be computed in real time through a novel polygonal strip representation. The technique also supports retargeting, which means that our gestures can be transferred to different characters. The framework is also very robust to different lighting conditions, which means that a differently lit photograph will lead to very similar outputs. The same applies for expressions. This is one of those highly desirable details that makes or breaks the usability of a new technique in production environments, and this one passed with flying colors. In these comparisons, you can also see that the quality of the results also smokes the competition. A variant of the technology can be downloaded through the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UPcR7S8ue1A",
        "paper_link": "http://www.hao-li.com/publications/papers/siggraphAsia2017ADFSIFRTR.pdf",
        "paper_title": "Avatar Digitization From a Single Image For Real-Time Rendering"
    },
    {
        "video_id": "iBaWVuaSQ-Q",
        "video_title": "A Photo Enhancer AI | Two Minute Papers #235",
        "position_in_playlist": 38,
        "description": "The paper \"DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks\" and its demo is available here:\nhttp://people.ee.ethz.ch/~ihnatova/\nhttp://phancer.com/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-3157391/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Some time ago, smartphone cameras were trying to outpace each other by adding more and more megapixels to their specification sheet. The difference between a half megapixel image and a 4 megapixel image was night and day. However, nowadays, we have entered into diminishing returns as most newer mobile cameras support 8 or more megapixels. At this point, a further resolution increase doesn't lead to significantly more convincing photos - and here is where the processing software takes the spotlight. This paper is about an AI-based technique that takes a poor quality photo and automatically enhances it. Here, you can already see what a difference software can make to these photos. Many of these photos were taken with an 8-year-old mobile camera and were enhanced by the AI. This is insanity. Now, before anyone thinks that by enhancement, I am referring to the classic workflow of adjusting white balance, color levels and hues. No, no, no. By enhancement, I mean the big, heavy hitters like recreating lost details via super resolution and image inpainting, image deblurring, denoising, and recovering colors that were not even recorded by the camera. The idea is the following: first, we shoot a lot of photos from the same viewpoint with a bunch of cameras, ranging from a relatively dated iPhone 3GS, other mid-tier mobile cameras, and a state of the art DSLR camera. Then, we hand over this huge bunch of data to a neural network that learns the typical features that are preserved by the better cameras and lost by the worse ones. The network does the same with relating the noise patterns and color profiles to each other. And then, we use this network to recover these lost features and pump up the quality of our lower tier camera to be as close as possible to a much more expensive model. Super smart idea, loving it. And you know what is even more brilliant? The validation of this work can take place in a scientific manner, because we don't need to take a group of photographers who will twirl their moustaches and judge these photos. Though, I'll note that this was also done for good measure. But, since we have the photos from the high-quality DSLR camera, we can take the bad photos, enhance them with the AI, and compare this output to the real DSLR's output. Absolutely brilliant. The source code and pre-trained networks and an online demo is also available. So, let the experiments begin, and make sure to leave a comment with your findings! What do you think about the outputs shown in the website? Did you try your own photo? Let me know in the comments section. A high-quality validation section, lots of results, candid discussion of the limitations in the paper, published source code, pretrained networks and online demos that everyone can try free of charge. Scientists at ETH Z\u00fcrich maxed this paper out. This is as good as it gets. If you have enjoyed this episode and would like to help us make better videos in the future, please consider supporting us on Patreon by clicking the letter P at the end screen of this video in a moment, or just have a look at the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iBaWVuaSQ-Q",
        "paper_link": "http://people.ee.ethz.ch/~ihnatova/",
        "paper_title": "DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks"
    },
    {
        "video_id": "pVgC-7QTr40",
        "video_title": "Building Blocks of AI Interpretability | Two Minute Papers #234",
        "position_in_playlist": 39,
        "description": "The paper \"Building Blocks of Interpretability\" is available here:\nhttps://distill.pub/2018/building-blocks/\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1210559/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Hold on to your papers because this is an exclusive look at a new neural network visualization paper that came from a collaboration between Google and the Carnegie Mellon University. The paper is as fresh as it gets because this is the first time I have been given an exclusive look before the paper came out, and this means that this video and the paper itself will be published at the same time. This is really cool and it's quite an honor, thank you very much! Neural networks are powerful learning-based tools that are super useful for tasks that are difficult to explain but easy to demonstrate. For instance, it is hard to mathematically define what a traffic sign is, but we have plenty of photographs of them. So the idea is simple, we label a bunch of photographs with additional data that says \"this is a traffic sign\", and \"this one isn't\", and feed this to the learning algorithm. As a result, neural networks have been able to perform traffic sign detection at a superhuman level for many years now. Scientists at Google DeepMind have also shown us that if we combine a neural network with reinforcement learning, we can get it to look at a screen and play computer games on a very high level. It is incredible to see problems that seemed impossible for many decades crumble one by one in quick succession over the last few years. However, we have a problem, and that problem is interpretability. There is no doubt that these neural networks are efficient, however, they cannot explain their decisions to us, at least not in a way that we can interpret. To alleviate this, earlier works tried to visualize these networks on the level of neurons, particularly, what kinds of inputs make these individual neurons extremely excited. This paper is about combining previously known techniques to unlock more powerful ways to visualize these networks. For instance, we can combine the individual neuron visualization with class attributions. This offers a better way of understanding how a neural network decides whether a photo depicts a labrador or a tiger cat. Here, we can see which part of the image activates a given neuron and what the neuron is looking for. Below, we see the final decision as to which class this image should belong to. This next visualization technique shows us which set of detectors contributed to a final decision, and how much they contributed exactly. Another way towards better interpretability is to decrease the overwhelming number of neurons into smaller groups with more semantic meaning. This process is referred to as factorization or neuron grouping in the paper. If we do this, we can obtain highly descriptive labels that we can endow with intuitive meanings. For instance, here, we see that in order for the network to classify the image as a labrador, it needs to see a combination of floppy ears, doggy forehead, doggy mouth, and a bunch of fur. And we can also construct a nice activation map to show which part of the image makes our groups excited. Please note that we have only scratched the surface. This is a beautiful paper, and it has tons of more results available exactly from this moment, with plenty of interactive examples you can play with. Not only that, but the code is open sourced, so you are also able to reproduce these visualizations with little to no setup. Make sure to have a look at it in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=pVgC-7QTr40",
        "paper_link": "https://distill.pub/2018/building-blocks/",
        "paper_title": "Building Blocks of Interpretability"
    },
    {
        "video_id": "izZofvgaIig",
        "video_title": "Why Should We Trust An AI? | Two Minute Papers #233",
        "position_in_playlist": 40,
        "description": "The paper \"Why Should I Trust You? - Explaining the Predictions of Any Classifier\" and its implementation is available here:\nhttp://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf\nhttps://github.com/marcotcr/lime\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-563428/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Through over 200 episodes of this series, we talked about many learning-based algorithms that are able to solve problems that previously seemed completely impossible. They can look at an image and describe what they depict in a sentence, or even turn video game graphics into reality and back. Amazing new results keep appearing every single week. However, an important thing that we need to solve is that if we deploy these neural networks in a production environment, we would want to know if we're relying on a good or bad AI's decision. The narrative is very simple: if we don't trust a classifier, we won't use it. And perhaps the best way of earning the trust of a human would be if the AI could explain how it came to a given decision. Strictly speaking, a neural network can explain it to us, but it would show us hundreds of thousands of neuron activations that are completely unusable for any sort of intuitive reasoning. So, what is even more difficult to solve is that this explanation happens in a way that we can interpret. An earlier approach used decision trees that described what the learner looks at and how it uses this information to arrive to a conclusion. This new work is quite different. For instance, imagine that a neural network would look at all the information we know about a patient and tell us that this patient likely has the flu. And, in the meantime, it could tell us that the fact that the patient has a headache and sneezes a lot contributed to the conclusion that he has the flu, but, the lack of fatigue is notable evidence against it. Our doctor could take this information and instead of blindly relying on the output, could make a more informed decision. A fine example of a case where AI does not replace, but augment human labor. An elegant tool for a more civilized age. Here, we see an example image where the classifier explains which region contributes to the decision that this image depicts a cat, and which region seems to be counterevidence. We can use this not only for tabulated patient data and images, but text as well. In this other example, we try to find out whether a piece of written text is about Christinanity or Atheism. Note that the decision itself is not as simple as looking for a few keywords, even a mid-tier classifier is much more sophisticated than that. But, it can tell us about the main contributing factors. A big additional selling point is that this technique is model agnostic, which means that it can be applied to other learning algorithms that are able to perform classification. It is also a possibility that an AI is only right by chance, and if this is the case, we should definitely know about that. And here, in this example, with the additional explanation, it is rather easy to find out that we have a bad model that looks at the background of the image and thinks it is the fur of a wolf. The tests indicate that humans make significantly better decisions when they lean on explanations that are extracted by this technique. The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=izZofvgaIig",
        "paper_link": "http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf",
        "paper_title": "Why Should I Trust You? - Explaining the Predictions of Any Classifier"
    },
    {
        "video_id": "hzpxXZJQNFg",
        "video_title": "DeepMind's WaveNet, 1000 Times Faster | Two Minute Papers #232",
        "position_in_playlist": 41,
        "description": "The paper \"Parallel WaveNet: Fast High-Fidelity Speech Synthesis\" is available here:\nhttps://arxiv.org/abs/1711.10433\n\nOur Patreon page: https://www.patreon.com/TwoMinutePapers\n\nDeepMind's Blog: https://deepmind.com/blog/wavenet-launches-google-assistant/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Nader Shakerin, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-3172471/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Due to popular demand, here is the new DeepMind paper on WaveNet. WaveNet is a text to speech algorithm that takes a sentence as an input, and gives us audio footage of these words being uttered by a person of our choice. Let's listen to some results from the original algorithm, note that these are all synthesized by the AI. All this requires is some training data from this person's voice, typically 10-30 hours, and, a ton of computational power. The computational power part is especially of interest, because we have to produce over 16 to 24 thousand samples for each second of continuous audio footage. And unfortunately, as you can see here, these new samples are generated one by one. And since today's graphics cards are highly parallel, this means that it is a waste to get them to have one compute unit that does all the work while the others are sitting there twiddling their thumbs. We need to make this more parallel somehow. So, the solution is simple, instead of one, we can just simply make more samples in parallel! No, no, no, it doesn't work like that, and the reason for this is that speech is not like random noise - it is highly coherent where the new samples are highly dependent on the previous ones. We can only create one new sample at a time. So how can we create the new waveform in one go using these many compute units in parallel? This new WaveNet variant starts out from white noise and applies changes to it over time to morph it into the output speech waveform. The changes take place in parallel over the entirety of the signal, so that's a good sign. It works by creating a reference network that is slow, but correct. Let's call this the teacher network. And the new algorithm arises as a student network, which tries to mimic what the teacher does, but the student tries to be more efficient at that. This has a similar vibe to Generative Adversarial Networks where we have two networks: one is actively trying to fool the other one, while this other one tries to better distinguish fake inputs from real ones. However, it is fundamentally different because of the fact that the student does not try to fool the teacher, but mimic it while being more efficient. And, this yields a blistering fast version of WaveNet that is over a 1000 times faster than its predecessor. It is not real time, it is 20 times faster than real time. And you know what the best part is? Usually, there are heavy tradeoffs for this, but this time, the validation section of the paper reveals that there is no perceived difference in the outputs from the original algorithm. Hell yeah! So, where can we try it? Well, it is already deployed online in Google Assistant in multiple English and Japanese voices. So, as you see, I was wrong. I said that a few papers down the line, it will definitely be done in real time. Apparently, with this new work, it is not a few more papers down the line, it is one, and it is not a bit faster but a thousand times faster. Things are getting out of hand real quick, and I mean this in the best possible way. What a time to be alive! This is one incredible, and highly inspiring work. Make sure to have a look at the paper, perfect training for the mind. As always, it is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hzpxXZJQNFg",
        "paper_link": "https://arxiv.org/abs/1711.10433",
        "paper_title": "Parallel WaveNet: Fast High-Fidelity Speech Synthesis"
    },
    {
        "video_id": "uGhyOBSzdTs",
        "video_title": "Bubble Collision Simulations in Milliseconds | Two Minute Papers #231",
        "position_in_playlist": 42,
        "description": "The paper \"A Hyperbolic Geometric Flow for Evolving Films and Foams\" is available here:\nhttps://sadashigeishida.bitbucket.io/hgf/index.html\n\nRecommended for you:\n1. Reddit discussion on bubble thickness measurements - https://www.reddit.com/r/askscience/comments/1wva6u/is_it_possible_to_measure_the_thickness_of_a_soap/\n2. An early episode on bubbles - https://www.youtube.com/watch?v=uj8b5mu0P7Y\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1916692/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about simulating on a computer what happens when bubbles collide. Prepare for lots of beautiful footage. This is typically done by simulating the Navier-Stokes equations that describe the evolution of the velocity and pressure within a piece of fluid over time. However, because the world around us is a continuum, we cannot compute these quantities in an infinite number of points. So, we have to subdivide the 3D space into a grid, and compute them only in these gridpoints. The finer the grid, the more details appear in our simulations. If we try to simulate what happens when these bubbles collide, we would need to create a grid that can capture these details. This is an issue, because the thickness of a bubble film is in the order of 10-800 nanometers, and this would require a hopelessly fine high-resolution grid. By the way, measuring the thickness of bubbles is a science of its own, there is a fantastic reddit discussion on it, I put a link to it in the video description, make sure to check it out. So, these overly fine grids take too long to compute, so what do we do? Well, first, we need to focus on how to directly compute how the shape of soap bubbles evolves over time. Fortunately, from Belgian physicist Joseph Plateau we know that they seek to reduce their surface area, but, retain their volume over time. One of the many beautiful phenomena in nature. So, this shall be the first step - we simulate forces that create the appropriate shape changes and proceed into an intermediate state. However, by pushing the film inwards, its volume has decreased, therefore this intermediate state is not how it should look in nature. This is to be remedied now where we apply a volume correction step. In the validation section, it is shown that the results follow to Plateau's laws quite closely. Also, you know well that my favorite kind of validation is when we let reality be our judge, and in this work, the results have been compared to a real-life experimental setup and proved to be very close to it. Take a little time to absorb this. We can write a computer program that reproduces what would happen in reality and result in lots of beautiful video footage. Loving it! And, the best part is that the first surface evolution step is done through an effective implementation of the hyperbolic mean curvature flow, which means that the entirety of the process is typically 3 to 20 times faster than the state of the art while being more robust in handling splitting and merging scenarios. The computation times are now in the order of milliseconds instead of seconds. The earlier work in this comparison was also showcased in Two Minute Papers, if I see it correctly, it was in episode number 18. Holy mother of papers, how far we have come since. I've put a link to it in the video description. The paper is beautifully written, and there are plenty of goodies therein, for instance, an issue with non-manifold junctions is addressed, so make sure to have a look. The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=uGhyOBSzdTs",
        "paper_link": "https://sadashigeishida.bitbucket.io/hgf/index.html",
        "paper_title": "A Hyperbolic Geometric Flow for Evolving Films and Foams"
    },
    {
        "video_id": "HANeLG0l2GA",
        "video_title": "This AI Sings | Two Minute Papers #230",
        "position_in_playlist": 43,
        "description": "The paper \"A Neural Parametric Singing Synthesizer\" is available here:\nhttp://www.dtic.upf.edu/~mblaauw/NPSS/\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nCheck out my new voice here! https://goo.gl/z6zxuT\n\nJean-Michel Jarre Vocoder song: https://open.spotify.com/album/0ZKglE5xlIqsWmtQHn9WxZ\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \nVocoder video credit: Shal Music/FX - https://www.youtube.com/watch?v=uXn1up-9D78\nThumbnail background image credit: https://flic.kr/p/GAhDRa\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about building an AI vocoder that is able to synthesize believable singing from MIDI and lyrics as inputs. But first, what is a vocoder? It works kinda like this. Fellow Scholars who are fans of Jean-Michel Jarre's music are likely very familiar with this effect, I've put a link to an example song in the video description. Make sure to leave a comment with your favorite songs with vocoders so I and other Fellow Scholars can also nerd out on them. And now about the MIDI and lyrics terms. The lyrics part is a simple text file containing the words that this synthesized voice should sing, and the MIDI is data that describes the pitch, length and the velocity of each sound. With a little simplification, we could say that the score is given as an input, and the algorithm has to output the singing footage. We will talk about the algorithm in a moment, but for now, let's listen to it. Wow. So this is a vocoder. This means it separates the pitch and timbre components of the voice, therefore the waveforms are not generated directly, which is a key difference from Google DeepMind's WaveNet. This leads to two big advantages: One, the generation times are quite favorable. And by favorable, I guess you're hoping for real time. Well, hold on to your papers, because it is not real time, it is 10-15 times real-time! And two, this way, the algorithm will only need a modest amount of training data to function well. Here, you can see the input phonemes that make up the syllables of the lyrics, each typically corresponding to one note. This is then connected to a modified WaveNet architecture that uses 2-by-1 dilated convolutions. This means that the dilation factor is doubled in each layer, thereby introducing an exponential growth in the receptive field of the model. This helps us keep the parameter count down, which enables training on small datasets. As validation, the mean opinion scores have been recorded, in a previous episode, we discussed that this is a number that describes how a sound sample would pass as genuine human speech or singing. The test showed that this new method is well ahead of the competition, approximately midway between the previous works and the reference singing footage. There are plenty of other tests in the paper, this is just one of many, so make sure to have a look. This is one important stepping stone towards synthesizing singing that is highly usable in digital media and where generation is faster than real time. Creating a MIDI input is a piece of cake with a midi master keyboard, or we can even draw the notes by hand in many digital audio workstation programs. After that, writing the lyrics is as simple as it gets and doesn't need any additional software. Tools like this are going to make this process accessible to everyone. Loving it. If you would like to help us create more elaborate videos, please consider supporting us on Patreon. We also support one-time payments through cryptos like Bitcoin, Ethereum and Litecoin. Everything is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HANeLG0l2GA",
        "paper_link": "http://www.dtic.upf.edu/~mblaauw/NPSS/",
        "paper_title": "A Neural Parametric Singing Synthesizer"
    },
    {
        "video_id": "3yOZxmlBG3Y",
        "video_title": "Pruning Makes Faster and Smaller Neural Networks | Two Minute Papers #229",
        "position_in_playlist": 44,
        "description": "The paper \"Learning to Prune Filters in Convolutional Neural Networks\" is available here:\nhttps://arxiv.org/pdf/1801.07365.pdf\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-3064187/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When we are talking about deep learning, we are talking about neural networks that have tens, sometimes hundreds of layers, and hundreds of neurons within these layers. This is an enormous number of parameters to train, and clearly, there should be some redundancy, some duplication in the information within. This paper is trying to throw out many of these neurons of the network without affecting its accuracy too much. This process we shall call pruning and it helps creating neural networks that are faster and smaller. The accuracy term I used typically means a score on a classification task, in other words, how good this learning algorithm is in telling what an image or video depicts. This particular technique is specialized for pruning Convolutional Neural Networks, where the neurons are endowed with a small receptive field and are better suited for images. These neurons are also commonly referred to as filters. So here, we have to provide a good mathematical definition of a proper pruning. The authors proposed a definition where we can specify a maximum accuracy drop that we deem to be acceptable, which will be denoted with the letter \"b\" in a moment. And the goal is to prune as many filters as we can, without going over the specified accuracy loss budget. The pruning process is controlled by an accuracy and efficiency term, and the goal is to have some sort of balance between the two. To get a more visual understanding of what is happening, here, the filters you see outlined with the red border are kept by the algorithm, and the rest are discarded. As you can see, the algorithm is not as trivial as many previous approaches that just prune away filters with weaker responses. Here you see the table with the b numbers. Initial tests reveal that around a quarter of the filters can be pruned with an accuracy loss of 0.3%, and with a higher b, we can prune more than 75% of the filters with a loss of around 3%. This is incredible. Image segmentation tasks are about finding the regions that different objects inhabit. Interestingly, when trying the pruning for this task, it not only introduces a minimal loss of accuracy, in some cases, the pruned version of the neural network performs even better. How cool is that! And of course, the best part is that we can choose a tradeoff that is appropriate for our application. For instance, if we are we looking for a light cleanup, we can use the first option at a minimal penalty, or, if we wish to have a tiny tiny neural network that can run on a mobile device, we can look for the more heavy-handed approach by sacrificing just a tiny bit more accuracy. And, we have everything in between. There is plenty more validation for the method in the paper, make sure to have a look! It is really great to see that new research works make neural networks not only more powerful over time, but there are efforts in making them smaller and more efficient at the same time. Great news indeed. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=3yOZxmlBG3Y",
        "paper_link": "https://arxiv.org/pdf/1801.07365.pdf",
        "paper_title": "Learning to Prune Filters in Convolutional Neural Networks"
    },
    {
        "video_id": "bdM9c2OFYuw",
        "video_title": "Google's Text Reader AI: Almost Perfect | Two Minute Papers #228",
        "position_in_playlist": 45,
        "description": "The paper \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\" is available here:\nhttps://google.github.io/tacotron/publications/tacotron2/index.html\nhttps://arxiv.org/abs/1712.05884\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nUnofficial implementations - proceed with care:\nhttps://github.com/candlewill/Tacotron-2\nhttps://github.com/r9y9/wavenet_vocoder\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2875123/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Earlier, we talked about Google's WaveNet, a learning-based text-to-speech engine. This means that we give it a piece of written text and after a training step using someone's voice, it has to read it aloud using this person's voice as convincingly as possible. And this followup work is about making it even more convincing. Before we go into it, let's marvel at these new results together. Hm-hm! As you can hear, it is great at prosody, stress and intonation, which leads to really believable human speech. The magic component in the original WaveNet paper was introducing dilated convolutions for this problem. This makes large skips in the input data so we have a better global view of it. It is a bit like increasing the receptive field of the eye so we can see the entire landscape, and not only a tree on a photograph. The magic component in this new work is using Mel spectrograms as an input to WaveNet. This is an intermediate representation that is based on the human perception that records not only how different words should be pronounced, but the expected volumes and intonations as well. The new model was trained on about 24 hours of speech data. And of course, no research work should come without some sort of validation. The first is recording the mean opinion scores for previous algorithms, this one and real, professional voice recordings. The mean opinion score is a number that describes how a sound sample would pass as genuine human speech. The new algorithm passed with flying colors. An even more practical evaluation was also done in the form of a user study where people were listening to the synthesized samples and professional voice narrators, and had to guess which one is which. And this is truly incredible, because most of the time, people had no idea which was which - if you don't believe it, we'll try this ourselves in a moment. A very small, but statistically significant tendency towards favoring the real footage was recorded, likely because some words, like \"merlot\" are mispronounced. Automatically voiced audiobooks, automatic voice narration for video games. Bring it on. What a time to be alive! Note that producing these waveforms is not real time and still takes quite a while. To progress along that direction, scientists as DeepMind wrote a heck of a paper where they sped WaveNet up a thousand times. Leave a comment if you would like to hear more about it in a future episode. And of course, new inventions like this will also raise new challenges down the line. It may be that voice recordings will become much easier to forge and be less useful as evidence unless we find new measures to verify their authenticity, for instance, to sign them like we do with software. In closing, a few audio sample pairs, one of them is real, one of them is synthesized. What do you think, which is which? Leave a comment below. I'll just leave a quick hint here that I found on the webpage. Hopp! There you go. If you have enjoyed this episode, please make sure to support us on Patreon. This is how we can keep the show running, and you know the drill, one dollar is almost nothing, but it keeps the papers coming. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bdM9c2OFYuw",
        "paper_link": "https://google.github.io/tacotron/publications/tacotron2/index.html",
        "paper_title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions"
    },
    {
        "video_id": "pAiiPNg0kDE",
        "video_title": "SLAC Dataset From MIT and Facebook  | Two Minute Papers #227",
        "position_in_playlist": 46,
        "description": "The paper \"SLAC: A Sparsely Labeled Dataset for Action Classification and Localization\" is available here:\nhttp://slac.csail.mit.edu/\nhttps://arxiv.org/abs/1712.09374\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-3011677/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This project is about a dataset created through a joint effort between MIT and Facebook. As it turns out, this dataset is way more useful than I initially thought, I'll tell you in a moment why. Datasets are used to train and test the quality of learning algorithms. This particular dataset contains short video clips. These clips are passed to a neural network which is asked to classify the kind of activity that is taking place in the video. In this dataset, there are many cases where everything is given to come to a logical answer that is wrong. We may be in a room with the climbing wall, but exercising is not necessarily happening. We could be around the swimming pool, but swimming is not necessarily happening. I am pretty sure this has happened to you too. This is a brilliant idea, because it is super easy for a neural network to assume that if there is a swimming pool, swimming is probably happening, but it takes a great deal of understanding to actually know what constitutes the swimming part. A few episodes ago, we discussed that this could potentially be a stepping stone towards creating machines that think like humans. Without looking into it, it would be easy to think that creating a dataset is basically throwing a bunch of training samples together and calling it a day. I can assure you that this is not the case and that creating a dataset like this was a herculean effort as it contains more than half a million videos, and almost two million annotations for 200 different activities. And, there are plenty of pre-processing steps that one has to perform to make it usable. The collection procedure contains a video crawling step where a large number of videos are obtained from YouTube, which are to be deduplicated, which means removing videos that are too similar to one already contained in the database. A classical case is many different kinds of commentary on the same footage. This amounted to the removal of more than 150 thousand videos. Then, all of these videos undergo a shot and person detection step where relevant subclips are extracted that contain some kind of human activity. These are then looked at by two different classifiers, and depending on whether there was a consensus between the two, a decision is made whether the clip is to be discarded or not. This step helps balancing the ratio of videos where there is some sort of relevant action compared to clips where there is no relevant action happening. This also makes the negative samples much harder because the context may be correct, but the expected activity may not be there. This is the classical hard case with the swimming pool and people in swimming suits twiddling their thumbs instead of swimming. And here comes the more interesting part - when trying to train a neural network for other, loosely related tasks, using this dataset for pre-training improves the scores significantly. I'll try to give a little context for the numbers, because these numbers are absolutely incredible. There are cases where the success rate is improved by over 30%, which speaks for itself. However, there are other cases where the difference is about 10-15% percent, that is also remarkable when we are talking about high numbers because the closer the classifier gets to a 100%, the more difficult the remaining corner cases are that improve the accuracy. In these cases, even a 3% improvement is remarkable. And before we go, greetings and best regards to Lucas, the little scholar who seems to be absorbing the papers along with the mother's milk. Excellent. You can't start early enough in the pursuit of knowledge. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=pAiiPNg0kDE",
        "paper_link": "http://slac.csail.mit.edu/",
        "paper_title": "SLAC: A Sparsely Labeled Dataset for Action Classification and Localization"
    },
    {
        "video_id": "WhaRsrlaXLk",
        "video_title": "DeepMind Control Suite | Two Minute Papers #226",
        "position_in_playlist": 47,
        "description": "The paper \"DeepMind Control Suite\" and its source code is available here:\nhttps://arxiv.org/pdf/1801.00690v1.pdf\nhttps://github.com/deepmind/dm_control\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2921430/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This footage that you see here came freshly from Google DeepMind's lab, and is about benchmarking reinforcement learning algorithms. Here, you see the classical cartpole swing-up task from this package. As the algorithm starts to play, a score is recorded that indicates how well it is doing, and the learner has to choose the appropriate actions depending on the state of the environment to maximize this score. Reinforcement learning is an established research subfield within machine learning with hundreds of papers appearing every year. However, we see that most of them cherry-pick a few problems and test against previous works on this very particular selection of tasks. This paper describes a package that is not about the algorithm itself, but about helping future research projects to be able to test their results against previous works on an equal footing. This is a great idea, which has been addressed earlier by OpenAI with their learning environment by the name Gym. So the first question is, why do we need a new one? The DeepMind Control Suite provides a few differentiating features. One, Gym contains both discrete and continuous tasks, where this one is concentrated on continuous problems only. This means that state, time and action are all continuous which is usually the hallmark of more challenging and life-like problems. For an algorithm to do well, it has to be able to learn the concept of velocity, acceleration and other meaningful physical concepts and understand their evolution over time. Two, there are domains where the new control suite is a superset of Gym, meaning that it offers equivalent tasks, and then some more. And three, the action and reward structures are standardized. This means that the results and learning curves are much more informative and easier to read. This is crucial because research scientists read hundreds of papers every year, and this means that they don't necessarily have to look at videos, they immediately have an intuition of how an algorithm works and how it relates to previous techniques just by looking at the learning curve plots. Many tasks also include a much more challenging variant with more sparse rewards. We discussed these sparse rewards in a bit more in detail in the previous episode, if you are interested, make sure to click the card on the lower right at the end of this video. The paper also contains an exciting roadmap for future development, including quadruped locomotion, multithreaded dynamics and more. Of course, the whole suite is available, free of charge for everyone. The link is available in the description. Super excited to see a deluge of upcoming AI papers and see how they beat the living hell out of each other in 2018. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=WhaRsrlaXLk",
        "paper_link": "https://arxiv.org/pdf/1801.00690v1.pdf",
        "paper_title": "DeepMind Control Suite"
    },
    {
        "video_id": "DW1AuOC9TQc",
        "video_title": "Reinforcement Learning With Noise (OpenAI) | Two Minute Papers #225",
        "position_in_playlist": 48,
        "description": "The paper \"Better Exploration with Parameter Noise\" and its source code is available here:\nhttps://arxiv.org/abs/1706.01905\nhttps://github.com/openai/baselines\n\nThe write-up and our Patreon page with the details:\nhttps://www.patreon.com/posts/technical-for-16738692\nhttps://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2560006/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about improving reinforcement learning. Reinforcement learning is a learning algorithm that we can use to choose a set of actions in an environment to maximize a score. Our classical example applications are helicopter control, where the score to be maximized would be proportional to the distance that we traveled safely, or any computer game of your choice where a score can describe how well we are doing. For instance, in Frostbite, our score describes how many jumps we have survived without dying and this score is subject to maximization. Earlier, scientists at DeepMind combined a reinforcement learner with a deep neural network so the algorithm could look at the screen and play the game much like a human player would. This problem is especially difficult when the rewards are sparse. This is similar to what a confused student would experience after a written exam when only one grade is given, but the results for the individual problems are not shown. It is quite hard to know where we did well and where we missed the mark, and it is much more challenging to choose the appropriate topics to study to do better next time. When starting out, the learner starts exploring the parameter space and performs crazy, seemingly nonsensical actions until it finds a few scenarios where it is able to do well. This can be thought of as adding noise to the actions of the agent. Scientists at OpenAI proposed an approach where they add noise not directly to the actions, but the parameters of the agent, which results in perturbations that depend on the information that the agent senses. This leads to less flailing and a more systematic exploration that substantially decreases the time taken to learn tasks with sparse rewards. For instance, it makes a profound difference if we use it in the walker game. As you can see here, the algorithm with the parameter space noise is able to learn the concept of galloping, while the traditional method does, well, I am not sure what it is doing to be honest, but it is significantly less efficient. The solution does not come without challenges. For instance, different layers respond differently to this added noise, and, the effect of the noise on the outputs grows over time, which requires changing the amount of noise to be added depending on its expected effect on the output. This technique is called adaptive noise scaling. There are plenty of comparisons and other cool details in the paper, make sure to have a look, it is available in the video description. DeepMind's deep reinforcement learning was published in 2015 with some breathtaking results and superhuman plays on a number of different games, and it has already been improved leaps and bounds beyond its initial version. And we are talking about OpenAI, so of course, the source code of this project is available under the permissive MIT license. In the meantime, we have recently been able to upgrade the entirety of our sound recording pipeline through your support on Patreon. I have been yearning for this for a long-long time now and not only that, but we could also extend our software pipeline with sound processing units that use AI and work like magic. Quite fitting for the series, right? Next up is a recording room or recording corner with acoustic treatment, depending on our budget. Again, thank you for your support, it makes a huge difference. A more detailed write-up on this is available in the video description, have a look. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=DW1AuOC9TQc",
        "paper_link": "https://arxiv.org/abs/1706.01905",
        "paper_title": "Better Exploration with Parameter Noise"
    },
    {
        "video_id": "FMEk8cHF-OA",
        "video_title": "DeepMind's AI Learns Object Sounds | Two Minute Papers #224",
        "position_in_playlist": 49,
        "description": "The paper \"Objects that Sound\" is available here:\nhttps://arxiv.org/abs/1712.06651\nhttps://www.youtube.com/watch?v=TFyohksFd48\nhttps://www.youtube.com/watch?v=x_qusr58ruU\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nRecommended for you:\nLook, Listen & Learn - https://www.youtube.com/watch?v=mL3CzZcBJZU\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-756326/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about creating an AI that can perform audio-visual correspondence. This means two really cool tasks: One, when given a piece of video and audio, it can guess whether they match each other. And two, it can localize the source of the sounds heard in the video. Hm-hmm! And wait, because this gets even better! As opposed to previous works, here, the entire network is trained from scratch and is able to perform cross-modal retrieval. Cross-modal retrieval means that we are able to give it an input sound and it will be able to find pictures that would produce similar sounds. Or vice versa. For instance, here, the input is the sound of a guitar, note the loudspeaker icon in the corner, and it shows us a bunch of either images or sounds that are similar. Marvelous. The training is unsupervised, which means that the algorithm is given a bunch of data and learns without additional labels or instructions. The architecture and results are compared to a previous work by the name Look, Listen & Learn that we covered earlier in the series, the link is available in the video description. As you can see, both of them run a convolutional neural network. This is one of my favorite parts about deep learning - the very same algorithm is able to process and understand signals of very different kinds: video and audio. The old work concatenates this information and produces a binary yes/no decision whether it thinks the two streams match. This new work tries to produce number that encodes the distance between the video and the audio. Kind of like the distance between two countries on a map, but both video and audio signals are embedded in the same map. And the output decision always depends on how small or big this distance is. This distance metric is quite useful: if we have an input video or audio signal, choosing other video and audio snippets that have a low distance is one of the important steps that opens up the door to this magical cross-modal retrieval. What a time to be alive! Some results are very easy to verify, others may spark some more debate, for instance, it is quite interesting to see that the algorithm highlights the entirety of the guitar string as a sound source. If you are curious about this mysterious blue image here, make sure to have a look at the paper for an explanation. Now this is a story that we would like to tell to as many people as possible. Everyone needs to hear about this. If you would like to help us with our quest, please consider supporting us on Patreon. You can also pick up some cool perks, like getting early access to these videos or deciding the order of upcoming episodes. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=FMEk8cHF-OA",
        "paper_link": "https://arxiv.org/abs/1712.06651",
        "paper_title": "Objects that Sound"
    },
    {
        "video_id": "uOiOhVgR3VA",
        "video_title": "Building Machines That Learn and Think Like People | Two Minute Papers #223",
        "position_in_playlist": 50,
        "description": "The paper \"Building Machines That Learn and Think Like People\" is available here:\nhttps://arxiv.org/abs/1604.00289\n\nDeepMind's commentary article: https://arxiv.org/ftp/arxiv/papers/1711/1711.08378.pdf\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nFrostbite gameplay video source: https://www.youtube.com/watch?v=J2oSbAbcOPg\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2981726/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper discusses possible roadmaps towards building machines that are endowed with humanlike thinking. And before we go into that, the first question would be, is there value in building machines that think like people? Do they really need to think like people? Isn't it a bit egotistical to say \"if they are to become any good at this and this task, they have to think like us\"? And the answer is, well, in some cases, yes. If you remember DeepMind's Deep Q-Learning algorithm, it was able to play on a superhuman level on 29 out of 49 different Atari games. For instance, it did quite well in Breakout, but less so in Frostbite. And by Frostbite, I mean not the game engine, but the Atari game from 1983 where we need to hop from ice floe to ice floe and construct an igloo. However, we are not meant to jump around arbitrarily - we can gather these pieces by jumping on the active ice floes only, and these are shown with white color. Have a look at this plot. It shows the score it was able to produce as a function of game experience in hours. As you can see, the original DQN is doing quite poorly, while the extended versions of the technique can reach a relatively high score over time. This looks really good...until we look at the x axis, because then we see that this takes around 462 hours and the scores plateau afterwards. Well, compare that to humans can do at least as well, or a bit better after a mere 2 hours of training. So clearly, there are cases where there is an argument to be made for the usefulness of humanlike AI. The paper describes several possible directions that may help us achieve this. Two of them is understanding intuitive physics and intuitive psychology. Even young infants understand that objects follow smooth paths and expect liquids to go around barriers. We can try to endow an AI with similar knowledge by feeding it with physics simulations and their evolution over time to get an understanding of similar phenomena. This could be used to augment already existing neural networks and give them a better understanding of the world around us. Intuitive psychology is also present in young infants. They can tell people from objects, or distinguish other social and anti-social agents. They also learn goal-based reasoning quite early. This means that a human who looks at an experienced player play Frostbite can easily derive the rules of the game in a matter of minutes. Kind of like what we are doing now. Neural networks also have a limited understanding of compositionality and causality, and often perform poorly when describing the content of images that contain previously known objects interacting in novel, unseen ways. There are several ways of achieving each of these elements described in the paper. If we manage to build an AI that is endowed with these properties, it may be able to think like humans, and through self-improvement, may achieve the kind of intelligence that we see in all these science fiction movies. There is lots more in the paper - learning to learn, approximate models for thinking faster, model-free reinforcement learning, and a nice Q&A section with responses to common questions and criticisms. It is a great read and is easy to understand for everyone, I encourage you to have a look at the video description for the link to it. Scientists at Google DeepMind have also written a commentary article where they largely agree with the premises described in this paper, and add some thoughts about the importance of autonomy in building humanlike intelligence. Both papers are available in the video description and both are great reads, so make sure to have a look at them! It is really cool that we have plenty of discussions on potential ways to create a more general intelligence that is at least as potent as humans in a variety of different tasks. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=uOiOhVgR3VA",
        "paper_link": "https://arxiv.org/abs/1604.00289",
        "paper_title": "Building Machines That Learn and Think Like People"
    },
    {
        "video_id": "fTBeNAu18_s",
        "video_title": "This Autonomous Robot Models Your House Interior | Two Minute Papers #222",
        "position_in_playlist": 51,
        "description": "The paper \"Autonomous Reconstruction of Unknown Indoor Scenes Guided by Time-varying Tensor Fields\" and its source code is available here:\nhttp://vcc.szu.edu.cn/research/2017/tfnav/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Shawn Azman, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2732939/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The goal of this work is to have a robot that automatically creates a 3D model of an indoor space, including path planning and controlling attention. Now this immediately sounds like quite a challenging task. This robot uses an RGBD camera, so beyond the colors, it also gets some depth information that describes how far the viewed objects are from the observer. From this information, it tries to create a 3D digital model of these interiors. It not only does that, but it also constantly replans its trajectory based on newly found areas. These paths have to be smooth and walkable without bumping into objects, and of course, adapt to the topology of the building. You'll see in a moment why even this mundane sounding smooth path part is really challenging to accomplish. Spoiler alert: it's about singularities. With this proposed technique, the robot takes a lap in the building and builds a rough representation of it, kind of like a minimap in your favorite computer games. Previous techniques worked with potential and gradient fields to guide the navigation. The issue with these is that the green dots that you see here represent singularities. These are degenerate points that introduce ambiguity to the path planning process and reduce the efficiency of the navigation. The red dots are sinks, which are even worse, because these can trap the robot. This new proposed tensor field representation contains a lot fewer singularities, and its favorable mathematical properties make it sink-free. This leads to much better path planning which is crucial for maintaining high reconstruction quality. If you have a look at the paper, you'll see several occurrences of the word \"advection\". This is particularly cool because these robot paths are planned in these gradient or tensor fields represent the vortices and flow directions similarly to how fluid flows are computed in simulations. Many of which you have seen in this series. Beautiful, love it. However, as advection can't guarantee that we'll have a full coverage of the building, this proposed technique borrows classic structures from graph theory. Graph theory is used to model the connections between railway stations or to represent people and their relationships in a social network. And here, a method to construct a minimum spanning tree was borrowed to help deciding which direction to take at intersections for optimal coverage with miminal effort. Robots, fluids, graph theory, some of my favorite topics of all time, so you probably know how happy I was to see all these theories come together to create something really practical. This is an amazing paper, make sure to have a look at it, it is available in the video description. The source code of this project is also available. If you have enjoyed this episode, make sure to subscribe to the series and click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fTBeNAu18_s",
        "paper_link": "http://vcc.szu.edu.cn/research/2017/tfnav/",
        "paper_title": "Autonomous Reconstruction of Unknown Indoor Scenes Guided by Time-varying Tensor Fields"
    },
    {
        "video_id": "Uo6hFVRsjpA",
        "video_title": "High-Resolution Neural Texture Synthesis | Two Minute Papers #221",
        "position_in_playlist": 52,
        "description": "The paper \"High-Resolution Multi-Scale Neural Texture Synthesis\" and its source code is available here:\nhttps://wxs.ca/research/multiscale-neural-synthesis/\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2929203/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Deep Learning means that we are working with neural networks that contain many inner layers. As neurons in each layer combine information from the layer before, the deeper we go in these networks, the more elaborate details we're going to see. Let's have a look at an example. For instance, if we train a neural network to recognize images of human faces, first we'll see an edge detector, and as a combination of edges, object parts will emerge in the next layer. And in the later layers, a combination of object parts create object models. Neural texture synthesis is about creating lots of new images based on an input texture, and these new images have to resemble, but not copy the input. Previous works on neural texture synthesis focused on how different features in a given layer relate to the ones before and after it. The issue is that because neurons in convolutional neural networks are endowed with a small receptive field, they can only look at an input texture at one scale. So for instance, if you look here, you see that with previous techniques, trying to create small-scale details in a synthesized texture is going to lead to rather poor results. This new method is about changing the inputs and the outputs of the network to be able to process these images at different scales. These scales range from coarser to finer versions of the same images. Sound simple enough, right? This simple idea makes all the difference - here, you can see the input texture, and here is the output. As you can see, it has different patterns but has very similar properties to the input, and if we zoom into both of these images, we see that this one is able to create beautiful, high-frequency details as well. Wow, this is some really, really crisp output. Now, it has to be emphasized that this means that the statistical properties of the original image are being mimiced really well. What it doesn't mean is that it takes into consideration the meaning of these images. Just have a look at the synthesized bubbles or the flowers here. The statistical properties of the synthesized textures may be correct, but the semantic meaning of the input is not captured well. In a future work, it would be super useful to extend this algorithm to have a greater understanding of the structure and the symmetries of the input images into consideration. The source code is available under the permissive MIT license, so don't hold back those crazy experiments. If you have enjoyed this episode and you think the series provides you value or entertainment, please consider supporting us on Patreon. One-time payments and cryptocurrencies like Bitcoin or Ethereum are also supported, and have been massively successful, I am really out of words, thank you so much. The details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Uo6hFVRsjpA",
        "paper_link": "https://wxs.ca/research/multiscale-neural-synthesis/",
        "paper_title": "High-Resolution Multi-Scale Neural Texture Synthesis"
    },
    {
        "video_id": "MCHw6fUyLMY",
        "video_title": "Efficient Viscoelastic Fluid Simulations | Two Minute Papers #220",
        "position_in_playlist": 53,
        "description": "The paper \"Conformation Constraints for Efficient Viscoelastic Fluid Simulation\" is available here:\nhttp://www.gmrv.es/Publications/2017/BGAO17/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1958464/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. It has been a while now since we've talked about fluid simulations, and now, it is time for us to have a look at an amazing technique that creates simulations with viscoelastic fluids, plus rigid and deformable bodies. This is a possibility with previous techniques but takes forever to compute and typically involves computational errors that add up over time and lead to a perceptible loss of viscoelasticity. I'll try to explain these terms and what's going on in a moment. There will be lots of eye candy to feast your eyes on throughout the video, but I can assure you that this scene takes the cake. The simulation phase here took place at one frame per second, which is still not yet fast enough for the much coveted real-time applications, however, is super competitive compared previous off-line techniques that could do this. So what does viscosity mean? Viscosity is the resistance of a fluid against deformation. Water has a low viscosity and rapidly takes the form of the cup we pour it into, where honey, ketchup and peanut butter have a higher viscosity and are much more resistant to these external forces. Elasticity is a little less elusive concept that describes to what degree a piece of fluid behaves like elastic solids. However, viscous and elastic are not binary yes or no concepts, there is a continuum between the two, and if we have the proper machinery, we can create viscoelastic fluid simulations. The tau parameter that you see here controls how viscous or elastic the fluid should be, and is referred to in the paper as relaxation time. As tau is increased towards infinity, the friction dominates the internal elastic forces and the polymer won't be able to recover its structure well. The opposite case is where we reduce the tau parameter to zero, then, the internal elastic forces will dominate and our polymer will be more rigid. The alpha parameter stands for compliance, which describes the fluidity of the model. Using a lower alpha leads to more solid behavior, and higher alphas lead to more fluid behavior. The cool thing is that as a combination of these two parameters, we can produce a lot of really cool materials ranging from viscous to elastoplastic to inviscid fluid simulations. Have a look at this honey pouring scene. Mmmm! This simulation uses more than a 100 thousand particles and 6 of these frames can be simulated in just 1 second. Wow. If we reduce the number of particles to a few tens of thousands, real-time human interaction with these simulations also becomes a possibility. A limitation of this technique is that most of our decisions involving the physical properties of the fluid are collapsed into the tau and alpha parameters, if we are looking for more esoteric fluid models, we should look elsewhere. I am hoping that since part of the algorithm runs on the graphics card, the speed of this technique can be further improved in the near future. That would be awesome. Admittedly, we've only been scratching the surface, so make sure to have a look at the paper for more details. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MCHw6fUyLMY",
        "paper_link": "http://www.gmrv.es/Publications/2017/BGAO17/",
        "paper_title": "Conformation Constraints for Efficient Viscoelastic Fluid Simulation"
    },
    {
        "video_id": "_BPJFFkxSbw",
        "video_title": "Deep Image Prior | Two Minute Papers #219",
        "position_in_playlist": 54,
        "description": "The paper \"Deep Image Prior\" and its source code is available here:\nhttps://dmitryulyanov.github.io/deep_image_prior\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/cfB62s\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about performing useful image restoration tasks with a convolutional neural network with an additional twist. Its main use cases are as follows: One, in the case of JPEG artifact removal, the input is this image with many blocky artifacts that materialized during compression, and the output is a restored version of this image. Two, image inpainting where some regions of the input image are missing and are to be filled with useful and hopefully plausible information. Three, super resolution where the input image is intact, but is very coarse and has low resolution, and the output should be a more detailed, higher resolution version of the same image. This is the classic enhance scenario from these CSI TV series. It is typically hard to do because there is a stupendously large number of possible high resolution image solutions that we could come up with as an output. Four, image denoising is also a possibility. The standard way of doing these is that we train such a network on a large database of images so that they can learn the concept of many object classes, such as humans, animals and more, and also the typical features and motifs that are used to construct such images. These networks have some sort of understanding of these images and hence, can perform these operations better than most handcrafted algorithms. So let's have a look at some comparisons. Do you see these bold lettered labels that classify these algorithms as trained or untrained? The bicubic interpolation is a classical untrained algorithm that almost naively tries to guess the pixel colors by averaging its neighbors. This is clearly untrained because it does not take a database of images to learn on. Understandably, the fact that the results are lackluster is to show that non-learning-based algorithms are not great at this. The SRResNet is a state of the art learning-based technique for super resolution that was trained on a large database of input images. It is clearly doing way better than bicubic interpolation. And look, here we have this Deep Prior algorithm that performs comparably well, but is labeled to be untrained. So what is going on here? And here comes the twist - this Convolutional Neural Network is actually untrained. This means that the neuron weights are randomly initialized, which generally leads to completely useless results on most problems. So no aspect of this network works through the data it has learned on, all the required information is contained within the structure of the network itself. We all know that the structure of these neural networks matter a great deal, but in this case, it is shown that it is at least as important as the training data itself. A very interesting and esoteric idea indeed, please make sure to have a look at the paper for details as there are many details to be understood to get a more complete view of this conclusion. In the comparisons, beyond the images, researchers often publish this PSNR number you see for each image. This is the Peak Signal To Noise ratio, which means how close the output image is to the ground truth, and this number is, of course, always subject to maximization. Remarkably, this untrained network performs well both on images with natural patterns and man-made objects. Reconstruction from a pair of flash and no-flash photography images is also a possibility and the algorithm does not contain the light leaks produced by a highly competitive handcrafted algorithm, a joint bilateral filter. Quite remarkable indeed. The supplementary materials and the project website contain a ton of comparisons against competing techniques, so make sure to have a look at that if you would like to know more. The source code of this project is available under the permissive Apache 2.0 license. If you have enjoyed this episode and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. One dollar is almost nothing, but it keeps the papers coming. Recently, we have also added the possibility of one-time payments through Paypal and cryptocurrencies. I was stunned to see how generous our crypto-loving Fellow Scholars are. Since most of these crypto donations are anonymous and it is not possible to say thank you to everyone individually, I would like to say a huge thanks to everyone who supports the series. And this applies to everyone regardless of contribution - just watching the series and spreading the word is already a great deal of help for us. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_BPJFFkxSbw",
        "paper_link": "https://dmitryulyanov.github.io/deep_image_prior",
        "paper_title": "Deep Image Prior"
    },
    {
        "video_id": "zjaz2mC1KhM",
        "video_title": "Distilling Neural Networks | Two Minute Papers #218",
        "position_in_playlist": 55,
        "description": "The paper \"Distilling a Neural Network Into a Soft Decision Tree\" is available here:\nhttps://arxiv.org/pdf/1711.09784.pdf\n\nDecision Trees and Boosting, XGBoost:\nhttps://www.youtube.com/watch?v=0Xc9LIb_HTw\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payment links are available below. Thank you very much for your generous support!\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nDecision tree image sources:\n1. https://github.com/SilverDecisions/SilverDecisions/wiki/Gallery\n2. https://commons.wikimedia.org/wiki/File:Decision_tree_model.png\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/gjWHVF\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#distillation",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Since the latest explosion in AI research, virtually no field of science remains untouched by neural networks. These are amazing tools that help us solve problems where the solutions are easy to identify, but difficult to explain. For instance, we all know a backflip when we see one, but mathematically defining all the required forces, rotations and torque is much more challenging. Neural networks excel at these kinds of tasks, provided that we can supply them a large number of training samples. If we peek inside these neural networks, we'll see more and more layers and more and more neurons within these layers as years go by. The final decision depends on what neurons are being activated by our inputs. They are highly efficient, however, trying to understand how a decision is being made by these networks is going to be a fruitless endeavor. This is especially troublesome when the network gives us a wrong answer that we, without having access to any sort of explanation, may erroneously accept without proper consideration. This piece of work is about distillation, which means that we take a neural network and try to express its inner workings in the form of a decision tree. Decision trees take into consideration a series of variables and provide a clear roadmap towards a decision based on them. For instance, they are useful in using the age and amount of free time of people to try to guess whether they are likely to play video games, or deciding who should get a loan from the bank based on their age, occupation and income. Yeah! This sounds great! However, the main issue is that decision trees are not good substitutes for neural networks. The theory says that we have a generalization versus interpretability tradeoff situation, which means that trees that provide us good decisions overfit the training data and generalize poorly, and the ones that are easy to interpret are inaccurate. So in order to break out of this tradeoff situation, a key idea of this piece of work is to ask the neural network to build a decision tree by taking an input dataset for training, trying to generate more training data that follows the same properties, and feed all this to the decision tree. Here are some results for the classical problem of identifying digits in the MNIST dataset. As each decision is meant to cut the number of output options in half, it shows really well that we can very effectively perform the classification in only 4 decisions from a given input. And not only that, but it also shows what it is looking for. For instance, here, we can see that the final decision before we conclude whether the input number is a 3 or 8, it looks for the presence of a tiny area that joins the ends of the 3 to make an 8. A different visualization of the Connect4 game dataset reveals that the neural network quickly tries to distinguish two types of strategies: one where the players start playing on the inner, and one with the outer region of the board. It is shown that these trees perform better than traditional decision trees. What's more, they are only slightly worse than the corresponding neural networks, but can explain their decisions much more clearly and are also faster. In summary, the rate of progress in machine learning research is truly insane these days, and I and am all for papers that try to provide us a greater understanding of what is happening under the hood. I am loving this idea in particular. We had an earlier episode on how to supercharge these decision trees via tree boosting. If you are interested in learning more about it, the link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=zjaz2mC1KhM",
        "paper_link": "https://arxiv.org/pdf/1711.09784.pdf",
        "paper_title": "Distilling a Neural Network Into a Soft Decision Tree"
    },
    {
        "video_id": "XhH2Cc4thJw",
        "video_title": "AI Learns Semantic Image Manipulation | Two Minute Papers #217",
        "position_in_playlist": 56,
        "description": "The paper \"High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs\" and its source code is available here:\nhttps://tcwang0509.github.io/pix2pixHD/\n\nOpenings at our Institute. Make sure to mention to the contact person that you found this through Two Minute Papers!\nhttps://www.cg.tuwien.ac.at/jobs/3dspatialization/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1721451/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This technique is about creating high resolution images from semantic maps. A semantic map is a colorful image where each of the colors denote an object class, such as pedestrians, cars, traffic signs and lights, buildings, and so on. Normally, we use light simulation programs or rasterization to render such an image, but AI researchers asked the question: why do we even need a renderer if we can code up a learning algorithm that synthesizes the images by itself? Whoa. This generative adversarial network takes this input semantic map, and synthesizes a high-resolution photorealistic image from it. Previous techniques were mostly capable of creating coarser, lower resolution images, and also they were rarely photorealistic. And get this, this one produces 2k by 1k pixel outputs, which is close to full HD in terms of pixel count. If we wish to change something in a photorealistic image, we'll likely need a graphic designer and lots of expertise in photoshop and similar tools. In the end, even simpler edits are very laborious to make because the human eye is very difficult to fool. An advantage of working with these semantic maps is that they are super easy to edit without any expertise. For instance, we can exert control on the outputs by choosing from a number of different possible options to fill the labels. These are often not just reskinned versions of the same car or road but can represent a vastly different solution, like changing the material of the road from concrete to dirt. Or, it is super easy to replace trees with buildings, all we have to do is rename the labels in the input image. These results are not restricted to outdoors traffic images. Individual parts of human faces are also editable. For instance, adding a moustache has never been easier. The results are compared to a previous technique by the name pix2pix and against cascaded refinement networks. You can see that the quality of the outputs vastly outperforms both of them, and the images are also of visibly higher resolution. It is quite interesting to say that these are \"previous work\", because both of these papers came out this year, for instance, our episode on pix2pix came 9 months ago and it has already been improved by a significant margin. The joys of machine learning research. Part of the trick is that the semantic map is not only used by itself, but a boundary map is also created to encourage the algorithm to create outputs with better segmentation. This boundary information turned out to be just as useful as the labels themselves. Another trick is to create multiple discriminator networks and run them on a variety of coarse to fine scale images. There is much, much more in the paper, make sure to have a look for more details. Since it is difficult to mathematically evaluate the quality of these images, a user study was carried out in the paper. In the end, if we take a practical mindset, these tools are to be used by artists and it is reasonable to say that whichever one is favored by humans should be accepted as a superior method for now. This tool is going to be a complete powerhouse for artists in the industry. And by this, I mean right now because the source code of this project is available to everyone, free of charge. Yipee! In the meantime, we have an opening at our Institute at the Vienna University of Technology for one PhD student and one PostDoc. The link is available in the video description, read it carefully to make sure you qualify, and if you apply through the e-mail address of Professor Michael Wimmer, make sure to mention Two Minute Papers in your message. This is an excellent opportunity to turn your life around, live in an amazing city, learn a lot and write amazing papers. It doesn't get any better than that. Deadline is end of January. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XhH2Cc4thJw",
        "paper_link": "https://tcwang0509.github.io/pix2pixHD/",
        "paper_title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"
    },
    {
        "video_id": "2ciR6rA85tg",
        "video_title": "AlphaZero: DeepMind's New Chess AI | Two Minute Papers #216",
        "position_in_playlist": 57,
        "description": "The paper \"Mastering Chess and Shogi by Self-Play with a\nGeneral Reinforcement Learning Algorithm\" is available here:\nhttps://arxiv.org/pdf/1712.01815.pdf\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nRecommendations:\nhttps://www.youtube.com/watch?v=akgalUq5vew\nhttps://www.youtube.com/watch?v=0g9SlVdv1PY\nhttps://www.youtube.com/watch?v=Ud8F-cNsa-k\nhttps://www.chess.com/news/view/google-s-alphazero-destroys-stockfish-in-100-game-match\nhttp://forum.computerschach.de/cgi-bin/mwf/topic_show.pl?tid=9653\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Evan Breznyik, Frank Goertzen, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nCredits:\nElo ratings: https://ratings.fide.com/top.phtml?list=men\nMagnus image source: https://www.youtube.com/watch?v=eLaOeXCAPbU\n400 point difference rule: https://www.fide.com/fide/handbook.html?id=172&view=article ctrl+f 400\nOne chess match source: https://chess24.com/en/watch/live-tournaments/alphazero-vs-stockfish/1/1/1\nStockfish: https://stockfishchess.org/\nThumbnail background image credit: https://pixabay.com/photo-1483735/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. After defeating pretty much every highly ranked professional player in the game of Go, Google DeepMind now ventured into the realm of Chess. They recently challenged not the best humans, no-no-no, that was long ago. They challenged Stockfish, the best computer chess engine in existence in quite possibly the most exciting chess-related event since Kasparov's matches against Deep Blue. I will note that I was told by DeepMind that this is the preliminary version of the paper, so now we shall have an initial look, and perhaps make a part 2 video with the newer results when the final paper drops. AlphaZero is based on a neural network and reinforcement learning and is trained entirely through self-play after being given the rules of the game. It is not to be confused with AlphaGo Zero that played Go. It is also noted that this is not simply AlphaGo Zero applied to chess. This is a new variant of the algorithm. The differences include: - one, the rules of chess are asymmetric, for instance pawns only move forward, castling is different on kingside and queenside, and this means that neural network-based techniques are less effective at it. - two, the algorithm not only has to predict a binary win or loss probability when given a move, but draws are also a possibility and that is to be taken into consideration. Sometimes a draw is the best we can do, actually. There are many more changes to the previous incarnation of the algorithm, please make sure to have a look at the paper for details. Before we start with the results and more details, a word on Elo ratings for perspective. The Elo rating is a number that measures the relative skill level of a player. Currently, the human player with the highest Elo rating, Magnus Carlssen is hovering around 2800. This man played chess blindfolded against 10 opponents simultaneously in Vienna a couple years ago and won most of these games. That's how good he is. And Stockfish is one of the best current chess engines, with Elo rating over 3300. A difference of 500 Elo points means that if it were to play against Magnus Carlssen, it would be expected to win at least 95 games out of a 100. Though it is noted that there is a rule suggesting a hard cutoff at around a 400 point difference. The two algorithms then played each other. AlphaZero versus Stockfish. They were both given 60 seconds of thinking time per move, which is considered to be plenty given that both of the algorithms take around 10 seconds at most per move. And here are the results. AlphaZero was able to outperform Stockfish in about 4 hours of learning from scratch. They played a 100 games - AlphaZero won 28 times, drew 72 times and never lost to Stockfish. Holy mother of papers, do you hear that? Stockfish is already unfathomably powerful compared to even the best human prodigies, and AlphaZero basically crushed it after four hours of self-play. And, it was run with a similar hardware as AlphaGo Zero, one machine with 4 Tensor Processing Units. This is hardly commodity hardware, but given the trajectory of the improvements we've seen lately, it might very well be in a couple of years. Note that Stockfish does not use machine learning and is a handcrafted algorithm. People like to refer to computer opponents in computer games as AI, but it is not doing any sort of learning. So, you know what the best part is? AlphaZero is a much more general algorithm that can also play Shogi on an extremely high level, which is also referred to as Japanese chess. And this is one of the most interesting points - AlphaZero would be highly useful even it if were slightly weaker than Stockfish, because it is built on more general learning algorithms that can be reused for other tasks without investing significant human effort. But in fact, it is more general, and it also crushes Stockfish. With every paper from DeepMind, the algorithm becomes better AND more and more general. I can tell you, this is very, very rarely the case. Total insanity. Two more interesting tidbits about the paper: one, all the domain knowledge the algorithm is given is stated precisely for clarity. two, one might think that as computers and processing power increases over time, all we have to do is add more brute force to the algorithm and just evaluate more positions. If you think this is the case, have a look at this - it is noted that AlphaZero was able to reliably defeat Stockfish WHILE evaluating ten times fewer positions per second. Maybe we could call this the AI equivalent of intuition, in other words, being able to identify a small number of promising moves and focusing on them. Chills run down my spine as I read this paper. Being a researcher is the best job in the world. And we are even being paid for this. Unreal. This is a hot paper, there is lot of discussions out there on this, lots of chess experts analyze and try to make sense of the games. I had a ton of fun reading and watching through some of these, as always, Two Minute Papers encourages you to explore and read more, and the video description is ample in useful materials. You will find videos with some really cool analysis from Grandmaster Daniel King, International Chess Master Daniel Rensch, and the YouTube channel ChessNetwork. All quality materials. And, if you have enjoyed this episode and you think that 8 of these videos a month is worth a few dollars, please throw a coin our way on Patreon, or, if you favor cryptocurrencies instead, you can throw Bitcoin or Ethereum our way. You support has been amazing as always and thanks so much for keeping with us through thick and thin, even in times when weird Patreon decisions happen. Luckily, this last one has been reverted. I am honored to have supporters like you Fellow Scholars. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2ciR6rA85tg",
        "paper_link": "https://arxiv.org/pdf/1712.01815.pdf",
        "paper_title": "Mastering Chess and Shogi by Self-Play with a\nGeneral Reinforcement Learning Algorithm"
    },
    {
        "video_id": "YjjTPV2pXY0",
        "video_title": "AI Learns Noise Filtering For Photorealistic Videos | Two Minute Papers #215",
        "position_in_playlist": 58,
        "description": "The paper \"Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder\" is available here:\nhttp://research.nvidia.com/publication/interactive-reconstruction-monte-carlo-image-sequences-using-recurrent-denoising\n\nThe paper with the notoriously difficult \"Spheres\" scene:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2379965/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is another one of those amazing papers that I am really excited about. And the reason for that is that this is in the intersection of computer graphics and machine learning, which, as you know, is already enough to make me happy, but when I've first seen the quality of the results, I was delighted to see that it delivered exactly what I was hoping for. Light simulation programs are an important subfield of computer graphics where we try to create a photorealistic image of a 3D digital scene by simulating the path of millions and millions of light rays. First, we start out with a noisy image, and as we compute more paths, it slowly clears up. However, it takes a very long time to get a perfectly clear image, and depending on the scene and the algorithm, it can take from minutes to hours. In an earlier work, we had a beautiful but pathological scene that took weeks to render on several machines, if you would like to hear more about that, the link is available in the video description. So in order to alleviate this problem, many noise filtering algorithms surfaced over the years. The goal of these algorithms is that instead of computing more and more paths until the image clears up, we stop at a noisy image and try to guess what the final image would look like. This often happens in the presence of some additional depth and geometry information, additional images that that are often referred to as feature buffers or auxiliary buffers. This information helps the noise filter to get a better understanding of the scene and produce higher quality outputs. Recently, a few learning-based algorithms emerged with excellent results. Well, excellent would be an understatement since these can take an extremely noisy image that we rendered with one ray per pixel. This is as noisy as it gets I'm afraid, and it is absolutely stunning that we can still get usable images out of this. However, these algorithms are not capable of dealing with sequences of data and are condemned to deal with each of these images in isolation. They have no understanding of the fact that we are dealing with an animation. What does this mean exactly? What this means is that the network has no memory of how it dealt with the previous image, and if we combine it with the fact that a trace amount of noise still remains in the images, we get a disturbing flickering effect. This is because the remainder of the noise is different from image to image. This technique uses a Recurrent Neural Network, which is able to deal with sequences of data, for instance, in our case, video. It remembers how it dealt with the previous images a few moments ago, and, as a result, it can adjust and produce outputs that are temporally stable. Computer graphics researchers like to call this spatiotemporal filtering. You can see in this camera panning experiment how much more smoother this new technique is. Let's try the same footage slowed down and see if we get a better view of the flickering. Yup, all good! Recurrent Neural Networks are by no means easy to train and need quite a few implementation details to get it right, so make sure to have a look at the paper for details. Temporally coherent light simulation reconstruction of noisy images from one sample per pixel. And for video. This is insanity. I would go out on a limb and say that in the very near future, we'll run learning-based noise filters that take images that are so noisy, they don't even have one ray sample per pixel. Maybe one every other pixel or so. This is going to be the new milestone. If someone told me that this would be possible when I started doing light transport as an undergrad student, I wouldn't have believed a word of it. Computer games, VR and all kinds of real-time applications will be able to get photorealistic light simulation graphics in real time. And, temporally stable. I need to take some time to digest this. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=YjjTPV2pXY0",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/",
        "paper_title": "Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder"
    },
    {
        "video_id": "QmIM24JDE3A",
        "video_title": "AI Beats Radiologists at Pneumonia Detection | Two Minute Papers #214",
        "position_in_playlist": 59,
        "description": "The paper \"CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\" is available here:\nhttps://stanfordmlgroup.github.io/projects/chexnet/\n\nInteresting commentary on the article, check this one out too!\nhttps://lukeoakdenrayner.wordpress.com/2017/11/18/quick-thoughts-on-chestxray14-performance-claims-and-clinical-tasks/\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/bCaBTq\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this work, a 121-layer convolutional neural network is trained to recognize pneumonia and 13 different diseases. Pneumonia is an inflammatory lung condition that is responsible for a million hospitalizations and 50,000 deaths per year in the US alone. Such an algorithm requires a training set of formidable size to work properly. This means a bunch of input-output pairs. In this case, one training sample is an input frontal X-ray image of the chest, and the outputs are annotations by experts who mark which of the 14 different sought diseases are present in this sample. So they say like this image contains pneumonia here, and this doesn't. This is not just a binary yes or no answer, but a more detailed heatmap of possible regions that fit the diagnosis. The training set used for this algorithm contained over a 100.000 images of over 30.000 patients. This is then given to the neural network, and its task is to learn the properties of these diseases by itself. Then, after the learning process took place, previously unseen images are given to the algorithm and a set of radiologists. This is called a test set, and of course, it is crucial that both the training and the test sets are reliable. If the training and test set is created by one expert radiologist, and then we again benchmark a neural network against a different, randomly picked radiologist, that's not a very reliable process because each of the humans may be wrong in more than a few cases. Instead, the training and test annotation data is created by asking multiple radiologists and taking a majority vote on their decisions. So now that the training and test data is reliable, we can properly benchmark a human versus a neural network. And here's the result: this learning algorithm outperforms the average human radiologist. The performance was measured in a 2D space, where sensitivity and specificity were the two interesting metrics. Sensitivity means the proportion of positive samples that were classified as positive, and specificity means the portion of negative samples that were classified as negative. The crosses mean the human doctors, and as you can see, whichever radiologist we look at, even though they have different false positive and negative ratios, they are all located below the blue curve which denotes the results of the learning algorithm. This is a simple diagram, but if you think about what it actually means, this is an incredible application of machine intelligence. And now, a word on limitations. It is noted that this was an isolated test, for instance, the radiologists were only given one image, and usually, when diagnosing someone, they know more about the history of the patient that may further help their decisions. For instance, a history of a strong cough and high fever is highly useful supplementary information for humans when diagnosing someone who may have pneumonia. Beyond only the frontal view of the chest, it is also standard practice to use the lateral views as well if the results are inconclusive. These views are not available in this dataset and it is conjectured that it may sway the comparison towards humans. However, I'll note that this information may also benefit the AI just as much as the radiologists, and this seems like a suitable direction for future work. Finally, this is not the only algorithm for pneumonia detection, and it has been compared to the state of the art for all 14 diseases, and this new technique came out on top on all of them. Also, have a look at the paper for details because training a 121-layer neural network requires some clever shenanigans as this was the case here too. It is really delightful to see that these learning algorithms can help diagnosing serious illnesses, and provide higher quality healthcare to more and more people around the world, especially in places where access to expert radiologists is limited. Everyone needs to hear about this. If you wish to help us spreading the word and telling these incredible stories to even more people, please consider supporting us on Patreon. We also know that many of you are crazy for Bitcoin, so we also set up a Bitcoin address as well. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=QmIM24JDE3A",
        "paper_link": "https://stanfordmlgroup.github.io/projects/chexnet/",
        "paper_title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning"
    },
    {
        "video_id": "v1oWke0Qf1E",
        "video_title": "Universal Neural Style Transfer | Two Minute Papers #213",
        "position_in_playlist": 60,
        "description": "The paper \"Universal Style Transfer via Feature Transforms\" and its source code is available here:\nhttps://arxiv.org/abs/1705.08086 \nhttps://github.com/Yijunmaverick/UniversalStyleTransfer\n\nRecommended for you:\nhttps://www.youtube.com/watch?v=Rdpbnd0pCiI - What is an Autoencoder?\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1978682/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Let's have a look at some recent results on neural style transfer. You know the drill, we take a photo with some content, and for example, a painting with the desired style, and the output is an image where this style is applied to our content. If this is done well and with good taste, it really looks like magic. However, for pretty much all of the previous techniques, there are always some mysterious styles that result in failure cases. And the reason for this is the fact that these techniques are trained on a set of style images, and if they face a style that is wildly different from these training images, the results won't be very usable. This new algorithm is also based on neural networks, it doesn't need to be trained on these style images, but it can perform high-quality style transfer, and it works on arbitrary styles. This sounds a bit like black magic. So how does this happen exactly? First, an autoencoder is trained for image reconstruction. An autoencoder is a neural network where the input and output image is supposed to be the same thing. So far, this doesn't make any sense, because all the neural network does is copy and pasting the inputs to the outputs. Not very useful. However, if we reduce the number of neurons in one of the middle layers to very very few neurons compared to the others, we get a bottleneck. This bottleneck essentially hamstrings the neural network, and forces it to first come up with a highly compressed representation of an image, this is the encoder network, and then reconstruct the full image from this compressed representation. This is called the decoder network. So encoding is compression, decoding is decompression or more intuitively, reconstruction. This compressed representation can be thought of as the essence of the image which is a very concise representation, but carefully crafted such that a full reconstruction of the image can take place based on it. Autoencoders are previous work, and if you would like to hear more about them, check the video description as we have dedicated an earlier episode to it. And now, the value proposition of this work comes from the fact that, we don't just use the autoencoder as-is, but rip this network in half, and use the encoder part on both the input style and content images. This way, the concept of style transfer is much much simpler in this compressed representation. In the end, we're not stuck with this compressed result, because if you remember, we also have a decoder, which is the second part of the neural network that performs a reconstruction of an image from this compressed essence. As a result, we don't have to train this neural network on the style images, and it will work with any chosen style. Hell yeah! With most style transfer techniques, we are given an output image and we either take it or leave it because and we can't apply any meaningful edits to it. A cool corollary of this design decision is that we can also get closer to our artistic vision by fiddling with parameters. For instance, the scale and weight of the style transfer can be changed on the fly to our liking. As always, the new technique is compared to a bunch of other competing algorithms. Due to the general and lightweight nature of this method, it seems to perform more consistently across a set of widely varying input styles. We can also create some mattes for our target image and apply different artistic styles to different parts of it. Local parts of a style can also be transferred. Remember, the first style transfer technique was amazing, but very limited and took an hour on a state of the art graphics card in a desktop computer. This one takes less than a second and works for any style. Now, as and more new phones contain chips for performing deep learning, we can likely look forward to a totally amazing future where style transfer can be done in our pockets and in real time. What a time it is to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=v1oWke0Qf1E",
        "paper_link": "https://arxiv.org/abs/1705.08086",
        "paper_title": "Universal Style Transfer via Feature Transforms"
    },
    {
        "video_id": "6JZNEb5uDu4",
        "video_title": "This Neural Network Optimizes Itself | Two Minute Papers #212",
        "position_in_playlist": 61,
        "description": "The paper \"Hierarchical Representations for Efficient Architecture Search\" is available here:\nhttps://arxiv.org/pdf/1711.00436.pdf\n\nGenetic algorithm (+ Mona Lisa problem) implementation:\n1. https://users.cg.tuwien.ac.at/zsolnai/gfx/mona_lisa_parallel_genetic_algorithm/\n2. https://users.cg.tuwien.ac.at/zsolnai/gfx/knapsack_genetic/\n\nAndrej Karpathy's online demo:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\n\nOverfitting and Regularization For Deep Learning - https://www.youtube.com/watch?v=6aF9sJrzxaM\nTraining Deep Neural Networks With Dropout - https://www.youtube.com/watch?v=LhhEv1dMpKE\nHow Do Genetic Algorithms Work? - https://www.youtube.com/watch?v=ziMHaGQJuSI\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2692456/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As we know from the series, neural network-based techniques are extraordinarily successful in defeating problems that were considered to be absolutely impossible as little as ten years ago. When we'd like to use them for something, choosing the right kind of neural network is one part of the task, but usually the even bigger problem is choosing the right architecture. Architecture typically, at a bare minimum, means the type and number of layers in the network, and the number of neurons to be used in each layer. Bigger networks can learn solutions for more complex problems. So it seems that the answer is quite easy: just throw the biggest possible neural network we can at the problem and hope for the best. But if you think that it is that easy or trivial, you need to think again. Here's why. Bigger networks come at a cost: they take longer to train, and even worse, if we have a networks that are too big, we bump into the problem of overfitting. Overfitting is the phenomenon when a learning algorithm starts essentially memorizing the training data without actually doing the learning. As a result, its knowledge is not going to generalize for unseen data at all. Imagine a student in a school who has a tremendous aptitude in memorizing everything from the textbook. If the exam questions happen to be the same, this student will do extremely well, but in the case of even the slightest deviations, well, too bad. Even though people like to call this rote learning, there is nothing about the whole process that resembles any kind of learning at all. A smaller neural network, a less knowledgeable student, who has done their homework properly would do way, way better. So this is overfitting, the bane of so many modern learning algorithms. It can be kind of defeated by using techniques like L1 and L2 regularization or dropout, these often help, but none of them are silver bullets. If you would like to hear more about these, we've covered them in an earlier episode, actually, two episodes, as always, the links are in the video description for the more curious Fellow Scholars out there. So, the algorithm itself is learning, but for some reason, we have to design their architecture by hand. As we discussed, some architectures, like some students, of course, significantly outperform other ones and we are left to perform a lengthy trial end error to find the best ones by hand. So, speaking about learning algorithms, why don't we make them learn their own architectures? And, this new algorithm is about architecture search that does exactly that. I'll note that this is by far not the first crack at this problem, but it definitely is a remarkable improvement over the state of the art. It represents the neural network architecture as an organism and makes it evolve via genetic programming. This is just as cool as you would think it is and not half as complex as you may imagine at first - we have an earlier episode on genetic algorithms, I wrote some source code as well which is available free of charge, for everyone, make sure to have a look at the video description for more on that, you'll love it! In this chart, you can see the number of evolution steps on the horizontal x axis, and the performance of these evolved architectures over time on the vertical y axis. Finally, after taking about 1.5 days to perform these few thousand evolutionary steps, the best architectures found by this algorithm are only slightly inferior to the best existing neural networks for many classical datasets, which is bloody amazing. Please refer to the paper for details and comparisons against state of the art neural networks and other architecture search approaches, there are lots of very easily readable results reported there. Note that this is still a preliminary work and uses hundreds of graphics cards in the process. However, if you remember how it went with AlphaGo, the computational costs were cut down by a factor of ten within a little more than a year. And until that happens, we have learning algorithms that learn to optimize themselves. This sounds like science fiction. How cool is that? Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6JZNEb5uDu4",
        "paper_link": "https://arxiv.org/pdf/1711.00436.pdf",
        "paper_title": "Hierarchical Representations for Efficient Architecture Search"
    },
    {
        "video_id": "1zvohULpe_0",
        "video_title": "How Do Neural Networks See The World? Pt 2. | Two Minute Papers #211",
        "position_in_playlist": 62,
        "description": "The paper \"Feature Visualization\" is available here:\nhttps://distill.pub/2017/feature-visualization/\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nOne-time payments:\nPayPal: https://www.paypal.me/TwoMinutePapers\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\nDistill journal:\nhttps://distill.pub/\n\nRecommended for you: \nHow Do Neural Networks See The World? https://www.youtube.com/watch?v=hBobYd8nNtQ\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This one is going to be a treat. As you know all too well after watching at least a few episodes of this series, neural networks offer us amazingly powerful tools to defeat problems that we didn't stand a chance against for a long long time. We are now in the golden age of AI and no business or field of science is going to remain unaffected by this revolution. However, this approach comes with its own disadvantage compared to previous handcrafted algorithms, it is harder to know what is really happening under the hood. That's also kind of the advantage of neural networks, because they can deal with complexities that we, humans are not built to comprehend. But still, it is always nice to peek within a neural network and see if it is trying to learn the correct concepts that are relevant to our application. Maybe later we'll be able to take a look into a neural network, learn what it is trying to do, simplify it and create a more reliable, handcrafted algorithm that mimics it. What's even more, maybe they will be able to write this piece of code by themselves. So clearly, there is lots of value to be had from the visualizations, however, this topic is way more complex than one would think at first. Earlier, we talked about a technique that we called activation maximization, which was about trying to find an input that makes a given neuron as excited as possible. Here you can see what several individual neurons have learned, when I trained them to recognize wooden patterns. In this first layer, it is looking for colors, then in the second layer, some basic patterns emerge. As we look into the third layer, we see that it starts to recognize horizontal, vertical and diagonal patterns, and in the fourth and fifth layers, it uses combinations of the previously seen features, and as you can see, beautiful, somewhat symmetric figures emerge. If you would like to see more on this, I put a link to a previous episode in the video description. Then, a followup work came for multifaceted neuron visualizations, that unveiled even more beautiful and relevant visualizations, a good example was showing which neuron is responsible for recognizing groceries. A new Distill article on this topic has recently appeared by Christopher Olah and his colleagues at Google. Distill is a journal that is about publishing clear explanations to common interesting phenomena in machine learning research. All their articles so far are beyond amazing, so make sure to have a look at this new journal as a whole, as always, the link is available in the video description. They usually include some web demos that you can also play with, I'll show you one in a moment. This article gives a nice rundown of recent works in optimization-based feature visualization. The optimization part can take place in a number different ways, but it generally means that we start out with a noisy image, and look to change this image to maximize the activation of a particular neuron. This means that we slowly morph this piece of noise into an image that provides us information on what the network has learned. It is indeed a powerful way to perform visualization, often more informative than just choosing the most exciting images for a neuron from the training database. It unveils exactly the information the neuron is looking for, not something that only correlates with that information. There is more about not only visualizing the neurons in isolation, but getting a more detailed understanding of the interactions between these neurons. After all, a neural network produces an output as a combination of these neuron activations, so we might as well try to get a detailed look at how they interact. Different regularization techniques to guide the visualization process towards more informative results are also discussed. You can also play with some of these web demos, for instance, this one shows the neuron activations with respect to the learning rates. There is so much more in the article, I urge you to read the whole thing, it doesn't take that long and it is a wondrous adventure into the imagination of neural networks. How cool is that? If you have enjoyed this episode, you can pick up some really cool perks on Patreon, like early access, voting on the order of the next few episodes, or getting your name in the video description as a key contributor. This also helps us make better videos in the future and we also use part of these funds to empower research projects and conferences. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1zvohULpe_0",
        "paper_link": "https://distill.pub/2017/feature-visualization/",
        "paper_title": "Feature Visualization"
    },
    {
        "video_id": "M_eaS7X-mIw",
        "video_title": "Meta Learning Shared Hierarchies | Two Minute Papers #210",
        "position_in_playlist": 63,
        "description": "The paper \"Meta Learning Shared Hierarchies\" and its source code is available here:\nhttps://arxiv.org/abs/1710.09767\nhttps://github.com/openai/mlsh\n\nA video from Robert Miles: https://www.youtube.com/watch?v=MUVbqQ3STFA\n\nWe have been experimenting with opening a bitcoin wallet. Let us know if it's working properly and thank you very much for your support! \nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1804496/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a technique where we have a virtual creature that tries to learn an optimal set of actions to maximize a reward in a changing environment. Playing video games, helicopter control, and even optimizing light transport simulations are among the more awesome example use cases for it. But if we train a reinforcement learner from scratch, we'll see that it typically starts out with a brute force search in the space of the simplest, lowest level actions. This not only leads to crazy behavior early on, but is also highly ineffective, requires way more experience than humans do, and, the obtained knowledge cannot be reused for similar tasks. It can learn the game it was trained on, often even on a superhuman level, but if we need it to function in a new environment, all this previous knowledge has to be thrown away. And this algorithm is very much like how humans learn - it breaks down a big and complex task into sequences of smaller actions. These are called sub-policies and can be shared between tasks. Learning to walk and crawl are excellent examples of that and will likely be reused for a variety of different problems and will lead to rapid learning on new, unseen tasks, even if they differ significantly from the previously seen problems. Not only that, but the search space over sub-policies can easily be a hundred or more times smaller than the original search space of all possible actions, therefore this kind of search is way more efficient than previous techniques. Of course, creating a good selection of sub-policies is challenging, because they have to be robust enough to be helpful on many possible tasks, but not too specific to one problem, otherwise they lose their utility. A few episodes ago, we mentioned a related technique by the name Neural Task Programming, and it seems that this one is capable of generalization not only over different variations of the same task, but across different tasks as well. These ants were trained to traverse several different mazes one after another and quickly realized that the basic movement directions should be retained. Creating more general learning algorithms is one of the holy grail problems of AI research, and this one seems to be a proper, proper step towards defeating it. We're not there yet, but it's hard not to be optimistic with this incredible rate of progress each year. Really excited to see how this area improves over the next few months. The source code of this project is also available. Oh, and before we go, make sure to check out the channel of Robert Miles who makes excellent videos about AI, and I'd recommend starting with one of his videos that you are objectively guaranteed to enjoy. If you wish to find out why, you'll see the link in the video description or just click the cat picture appearing here on the screen in a moment. If you indeed enjoyed it, make sure to subscribe to his channel. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=M_eaS7X-mIw",
        "paper_link": "https://arxiv.org/abs/1710.09767",
        "paper_title": "Meta Learning Shared Hierarchies"
    },
    {
        "video_id": "6DVng5JVuhI",
        "video_title": "Image Matting With Deep Neural Networks | Two Minute Papers #209",
        "position_in_playlist": 64,
        "description": "The paper \"Deep Image Matting\" and a (seemingly) unofficial implementation by someone else is available here:\nhttps://sites.google.com/view/deepimagematting\nhttps://github.com/Joker316701882/Deep-Image-Matting\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/95VjEC\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Image matting is the process of taking an input image, and separating its foreground from the background. It is an important preliminary step for creating visual effects where we cut an actor out from green-screen footage and change the background to something else, and image matting is also an important part of these new awesome portrait mode selfies where the background looks blurry and out of focus for a neat artistic effect. To perform this properly, we need to know how to separate the foreground from the background. Matting human hair and telling accurately which hair strand is the foreground and which is the background is one of the more difficult parts of this problem. This is also the reason for many of the failure cases of the portrait mode photos made with new iPhone and Pixel cameras. The input of this problem formulation is a colored image or video, and the output is an alpha matte where white and lighter colors encode the foreground, and darker colors are assigned to the background. After this step, it is easy to separate and cut out the different layers and selectively replace some of them. Traditional techniques rely on useful heuristics, like assuming that the foreground and the background are dominated by different colors. This is useful, but of course, it's not always true, and clearly, we would get the best results if we had a human artist creating these alpha mattes. Of course, this is usually prohibitively expensive for real-world use and costs a ton of time and money. The main reason why humans are successful at this is that they have an understanding of the objects in the scene. So perhaps, we could come up with a neural network-based learning solution that could replicate this ideal case. The first part of this algorithm is a deep neural network that takes images as an input and outputs an alpha matte, which was trained on close to 50 thousand input-output pairs. So here comes the second, refinement stage where we take the output matte from the first step and use a more shallow neural network to further refine the edges and sharper details. There are a ton of comparisons in the paper, and we are going to have a look at some of them, and as you can see, it works remarkably well for difficult situations where many tiny hair strands are to be matted properly. If you look closely here, you can also see the minute differences between the results of the raw and refined steps, and it is shown that the refined version is more similar to the ground truth solution and is abbreviated with GT here. By the way, creating a dataset with tons of ground truth data is also a huge endeavor in and of itself, so thank you very much for the folks at alphamatting.com for creating this dataset, and you can see how important this kind of work is to make it easier to compare state of the art research works more easily. Adobe was a part of this research project so if everything goes well, we can soon expect such a feature to appear in their products. Also, if you're interested, we also have some nice Two Minute Papers shirts for your enjoyment. If you are located in the US, check twominutepapers.com, and for worldwide shipping, check the video description for the links. All photos of you wearing them are appreciated. Plus scholarly points if it depicts you reading a paper! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6DVng5JVuhI",
        "paper_link": "https://sites.google.com/view/deepimagematting",
        "paper_title": "Deep Image Matting"
    },
    {
        "video_id": "NEscK5RCtlo",
        "video_title": "Terrain Generation With Deep Learning | Two Minute Papers #208",
        "position_in_playlist": 65,
        "description": "The paper \"Interactive Example-Based Terrain Authoring with\nConditional Generative Adversarial Networks\" is available here:\nhttps://hal.archives-ouvertes.fr/hal-01583706/file/tog.pdf\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have recently witnessed the emergence of neural network-based techniques that are able to synthesize all sorts of images. Our previous episode was about NVIDIA's algorithm that created high resolution images of imaginary celebrities that was a really cool application of Generative Adversarial Networks. This architecture means that we have a pair of neural networks, one that learns to generate new images, and the other learns to tell a fake image from a real one. As they compete against each other, they get better and better without any human interaction. So we can clearly use them to create 2D images, but why stop there? Why not use this technique, for instance, to create assets for digital media? So instead of 2D images, let's try to adapt these networks to generate high-resolution 3D models of terrains that we can use to populate a virtual world. Both computer games and the motion picture industry could benefit greatly from such a tool. This process is typically done via procedural generation, which is basically a sort of guided random terrain generation. Here, we can have a more direct effect on the output without putting in tens of hours of work to get the job done. In the first training step, this technique learns how an image of a terrain corresponds to input drawings. Then, we will be able to sketch a draft of a landscape with rivers, ridges, valleys and the algorithm will output a high quality model of the terrain itself. During this process, we can have a look at the current output and refine our drawings in the meantime, leading to a super efficient process where we can go from a thought to a high-quality final result within a few seconds without being bogged down with the technical details. What's more, it can also not only deal with erased subregions, but it can also automatically fill them with sensible information to save time for us. What an outstanding convenience feature! And, the algorithm can also perform physical manipulations like erosion to the final results. After the training for the erosion step is done, the computational cost is practically zero, for instance, running an erosion simulator on this piece of data would take around 40 seconds, where the neural network can do it in 25 milliseconds. The full simulation would almost be a minute, where the network can mimic its results practically instantaneously. A limitation of this technique is that if the input is too sparse, unpleasant grid artifacts may appear. There are tons of more cool features in the paper, make sure to have a look, as always, it is available in the video description. This is a really well thought out and well-presented work that I expect to be a true powerhouse for terrain authoring in the future. And, in the meantime, we have reached a hundred thousand subscribers. A hundred thousand Fellow Scholars. Wow. This is absolutely amazing and honestly I never thought that this would ever happen. So, happy paperversary, thank you very much for coming along on this journey of science and I am very happy to see that the series brings joy and learning to more people than ever. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=NEscK5RCtlo",
        "paper_link": "https://hal.archives-ouvertes.fr/hal-01583706/file/tog.pdf",
        "paper_title": "Interactive Example-Based Terrain Authoring with\nConditional Generative Adversarial Networks"
    },
    {
        "video_id": "VrgYtFhVGmg",
        "video_title": "NVIDIA's AI Dreams Up Imaginary Celebrities | Two Minute Papers #207",
        "position_in_playlist": 66,
        "description": "The paper \"Progressive Growing of GANs for Improved Quality, Stability, and Variation\" and its source code is available here:\nhttp://research.nvidia.com/publication/2017-10_Progressive-Growing-of\n\nOur Patreon page with some really cool perks:\nhttps://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Raul Ara\u00fajo da Silva, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2911332/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Hold on to your papers because these results are completely out of this world, you'll soon see why. In this work, high-resolution images of imaginary celebrities are generated via a generative adversarial network. This is an architecture where two neural networks battle each other: the generator network is the artist who tries to create convincing, real-looking images and the discriminator network, the critic tries to tell a fake image from a real one. The artist learns from the feedback of the critic and will improve itself to come up with better quality images, and in the meantime, the critic also develops a sharp eye for fake images. These two adversaries push each other until they are both adept at their tasks. A classical drawback of this architecture is that it is typically extremely slow to train and these networks are often quite shallow, which means that we get low-resolution images that are devoid of sharp details. However, as you can see here, these are high resolution images with tons of details. So, how is that possible? So here comes the solution from scientists at NVIDIA. Initially, they start out with tiny, shallow neural networks for both the artist and the critic, and as time goes by, both of these neural networks are progressively grown. They get deeper and deeper over time. This way, the training process is more stable than using deeper neural networks from scratch. It not only generates pictures, but it can also compute high resolution intermediate images via latent space interpolation. It can also learn object categories from a bunch of training data and generate new samples. And, if you take a look at the roster of scientists on this project, you will see that they are computer graphics researchers who recently set foot in the world of machine learning. And man, do they know their stuff and how to present a piece of work! And now comes something, that is the absolute most important part of the evaluation that should be a must for every single paper in this area. These neural networks were trained on a bunch of images of celebrities, and are now generating new ones. However, if all we are shown is a new image, we don't know how close it is to the closest image in the training set. If the network is severely overfitting, it would essentially copy/paste samples from there. Like a student in class who hasn't learned a single thing, just memorized the textbook. Actually, what is even worse is that this would mean that the worst learning algorithm that hasn't learned anything but memorized the whole database would look the best! That's not useful knowledge. And here you see the nearest neighbors, the images that are the closest in this database to the newly synthesized images. It shows really well that the AI has learned the concept of a human face extremely well and can synthesize convincing looking new images that are not just copy-pasted from the training set. The source code, pre-trained network and one hour of imaginary celebrities are also available in the description, check them out! Premium quality service right there. And, if you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. You can also get really cool additional perks like early access, and it helps us to make better videos, grow, and tell these incredible stories to a larger audience. Details are available in the description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=VrgYtFhVGmg",
        "paper_link": "http://research.nvidia.com/publication/2017-10_Progressive-Growing-of",
        "paper_title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation"
    },
    {
        "video_id": "Lcxz6dtYjI4",
        "video_title": "Generalizing AI With Neural Task Programming | Two Minute Papers #206",
        "position_in_playlist": 67,
        "description": "The paper \"Neural Task Programming:\nLearning to Generalize Across Hierarchical Tasks\" is available here:\nhttps://stanfordvl.github.io/ntp/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Robin Graham, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2137333/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. One of the holy grail problems of machine learning research is to achieve artificial general intelligence, AGI in short. Deep Blue was able to defeat the genius Kasparov in Chess, but it was unable to tell us what the time was. Algorithms of this type we often refer to as a weak AI, or narrow AI, a technique that excels, or is maybe even on a superhuman level at a task, but has zero or no knowledge about anything else. A key to extend these algorithms would be to design them in a way that their knowledge generalizes well to other problems. This is what we call transfer learning, and this collaboration between the Stanford AI lab and Caltech goes by the name Neural Task Programming, and tries to tackle this problem. A solution to practically any problem we're trying to solve can be written as series of tasks. These are typically complex actions, like cleaning a table, or performing a backflip that are difficult to transfer to a different problem. This technique is a bit like divide and conquer type algorithms that aggressively try to decompose big, difficult tasks into smaller, more manageable pieces. The smaller and easier to understand the pieces are, the more reusable they are and the better they generalize. Let's have a look at an example. For instance, in a problem where we need to pick and place objects, this series of tasks can be decomposed into picking and placing. These can be further diced into a series of even smaller tasks, such as gripping, moving, and releasing actions. However, if the learning takes place like this, we can now specify different variations of these tasks, and the algorithm will quickly understand how to adapt the structure of these small tasks to efficiently to solve new problems. The new algorithm generalizes really well for tasks with different lengths, topologies, and changing objectives. If you take a look at the paper, you'll also find some more information on adversarial dynamics, which lists some problem variants where a really unpleasant adversary pushes things around on the table from time to time to mess with the program, and there are some results that show that the algorithm is able to recover from these failure states quite well. Really cool. Now, please don't take this as a complete solution for AGI, because it is a fantastic piece of work, but it's definitely not that. However, it may be a valuable puzzle piece to build towards the final solution. This is research. We advance one step at a time. Man, what an amazing time to be alive. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Lcxz6dtYjI4",
        "paper_link": "https://stanfordvl.github.io/ntp/",
        "paper_title": "Neural Task Programming:\nLearning to Generalize Across Hierarchical Tasks"
    },
    {
        "video_id": "p831XtyLA5M",
        "video_title": "AI Competitive Self-Play | Two Minute Papers #205",
        "position_in_playlist": 68,
        "description": "The paper \"Emergent Complexity via Multi-Agent Competition\" and its source code is available here:\nhttps://arxiv.org/abs/1710.03748\nhttps://github.com/openai/multiagent-competition\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/5BaDVq\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Earlier we had an episode about OpenAI's absolutely amazing algorithm that mastered DOTA 2, a competitive online battle arena game, and managed to beat some of the best players in the world in a relatively limited 1 versus 1 game mode. While the full 5 versus 5 version of this learning algorithm is still in the works, scientists at OpenAI experimented with some self-play in other games and have found some remarkable results. You can see here that most of these amusing experiments take place in a made up 3D game with simulated physics. For instance, performing well with these humanoid creatures means controlling 17 actuated joints properly. These agents use a reinforcement learning algorithm to maximize a reward, for instance, a sumo warrior gets a thousand points for pushing their opponent out of the ring. The first interesting thing is that a learning curriculum was used, which means that the algorithm was allowed to explore on their own by relaxing the strict scores that are given only when winning. This is combined with the fact that these agents play against themselves led to some remarkable emergent behaviors. Here you can see with the score how much of a difference this curriculum makes, and you also see that whenever a plot is symmetric that means that they are zero-sum games, so if one agent wins a given number of points, the other loses the same amount. The self-play part is also particularly interesting as many agents are being trained in parallel at the same time, and if we're talking about one versus one games, we have to create some useful logic to decide who to pair with whom. It seems that training against an older version of a previously challenged opponent was the best strategy. This makes sense because they are running a similar algorithm, and for self-play, this means that the algorithm is asked to defeat an older version of itself. If it can reliably do that, it will lead to a smooth and predictable learning process. It is kinda incredible to think about the fact that we have a virtual world with a bunch of simulated learning creatures and we are omnipotent beings trying to craft the optimal learning experience for them. The perks of being a researcher in machine learning. And we are even being paid for this! Isn't this incredible? Psst, don't tell anyone about this. There are so many interesting results here and so much to talk about. For instance, we haven't even talked about transfer learning, where these creatures learn to generalize their knowledge learned from previous tasks to tackle new challenges more efficiently. Make sure to have a look at the paper, and the source code is available for everyone free of charge. If you are one of those fellow tinkerers, you'll be more than happy to look into the video description. If you wish to hear more about transfer learning, subscribe and turn on notifications because the next episode is going to about some really cool results in this area. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=p831XtyLA5M",
        "paper_link": "https://arxiv.org/abs/1710.03748",
        "paper_title": "Emergent Complexity via Multi-Agent Competition"
    },
    {
        "video_id": "7wt-9fjPDjQ",
        "video_title": "Disney's AI Learns To Render Clouds | Two Minute Papers #204",
        "position_in_playlist": 69,
        "description": "The paper \"Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting Neural Networks\" is available here:\nhttp://drz.disneyresearch.com/~jnovak/publications/DeepScattering/\nhttp://simon-kallweit.me/deepscattering/\nhttps://tom94.net/data/publications/kallweit17deep/interactive-viewer/\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2920167/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a fully in-house Disney paper on how to teach a neural network to capture the appearance of clouds. This topic is one of my absolute favorites because it is in the intersection of the two topics I love most - computer graphics and machine learning. Hell yeah! Generally, we use light simulation programs to render these clouds, and the difficult part of this is that we have to perform something that is called volumetric path tracing. This is a technique where we have to simulate rays of light that do not necessarily bounce off of the surface of objects, but may penetrate their surfaces and undergo many scattering events. Understandably, in the case of clouds, capturing volumetric scattering properly is a key element in modeling their physical appearance. However, we have to simulate millions and millions of light paths with potentially hundreds of scattering events, which is a computationally demanding task even in the age of rapidly improving hardware. As you can see here, the more we bump up the number of possible simulated scattering events, the closer we get to reality, but the longer it takes to render an image. In the case of bright clouds here, rendering an image like this can take up to 30 hours. In this work, a nice hybrid approach is proposed where a neural network learns the concept of in-scattered radiance and predicts it rapidly so this part we don't have to compute ourselves. It is a hybrid because some parts of the renderer are still using the traditional algorithms. The dataset used for training the neural network contains 75 different clouds, some of which are procedurally generated by a computer, and some are drawn by artists to expose the learning algorithm to a large variety of cases. As a result, these images can be rendered in a matter of seconds to minutes. Normally, this would take many-many hours on a powerful computer. Here's another result with traditional path tracing. And now the same with deep scattering. Yep, that's how long it takes. The scattering parameters can also be interactively edited without us having to wait for hours to see if the new settings are better than the previous ones. Dialing in the perfect results typically takes an extremely lengthy trial and error phase which now can be done almost instantaneously. The technique also supports a variety of different scattering models. As with all results, they have to be compared to the ground truth renderings, and as you can see here, they seem mostly indistinguishable from reality. It is also temporally stable, so animation rendering can take place flicker-free as is demonstrated here in the video. I think this work is also a great testament to show how these incredible learning algorithms can accelerate progress in practically all fields of science. And given that this work was done by Disney, I am pretty sure we can expect tons of photorealistic clouds in their upcoming movies in the near future. There are tons of more details discussed in the paper, which is remarkably well produced, make sure to have a look, the link is in the video description. This is a proper, proper paper, you don't want to miss out on this one. And, if you enjoyed this episode and you feel that the series provides you value in the form of enjoyment or learning, please consider supporting us on Patreon. You can pick up cool perks there like deciding the order of the next few episodes, and you also help us make better videos in the future. Details are available in the description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7wt-9fjPDjQ",
        "paper_link": "http://drz.disneyresearch.com/~jnovak/publications/DeepScattering/",
        "paper_title": "Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting Neural Networks"
    },
    {
        "video_id": "dqxqbvyOnMY",
        "video_title": "Video Game Graphics To Reality And Back | Two Minute Papers #203",
        "position_in_playlist": 70,
        "description": "The paper \"Unsupervised Image-to-Image Translation Networks\" and its source code is available here:\nhttps://arxiv.org/pdf/1703.00848.pdf\nhttps://github.com/mingyuliutw/UNIT\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/UASi2i\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Researchers at NVIDIA have been hitting it out of the park lately, and this work is no different. This technique performs image translation, which means that the input is an image of our choice, and the output is the same image with somewhat different semantics. For instance, an image of a city can be translated to a map of this city, or a daytime photo or video can be converted to appear as if it were shot during the night. And throughout the video, you'll see so much more of these exciting applications. A typical way of accomplishing this is done by using Generative Adversarial Networks. This is a pair of neural networks where the generator network creates new synthetic images trying to fool a discriminator network which learns to tell a fake, synthesized image from a real one. These two neural networks learn together, where one tries to come up with better solutions to fool the discriminator, where the discriminator seeks to get better at telling forgeries from the real photographs. In the end, this rivalry makes both of them get better and better and the final result is an excellent technique to create convincing image translations. In this work, not two, but 6 of these networks are being used, so make sure to have a look at the paper for details. There was an earlier work that was able to perform image translation by leaning on a novel cycle consistency constraint. This means that we assume that the source image can be translated to the target image and then this target image can be translated back to look exactly like the source. This kinda means that these translations are not arbitrary and are mathematically meaningful operations. Here, the new technique builds on a novel assumption that there exists a latent space in which the input and output images can both coexist. This latent space is basically an intuitive and concise representation of some more complicated data. For instance, earlier, we experimented a bit with fonts and had seen that even though the theory of font design is not easy, we can create a 2 dimensional latent space that encodes simple properties like curvature that can describe many many fonts in an intuitive manner. Remarkably, with this new work, converting dogs and cats into different breeds is also a possibility. Interestingly, it can also perform real to synthetic image translation and vice versa. So that means that it can create video game footage from our real world videos, and even more remarkably, convert video game footage to real world video. This is insanity, one of the craziest ideas I've seen in a while. Bravo. And now, hold on to your papers because it can also perform attribute-based image translation. This means that for instance, we can grab an image of a human face and transform the model's hair to blonde, add sunglasses or smiles to it at will. A limitation of this technique is that training is still non-trivial as it still relies on generative adversarial networks, and it is not yet clear whether there is a point to which the training converges or not. The source code of this project is also available. Make sure to take a good look at the license before doing anything because it is under the creative commons non-commercial and no derivatives license. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dqxqbvyOnMY",
        "paper_link": "https://arxiv.org/pdf/1703.00848.pdf",
        "paper_title": "Unsupervised Image-to-Image Translation Networks"
    },
    {
        "video_id": "mmeoUZ_wRm4",
        "video_title": "Transferring AI To The Real World (OpenAI) | Two Minute Papers #202",
        "position_in_playlist": 71,
        "description": "The paper \"Domain Randomization for Transferring Deep Neural  Networks from Simulation to the Real World\" is available here:\nhttps://arxiv.org/pdf/1703.06907.pdf\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2874016/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we talk a lot about different AI algorithms that solve a variety of super difficult tasks. These are typically tested within a software environment in the form of a simulation program. However, this often leaves the question open whether these algorithms would really work in real world environments. So what about that? This work from OpenAI goes by the name domain randomization, and is about training an AI on relatively crude computer simulations in a way that can be transferred to the real world. The problem used to demonstrate this was localizing and grasping objects. Note that this algorithm has never seen any real images and was trained using simulated data. It only played a computer game, if you will. Now, the question we immediately think about is what the term domain randomization has to do with transferring simulation knowledge into reality? The key observation is that using simulated training data is okay, but we have to make sure that the AI is exposed to a diverse enough set of circumstances to obtain knowledge that generalizes properly, hence the term domain randomization. In these experiments, the following parameters were heavily randomized: number of shapes and distractor objects on the table, positions and textures on the objects, table and the environment, number of lights, material properties, and the algorithm was even exposed to some random noise as well in the images. And it turns out that if we do this properly, leaning on the knowledge of only a few thousand images, when the algorithm is uploaded to a real robot arm, it becomes capable of grasping the correct prescribed objects. In this case, the objective was, spam detection. Very amusing. I think the very interesting part is that it is not even using photorealistic rendering and light simulations - these programs are able to create high quality images that resemble the real world around us, and it is mostly clear that those would be useful to train such an algorithm. However, this only uses extremely crude data and the knowledge of the AI still generalizes to the real world. How about that! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mmeoUZ_wRm4",
        "paper_link": "https://arxiv.org/pdf/1703.06907.pdf",
        "paper_title": "Domain Randomization for Transferring Deep Neural  Networks from Simulation to the Real World"
    },
    {
        "video_id": "9xlSy9F5WtE",
        "video_title": "New DeepMind AI Beats AlphaGo 100-0 | Two Minute Papers #201",
        "position_in_playlist": 72,
        "description": "The AlphaGo Zero paper \"Mastering the Game of Go without Human Knowledge\" is available here:\nhttps://deepmind.com/blog/alphago-zero-learning-scratch/\nhttps://deepmind.com/documents/119/agz_unformatted_nature.pdf\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nPhoto credits: \nWatson - AP Photo/Jeopardy Productions, Inc. \nFan Hui match photo - Google DeepMind - https://www.youtube.com/watch?v=SUbqy... \n\nGo board image credits (all CC BY 2.0): \nRenato Ganoza - https://flic.kr/p/7nX4kK \nJaro Larnos (changes were applied, mostly recoloring) - https://flic.kr/p/dDeQU9 \nLuis de Bethencourt - https://flic.kr/p/4c5RaR\n\nGo ratings:\nhttps://www.goratings.org/en/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/skJBM1\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Hold on to your papers, because this work on AlphaGo is absolute insanity. In the game of Go, the players put stones on a table where the objective is to surround more territory than the opponent. This is a beautiful game that is particularly interesting for AI research, because the space of possible moves is vastly larger than in chess, which means that using any sort of exhaustive search is out of question and we have to resort to smart algorithms that are able to identify a small number of strong moves within this stupendously large search space. The first incarnation of DeepMind's Go AI, AlphaGo uses a combination of a policy network that is responsible for predicting the moves, and a value network that predicts the winner of the game after it plays it to the end against itself. These are both deep neural networks and they are then combined with a technique called Monte Carlo Tree Search to be able to narrow down the search in this this large search space. This algorithm started out with a bootstrapping process where it was shown thousands of games that were used to learn the basics of Go. Based on this, it is clear that such an algorithm can learn to be as good as formidable human players. But the big question was, how could it possibly become even better than the professionals that it has observed? How could the disciple become better than its master? The solution is that after it has learned what it can from these games, it plays against itself many-many times to improve its skills. This second phase is the main part of the training that takes the most time. Let's call this base algorithm AlphaGo Fan, which was used to play against Fan Hui a 2-dan European Go champion, who was defeated 5 to 0. This was a historic moment and the first time an AI beat a professional Go player without a handicap. Fan Hui described his experience as playing against a very strong and stable player and he also mentioned that the algorithm felt very human-like. Some voiced their doubts within the Go community and noted that the algorithm would never be able to beat Lee Sedol, a 9-dan world champion, and winner of 18 international titles. Just to give you an intuition of the difference, based on their Elo points, Lee Sedol is expected to beat Fan Hui 97 times out of 100 games. So a few months later, DeepMind organized a huge media event where they would challenge him to play against AlphaGo. This was a slightly modified version of the base algorithm that used a deeper neural network with more layers and was trained using more resources than the previous version. There was also an algorithmic change to the policy networks, the details on this are available in the paper in the description, it is a great read, make sure to have a look. Let's call this algorithm AlphaGo Lee. This event was watched all around the world and can perhaps be compared to Kasparov's public chess games against Deep Blue. I have the fondest memories of waking up super early in the morning, jumping out of the bed in excitement to watch all these Go matches. And in a long and nailbiting series, Lee Sedol was defeated 4 to 1 by the AI. With significantly less media attention, the next phase came bearing the name AlphaGo Master, which used around ten times less tensor processing units than the AlphaGo Lee and became an even stronger player. This algorithm played against human professionals online in January 2017 and won all 60 matches it had played. This is insanity, but if you think that's it, well, hold on to your papers now. In this newest work, AlphaGo has reached its next form, AlphaGo Zero. This variant does not have access to any human played games in the first phase and learns completely through self-play. It starts out from absolutely nothing, with just the knowledge of the rules of the game. It was trained for 40 days, and by day 3, it reached the level of AlphaGo Lee, this is above World champion level. Around day 21, it hits the level of AlphaGo Master, which is practically unbeatable to all human beings. And get this, at 40 days, this version surpasses all previous AlphaGo versions and defeats the previously published worldbeater version 100-0. This has kept me up for several nights now and I am completely out of words. In this version, the two neural networks are fused into one, which can be trained more efficiently. It is beautiful to see these curves as they show this neural network starting from a random initialization. It knows the rules, but beyond that, it is completely clueless about the game itself, and it rapidly becomes practically unbeatable. And I left the best part for last - it uses only one single machine. I think it is fair to say that is history unfolding before our eyes. What a time to be alive! Congratulations to the DeepMind team for this remarkable achievement. And, for me, I love talking about research to a wider audience and it is a true privilege to be able to tell these stories to you. Thank you very much for your generous support on Patreon and making me able to spend more and more time with what I love most. Absolutely amazing. And now, I know it's a bit redundant, but from muscle memory, I'll sign out the usual way. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9xlSy9F5WtE"
    },
    {
        "video_id": "mECv52eSjBo",
        "video_title": "Real-Time Global Illumination With Radiance Probes | Two Minute Papers #200",
        "position_in_playlist": 73,
        "description": "The paper \"Real-time Global Illumination by Precomputed Local Reconstruction\nfrom Sparse Radiance Probes\" is available here:\nhttps://arisilvennoinen.github.io/Publications/Real-time_Global_Illumination_by_Precomputed_Local_Reconstruction_from_Sparse_Radiance_Probes.pdf\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Christoph Jadanowski, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Steve Messina, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is our 200th episode, so I know you're expecting something great. See how you like this one! One of the most sought after effect in light transport simulations is capturing indirect illumination. This is a beautiful effect where the color of multiple diffuse, matte surfaces bleed onto each other. And of course, computing such an effect is as costly as it is beautiful because it requires following the path of millions and millions of light rays. This usually means several hours of waiting time. There have been countless research papers written on how to do this in real time, but the limitations were often much too crippling for practical use. But this time around, you will see soon that these results are just outstanding, and we will have a word on limitations at the end of this video. The key contribution of this work is that instead of computing the light transport between all possible point pairs in the scene, it uses radiance probes that measure the nearby illumination, and tries to reconstruct the missing information from this sparse set of radiance probes. After that, we place a bunch of receiver points around the scene to places where we would like to know how the indirect illumination looks. There are several things to be taken care of in the implementation of this idea. For instance, in previous works, the hierarchy of these sender and receiver points was typically fixed. In this new work, it is shown that a much sparser set of carefully placed radiance probes is sufficient to create high-quality reconstructions. This seemingly small difference also gives rise to a lot of ambiguous cases that the researchers needed to work out how to deal with. For instance, possible occlusions between the probes and receiver points need special care. The entire algorithm is explained in a remarkably intuitive way in the paper, make sure to have a look at that. And, given that we can create images by performing much less computation, with this technique, we can perform real-time light simulations. As you can see, 3.9 milliseconds is a typical value for computing an entire image, which means that this can be done with over 250 frames per second. That's not only real time, that's several times real time, if you will. Outstanding! And of course, now that we know that this technique is fast, the next question is: how accurate is it? As expected, the outputs are always compared to the reference footage so we can see how accurate the proposed technique is. Clearly, there are differences, however, probably many of us would fail to notice that we're not looking at the reference footage, especially if we don't have access to it, which is the case in most applications. And note that normally, we would have to wait for hours for results like this. Isn't this incredible? There are also tons of more comparisons in the paper, for instance, it is also shown how the density of radiance probes relates to the output quality and where the possible sweet spots are for industry practitioners. It is also tested against many competing solutions. Not only the results, but the number and quality of comparisons is also top tier in this paper. However, like with all research works, no new idea comes without limitations. This method works extremely well for static scenes where not a lot of objects move around. Some movement is still fine as it is shown in the video here, but drastic changes to the structure of the scene, like a large opening door that remains unaccounted for by the probes will lead to dips in the quality of the reconstruction. I think this is an excellent direction for future research works. If you enjoyed this episode, make sure to subscribe and click the bell icon, we have some more amazing papers coming up, you don't want to miss that. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mECv52eSjBo",
        "paper_link": "https://arisilvennoinen.github.io/Publications/Real-time_Global_Illumination_by_Precomputed_Local_Reconstruction_from_Sparse_Radiance_Probes.pdf",
        "paper_title": "Real-time Global Illumination by Precomputed Local Reconstruction\nfrom Sparse Radiance Probes"
    },
    {
        "video_id": "kfJMUeQO0S0",
        "video_title": "Learning to Model Other Minds (OpenAI) | Two Minute Papers #199",
        "position_in_playlist": 74,
        "description": "The paper \"Learning with Opponent-Learning Awareness\" is available here:\nhttps://arxiv.org/abs/1709.04326\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nShowcased episodes:\nReal-Time Oil Painting on Mobile - https://www.youtube.com/watch?v=1SHW1-qKKpY\nReal-Time Modeling and Animation of Climbing Plants - https://www.youtube.com/watch?v=aAsejHZC5EE\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2629752/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work doesn't have a ton of viewable footage, but I think it is an absolutely amazing piece of craftsmanship, so in the first half of this video, we'll roll some footage from earlier episodes, and in the second half, you'll see the new stuff. In this series, we often talk about reinforcement learning, which is a learning technique where an agent chooses an optimal series of actions in an environment to maximize a score. Playing computer games is a good example of a clearly defined score that is to be maximized. As long as we can say that the higher the score, the better the learning, the concept will work for helicopter control, choosing the best spot for wifi connectivity or a large variety of different tasks. However, what about environments where multiple agents or players are present? Not all games are single player focused, and not all helicopters have to fly alone. So what about that? To deal with cases like this, scientists at OpenAI and the University of Oxford came up with a work by the name \"Learning with Opponent-Learning Awareness\", LOLA in short, or lola. I have to say that the naming game at OpenAI has been quite strong lately. This is about multiplayer reinforcement learning, if you will. This new agent does not only care about maximizing its own score but also inserts a new term into the equation which is about anticipating the actions of other players in the environment. It is not only possible to do this, but they also show that it can be done in an effective way, and, the best part is that it also gives rise to classical strategies that game theory practitioners will immediately recognize. For instance, it can learn tit for tat, which is a strategy that mirrors the other player's actions. This means that if the other player is cooperative, it will remain cooperative, but if it gets screwed over, it will also try to screw others over. You'll see in a moment why this is a big deal. The prisoner's dilemma is a game where two criminals are caught and are independently interrogated, and have to choose whether they snitch on the other one or not. If any one snitches out, there will be hell to pay for the other one. If they both defect, they both serve a fair amount time in prison. The score to be minimized is therefore this time spent in prison. The optimal solution of this game is when both criminals remain silent, and this strategy is something that we call the Nash equilibrium. In other words, this is the best set of actions if we consider the options of the other actor as well and expect that they do the same for us. And now, the first cool result is that if we run the prisoner's dilemma with two of these new LOLA agents, they quickly find the Nash equilibrium. This is great. But wait, we have talked about this tit for tat thing, so what's the big deal with that? There is an iterated version of the prisoner's dilemma game, where this snitching or cooperating game is replayed many many times. It is an ideal benchmark because an advanced agent would know that we cooperated the last time, so it is likely that we can partner up this time around too! And now comes the even cooler thing! This is where the tit for tat strategy emerges - these LOLA agents know that if the previous time, they cooperated, they will immediately give each other another chance, and again, get away with the least amount of prison time. As you can see here, the results vastly outperform other naive agents, and from the scores it seems that previous techniques enter a snitching revenge war against each other and both will serve plenty of time in prison. Other games are also benchmarked against naive, uncooperative agents, vastly outperforming them. This is a fantastic paper, make sure to check it out in the video description for more details. I found it to be very readable, so do not despair if your math kung fu is not that strong. Just dive into it! Videos like this tend to get less views because they have less visual fireworks than most other works we're discussing in the series. Fortunately, we are super lucky because we have your support on Patreon and can tell these important stories without worrying about going viral. And, if you have enjoyed this episode and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. One buck is almost nothing, but it keeps the papers coming. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kfJMUeQO0S0",
        "paper_link": "https://arxiv.org/abs/1709.04326",
        "paper_title": "Learning with Opponent-Learning Awareness"
    },
    {
        "video_id": "9BOdng9MpzU",
        "video_title": "AI Learns 3D Face Reconstruction | Two Minute Papers #198",
        "position_in_playlist": 75,
        "description": "The paper \"Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression\" is available here:\nhttp://aaronsplace.co.uk/papers/jackson2017recon/\n\nOnline demo:\nhttp://cvl-demos.cs.nott.ac.uk/vrn/\n\nSource code:\nhttps://github.com/AaronJackson/vrn\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1836445/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Now that facial recognition is becoming more and more of a hot topic, let's talk a bit about 3D face reconstruction! This is a problem where we have a 2D input photograph, or a video of a person, and the goal is to create a piece of 3D geometry from it. To accomplish this, previous works often required a combination of proper alignment of the face, multiple photographs and dense correspondences, which is a fancy name for additional data that identifies the same regions across these photographs. But this new formulation is the holy grail of all possible versions of this problem, because it requires nothing else but one 2D photograph. The weapon of choice for this work was a Convolutional Neural Network, and the dataset the algorithm was trained on couldn't be simpler: it was given a large database of 2D input image and 3D output geometry pairs. This means that the neural network can look at a lot of these pairs and learn how these input photographs are mapped to 3D geometry. And as you can see, the results are absolutely insane, especially given the fact that it works for arbitrary face positions and many different expressions, and even with occlusions. However, this is not your classical Convolutional Neural Network, because as we mentioned, the input is 2D and the output is 3D. So the question immediately arises: what kind of data structure should be used for the output? The authors went for a 3D voxel array, which is essentially a cube in which we build up the face from small, identical Lego pieces. This representation is similar to the terrain in the game Minecraft, only the resolution of these blocks is finer. The process of guessing how these voxel arrays should look based the input photograph is referred to in the research community as volumetric regression. This is what this work is about. And now comes the best part! An online demo is also available where we can either try some prepared images, or, we can also upload our own. So while I run my own experiments, don't leave me out of the good stuff and make sure you post your results in the comments section! The source code is also available for you fellow tinkerers out there. The limitations of this technique includes the inability of detecting expressions that are very far away from the ones seen in the training set, and as you can see in the videos, temporal coherence could also use some help. This means that if we have video input, the reconstruction has some tiny differences in each frame. Maybe a Recurrent Neural Network, like some variant of Long Short Term Memory could address this in the near future. However, those are trickier and more resource-intensive to train properly. Very excited to see how these solutions evolve, and of course, Two Minute Papers is going to be here for you to talk about some amazing upcoming works. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9BOdng9MpzU",
        "paper_link": "http://aaronsplace.co.uk/papers/jackson2017recon/",
        "paper_title": "Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression"
    },
    {
        "video_id": "T_g6S3f0Z5I",
        "video_title": "AI Learns Video Frame Interpolation | Two Minute Papers #197",
        "position_in_playlist": 76,
        "description": "The paper \"Video Frame Interpolation via Adaptive Separable Convolution\" and its source code is available here:\nhttps://arxiv.org/abs/1708.01692\nhttps://github.com/sniklaus/pytorch-sepconv\n\nTwo Minute Papers subreddit:\nhttps://www.reddit.com/r/twominutepapers/comments/76j145/ai_learns_video_frame_interpolation_two_minute/\n\nRecommended for you:\n1. Separable Subsurface Scattering (with convolutions) - https://www.youtube.com/watch?v=72_iAlYwl0c\n2. https://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n3. Rocking Out With Convolutions - https://www.youtube.com/watch?v=JKYQOAZRZu4\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2842576/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. With today's graphics technology, we can enjoy many really smooth videos that were created using 60 frames per second. We love it too, and we hope that you noticed that our last hundred or maybe even more episodes have been available in 60hz. However, it oftentimes happens that we're given videos that have anything from 20 to 30 frames per second. This means that if we play them on a 60 fps timeline, half or even more of these frames will not provide any new information. As we try to slow down the videos for some nice slow-motion action, this ratio is even worse, creating an extremely choppy output video. Fortunately, there are techniques that are able to guess what happens in these intermediate frames and give them to us. This is what we call frame interpolation. We have had some previous experiments in this area where we tried to create an amazing slow motion version of a video with some bubbles merging. A simple and standard way of doing frame interpolation is called frame blending, which is a simple averaging of the closest two known frames. The more advanced techniques are optical flow-based, which is a method to determine what motions happened between the two frames, and create new images based on that knowledge, leading to higher quality results in most cases. This technique uses a convolutional neural network to accomplish something similar, but in the end, it doesn't give us an image, but a set of convolution kernels. This is a transformation that is applied to the previous and the next frame to produce the intermediate image. It is not the image itself, but a recipe of how to produce it, if you will. We've had a ton of fun with convolutions earlier, where we used them to create beautiful subsurface scattering effects for translucent materials in real time, and our more loyal Fellow Scholars remember that at some point, I also pulled out my guitar and showed what it would sound like inside a church using a convolution-based reverberation technique. The links are available in the video description, make sure to check them out! Since we have a neural network over here, it goes without saying that the training takes place on a large number of before-after image pairs, so that the network is able to produce these convolution kernels. Of course, to validate this algorithm, we also need to have access to a ground truth reference to compare against - we can accomplish this by withholding some information about a few intermediate frames so we have the true images which the algorithm would have to reproduce without seeing it. Kinda like giving a test to a student when we already know the answers. You can see such a comparison here. And now, let's have a look at these results! As you can see, they are extremely smooth, and the technique retains a lot of high-frequency details in these images. The videos also seem temporally coherent, which means that it's devoid of the annoying flickering effect where the reconstruction takes place in a way that's a bit different in each subsequent frame. None of that happens here, which is an excellent property of this technique. The python source code for this technique is available and is free for non-commercial uses. I've put a link in the description, if you have given it a try and have some results of your own, make sure to post them in the comments section or our subreddit discussion. The link is available in the description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=T_g6S3f0Z5I",
        "paper_link": "https://arxiv.org/abs/1708.01692",
        "paper_title": "Video Frame Interpolation via Adaptive Separable Convolution"
    },
    {
        "video_id": "WT0WtoYz2jE",
        "video_title": "Deep Learning From Human Preferences | Two Minute Papers #196",
        "position_in_playlist": 77,
        "description": "The paper \"Deep Reinforcement Learning from Human Preferences\" is available here:\nhttps://arxiv.org/pdf/1706.03741.pdf\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Malek Cellier, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2386034/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this new age of AI, there is no shortage of articles and discussion about AI safety, and of course, rightfully so: these new learning algorithms started solving problems that were previously thought to be impossible in quick succession. Only ten years ago, if we told someone about half of the things that have been covered in the last few Two Minute Papers episodes, we'd have been declared insane. And of course, having such powerful algorithms, we have to make sure that they are used for good. This work is a collaboration between OpenAI and DeepMind's security team and is about introducing more human control in reinforcement learning problems. The goal was to learn to perform a backflip through reinforcement learning. This is an algorithm that tries to perform a series of actions to maximize a score. Kind of like playing computer games. For instance, in Atari Breakout - if we break a lot of bricks, we get a high score so we know we did something well. If we see that happening, we keep doing what led to this result, if not, we go back to the drawing board and try something new. But this work is about no ordinary reinforcement learning algorithm, because the score to be maximized comes from a human supervisor and we're trying to teach a digital creature to perform a backflip. I particularly like the choice of the backflip here because we can tell when we see one, but a mathematical specification of this in terms of movement actions is rather challenging. This is a problem formulation in which humans can overlook and control the learning process, which is going to be an increasingly important aspect of learning algorithms in the future. The feedback option is very simple: we just specify whether this sequence of motions achieved our prescribed goal or not. Did it fall or did it perform the backflip successfully. After around 700 human feedbacks, the algorithm was able to learn the concept of a backflip, which is quite remarkable given that these binary yes/no scores are extremely difficult to use for any sort of learning. In an earlier episode, we illustrated a similar case with a careless teacher who refuses to give out points for each problem on a written exam and only announces whether we have failed or passed. This clearly makes a dreadful learning experience, and it is incredible that the algorithm is still able to learn using these. We provide feedback on less than 1% of the actions the algorithm makes, and it can still learn difficult concepts off of these extremely sparse and vague rewards. Low-quality teaching leads to high-quality learing. How about that!? This is significantly more complex than what other techniques were able to learn with human feedback. And, it works with other games too! A word about the collaboration itself. When a company hires a bunch of super smart scientists and a spends a ton of money on research, it is understandable that they want to get an edge through these projects, which often means keeping the results for themselves. This leads to excessive secrecy and a lack of collaboration with other groups as everyone wants to keep their cards close to their chest. The fact that such collaborations can happen between these two AI research giants is a testament to how devoted they are to working together and sharing their findings with everyone, free of charge for the greater good. Awesome. As the media is all up in arms about the demise of the human race I feel that it is important to show the other side of the coin as well. We have top people working on AI safety right now. If you wish to help us tell these stories to more people, please consider supporting us on Patreon. Details are available in the video description, or just click the letter p that appears on the screen in a moment. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=WT0WtoYz2jE",
        "paper_link": "https://arxiv.org/pdf/1706.03741.pdf",
        "paper_title": "Deep Reinforcement Learning from Human Preferences"
    },
    {
        "video_id": "2VyhmbEjs9A",
        "video_title": "AI Learns To Recreate Computer Games | Two Minute Papers #195",
        "position_in_playlist": 78,
        "description": "The paper \"Game Engine Learning from Video\" is available here:\nhttps://www.cc.gatech.edu/~riedl/pubs/ijcai17.pdf\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nGenerative Adversarial Networks (GANs):\nhttps://www.youtube.com/watch?v=pqkpIfu36Os\n\nGenerative Latent Optimization (GLO):\nhttps://www.youtube.com/watch?v=aR6M0MQBo2w\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Eric Haddad, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1558063/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In most video games that we've seen running for at least a few moments, we learn to anticipate what is going to happen in the next second, and even more, if given the patience and skills, we could attempt to recreate parts of the game itself. And what you see here in this work is actually even better, because it requires neither the patience nor the skills to do that. So here is the million dollar idea: let's have a learning algorithm look at some video game footage, and then, ask it to recreate that so we can indulge in playing it. The concept is demonstrated on the Super Mario game and later, you will also see some results with the millenial childhood favorite, Mega Man. There are many previous works that hook into the source code of these games and try to read and predict what happens next by reading the code-level instructions. But not in this case, because this technique looks at the video output and the learning takes place on the level of pixels, therefore, no access to the inner workings of the game is necessary. The algorithm is given two things for the learning process: one, a sprite pallette that contains all the possible elements that can appear in the game, including landscape tiles, enemies, coins and so on. And two, we also provide an input video sequence with one playthrough of the game to demonstrate the mechanics and possible interactions between these game elements. A video is a series of frames, from which the technique learns how a frame can be advanced to the next one. After it has been exposed to enough training samples, it will be able to do this prediction by itself on unknown frames that it hasn't seen before. This pretty much means that we can start playing the game that it tries to mimic. And there are similarities across many games that could be exploited, endowing the learning algorithms with knowledge reused from other games, making them able to recreate even higher quality computer games, even in cases where a given scenario hasn't played out in the training footage. It used to be the privilege of computer graphics researchers to play video games during work hours, but apparently, scientists in machine learning also caught up in this regard. Way to go! A word about limitations. As the predictions are not very speedy and are based off of a set of facts learned from the video sequences, it is a question as to how well this technique would generalize to more complex 3d video games. As almost all research works, this is a stepping stone, but a very important one at that as this is a proof of concept for a really cool idea. You know the drill, a couple papers down the line and we'll see the idea significantly improved. The results are clearly not perfect, but it is a nice demonstration of a new concept, and knowing the rate of progress in machine learning research, you will very soon see some absolutely unreal results. What's even more, I expect that new levels, enemy types and mechanics will soon be synthesized to already existing games via generative adversarial networks, or generative latent optimization. If you would like to hear more about these, as always, the links are available in the video description. Also, if you enjoyed this episode, please make sure to help us tell these incredible stories to more and more people by supporting us on Patreon. Your support has always been absolutely amazing. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2VyhmbEjs9A",
        "paper_link": "https://www.cc.gatech.edu/~riedl/pubs/ijcai17.pdf",
        "paper_title": "Game Engine Learning from Video"
    },
    {
        "video_id": "nsuAQcvafCs",
        "video_title": "Audio To Obama: AI Learns Lip Sync from Audio | Two Minute Papers #194",
        "position_in_playlist": 79,
        "description": "The paper \"Synthesizing Obama: Learning Lip Sync from Audio\" is available here:\nhttps://grail.cs.washington.edu/projects/AudioToObama/\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nPatreon notes:\nhttps://www.patreon.com/TwoMinutePapers/posts?tag=what%27s%20new\n\nRecommended for you:\nWaveNet: https://www.youtube.com/watch?v=CqFIVCD1WWo\nFace2face: https://www.youtube.com/watch?v=_S1lyQbbJM4\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1174489/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Deepfake #Face2Face #Audio2Obama",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is doing something truly remarkable: if we have a piece of audio of a real person speaking, and a target video footage, it will retime and change the video so that the target person appears to be uttering these words. Whoa! This is different from what we've seen a few episodes ago, where scientists at NVIDIA worked on synthesizing lip sync geometry for digital characters solely relying on audio footage. The results were quite amazing, have  a look. This was great for animating digital characters when all we have is sound. But this time around, we're interested in reanimating the footage or real, existing people. A prerequisite to do this with a learning algorithm is to have a ton of data to train on - which we have in our possession as there are many hours of footage of the former president speaking during his weekly address. This is done using a recurrent neural network. Recurrent neural networks are learning algorithms where the inputs and outputs can be sequences of data. So here, in the first part, the input can be a piece of audio with the person saying something, and it is able to synthesize the appropriate mouth shapes and their evolution over time to match the audio. The next step is creating an actual mouth texture from this rough shape that comes from the learning algorithm, which is then used as an input to the synthesizer. Furthermore, the algorithm is also endowed with an additional pose matching module to make sure that the synthesized mouth texture aligns with the posture of the head properly. The final retiming step makes sure that the head motions follow the speech correctly. If you have any doubts whether this is required, here are some results with and without the retiming step. You can see that this indeed substantially enhances the realism of the final footage. Even better, when combined with Google DeepMind's WaveNet, given enough training data, we could skip the audio footage altogether and just write a piece of text, making Obama, or someone else say what we've written. There are also a ton of other details to be worked out, for instance, there are cases where the mouth moves before the person starts to speak, which is to be taken into consideration. The dreaded \"umm\"-s and \"ahh\"-s are classical examples of that There is also an important jaw correction step and more. This is a brilliant piece of work with many non-trivial decisions that are described in the paper - make sure to have a look at it for details, as always, there is a link to it is available the video description. The results are also compared to the Face2face paper from last year that we also covered in the series. It is absolutely insane to see this rate of progress over the lapse of only one year. If you have enjoyed this episode and you feel that eight of these videos a month is worth a dollar, please consider supporting us on Patreon. You can pick up some really cool perks there and it is also a great deal of help for us to make better videos for you in the future. Earlier I also wrote a few words about the changes we were able to make because of your amazing support. Details are available in the description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=nsuAQcvafCs",
        "paper_link": "https://grail.cs.washington.edu/projects/AudioToObama/",
        "paper_title": "Synthesizing Obama: Learning Lip Sync from Audio"
    },
    {
        "video_id": "GNx8rgNcw5c",
        "video_title": "Light Transport on Specular Microstructure | Two Minute Papers #193",
        "position_in_playlist": 80,
        "description": "The paper \"Position-Normal Distributions for Efficient Rendering of Specular Microstructure\" is available here:\nhttp://people.eecs.berkeley.edu/~lingqi/publications/paper_glints2.pdf\nhttp://people.eecs.berkeley.edu/~lingqi/\n\nVienna Rendering course:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2796240/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Using light simulation programs, we are able to populate a virtual scene with objects, assign material models to them, and create a beautiful, photorealistic image of this digital scene. The theory of light transport follows the physics of light, therefore these images should be indistinguishable from reality, and fortunately, they often are. However, there are some cases when we can tell them apart from real images. And the reason for this is not the inaccuracies of the light transport algorithm, but the oversimplifications used in the geometry and material models. The main issue is that in our mathematical models, these materials are often defined to be too perfect. But in reality, metals are rarely perfectly polished, and the classical material models in light transport can rarely capture these microstructures that make surfaces imperfect. This algorithm is about rendering new material models that can represent the imperfect materials like scratched coating and metallic flakes on carpaint, a leather sofa, wooden floor, or a teapot. Just look at these phenomenal images. Previous techniques exist to solve this problem, but they take extremely long and are typically limited to flat surfaces. One of the main difficulties of the problem is that these tiny flakes and scratches are typically orders of magnitude smaller than a pixel, and therefore they require a lot of care and additional computations to render. This work provides an exact, closed-form solution to this that is highly efficient to render. It is over a hundred times faster than previous techniques, has less limitations as it works on curved surfaces, and, it only takes 40% longer to render it compared to the standard perfect material models. Only 40% more time for this? Sign me up! It is truly incredible that we can create images of this sophistication using science. It is also highly practical as it can be plugged in as a standard material model without any crazy modifications to the simulation program. Looking forward to seeing some amazing animations using more and more realistic material models in the near future. If you would like to learn more about light simulations, I have been holding a full master-level course on it at the Technical University of Vienna for a few years now. After a while, I got a strong feeling that the teachings shouldn't only be available for the lucky 30 people in the classroom who can afford a college education. The teachings should be available for everyone. And now, the entirety of this course is available free of charge for everyone where we learn the theory of light from scratch and implement a really cool light simulation program together. If you want to solve a few infinite dimensional integrals with me, give it a go! As always, details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=GNx8rgNcw5c",
        "paper_link": "http://people.eecs.berkeley.edu/~lingqi/publications/paper_glints2.pdf",
        "paper_title": "Position-Normal Distributions for Efficient Rendering of Specular Microstructure"
    },
    {
        "video_id": "Dvd1jQe3pq0",
        "video_title": "Hindsight Experience Replay | Two Minute Papers #192",
        "position_in_playlist": 81,
        "description": "The paper \"Hindsight Experience Replay\" is available here:\nhttps://arxiv.org/pdf/1707.01495.pdf\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nRecommended for you:\nDeep Reinforcement Terrain Learning - https://www.youtube.com/watch?v=wBrwN4dS-DA&t=109s\nDigital Creatures Learn To Walk - https://www.youtube.com/watch?v=kQ2bqz3HPJE\nTask-based Animation of Virtual Characters - https://www.youtube.com/watch?v=ZHoNpxUHewQ\nReal-Time Character Control With Phase-Functioned Neural Networks - https://www.youtube.com/watch?v=wlndIQHtiFw\nDeepMind's AI Learns Locomotion From Scratch - https://www.youtube.com/watch?v=14zkfDTN_qo\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1193318/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is an awesome algorithm that is able to play computer games, navigate helicopters, hit a baseball, or even defeat Go champions when combined together with a neural network and Monte Carlo tree search. It is a quite general algorithm that is able to take on a variety of difficult problems that involve observing an environment and coming up with a series of actions to maximize a score. In a previous episode, we had a look at DeepMind's algorithm where a set of movement actions had to be chosen to navigate in a difficult 3D environment efficiently. The score to be maximized was the distance measured from the starting point, the further our character went, the higher score it was given, and it has successfully learned the concept of locomotion. Really cool! A prerequisite for a reinforcement learner to work properly is that it has to be given informative reward signals. For instance, if we go to a written exam, as an output, we would like to get a detailed breakdown of the number of points we got for each problem. This way, we know where we did well and which kinds of problems need some more work. However, imagine having a really careless teacher who never tells us the points, but would only tell us whether we have failed of passed. No explanation, no points for individual tasks, no telling whether we failed by a lot or just by a tiny bit. Nothing. First attempt, we failed. Next time, we failed again. And again and again and again. Now this would be a dreadful learning experience because we would have absolutely no idea what to improve. Clearly, this teacher would have to be fired. However, when formulating a reinforcement learning problem, instead of using more informative scores, it is much easier to just tell whether the algorithm was successful or not. It is very convenient for us to be this careless teacher. Otherwise, what score would make sense for a helicopter control problem when we almost crash into a tree? This part is called reward engineering and the main issue is that we have to adapt the problem to the algorithm, where the best would be if the algorithm would adapt to the problem. This has been a long-standing problem in reinforcement learning research, and a potential solution would open up the possibility of solving even harder and more interesting problems with learning algorithms. And this is exactly the what researchers at OpenAI try to solve by introducing Hindsight Experience Replay, HER, or her in short. Very apt. This algorithm takes on problems where the scores are binary, which means that it either passed or failed the prescribed task. A classic careless teacher scenario. And these rewards are not only binary, but very sparse as well, which further exacerbates the difficulty of the problem. In the video, you can see a comparison with a previous algorithm with and without the HER extension. The higher the number of epochs you see above, the longer the algorithm was able to train. The incredible thing here is that it is able to achieve a goal even if it had never been able to reach it during training. The key idea is that we can learn just as much from undesirable outcomes as from desirable ones. Let me quote the authors. Imagine that you are learning how to play hockey and are trying to shoot a puck into a net. You hit the puck but it misses the net on the right side. The conclusion drawn by a standard reinforcement learning algorithm in such a situation would be that the performed sequence of actions does not lead to a successful shot, and little (if anything) would be learned. It is however possible to draw another conclusion, namely that this sequence of actions would be successful if the net had been placed further to the right. They have achieved this by storing and replaying previous experiences with different potential goals. As always, the details are available in the paper, make sure to have a look. Now, it is always good to test things out whether the whole system works well in software, however, its usefulness has been demonstrated by deploying it on a real robot arm. You can see the goal written on the screen alongside with the results. A really cool piece of work that can potentially open up new ways of thinking about reinforcement learning. After all, it's great to have learning algorithms that are so good, they can solve problems that we formulate in such a lazy way that we'd have to be fired. And here's a quick question: do you think 8 of these videos a month is worth a dollar? If you have enjoyed this episode and your answer is yes, please consider supporting us on Patreon. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Dvd1jQe3pq0",
        "paper_link": "https://arxiv.org/pdf/1707.01495.pdf",
        "paper_title": "Hindsight Experience Replay"
    },
    {
        "video_id": "aR6M0MQBo2w",
        "video_title": "Latent Space Human Face Synthesis | Two Minute Papers #191",
        "position_in_playlist": 82,
        "description": "The paper \"Optimizing the Latent Space of Generative Networks\" is available here:\nhttps://arxiv.org/pdf/1707.05776.pdf\n\nKhan Academy's video on the Nash equilibrium:\nhttps://www.khanacademy.org/economics-finance-domain/microeconomics/nash-equilibrium-tutorial/nash-eq-tutorial/v/prisoners-dilemma-and-nash-equilibrium\n\nEarlier episodes showcased in the video:\nImage Editing with Generative Adversarial Networks - https://www.youtube.com/watch?v=pqkpIfu36Os\nAI Learns to Synthesize Pictures of Animals - https://www.youtube.com/watch?v=D4C1dB9UheQ\nAI Makes 3D Models From Photos - https://www.youtube.com/watch?v=HO1LYJb818Q\n\nFont paper:\nhttp://vecg.cs.ucl.ac.uk/Projects/projects_fonts/projects_fonts.html\n\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2589641/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In many previous episodes, we talked about generative adversarial networks, a recent new line in machine learning research with some absolutely fantastic results in a variety of areas. They can synthesize new images of animals, create 3d models from photos, or dream up new products based on our edits of an image. A generative adversarial network means that we have two neural networks battling each other in an arms race. The generator network tries to create more and more realistic images, and these are passed to the discriminator network which tries to learn the difference between real photographs and fake, forged images. During this process, the two neural networks learn and improve together until they become experts at their own craft. And as you can see, the results are fantastic. However, training these networks against each other is anything but roses and sunshine. We don't know if the process converges or if we reach Nash equilibrium. Nash equilibrium is a state where both actors believe they have found an optimal strategy while taking into account the other actor's possible decisions, and neither of them have interest in changing their strategy. This is a classical scenario in game theory where two convicted criminals are pondering whether they should snitch on each other without knowing how the other decided to act. If you wish to hear more about the Nash-equilibrium, I've a put a link to Khan Academy's video in the description, make sure to check it out, you'll love it! I find it highly exciting that there are parallels in AI and game theory, however, the even cooler thing is that here, we try to build a system where we don't have to deal with such a situation. This is called Generative Latent Optimization, GLO in short and it is about introducing tricks to do this by only using a generator network. If you have ever read up on font design, you know that it is a highly complex field. However, if we'd like to create a new font type, what we're typically interested in is only a few features, like how curvy they are, or whether we're dealing with a serif kind of font, and simple descriptions like that. The same principle can be applied to human faces, animals, and most topics you can imagine. This means that there are many complex concepts that contain a ton of information, most of which can be captured by a simple description with only a few features. This is done by projecting this high-dimensional data onto a low-dimensional latent space. This latent space helps eliminating adversarial optimization, which makes this system much easier to train, and the main selling point is that it still retains the attractive properties of generative adversarial networks. This means that it can synthesize new samples from the learned dataset. If it had learned the concept of birds, it will be able to synthesize new bird species. It can perform continuous interpolation between data points. This means that for instance, we can produce intermediate states between two chosen furniture types or light fixtures. It is also able to perform simple arithmetic operations between any number of data points. For instance, if A is males with sunglasses, B are males without sunglasses, and C are females, then A-B+C is going to generate females in sunglasses. It can also do super resolution and much, much more, make sure to have a look at the paper in the video description. Now, before we go, we shall address the elephant in the room: these images are tiny. Our seasoned Fellow Scholars know that for generative adversarial networks, there are plenty of works on how to synthesize high resolution images with more details. This means that this is a piece of work that opens up exciting new horizons, but it is not to be measured against the tenth followup work on top of a more established line of research. Two Minute Papers will be here for you to keep you updated on the progress, which is, as we know, staggeringly quick in machine learning research. Don't forget to subscribe and click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=aR6M0MQBo2w",
        "paper_link": "https://arxiv.org/pdf/1707.05776.pdf",
        "paper_title": "Optimizing the Latent Space of Generative Networks"
    },
    {
        "video_id": "14zkfDTN_qo",
        "video_title": "DeepMind's AI Learns Locomotion From Scratch | Two Minute Papers #190",
        "position_in_playlist": 83,
        "description": "The paper \"Emergence of Locomotion Behaviours in Rich Environments\" is available here:\nhttps://arxiv.org/abs/1707.02286\n\nOur Patreon page with the details is available here:\nhttps://www.patreon.com/TwoMinutePapers\n\nRecommended for you:\nDigital Creatures Learn To Walk - https://www.youtube.com/watch?v=kQ2bqz3HPJE\nReal-Time Character Control With Phase-Functioned Neural Networks -\nhttps://www.youtube.com/watch?v=wlndIQHtiFw\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1834465/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have talked about some awesome previous works where we used learning algorithms to teach digital creatures to navigate in complex environments. The input is a terrain and a set of joints, feet, and movement types, and the output has to be a series of motions that maximizes some kind of reward. This previous technique borrowed smaller snippets of movements from a previously existing database of motions and learned to stitch them together in a way that looks natural. And as you can see, these results are phenomenal. And the selling point of this new one, which you might say, looks less elaborate, however, it synthesizes them from scratch. This problem is typically solved via reinforcement learning, which is a technique that comes up with a series of decisions to maximize a prescribed score. This score typically needs to be something reasonably complex, otherwise the algorithm is given too much freedom to maximize it. For instance, we may want to teach a digital character to run or jump hurdles, but it may start crawling instead, which is still completely fine if our objective is too simple, for instance, just maximizing the distance from the starting point. To alleviate this, we typically resort to reward engineering, which means that we add additional terms to this reward function to regularize the behavior of these creatures. For instance, we can specify that throughout these motions, the body has to remain upright which likely favors locomotion-type solutions. However, one of the main advantages of machine learning is that we can reuse our solutions for a large set of problems. If we have to specialize our algorithm for all terrain and motion types, and different kinds of games, we lose out on one of the biggest advantage of learning techniques. So researchers at DeepMind decided that they are going to solve this problem with a reward function which is nothing else but forward progress. That's it. The further we get, the higher score we obtain. This is amazing because it doesn't require any specialized reward function but at the same time, there are a ton of different solutions that get us far in these terrains. And as you see here, beyond bipeds, a bunch of different agent types are supported. The key factors to make this happen is to apply two modifications to the original reinforcement learning algorithm. One makes the learning process more robust and less dependent on what parameters we choose, and the other one makes it more scalable, which means that it is able to efficiently deal with larger problems. Furthermore, the training process itself happens on a rich, carefully selected set of challenging levels. Make sure to have a look at the paper for details. A byproduct of this kind of problem formulation, is, as you can see, that even though this humanoid does its job with its lower body well, but in the meantime, it is flailing its arms like a madman. The reason is likely because there is not much of a difference in the reward between different arm motions. This means that we most likely get through a maze or a heightfield even when flailing, therefore the algorithm doesn't have any reason to favor more natural looking movements for the upper body. It will probably choose a random one, which is highly unlikely to be a natural motion. This creates high quality, albeit amusing results that I am sure some residents of the internet will honor with a sped-up remix video with some Benny Hill music. In summary, no precomputed motion database, no handcrafting of rewards, and no additional wizardry needed. Everything is learned from scratch with a few small modifications to the reinforcement learning algorithm. Highly remarkable work. If you've enjoyed this episode and would like to help us and support the series, have a look at our Patreon page. Details and cool perks are available in the video description, or just click the letter P at the end of this video. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=14zkfDTN_qo",
        "paper_link": "https://arxiv.org/abs/1707.02286",
        "paper_title": "Emergence of Locomotion Behaviours in Rich Environments"
    },
    {
        "video_id": "TItYXBoJ1sc",
        "video_title": "What is The Best Way To Simulate Liquids? | Two Minute Papers #189",
        "position_in_playlist": 84,
        "description": "The paper \"Perceptual Evaluation of Liquid Simulation Methods\" is available here:\nhttps://ge.in.tum.de/publications/2017-sig-um/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2753740/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As you know from this series, fluid simulation techniques that are able to create high fidelity video footage are in abundance in computer graphics research. These techniques all have their own tradeoffs, and when we evaluate them, we often use terms like first or second-order accuracy, which are mathematical terms. We often have to evaluate these techniques against each other by means of mathematics, because this way, we can set up consistent and unbiased comparisons that everyone understands and agrees upon. However, ultimately, in the show business, what matters is how the viewers perceive the end result, whether they think it looks fake, or if it keeps their suspension of disbelief. We have the choice of not only simulation techniques, but all of them also have their own set of parameters. For instance, the higher the resolution of our simulations, the more high-frequency details appear in the footage. However, after a point, increasing the resolution further is extremely costly, and while we know what is to be gained in terms of mathematics, it is still unknown how well it would do with the users. So the ultimate question is this: what do I get for my money and time? This paper provides an exhaustive user study to answer this question, where the users are asked to look at two different simulations and as a binary choice, tell us which is the one they perceived to be closer to the reference. The reference footage is a real-world video of a water sloshing in a tank, and the other footage that is to be judged is created via a fluid simulation algorithm. Turns out that the reference footage can be almost anything as long as there are some splashing and sloshing going on in it. It also turns out that after a relatively favorable breaking point which is denoted by 2x, further increasing the resolution does not make a significant difference in the user scores. But boy, does it change the computation times! So this is why such studies are super useful, and it's great to see that the accuracy of these techniques are measured both mathematically, and also how convincing they actually look for users. Another curious finding is that if we deny access to the reference footage, we see a large change in different responses and a similar jump in ambiguity. This means that we are reasonably bad at predicting the fine details, therefore, if the simulation pushes the right buttons, the users will easily believe it to be correct even if it is far away from the ground truth solution. Here is a matrix with a ton of rendered footage. Horizontally, you see the same thing with different simulation techniques, and vertically, we slowly go from transparent above to opaque below. To keep things fair and really reveal which choices are the best bang for the buck, there are also comparisons between techniques that have a similar computation time. In these cases, the Fluid Implicit Particle. FLIP in short and the Affine Particle in Cell, are almost unanimously favorable. These are advanced techniques that combine particle and grid-based simulations. I think this is highly useful information for more time critical applications, so make sure to have a look at the paper for details. There are similar user studies with glossy and translucent material models and much more in the paper. The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=TItYXBoJ1sc",
        "paper_link": "https://ge.in.tum.de/publications/2017-sig-um/",
        "paper_title": "Perceptual Evaluation of Liquid Simulation Methods"
    },
    {
        "video_id": "Mu0ew2F-SSA",
        "video_title": "AI Learns To Improve Smoke Simulations | Two Minute Papers #188",
        "position_in_playlist": 85,
        "description": "The paper \"Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors\" is available here:\nhttps://ge.in.tum.de/publications/2017-sig-chu/\n\nRecommended for you:\nWavelet Turbulence - https://www.youtube.com/watch?v=5xLSbj5SsSE\nNeural Network Learns The Physics of Fluids and Smoke - https://www.youtube.com/watch?v=iOWamCtnwTc\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2571245/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about using AI to create super detailed smoke simulations. Typically, creating a crude simulation doesn't take very long, but as we increase the resolution, the execution time and memory consumption skyrockets. In the age of AI, it only sounds logical to try to include some learning algorithms in this process. So what if we had an AI-based technique that would have some sort of understanding of smoke simulations, take our crude data and add the fine details to it? This way, we could obtain a high resolution smoke simulation without waiting several days or weeks for the computation. Now if you are a truly seasoned Fellow Scholar, you may remember an earlier work by the name Wavelet Turbulence, which is one of my favorite papers of all time. So much so that it got the distinction of being showcased in the very first Two Minute Papers episode. I was a sophomore college student back then when I've first seen it and was absolutely shocked by the quality of the results. That was an experience I'll never forget. It also won a technical Oscar award and it is not an overstatement to say that this was one of the most influential works that made me realize that research is my true calling. The link to the first episode is available in the video description and if you want to see how embarrassing it is, make sure to check it out. It did something similar, but instead of using AI, it used some heuristics that describe what is the ratio and distribution of smaller and bigger vortices in a piece of fluid or smoke. Using this information, it could create a somewhat similar effect, but ultimately, that technique had an understanding of smoke simulations in general, but it didn't know anything about the scene that we have at hand right now. Another work that is related to this is showing a bunch of smoke simulation videos to an AI and teach it to continue these simulations by itself. I would place this work as a middle ground solution, because this work says that we should take a step back and not try to synthesize everything from scratch. Let's create a database of simulations, dice them up into tiny tiny patches, look at the same footage in low and high resolutions, and learn how they relate to each other. This way, we can hand the neural network some low resolution footage and it will be able to make an educated guess as to which high resolution patch should be the best match for it. When we found the right patch, we just switch the coarse simulation to the most fitting high-resolution patch in the database. You might say that in theory, creating such a Frankenstein smoke simulation sounds like a dreadful idea. But have a look at the results, as they are absolutely brilliant! And as you can see, it takes a really crude base simulation and adds so many details to it, it's truly an incredible achievement. One neural network is trained to capture similarities in densities, and one for vorticity. Using the two neural networks in tandem, we can take a low resolution fluid flow and synthesize the fine details on top of it in a way that is hardly believable. It also handles boundary conditions, which means that these details are correctly added even if our smoke puff hits an object. This was an issue with Wavelet Turbulence which had to be addressed with several followup works. There are also comparisons against this legendary algorithm, and as you can see, the new technique smokes it. However, it took 9 years to do this. This is exactly 9 eternities in the world of research, which is a huge testament to how powerful the original algorithm was. It is also really cool to get more and more messages where I get to know more about you Fellow Scholars. I was informed that the series is used in school classes in Brazil, it is also used to augment college education, and it is a great topic for fun family conversations over dinner. That's just absolutely fantastic. Loving the fact that the series is an inspiration for many of you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Mu0ew2F-SSA",
        "paper_link": "https://ge.in.tum.de/publications/2017-sig-chu/",
        "paper_title": "Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors"
    },
    {
        "video_id": "bVGubOt_jLI",
        "video_title": "Physics-based Image and Video Editing | Two Minute Papers #187",
        "position_in_playlist": 86,
        "description": "The paper \"Calipso: Physics-based Image and Video Editing through CAD Model Proxies\" is available here:\nhttps://arxiv.org/abs/1708.03748\n\nProject page: http://mimesis.inria.fr/calipso/\nPhysics simulation by SOFA: http://www.sofa-framework.org \n\nRecommended for you: https://www.youtube.com/watch?v=BjwhMDhbqAs\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Evan Breznyik, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2714673/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about changing already existing images and videos by adding new objects to them, or editing their physical attributes. These editable attributes include gravity, mass, stiffness, or we can even add new physical forces to the scene. For instance, we can change the stiffness of the watches in this Dal\u00ed painting and create an animation from it. Physically accurate animations from paintings. How cool is that? This is approaching science fiction levels of craziness. Animating a stationary clothesline by adding a virtual wind effect to the scene, or bending a bridge by changing its mass is also a possibility. The first reaction I had when I've looked at this work was \"are you kidding me? you can't edit a photograph!\". Especially that I've seen plenty of earlier works that tried to do something similar but each time the limitations were just too crippling for real-world usage. And the other ultimate question is always - how much user interaction does this need? Is this trivial to use, or is it a laborious process? What we need to do is roughly highlight the outline of the object that we'd like to manipulate. The algorithm uses a previously published technique to make sure that the outlines are accurately captured, and then tries to create a 3D digital model from the selected area. We need one more step where we align the 3D model to the image or video input. Finally, the attribute changes and edits take place not on the video footage, but on this 3D model through a physics simulation technique. A truly refreshing combination of old and new techniques with some killer applications. Loving it! The biggest challenge is to make sure that the geometry and the visual consistency of the scene is preserved through these changes. There are plenty of details discussed in the paper, make sure to have a look at that, a link to it is available in the video description. As these 2D photo to 3D model generator algorithms improve, so will the quality of these editing techniques in the near future. Our previous episode was on this topic, make sure to have a look at that. Also, if you would like to get more updates on the newest and coolest works in this rapidly improving field, make sure to subscribe and click the bell icon to be notified when new Two Minute Papers videos come up. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bVGubOt_jLI",
        "paper_link": "https://arxiv.org/abs/1708.03748",
        "paper_title": "Calipso: Physics-based Image and Video Editing through CAD Model Proxies"
    },
    {
        "video_id": "BjwhMDhbqAs",
        "video_title": "AI Creates 3D Models From Images | Two Minute Papers #186",
        "position_in_playlist": 87,
        "description": "The paper \"Hierarchical Surface Prediction for 3D Object Reconstruction\" is available here:\nhttps://arxiv.org/abs/1704.00710\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Brian Gilman, Dave Rushton-Smith, Dennis Abts, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2717506/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we're going to talk about a task that humans are remarkably good at, but learning algorithms mostly flounder. And that is creating 3D geometry by looking at a 2D color image. In video games and animation films, this is a scenario that comes up very often - if we need a new weapon model in the game, we typically give the artist a photo, who will sit down with a 3D modeler program, and spends a few hours sculpting a similar 3D geometry. And I will quickly note that our binocular vision is not entirely necessary to make this happen. We can look at 2D images all day long and still have a good idea about the shape of an airplane, even with one eye closed. We had previous episodes on this problem, and the verdict was that that the results with previous techniques are great, but not very detailed. Mathematicians like to say that this algorithm has a cubic complexity or cubic scaling, which means that if we wish to increase the resolution of the 3D model just a tiny bit, we have wait not a tiny bit longer, but significantly longer. And the cubic part means that this tradeoff becomes unbearable even for moderately high resolutions. This paper offers a technique to break through this limitation. This new technique still uses a learning algorithm to predict the geometry, but it creates these 3D models hierarchically. This means that it starts out approximating the coarse geometry of the output, and restarts the process by adding more and more fine details to it. The geometry becomes more and more refined over several steps. Now, this refinement doesn't just work unless we have a carefully designed algorithm around it. The refinement happens by using additional information in each step from the created model. Namely, we imagine our predicted 3D geometry as a collection of small blocks, and each block is classified as either free space, occupied space, or as a surface. After this classification happened, we have the possibility to focus our efforts on refining the surface of the model, leading to a significant improvement in the execution time of the algorithm. As a result, we get 3D models that are of higher quality than the ones offered by previous techniques. The outputs are still not super high resolution, but they capture a fair number of surface detail. And you know the drill, research is a process, and every paper is a stepping stone. And this is one of those stepping stones that can potentially save many hours of work for 3D artists in the industry. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=BjwhMDhbqAs",
        "paper_link": "https://arxiv.org/abs/1704.00710",
        "paper_title": "Hierarchical Surface Prediction for 3D Object Reconstruction"
    },
    {
        "video_id": "ZtP3gl_2kBM",
        "video_title": "AI Creates Facial Animation From Audio | Two Minute Papers #185",
        "position_in_playlist": 88,
        "description": "The paper \"Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion\" is available here:\nhttp://research.nvidia.com/publication/2017-07_Audio-Driven-Facial-Animation\n\nOur Patreon page and the newest post on empowering research projects:\nhttps://www.patreon.com/TwoMinutePapers\nhttps://www.patreon.com/posts/14199475\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Dave Rushton-Smith, Dennis Abts, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2308464/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about creating facial animation from speech in real time. This means that after recording the audio footage of us speaking, we give it to a learning algorithm, which creates a high-quality animation depicting our digital characters uttering these words. This learning algorithm is a Convolutional Neural Network, which was trained on as little as 3 to 5 minutes of footage per actor, and was able to generalize its knowledge from this training data to a variety of real-world expressions and words. And if you think you've seen everything, you should watch until the end of the video as it gets better than that because of two reasons. Reason number one, it not only takes audio input, but we can also specify an emotional state that the character should express when uttering these words. Number two, and this is the best part, we can also combine this together with DeepMind's WaveNet, which synthesizes audio from our text input. It basically synthesizes a believable human voice and says whatever text we write down. And then that sound clip can be used with this technique to make a digital character say what we've written. So we can go from text to speech with WaveNet, and put the speech onto a virtual actor with this work. This way, we get a whole pipeline that works by learning and does everything for us in the most convenient way. No actors needed for voiceovers. No motion capture for animations. This is truly incredible. And if you look at the left, side, you can see that in their video, there is some Two Minute Papers action going on. How cool is that? Make sure to have a look at the paper to see the three-way loss function the authors came up with to make sure that the results work correctly for longer animations. And of course, in research, we have to prove that our results are better than previous techniques. To accomplish this, there are plenty of comparisons in the supplementary video. But we need more than that. Since these results cannot be boiled down to a mathematical theorem that we need to prove, we have to do it some other way. And the ultimate goal is that a human being would judge these videos as being real with a higher chance than one made with a previous technique. This is the core idea behind the user study carried out in the paper. We bring in a bunch of people, present them with a video of the old and new technique without knowing which is which, and ask them which one they feel to be more natural. And the result was not even close - the new method is not only better overall, but I haven't found a single case, scenario or language where it didn't come out ahead. And that's extremely rare in research. Typically, in a maturing field, new techniques introduce a different kind of tradeoff, for instance, less execution time but at the cost of higher memory consumption is a classical case. But here, it's just simply better in every regard. Excellent. If you enjoyed this episode, and would like to help us make better videos in the future, please consider supporting us on Patreon. You can pick up cool perks like watching these episodes in early access. Details are available in the video description. Beyond telling these important research stories, we're also using part of these funds to empower other research projects. I just made a small write-up about this which is available on our Patreon page. That link is in the video description, make sure to have a look. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZtP3gl_2kBM",
        "paper_link": "http://research.nvidia.com/publication/2017-07_Audio-Driven-Facial-Animation",
        "paper_title": "Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion"
    },
    {
        "video_id": "mL3CzZcBJZU",
        "video_title": "DeepMind's AI Learns Audio And Video Concepts By Itself | Two Minute Papers #184",
        "position_in_playlist": 89,
        "description": "The paper \"Look, Listen and Learn\" is available here:\nhttps://arxiv.org/abs/1705.08168\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nRecommended for you:\nhttps://www.youtube.com/watch?v=hBobYd8nNtQ\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, Emmanuel Mwangi, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Jensen, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1838412/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In our earlier episodes, when it came to learning techniques, we almost always talked about supervised learning. This means that we give the algorithm a bunch of images, and some additional information, for instance, that these images depict dogs or cats. Then, the learning algorithm is exposed to new images that it had never seen before and has to be able to classify them correctly. It is kind of like a teacher sitting next to a student, providing supervision. Then, the exam comes with new questions. This is supervised learning, and as you have seen from more than 180 episodes of Two Minute Papers, there is no doubt that it is an enormously successful field of research. However, this means that we have to label our datasets, so we have to add some additional information to every image we have. This is a very laborious task, which is typically performed by researchers or through crowdsourcing, both of which takes a lot of funding and hundreds of work hours. But if we think about it, we have a ton of videos on the internet, you always hear these mind melting new statistics on how many hours of video footage is uploaded to YouTube every day. Of course, we could hire all the employees in the world to annotate these videos frame by frame to tell the algorithm that this is a guitar, this is an accordion, or a keyboard, and we would still not be able to learn on most of what's uploaded. But it would be so great to have an algorithm that can learn on unlabeled data. However, there are learning techniques in the field of unsupervised learning, which means that the algorithm is given a bunch of images, or any media, and is instructed to learn on it without any additional information. There is no teacher to supervise the learning. The algorithm learns by itself. And in this work, the objective is to learn both visual and audio-related tasks in an unsupervised manner. So for instance, if we look at the this layer of the visual subnetwork, we'll find neurons that get very excited when they see, for instance, someone playing an accordion. And each of the neurons in this layer belong to different object classes. I surely have something like this for papers. And here comes the K\u00e1roly goes crazy part one: this technique not only classifies the frames of the videos, but it also creates semantic heatmaps, which show us which part of the image is responsible for the sounds that we hear. This is insanity! To accomplish this, they ran a vision subnetwork on the video part, and a separate audio subnetwork to learn about the sounds, and at the last step, all this information is fused together to obtain K\u00e1roly goes crazy part two: this makes the network able to guess whether the audio and the video stream correspond to each other. It looks at a man with a fiddle, listens to a sound clip and will say whether the two correspond to each other. Wow! The audio subnetwork also learned the concept of human voices, the sound of water, wind, music, live concerts and much, much more. And the answer is yes, it is remarkably close to human-level performance on sound classification. And all this is provided by the two networks that were trained from scratch, and, no supervision is required. We don't need to annotate these videos. Nailed it. And please don't get this wrong, it's not like DeepMind has suddenly invented unsupervised learning, not at all. This is a field that has been actively researched for decades, it's just that we rarely see really punchy results like these ones here. Truly incredible work. If you enjoyed this episode, and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mL3CzZcBJZU",
        "paper_link": "https://arxiv.org/abs/1705.08168",
        "paper_title": "Look, Listen and Learn"
    },
    {
        "video_id": "qKhSZmS6aWw",
        "video_title": "Photorealistic Fur With Multi-Scale Rendering | Two Minute Papers #183",
        "position_in_playlist": 90,
        "description": "The paper \"An Efficient and Practical Near and Far Field Fur Reflectance Model\" is available here:\nhttps://people.eecs.berkeley.edu/~lingqi/publications/paper_fur2.pdf\nhttps://people.eecs.berkeley.edu/~lingqi/\n\nThe free Rendering course is available on YouTube here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, Emmanuel Mwangi, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1238238/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Creating a photorealistic image with fur and hair is hard. It is typically done by using light simulation programs where we use the laws of physics to simulate the path of millions and millions of light rays as they bounce off of different objects in a scene. This typically takes from minutes to hours if we're lucky. However, in the presence of materials like hair and fur, this problem becomes even more difficult, because fur fibers have inner scattering media. This means that we not only have to bounce these rays off of the surface of objects, but also have to simulate how light is transmitted between these inner layers. And initially, we start out with a noisy image, and this noise gets slowly eliminated as we compute more and more rays for the simulation. Spp means samples per pixel, which is the number of rays we compute for each pixel in our image. And you can see that with previous techniques, using 256 samples per pixel leads to a very noisy image and we need to spend significantly more time to obtain a clear, converged image. And this new technique enables us to get the most out of our samples, and if we render an image with 256 spp, we get a roughly equivalent quality to a previous technique using around six times as many samples. If we had a film studio and someone walked up on us and said that we can render the next Guardians of The Galaxy film six times cheaper, we'd surely be all over it. This would save us millions of dollars. The main selling point is that this work introduces a multi-scale model for rendering hair and fur. This means that it computes near and far-field scattering separately. The far-field scattering model contains simplifications, which means that it's way faster to compute. This simplification is sufficient if we look at a model from afar or we look closely at a hair model that is way thinner than human hair strands. The near-field model is more faithful to reality, but also more expensive to compute. And the final, most important puzzle piece is stitching together the two: whenever we can get away with it, we should use the far-field model, and compute the expensive near-field model only when it makes a difference visually. And one more thing: as these hamsters get closer or further away from the camera, we need to make sure that there is no annoying jump when we're switching models. And as you can see, the animations are buttery smooth, and when we look at it, we see beautiful rendered images, and if we didn't know a bit about the theory, we would have no idea about the multi-scale wizardry under the hood. Excellent work. The paper also contains a set of decompositions for different light paths, for instance, here, you can see a fully rendered image on the left, and different combinations of light reflection and transmission events. For instance, R stands for one light reflection, TT for two transmission events, and so on. The S in the superscript denotes light scattering events. Adding up all the possible combinations of these Ts and Rs, we get the photorealistic image on the left. That's really cool, loving it! If you would like to learn more about light simulations, I am holding a full master-level course on it at the Technical University of Vienna. And the entirety of this course is available free of charge for everyone. I got some feedback from you Fellow Scholars that you watched it and enjoyed it quite a bit. Give it a go! As always, details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=qKhSZmS6aWw",
        "paper_link": "https://people.eecs.berkeley.edu/~lingqi/publications/paper_fur2.pdf",
        "paper_title": "An Efficient and Practical Near and Far Field Fur Reflectance Model"
    },
    {
        "video_id": "St5lxIxYGkI",
        "video_title": "DeepMind Publishes StarCraft II Learning Environment | Two Minute Papers #182",
        "position_in_playlist": 91,
        "description": "The paper \"StarCraft II: A New Challenge for Reinforcement Learning\" and its source code is available here:\nhttps://arxiv.org/abs/1708.04782\nhttps://github.com/Blizzard/s2client-proto\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: Blizzard - http://media.blizzard.com/sc2/media/wallpapers/wall000/wall000-1600x1200.jpg\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This topic has been perhaps the most highly anticipated by you Fellow Scholars and I am extremely excited to show you the first joint paper between DeepMind and Blizzard on creating an AI program to play Stacraft II. Hell yeah! And fortunately, we have a paper where every detail is meticulously described, so there's much less room for misunderstandings. And before we start, note that this is a preliminary work, so please don't expect superhuman performance. However difficult you thought this problem was, you'll see in a minute that it's way more complex than most people would think. But before we start - what is Starcraft 2? It is a highly technical strategy game which will be a huge challenge to write a formidable AI for because of the three reasons. One, we have imperfect information with a partially observed map. If we wish to see what the other opponent is up to, we have to devote resources to scouting, which may or may not be successful depending on the vigilance of the other player. Two, we need to select and control hundreds of units under heavy time pressure. One wrong decision, and we can quickly lose most of our units and become unable to recover from it. And three, perhaps the most important part - long-term strategies need to be developed, where a poor decision in the early game can lead to a crushing defeat several thousands of actions later. These cases are especially difficult to identify and learn. However, we ran a bit too far ahead to the gameplay part. What needs to be emphasized is that there is a step number one before that. And that step number one is making sure that the AI is able communicate and interact with the game, which requires a significant engineering effort. In this paper, a Python-based interface is described to make this happen. It is great to have companies like DeepMind and OpenAI who are devoted to lay down the foundations for such an interface, which is a herculean task. This work would likely had never seen the light of day if AI research would only take place in academia. Huge respect and much thanks for the DeepMind guys for making this happen. To play the game, Deep Reinforcement Learning is used, which you heard about earlier in the series. This is a powerful learning algorithm where a neural network is used to process the video input and is combined with a reinforcement learner. With reinforcement learning, we're observing the environment around us and choose the next action to maximize a score or reward. However, defining score was very easy in Atari Breakout, because we knew that if the number of our lives drops to zero, we lost, and if we break a lot of bricks, our score improves. Simple. Not so much in Starcraft 2, because how do we know exactly if we're winning? What is the score we're trying to maximize? In this work, there are two definitions for score: one that we get to know at the very end that describes whether we won, had a tie, or lost. This is the score that ultimately matters. However, this information is not available throughout the game to drive the reinforcement learner, so there is an intermediate score that is referred to as Blizzard score in the paper, which involves a weighted sum of current resources and upgrades, as well as our units and buildings. This we have access to throughout the game. This sounds good for a first approximation, since it is monotonically increasing if we win battles and manage our resources well, and decreases when we're losing. However, there are many matches where the player with the more resources does not have enough time to spend it and ultimately loses a deciding encounter. It remains to be seen whether this is exactly what we need to maximize to beat a formidable human player. There are also non-trivial engineering decisions on how to process the video stream. The current system uses a set of feature layers, which encode relevant information for the AI, such as terrain height, the camera location, hit points for the units on the screen and much, much more. There is a huge amount of information that the convolutional neural network has to make sense of. And I think it is now easy to see that starting out with throwing the AI in the deep water and expecting it to perform well on a full one versus one match, at this point, is a forlorn effort. The paper describes a set of minigames, where the algorithm can learn different aspects of the game in isolation, such as picking up mineral shards scattered around the map, defeating enemy units in small skirmishes, building units or harvesting resources. In these minigames, the AI has reached the level of a novice human player, which is quite amazing given the magnitude and the complexity of the problem. The authors also encourage the community to create more minigames for the AI to train on. I really love the openness and the community effort aspects of this work! And we've only just scratched the surface, there is so much more in the paper, with a lot more non-trivial design decisions and a database with tens of thousands of recorded games. And, the best part is that the source code for this environment is available right now for the fellow tinkerers out there. I've put a link to this in the video description. This is going to be one heck of a challenge for even the brightest AI researchers of our time. I can't wait to get my hands on the code and also, I am very excited to read some followup papers on this. I expect there will be many of those in the following months. In the meantime, as we know, OpenAI is also working on DOTA with remarkable results, and there's lots of discussion whether a DOTA 5 versus 5 or a Starcraft 2 1 versus 1 game is more complex for the AI to learn. If you have an opinion on this, make sure to leave a comment below this video. Which is more complex? Why? This also signals that there's going to be tons of fun to be had with AI and video games this year. Stay tuned! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=St5lxIxYGkI",
        "paper_link": "https://arxiv.org/abs/1708.04782",
        "paper_title": "StarCraft II: A New Challenge for Reinforcement Learning"
    },
    {
        "video_id": "HSmm_vEVs10",
        "video_title": "Real-Time Noise Filtering For Light Simulations | Two Minute Papers #181",
        "position_in_playlist": 92,
        "description": "The paper \"Spatiotemporal Variance-Guided Filtering: Real-Time Reconstruction for Path-Traced Global Illumination\" is available here:\nhttp://cg.ivd.kit.edu/svgf.php\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we talk a lot about photorealistic rendering, which is one of the most exciting areas in computer graphics research. Photorealistic rendering means that we put virtual objects in a scene, assign material models to them, and then run a light simulation program to create a beautiful image. This image depicts how these objects would look like in reality. This is particularly useful for the film industry because we can create highly realistic scenes and set them up in a way that we couldn't do in real life. We can have any possible object we can imagine, light sources that we wouldn't ever be able to buy, change the time of the day, or even the planet we're on. Practically, we have an infinite budget. That's amazing! However, creating such an image takes a long time, often in the order of hours to days. Here you can see me render an image and even if the footage is sped up significantly, you can see that this is going to take a long, long time. Spp means samples per pixel, so this the number of light rays we compute for every pixel. The more spp, the cleaner, more converged image we get. This technique performs spatiotemporal filtering. This means that we take a noisy input video and try to eliminate the noise and try to guess how the final image would look like. And it can create almost fully converged images from extremely noisy inputs. Well, as you can see, these videos are created with one sample per pixel, which is as noisy as it gets. These images with the one sample per pixel can be created extremely quickly, in less than ten milliseconds per image, and this new denoiser also takes around 10 milliseconds to reconstruct the final image from the noisy input. And yes, you heard it right, this is finally a real-time result. This all happens through decoupling the direct and indirect effect of light sources and denoising them separately. In the meantime, the algorithm also tries to estimate the amount of noise in different parts of the image to provide more useful information for the denoising routines. The fact that the entire pipeline runs on the graphics card is a great testament to the simplicity of this algorithm. Whenever you see the term SVGF, you'll see the results of the new technique. So we have these noisy input videos with 1 spp. And...look at that! Wow! This is one of those papers that looks like magic. And, no neural networks or learning algorithms have been used in this work. Not so long ago, I speculated, or more accurately, hoped that real-time photorealistic rendering would be a possibility during my lifetime. And just a few years later, this paper appears. We know that the rate of progress in computer graphics research is just staggering, but this is, this is too much to handle. Super excited to see where the artists will take this, and of course, I'll be here to show you the coolest followup works. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HSmm_vEVs10",
        "paper_link": "http://cg.ivd.kit.edu/svgf.php",
        "paper_title": "Spatiotemporal Variance-Guided Filtering: Real-Time Reconstruction for Path-Traced Global Illumination"
    },
    {
        "video_id": "cLC_GHZCOVQ",
        "video_title": "OpenAI's Bot Beats DOTA World Champion Dendi | Two Minute Papers #180",
        "position_in_playlist": 93,
        "description": "Some updates and clarifications follow:\nUpdate 1: we seem to have conflicting information on the training times - both 24 hours and 2 weeks was mentioned. We'll make sure to address this when the official paper appears. \nUpdate 2: more from OpenAI - https://blog.openai.com/more-on-dota-2/ \nUpdate 3: more reddit discussion on how to trick the bot into defeat: https://www.reddit.com/r/DotA2/comments/6t8qvs/openai_bots_were_defeated_atleast_50_times/ (thanks to nikre for the link)\nUpdate 4: an OpenAI employee provides more clarification on the training process - https://news.ycombinator.com/item?id=15001521\n\nApologies for the inaccuracies - I've watched every video and interview I could get my hands on and found quite a bit of conflicting information. I'll take this into consideration next time when something comes up without an official research paper.\n\nOpenAI's materials on their DOTA bot: https://blog.openai.com/dota-2/\n\nDay9's DOTA learning videos are available here:\nhttps://www.youtube.com/playlist?list=PLgmCLtUkEutILNA9EM0BON6ShoQGZhd3P\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image source: https://blog.openai.com/dota-2/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. It is time for some minds to be blown. DOTA 2 is a multiplayer online battle arena game with a huge cult following and world championship events with a prize pool of over 20 million dollars. In this game, players form two teams and control a hero each and use their strategy and special abilities to defeat the other team. OpenAI recently created an AI for this game that is so good that they challenged the best players in the world. Now note that this program is not playing the full feature set of the game, but a version that is limited to one versus one encounters with several other elements of the game disabled. Since lots of strategy is involved, we always discuss in these episodes that long-term planning is the Achilles-heel of these learning algorithms. A small blunder in the early game can often snowball out of control by the end of the match, and it is hard for the AI, and sometimes, to even humans to identify these cases. And this game is a huge challenge because unlike chess and go, it has lots of incomplete information, and even the simplified one versus one mode involves a reasonable amount of long-term planning. It also involves attacks, trickery and deceiving an opponent and can be imagined as a strategy game that also requires significant technical prowess to pull off the most spectacular moves. This game is also designed in a way that new and unfamiliar situations come up all the time which require lots of experience and split-second decisionmaking to master. This is a true test for any kind of AI. And note that this AI wasn't told anything about the game, not even the rules, and was just instructed to try to find a way to win. The algorithm was trained in 24 hours, and during this time, it not only learned the rules and objectives of the game, but it also pulls off remarkable tactics. For instance, other players were very surprised that the bot didn't take the bait, which typically means a smart tactic involving giving up a smaller battle in favor of winning a bigger objective. The AI has a ton of experience playing the game and typically sees through these shenanigans. In this game, there are also neutral units that we call creep. When killed, they grant precious gold and experience to our opponent, so we typically try to deny that. If these units encounter an obstacle, they go around it, so players developed a technique by the name creep blocking, which is the art of holding them up by the hero character to minimize the distance traveled by them in a unit of time. And the AI has not only learned about the existence of this technique by itself, but it also executes it with stunning precision, which is quite remarkable. And again, during the training phase, it had never seen any human play the game and do something like this. The other remarkable thing is that when a player disappears in the darkness, the AI predicts what he could be doing, plans around it, and strikes where the player is expected to show up. If you remember, DeepMind's initial Go algorithm contained a bootstrapping step where it was fed a large amount of games by players to grasp the basics. The truly remarkable thing is that none of that happened here. This algorithm was trained for only 24 hours and it only played against itself. When it finally played against Dendi, the reigning world champion, the first match was such a treat, and I was shocked to see that the AI has outplayed him. In the second game, the player tried to create a situation that he thought the AI hasn't encountered before by giving up some creep to it. The program ruthlessly took advantage of this mistake and defeated him almost immediately. OpenAI's bot not only won, but apparently also broke the will of Dendi, who tapped out after two matches. I feel like someone being hit by a sledgehammer. I didn't even know this was being worked on! This is such a remarkable achievement. Usually, the first argument I hear is that of course, the AI can play non-stop without bathroom breaks or sleep. While, admittedly, this is also true for some players, the algorithm was only trained for 24 hours. Note that this still means a stupendous amount of games played, but in terms of training time, given that these algorithms typically take from weeks to months to train properly, 24 hours is nothing. The second argument that I often hear is that the AI should of course win every time, because it has close to 0 reaction time and can perform thousands of actions every second. For instance, if we would play a game where the goal is to perform the most amount of actions per minute, clearly, humans with biological limitations would stand no chance against a computer program. However, in this case, the number of actions that this algorithm performs in a minute is comparable to that of a human player. This means that these results stem from superior technical abilities and planning, and not from the fact that we're talking about a computer. We can look at this result from two different directions. One could be saying, well, no big deal, because this is only a highly limited and hamstrung version of the game, which is way less complex than a fully-fledged 5 versus 5 team match. Or, two, we could say that the algorithm had shown a remarkable aptitude for learning highly sophisticated technical maneuvers and longer-term strategy in a difficult game. And the rest is only a matter of time. In fact, in 5 versus 5, there is even more room for a highly intelligent program to shine and create new tactics that we've never thought of. I would bet that if anything, we're going to be even more surprised by the 5 versus 5 results later. We are still lacking in details a bit, but I have contacted the OpenAI guys who noted that there will be more information available in the next few days. Whenever something new appears, I'll be here to cover it for you Fellow Scholars. If you are new to the series and enjoyed this episode, make sure to subscribe and click the bell icon for two super fun science videos a week. And if you find yourself interested in DOTA 2, and admittedly, it's hard not to, and would like to catch up a bit on the basics, make sure to visit Day9's channel who has a really nice playlist about the fundamentals of the game. There is a link in the description for it, check it out. If you go to his channel, make sure to leave him a kind scholarly comment. Let the world see how courteous the Two Minute Papers listeners are. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cLC_GHZCOVQ"
    },
    {
        "video_id": "_DN2rzHkpZE",
        "video_title": "Verifying Mission-Critical AI Programs | Two Minute Papers #179",
        "position_in_playlist": 94,
        "description": "The paper \"Reluplex: An Efficient SMT Solver for Verifying\nDeep Neural Networks\" is available here:\nhttps://arxiv.org/pdf/1702.01135.pdf\n\nOut Patreon page:\nhttps://www.patreon.com/TwoMinutePapers\n\nEarlier episodes that were showcased:\npix2pix - https://www.youtube.com/watch?v=u7kQ5lNfUfg\nBreaking DeepMind's Game AI System - https://www.youtube.com/watch?v=QFu0vZgMcqk\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2072618/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper does not contain the usual fireworks that you're used to in Two Minute Papers, but I feel that this is a very important story that needs to be told to everyone. In computer science, we encounter many interesting problems, like finding the shortest path between two given streets in a city, or measuring the stability of a bridge. Up until a few years ago, these were almost exclusively solved by traditional, handcrafted techniques. This means a class of techniques that were designed by hand by scientists and are often specific to the problem we have at hand. Different problem, different algorithm. And, fast forward to a few years ago, we witnessed an amazing resurgence of neural networks and learning algorithms. Many problems that were previously thought to be unsolvable, crumbled quickly one after another. Now it is clear that the age of AI is coming, and clearly, there are possible applications of it that we need to be very cautious with. Since we design these traditional techniques by hand, the failure cases are often known because these algorithms are simple enough that we can look under the hood and make reasonable assumptions. This is not the case with deep neural networks. We know that in some cases, neural networks are unreliable. But it is remarkably hard to identify these failure cases. For instance, earlier, we talked about this technique by the name pix2pix where we could make a crude drawing of a cat and it would translate it to a real image. It worked spectacularly in many cases, but twitter was also full of examples with really amusing failure cases. Beyond the unreliability, we have a much bigger problem. And that problem is adversarial examples. In an earlier episode, we discussed an adversarial algorithm, where in an amusing example, they added a tiny bit of barely perceptible noise to this image, to make the deep neural network misidentify a bus for an ostrich. We can even train a new neural network that is specifically tailored to break the one we have, opening up the possibility of targeted attacks against it. To alleviate this problem, it is always a good idea to make sure that these neural networks are also trained on adversarial inputs as well. But how do we know how many possible other adversarial examples exist that we haven't found yet? The paper discusses a way of verifying important properties of neural networks. For instance, it can measure the adversarial robustness of such a network, and this is super useful, because it gives us information whether there are possible forged inputs that could break our learning systems. The paper also contains a nice little experiment with airborne collision avoidance systems. The goal here is avoiding midair collisions between commercial aircrafts while minimizing the number of alerts. As a small-scale thought experiment, we can train a neural network to replace an existing system, but in this case, such a neural network would have to be verified. And it is now finally a possibility. Now, make no mistake, this does not mean that there are any sort of aircraft safety systems deployed in the industry that are relying on neural networks. No no no, absolutely not. This is a small-scale \"what if\" kind of experiment that may prove to be a first step towards something really exciting. This is one of those incredible papers that, even without the usual visual fireworks, makes me feel that I am a part of the future. This is a step towards a future where we can prove that a learning algorithm is guaranteed to work in mission critical systems. I would also like to note that even if this episode is not meant to go viral on the internet, it is still an important story to be told. Normally, creating videos like this would be a financial suicide, but we're not hurt by this at all because we get stable support from you on Patreon. And that's what it is all about - worrying less about views and spending more time talking about what's really important. Absolutely amazing. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_DN2rzHkpZE",
        "paper_link": "https://arxiv.org/pdf/1702.01135.pdf",
        "paper_title": "Reluplex: An Efficient SMT Solver for Verifying\nDeep Neural Networks"
    },
    {
        "video_id": "xp-YOPcjkFw",
        "video_title": "DeepMind's AI Learns Imagination-Based Planning | Two Minute Papers #178",
        "position_in_playlist": 95,
        "description": "The paper \"Imagination-Augmented Agents for Deep Reinforcement Learning\" is available here:\nhttps://arxiv.org/abs/1707.06203\n\nOut Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Eric Swenson, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-767781/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A bit more than two years ago, the DeepMind guys implemented an algorithm that could play Atari Breakout on a superhuman level by looking at the video feed that you see here. And the news immediately took the world by storm. This original paper is a bit more than 2 years old and has already been referenced in well over a thousand other research papers. That is one powerful paper! This algorithm was based on a combination of a neural network and reinforcement learning. The neural network was used to understand the video feed, and reinforcement learning is there to come up with the appropriate actions. This is the part that plays the game. Reinforcement learning is very suitable for tasks where we are in a changing environment and we need to choose an appropriate action based on our surroundings to maximize some sort of score. This score can be for instance, how far we've gotten in a labyrinth, or how many collisions we have avoided with a helicopter, or any sort of score that reflects how well we're currently doing. And this algorithm works similarly to how an animal learns new things. It observes the environment, tries different actions and sees if they worked well. If yes, it will keep doing that, if not, well, let's try something else. Pavlov's dog with the bell is an excellent example of that. There are many existing works in this area and it performs remarkably well for a number of problems and computer games, but only if the reward comes relatively quickly after the action. For instance, in Breakout, if we miss the ball, we lose a life immediately, but if we hit it, we'll almost immediately break some bricks and increase our score. This is more than suitable for a well-built reinforcement learner algorithm. However, this earlier work didn't perform well on any other games that required long-term planning. If Pavlov gave his dog a treat for something that it did two days ago, the animal would have no clue as to which action led to this tasty reward. And this work's subject is a game where we control this green character and our goal is to push the boxes onto the red dots. This game is particularly difficult, not only for algorithms, but even humans, because of two important reasons: one, it requires long-term planning, which, as we know, is a huge issue for reinforcement learning algorithms. Just because a box is next to a dot doesn't mean that it is the one that belongs there. This is a particularly nasty property of the game. And two, some mistakes we make are irreversible, for instance, pushing a box in a corner can make it impossible to complete the level. If we have an algorithm that tries a bunch of actions and sees if they stick, well, that's not going to work here! It is now hopefully easy to see that this is an obscenely difficult problem, and the DeepMind guys just came up with Imagination-Augmented Agents as a solution for it. So what is behind this really cool name? The interesting part about this novel architecture is that it uses imagination, which is a routine to cook up not only one action, re plans consisting of several steps, and finally, choose one that has the greatest expected reward over the long term. It takes information about the present and imagines possible futures, and chooses the one with the most handsome reward. And as you can see, this is only the first paper on this new architecture and it can already solve a problem with seven boxes. This is just unreal. Absolutely amazing work. And please note that this is a fairly general algorithm that can be used for a number of different problems. This particular game was just one way of demonstrating the attractive properties of this new technique. The paper contains more results and is a great read, make sure to have a look. Also, if you've enjoyed this episode, please consider supporting Two Minute Papers on Patreon. Details are available in the video description, have a look! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=xp-YOPcjkFw",
        "paper_link": "https://arxiv.org/abs/1707.06203",
        "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning"
    },
    {
        "video_id": "vmkqFRyNUWo",
        "video_title": "AI Learns Semantic Style Transfer | Two Minute Papers #177",
        "position_in_playlist": 96,
        "description": "The paper \"Visual Attribute Transfer through Deep Image Analogy\" and its source code is available here:\nhttps://arxiv.org/pdf/1705.01088.pdf\nhttps://github.com/msracver/Deep-Image-Analogy\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1895653/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Style transfer is an amazing area in machine learning and AI research, where we take two images. Image number one is an input photograph, and image number two, is the desired style. And the output of this process is the content of image number one with the style of image number two. This first paper opened up an incredible new area of research. As a result, a ton of different variants have emerged in the last two years. Feedforward style transfer for close to real-time results, temporally coherent style transfer for videos, and much, much more. And this one not only outperforms previously existing techniques, but also broadens the horizon of possible style transfer applications. And obviously, a human would be best at doing this because a human has an understanding of the objects seen in these images. And now, hold on to your papers, because the main objective of this is method to create semantically meaningful results for style transfer. It is meant to do well with input image pairs that may look completely different visually, but have some semantic components that are similar. For instance, a photograph of a human face and a drawing of a virtual character is excellent example of that. In this case, this learning algorithm recognizes that they both have noses and uses this valuable information in the style transfer process. As a result, it has three super cool applications. First, the regular photo to style transfer that we all know and love. Second, it is also capable of swapping the style of two input images. Third, and hold on to your papers because this is going to be even more insane. Style or sketch to photo. And we have a plus one here as well, so, fourth, it also supports color transfer between photographs, which will allow creating amazing time-lapse videos. I always try to lure you Fellow Scholars into looking at these papers, so make sure to have a look at the paper for some more results on this. And you can see here that this method was compared to several other techniques, for instance, you can see the cycle consistency paper and PatchMatch. And this is one of those moments when I get super happy, because more than a 170 episodes into the series, we can not only appreciate the quality of these these new results, but we also had previous episodes about both of these algorithms. As always, the links are available in the video description, make sure to have a look, it's going to be a lot of fun. The source code of this project is also available. We also have a ton of episodes on computer graphics in the series, make sure to have a look at those as well. Every now and then I get e-mails from viewers who say that they came for the AI videos, and just in case, watched a recent episode on computer graphics and were completely hooked. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=vmkqFRyNUWo",
        "paper_link": "https://arxiv.org/pdf/1705.01088.pdf",
        "paper_title": "Visual Attribute Transfer through Deep Image Analogy"
    },
    {
        "video_id": "RygQnpQMdPI",
        "video_title": "Elastoplastic Hair and Cloth Simulations | Two Minute Papers #176",
        "position_in_playlist": 97,
        "description": "The paper \"Anisotropic Elastoplasticity for Cloth, Knit and Hair Frictional Contact\" is available here:\nhttp://www.math.ucla.edu/~jteran/papers/JGT17.pdf\nhttp://dl.acm.org/citation.cfm?id=3073623\n\nOur Patreon page with the details:\nhttps://www.patreon.com/TwoMinutePapers\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-791886/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a piece of elastic cloth modeled from more than a million tiny triangles and its interaction with 7 million colored grains of sand. This is super challenging because of two things: one, we have to compute the elastic deformations when these millions of tiny elements collide, and two, all this, while maintaining two-way coupling. This means that the cloth has an effect on the sand, but the effect of the sand is also simulated on the cloth. In elastic deformations, there are potential interactions between distant parts of the same material and self-collisions may also occur. Previous state of the art techniques were either lacking in these self-collision effects, or the ones that were able to process that also included the fracturing of the material. With this novel work, it is possible to simulate both elastic deformations as you can see here, but, it also supports simulating plasticity as you can see here with the cloth pieces sliding off of each other. Beautiful! This new technique also supports simulating a variety of different types of materials, knitted cloth ponchos, shag carpets, twisting cloth, hair, tearing fiber and more. And it does all this with a typical execution time between 10 to 90 seconds per frame. In these black screens, you see the timing information and the number of particles and triangles used in these simulations. And you will see that there are many scenes where millions of triangles and particles are processed in very little time. It is very rare that we can implement only one technique that takes care of so many kinds of interactions while still obtaining results very quickly. This is insanity. This paper is absolutely top tier bang for the buck and I am really excited to see some more elastoplastic simulations in all kinds of digital media in the future. You know our motto, a couple more papers down the line and having something like this in real-time applications may become a reality. Really cool! If you enjoyed this episode and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. One dollar per month really doesn't break the bank but it is a great deal of help for us in keeping the series going. And your support has always been absolutely amazing and I am so grateful to have so many devoted Fellow Scholars like you in our ranks. Details are in the video description, have a look. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=RygQnpQMdPI",
        "paper_link": "http://www.math.ucla.edu/~jteran/papers/JGT17.pdf",
        "paper_title": "Anisotropic Elastoplasticity for Cloth, Knit and Hair Frictional Contact"
    },
    {
        "video_id": "6c2T2cykE_A",
        "video_title": "Animating Elastic Rods With Sound | Two Minute Papers #175",
        "position_in_playlist": 98,
        "description": "The paper \"Animating Elastic Rods with Sound\" is available here:\nhttps://www.cs.cornell.edu/projects/rodsound/\n\nWatch the original video with the sound samples here:\nhttps://www.youtube.com/watch?v=ePySSLiyghs\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1681565/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the series, we talk a lot about photorealistic rendering and making sure that the appearance of our virtual objects is simulated properly. A lot of works on how things look. However, in order to create a more complete sensorial experience, we also have to simulate how these things sound. And today, we're going to have a look at a really cool piece of work that simulates the sound of virtual elastic rods made of aluminum, steel, oak tree, and rubber. And of course, before you ask, this also means that there will be sound simulations of everyone's favorite toy - the walking slinky. As for all papers that have anything to do with sound synthesis, I recommend using a pair of headphones for this episode. The sound emerging from these elastic rods is particularly difficult to simulate because of the fact that sound frequencies vary quite a bit over time, and the objects themselves are also in motion and subject to deformations during the simulation. And as you will see with the slinky, we potentially have tens of thousands of contact events in the meantime. Let's have a look at some results! For the Fellow Scholars who are worried about the validity of these Star Wars sounds, I know you're out there, make sure to watch the video until the end. The authors of the paper proposed a dipole model to create these simulations. Dipoles are typically used to approximate electric and magnetic fields in physics, and in this case, it is really amazing to see an application of it for sound synthesis. For instance, in most cases, these sound waves are typically symmetric around 2D cross sections of these objects, which can be described by a dipole model quite well. Also, it is computationally quite effective and can eliminate these lengthy pre-computation steps that are typically present in previous techniques. There are also comparisons against the state of the art, and we can hear how much richer the sound of this new technique is. And as you know all too well, I love all papers that have something to do with the real world around us. And the reason for this is that we can try the very best kind of validation for these algorithms - and this is, when we let reality be our judge. Some frequency plots are also available to validate the output of the algorithm against the real-world sound samples from the lab. It is really amazing to see that we can use science to breathe more life in our virtual worlds. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6c2T2cykE_A",
        "paper_link": "https://www.cs.cornell.edu/projects/rodsound/",
        "paper_title": "Animating Elastic Rods with Sound"
    },
    {
        "video_id": "343n8xwozJI",
        "video_title": "Interactive Green-Screen Keying | Two Minute Papers #174",
        "position_in_playlist": 99,
        "description": "The paper \"Interactive High-Quality Green-Screen Keying via Color Unmixing\" is available here:\nhttp://people.inf.ethz.ch/aksoyy/keying/\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://flic.kr/p/RiCCF2\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the film industry, we can often see footage of a human walking on the Moon, fighting underwater, or appearing in any environment without actually going there. To do this, a piece of footage of the actor is recorded in front of a green screen, and then, the background of the scene is changed to something else. This process is called green-screen keying, and in theory, this sounds simple enough, but make no mistake - this is a challenging problem. Here's why. Issue number one is that separating the foreground from the background is non-trivial and is not a fully automatic process. Let's call this semi-automatic because the compositing artist starts drawing these separation masks, and even though there is some help from pre-existing software, it still takes quite a bit of manual labor. For instance, in this example, it is extremely difficult to create a perfect separation between the background and the hair of the actor. Our eyes are extremely keen on catching such details, so even the slightest inaccuracies are going to appear as glaring mistakes. This takes a ton of time and effort from the side of the artist, and we haven't even talked about tracking the changes between frames as we're talking about video animations. I think it is now easy to see that this is a hugely relevant problem in the post production of feature films. And now, on to issue number two, which is subtracting indirect illumination from this footage. This is a beautiful light transport effect where the color of different diffuse objects bleed onto each other. In this case, the green color of the background bleeds onto the karate uniform. That is normally a beautiful effect, but here, it is highly undesirable because if we put this character in a different environment, it won't look like it belongs there. It will look more like one of those super fake Photoshop disasters that we see everywhere on the internet. And this technique offers a novel solution to this keying problem. First, we are asked to scribble on the screen and mark the most dominant colors of the scene. This we only have to do once even though we're processing an entire video. As a result, we get an initial map where we can easily fix some of the issues. This is very easy and intuitive, not like those long sessions spent with pixel-by-pixel editing. These colors are then propagated to the entirety of the animation. The final results are compared to a ton of already existing methods on the market and this one smokes them all. However, what is even more surprising is that it is also way better than what an independent artist produced which took ten times that long. Similar comparisons are also made for removing indirect illumination, which is also referred to as color unmixing in the paper. It is also shown that the algorithm is not too sensitive to this choice of dominant colors, so there is room for amazing followup papers to make the process a bit more automatic. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=343n8xwozJI",
        "paper_link": "http://people.inf.ethz.ch/aksoyy/keying/",
        "paper_title": "Interactive High-Quality Green-Screen Keying via Color Unmixing"
    },
    {
        "video_id": "EGnbAgbRIh4",
        "video_title": "Refocusing Videos With Neural Networks | Two Minute Papers #173",
        "position_in_playlist": 100,
        "description": "The paper \"Light Field Video Capture Using a Learning-Based Hybrid Imaging System\" and its implementation is available here:\nhttps://arxiv.org/abs/1705.02997\nhttps://github.com/junyanz/light-field-video\n\nRecommended for you:\nAmazing Slow Motion Videos With Optical Flow - https://www.youtube.com/watch?v=7aLda2E0Yyg\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Steef, Sunil Kim, Torsten Reil, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-272263/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Whenever we take an image with our camera, and look at it after an event, we often feel that many of them are close to perfect, if only it was less blurry, or the focus distance was a bit further away. But the magic moment is now gone, and there is nothing to do other than cursing at the blurry footage that we're left with when showing it to our friends. However, if we have access to light fields, we can change some camera parameters after the photo was taken. This includes changing the focal distance, or even slightly adjusting the viewpoint of the camera. How cool is that! This can be accomplished by a light field camera which is also referred to as a plenoptic camera. This tries to record not only light intensities, but the direction of incoming light as well. Earlier, this was typically achieved by using an array of cameras. That's both expensive and cumbersome. And here comes the problem with using only one light field camera: because of the increased amount of data that they have to record, current light field cameras are only able to take 3 frames per second. That's hardly satisfying if we wish to do this sort of post-editing for videos. This work offers a novel technique to remedy this situation by attaching a standard camera to this light field camera. The goal is that the standard camera has 30, so tons of frames per second, but with little additional information, and the light field camera, which has only a few frames per second, but, with a ton of additional information. If we stitch all this information together in a smart way, maybe it is a possibility to get full light field editing for videos. Earlier, we have talked about interpolation techniques that can fill some of the missing frames in videos. This way, we can fill in maybe every other frame in a footage, or we can be a bit more generous than that. However, if we're shown 3 frames a second, and we have to create a smooth video by filling the blanks would almost be like asking an algorithm to create a movie from a comic book. This would be awesome, but we're not there yet. Too much information is missing. This stitching process works with a bit more information than this, and the key idea is to use two convolutional neural networks to fill in the blanks: one is used to predict flows, which describe the movements and rotations of the objects in the scene, and one to predict the final appearance of the objects. Basically, one for how they move, and one for how they look. And the results are just absolutely incredible. It is also blazing fast and takes less than a tenth of a second to create one of these new views. Here, you can see how the final program is able to change the focal distance of any of the frames in our video, or we can even click on something in the image to get it in focus. And all this is done after the video has been taken. The source code of this project is also available. With some more improvements, this could be tremendously useful in the film industry, because the directors could adjust their scenes after the shooting, and not just sigh over the inaccuracies and missed opportunities. And this is just one of many possible other applications. Absolutely amazing. If you enjoyed this episode, don't forget to subscribe to Two Minute Papers and also make sure to click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=EGnbAgbRIh4",
        "paper_link": "https://arxiv.org/abs/1705.02997",
        "paper_title": "Light Field Video Capture Using a Learning-Based Hybrid Imaging System"
    },
    {
        "video_id": "twWHwVaBfM8",
        "video_title": "Phace: Physics-based Face Modeling and Animation | Two Minute Papers #172",
        "position_in_playlist": 101,
        "description": "The paper \"Phace: Physics-based Face Modeling and Animation\" is available here:\nhttp://lgg.epfl.ch/publications/2017/Phace/index.php\n\nOur Patreon page:\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Sunil Kim, Torsten Reil, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-984031/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about transferring our gestures onto a virtual human's face in a way that is physically correct. This means that not only the changes in the facial geometry are transferred to a digital character, no, no, no. Here's how it works. This piece of work uses a really cool digital representation of our face that contains not only geometry, but there is also information about the bone and flesh and muscle structures as well. This means that it builds on a physically accurate model, which synthesizes animations where this human face is actuated by the appropriate muscles. We start out with a surface scan of the user, which through a registration step, is then converted to a set of expressions that we wish to achieve. The inverse physics module tries to guess exactly which muscles are used and how they are used to achieve these target expressions. The animation step takes the information of how the desired target expressions evolve in time, and some physics information, such as gravity or wind, and the forward physics unit computes the final simulation of the digital character. So while we're talking about the effects of gravity and wind, here you can see how this can create more convincing outputs because these characters really become a part of their digital environment. As a result, the body mass index of a character can also be changed in both directions, slimming or fattening the face. Lip enhancement is also a possibility. If we had super high resolution facial scans, maybe a followup work could simulate the effects of botox injections. How cool would that be? Also, one of my favorite features of this technique is that it also enables artistic editing. By means of drawing, we can also specify a map of stiffness and mass distributions, and if we feel cruel enough, we can create a barely functioning human face to model and animate virtual zombies. Imagine what artists could do with this, especially in the presence of super high resolution textures and photorealistic rendering. Oh, my! Another glimpse of the future of computer graphics and animation. Make sure to have a look at the paper for more applications, for instance, they also demonstrate the possibility of modifying the chin and the jawbone. They even have some result in simulating the effect of Bell's palsy, which is the paralysis of facial muscles on one side. While we're at this high note of illnesses, if you enjoyed this episode and would like to support us, you can pick up really cool perks like early access for all of these episodes on Patreon. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=twWHwVaBfM8",
        "paper_link": "http://lgg.epfl.ch/publications/2017/Phace/index.php",
        "paper_title": "Phace: Physics-based Face Modeling and Animation"
    },
    {
        "video_id": "7x2UvvD48Fw",
        "video_title": "Real-Time Hair Rendering With Deep Opacity Maps | Two Minute Papers #171",
        "position_in_playlist": 102,
        "description": "The paper \"Deep Opacity Maps\" is available here:\nhttp://www.cemyuksel.com/research/deepopacity/\n\nUnofficial implementation:\nhttp://prideout.net/blog/?p=69\n\nRecommended for you:\nThe Dunning-Kruger Effect - https://www.youtube.com/watch?v=4Y7RIAgOpn0\nAre We Living In a Computer Simulation? - https://www.youtube.com/watch?v=ATN9oqMF_qk\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Sunil Kim, Torsten Reil, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1853957/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In earlier episodes, we've seen plenty of video footage about hair simulations and rendering. And today we're going to look at a cool new technique that produces self-shadowing effects for hair and fur. In this image pair, you can see this drastic difference that shows how prominent this effect is in the visual appearance of hair. Just look at that. Beautiful. But computing such a thing is extremely costly. Since we have a dense piece of geometry, for instance, hundreds of thousands of hair strands, we have to know how each one occludes the other ones. This would take hopelessly long to compute. To even get a program that executes in a reasonable amount of time, we clearly need to simplify the problem further. An earlier technique takes a few planes that cut the hair volume into layers. These planes are typically regularly spaced outward from the light sources and it is much easier to work with a handful of these volume segments than with the full geometry. The more planes we use, the more layers we obtain, and the higher quality results we can expect. However, even if we can do this in real time, we will produce unrealistic images when using around 16 layers. Well of course, we should then crank up the number of layers some more! If we do that, for instance by now using 128 layers, we can expect better quality results, but we'll be able to process an image only twice a second, which is far from competitive. And even then, the final results still contain layering artifacts and are not very close to the ground truth. There has to be a better way to do this. And with this new technique called Deep Opacity Maps, these layers are chosen more wisely, and this way, we can achieve higher quality results with only using 3 layers, and it runs easily in real time. It is also more memory efficient than previous techniques. The key idea is that if we look at the hair from the light source's point of view, we can record how far away different parts of the geometry are from the light source. Then, we can create the new layers further and further away according to this shape. This way, the layers are not planar anymore, they adapt to the scene that we have at hand and contain significantly more useful occlusion information. As you can see, this new technique blows all previous methods away and is incredibly simple. I have found an implementation from Philip Rideout, the link to this is available in the video description. If you have found more, let me know and I'll include your findings in the video description for the fellow tinkerers out there. The paper is ample in comparisons, make sure to have a look at that too. And sometimes I get some messages saying \"K\u00e1roly, why do you bother covering papers from so many years ago, it doesn't make any sense!\". And here you can see that part of the excitement of Two Minute Papers is that the next episode can be about absolutely anything. The series has been mostly focusing on computer graphics and machine learning papers, but don't forget, that we also have an episode on whether we're living in a simulation, or the Dunning-Kruger effect and so much more. I've put a link to both of them in the video description for your enjoyment. The other reason for covering older papers is that a lot of people don't know about them and if we can help just a tiny bit to make sure these incredible works see more widespread adoption, we've done our job well. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7x2UvvD48Fw",
        "paper_link": "http://www.cemyuksel.com/research/deepopacity/",
        "paper_title": "Deep Opacity Maps"
    },
    {
        "video_id": "HUFh8cEDeII",
        "video_title": "Visualizing Fluid Flow With Clebsch Maps | Two Minute Papers #170",
        "position_in_playlist": 103,
        "description": "The paper \"Inside Fluids: Clebsch Maps for Visualization and Processing\" and its source code are available here:\nhttp://multires.caltech.edu/pubs/Clebsch.pdf\nhttp://multires.caltech.edu/pubs/ClebschCodes.zip\n\nRecommended for you:\nSchr\u00f6dinger's Smoke - https://www.youtube.com/watch?v=heY2gfXSHBo\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Sunil Kim, Torsten Reil, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2427263/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Everyone who watches this series knows that among many other scientific topics, I am severely addicted to fluid simulations, and today, it's time to relapse! And this time, we're going to run wind tunnel tests on hummingbirds. Typically, when a new engine, airplane, or even a new phone is being designed, we're interested in knowing how the heat flow and dissipation will look like, preferably before we're designing an object. To do so, we often run some virtual wind tunnel tests and optimize our design until we're happy with the results. Then, we can proceed to build these new contraptions. Simulating the pressure distribution and aerodynamic forces is a large topic, however, visualizing these results is at least as well-studied and difficult as writing a simulator. What is it exactly that we're interested in? Even if we have an intuitive particle-based simulation, millions and millions of particles, it is clearly impossible to show the path for every one of them. Grid-based simulations are often even more challenging to visualize well. So how do we choose what to visualize and what not to show on the screen? And in this paper, we can witness a new way of visualizing velocity and vorticity fields. And this visualization happens through Clebsch-maps. This is a mathematical transformation where we create a sphere, and a set of points on this sphere correspond to vortex lines and their evolution over time. However, if instead of only points, we pick an entire region on this sphere, as you can see the north and south pole regions here, we obtain vortex tubes. These vortex tubes provide an accurate representation of the vorticity information within the simulation, and this is one of the rare cases where the validity of such a solution can also be shown. Such a crazy idea, loving it! And with this, we can get a better understanding of the air flow around the wings of a hummingbird, but we can also learn more from pre-existing NASA aircraft datasets. Have a look at these incredible results. Publishing a paper at the SIGGRAPH conference is an incredible feat that typically takes a few brilliant guys and several years of unbelievably hard work. Well, apparently this is not such a challenge for Albert Chern, who was also the first author of this and the Schr\u00f6dinger's Smoke paper just a year ago that we reported on. He is doing incredible work at taking a piece of mathematical theory and showing remarkable applications of it in new areas where we would think it doesn't belong at all. The link is available in the video description, for both this and the previous works, make sure to have a look. There is lots of beautifully written mathematics to be read there that seems to be from another world. It's a truly unique experience. The paper reports that the source code is also available, but I was unable to find it yet. If you have found a public implementation, please let me know and I'll update the video description with your link. Thanks for watching and for your generous support, and I'll see you next time.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HUFh8cEDeII",
        "paper_link": "http://multires.caltech.edu/pubs/Clebsch.pdf",
        "paper_title": "Inside Fluids: Clebsch Maps for Visualization and Processing"
    },
    {
        "video_id": "XgB3Xg5st2U",
        "video_title": "AI Learns Visual Common Sense With New Dataset | Two Minute Papers #169",
        "position_in_playlist": 104,
        "description": "The paper \"The \"something something\" video database for learning and evaluating visual common sense\" is available here:\nhttps://arxiv.org/abs/1706.04261\n\nSource for the video results:\nhttps://medium.com/@raghavgoyal14/7383596f58df\n\nRecommended for you:\nRecurrent Neural Network Writes Sentences About Images - https://www.youtube.com/watch?v=e-WB4lfg30M\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Michael Orenstein, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-569070/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we are going to talk about a new endeavor to teach some more common sense to learning algorithms. If you remember, in an earlier episode, we talked about an excellent work by Andrej Karpathy, who built an algorithm that looked at an input image, and described, in a full, well-formed sentence what is depicted there. By the way, he recently became director of AI at Tesla. Before that, he worked at OpenAI, freshly after graduating with a PhD. Now that is a scholarly career if I've ever seen one! Reading about this earlier work was one of those moments when I really had to hold on to my papers not to fall out of the chair, but of course, as it should be with every new breakthrough, the failure cases were thoroughly discussed. One of the the motivations for this new work is that we could improve the results by creating a video database that contains a ton of commonly occurring events that would be useful to learn. These events include, moving and picking up, or holding, poking, throwing, pouring, or plugging in different things, and much more. The goal is that these neural algorithms would get tons of training data for these, and would be able to distinguish whether a human is showing them something, or just moving things about. The already existing video databases are surprisingly sparse in this sort of information, and in this new, freshly published dataset, we can learn on a 100.000 labeled videos to accelerate research in this direction. I love how many these works are intertwined and how followup research works try to address the weaknesses of previous techniques. Some initial results with learning on this dataset are also reported to kick things off, and they seem quite good if you look at the results here, but since this was not the focus of the paper, we shouldn't expect superhuman performance. However, as almost all papers in research are stepping stones, two more followup papers down the line, this will be an entirely different discussion. I'd love to report back to you on the progress later. Super excited for that. Thanks for watching and for your generous support, and I'll see you next time.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XgB3Xg5st2U",
        "paper_link": "https://arxiv.org/abs/1706.04261",
        "paper_title": "The "
    },
    {
        "video_id": "vzg5Qe0pTKk",
        "video_title": "DeepMind's AI Learns Superhuman Relational Reasoning | Two Minute Papers #168",
        "position_in_playlist": 105,
        "description": "The paper \"A simple neural network module\nfor relational reasoning\" is available here:\nhttps://arxiv.org/abs/1706.01427\n\nDetails on our Patreon page:\nhttps://www.patreon.com/TwoMinutePapers\n\nMore on Long Short-Term Memory:\nRecurrent Neural Network Writes Music and Shakespeare Novels - https://www.youtube.com/watch?v=Jkkjy7dVdaY\nRecurrent Neural Network Writes Sentences About Images - https://www.youtube.com/watch?v=e-WB4lfg30M\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-674828/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is from the Google DeepMind guys, and is about teaching neural networks to be capable of relational reasoning. This means that we can present the algorithm with an image and ask it relatively complex relational questions. For instance, if we show it this image and ask \"What is the color of the object that is closest to the blue object?\", it would answer \"red\". This is a particularly difficult problem because all the algorithm has access to is a bunch of pixels. In computer code, it is near impossible to mathematically express that in an image, something is below or next to something else, especially in 3 dimensional scenes. Beyond a list of colors, this requires a cognitive understanding of the entirety of the image. This is something that we humans are amazingly good at, but computer algorithms are dreadful for this type of work. And this work almost feels like teaching common sense to a learning algorithm. This is accomplished by augmenting and already existing neural network with a relational network module. This is implemented on top of a recurrent neural network that we call long short-term memory or LSTM, that is able to process sequences of information, for instance, an input sentence. The more seasoned Fellow Scholars know that we've talked about LSTMs in earlier episodes, and of course, as always, the video description contains these episodes for your enjoyment. Make sure to have a look, you'll love it. As you can see in this result, this relational reasoning also works for three dimensional scenes as well. The aggregated results in the paper show that this method is not only leaps and bounds beyond the capabilities of already existing algorithms, but, and now, hold on to your papers - in many cases, it also shows superhuman performance. I love seeing these charts in machine learning papers where several learning algorithms and humans are benchmarked on the same tasks. This paper was barely published and there is already a first, unofficial public implementation and two research papers have already referenced it. This is such a great testament to the incredible pace of machine learning research these days. To say that it is competitive would be a huge understatement. Achieving high quality results in relational reasoning is an important cornerstone for achieving general intelligence, and even though there are still much, much more to do, today is one of those days when we can feel that we're a part of the future. The failure cases are also reported in the paper and are definitely worthy of your time and attention. When I asked for permissions to cover this paper in the series, all three scientists from DeepMind happily answered yes within 30 minutes. That's unbelievable. Thanks guys! Also, some of these questions sound like ones that we would get in the easier part of an IQ test. I wouldn't be very surprised to see a learning algorithm complete a full IQ test with flying colors in the near future. If you enjoyed this episode, and you feel that 8 of these videos a month is worth a dollar, please consider supporting us on Patreon. This way, we can make better videos for your enjoyment. We've recently reached a new milestone, which means that part of these funds will be used to empower research projects. Details are available in the video description. Thanks for watching and for your generous support, and I'll see you next time.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=vzg5Qe0pTKk",
        "paper_link": "https://arxiv.org/abs/1706.01427",
        "paper_title": "A simple neural network module\nfor relational reasoning"
    },
    {
        "video_id": "ldO7RD3s4_s",
        "video_title": "Text-based Editing of Audio Narration | Two Minute Papers #167",
        "position_in_playlist": 106,
        "description": "The paper \"VoCo: Text-based Insertion and Replacement in Audio Narration\" is available here:\nhttp://gfx.cs.princeton.edu/pubs/Jin_2017_VTI/\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1109588/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Close enough! As you have probably noticed, today we are going to talk about text to speech, or TTS in short. TTS means that we write a piece of text, and a computer synthesized voice will read it aloud for us. This is really useful for reading the news, or creating audiobooks that don't have any official voiceovers. This work was done by researchers at Princeton University and Adobe and is about text-based audio narration editing. This one is going to be crazy good. The Adobe guys like to call this the Photoshop of voiceovers. In a normal situation, we have access to a waveform and if we wish to change anything in a voiceover, we need to edit it. Editing waveforms by hand is extremely difficult - traditional techniques often can't even reliably find the boundaries between words or letters, let alone edit them. And with this technique, we can cut, copy, and even edit this text and the waveforms will automatically be transformed appropriately using the same voice. We can even use new words that have never been uttered in the original narration. It solves an optimization problem where the similarity, smoothness and the pace of the original footage is to be matched as closely as possible. One of the excellent new features is that we can even choose from several different voicings for the new word and insert the one that we deem the most appropriate. For expert users, the pitch and duration is also editable. It is always important to have a look at a new technique and make sure that it works well in practice, but in science, this is only the first step. There has to be more proof that a new proposed method works well in a variety of cases. In this case, a theoretical proof by means of mathematics is not feasible, therefore a user study was carried out where listeners were shown synthesized and real audio samples and had to blindly decide which was which. The algorithm was remarkably successful at deceiving the test subjects. Make sure to have a look at the paper in the description for more details. This technique is traditional in a sense that it doesn't use any sort of neural networks, however, there are great strides being made in that area as well, which I am quite excited to show you in future episodes. And due to some of these newer video and audio editing techniques, I expect that within the internet forums, fake news is going to be an enduring topic. I hope that in parallel with better and better text and video synthesis, there will be an arms race with other methods that are designed to identify these cases. A neural detective, if you will. And now, if you excuse me, I'll give this publicly available TTS one more try and see if I can retire from narrating videos. Thanks for watching and for your generous support, and I'll see you next time. Yep. Exact same thing. Bet you didn't even notice it.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ldO7RD3s4_s",
        "paper_link": "http://gfx.cs.princeton.edu/pubs/Jin_2017_VTI/",
        "paper_title": "VoCo: Text-based Insertion and Replacement in Audio Narration"
    },
    {
        "video_id": "oltKUPTBz9Q",
        "video_title": "Efficient Yarn-based Cloth Simulations | Two Minute Papers #166",
        "position_in_playlist": 107,
        "description": "The paper \"Efficient Yarn-based Cloth with Adaptive Contact Linearization\" is available here:\nhttps://www.cs.cornell.edu/projects/YarnCloth/\nhttps://www.cs.cornell.edu/projects/YarnCloth/sg10_acl.pdf\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Kaben Gabriel Nanlohy, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1142179/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about creating stunning cloth simulations that are rich in yarn-to-yarn contact. Normally, this is a challenging problem because finding and simulating all the possible contacts between tens of thousands of interlinked pieces of geometry is a prohibitively long process. Also, due to the many different kinds of possible loop configurations, these contacts can take an awful lot of different shapes, which all need to be taken into consideration. Since we are so used to see these garments moving about in real life, if someone writes a simulator that is off just by a tiny bit, we'll immediately spot the difference. I think it is now easy to see why this is a highly challenging problem. This technique optimizes this process by only computing some of the forces that emerge from these yarns pulling each other, and only trying to approximate the rest. The good news is that this approximation is carried out with temporal coherence. This means that these contact models are retained through time and are only rebuilt when it is absolutely necessary. The regions marked with red in these simulations show the domains that are found to be undergoing significant deformation, therefore we need to focus most of our efforts in rebuilding the simulation model for these regions. Look at these results, this is unbelievable. There is so much detail in these simulations. And all this was done seven years ago. In research and technology, this is an eternity. This just blows my mind. The results are also compared against the expensive reference technique as well. And you can see that the differences are minuscule, but the new, improved technique offers a 4 to 5-time speedup over that. For my research project, I also run many of these simulations myself, and many of these tasks take several all nighters to compute. If someone would say that each of my all-nighters would now count as 5, I'd be absolutely delighted. If you haven't subscribed to the series, please make sure to do so and please also click the bell icon to never miss an episode. We have tons of awesome papers to come in the next few episodes. Looking forward to seeing you there! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=oltKUPTBz9Q",
        "paper_link": "https://www.cs.cornell.edu/projects/YarnCloth/",
        "paper_title": "Efficient Yarn-based Cloth with Adaptive Contact Linearization"
    },
    {
        "video_id": "SauCsNkGr-E",
        "video_title": "Iridescent Light Simulations | Two Minute Papers #165",
        "position_in_playlist": 108,
        "description": "The paper \"A Practical Extension to Microfacet Theory for the Modeling of Varying Iridescence\" and its source code is available here:\nhttps://belcour.github.io/blog/research/2017/05/01/brdf-thin-film.html\n\nAdditional reading:\n1. http://www.care2.com/greenliving/amazing-iridescent-fruit-worlds-most-intense-color.html\n2. https://academy.allaboutbirds.org/how-birds-make-colorful-feathers/\n3. http://www.cam.ac.uk/research/news/african-fruit-brightest-thing-in-nature-but-does-not-use-pigment-to-create-its-extraordinary-colour\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2139279/\nPollia condensata fruit image credit: Silvia Vignolini - http://www.cam.ac.uk/research/news/african-fruit-brightest-thing-in-nature-but-does-not-use-pigment-to-create-its-extraordinary-colour\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. From many fond childhood memories, most of us are quite fond of the colorful physical appearance of bubbles and fuel-water mixtures. We also know surprisingly little about this peculiar phenomenon where the color of an object changes when we either turn our head or change the lighting. This happens in a very colorful manner, and physicists like to call this iridescence or goniochromism. What is even less known, is that if we try to use a light simulation program to make an image with leather, we'll be surprised to see that it also shows a pronounced goniochromatic effect. An even more less known fact is that quite a few birds, insects, minerals, sea shells, and even some fruits are iridescent as well. I've added links to some really cool additional readings to the video description for your enjoyment. This effect is caused by materials that scatter different colors of light in different directions. A white incoming light is therefore scattered not in one direction, but in a number of different directions, sorted by their colors. This is why we get these beautiful, rainbow-colored patterns that we all love so much. Now that we know what iridescence is, the next step is obviously to infuse our light simulation programs to have this awesome feature. This paper is about simulating this effect with microfacets, which are tiny microstructures on the surface of rough objects. And with this, it is now suddenly possible to put a thin iridescent film onto a virtual object and create a photorealistic image out of it. If you are into math and would like to read about some tasty spectral integration in the frequency space with Fourier transforms, this paper is for you. If you are not a mathematician, also make sure to have a look because the production quality of this paper is through the roof. The methodology, derivations, comparisons are all really crisp. Loving it. If you have a look, you will get a glimpse of what it takes to create a work of this quality. This is one of the best papers in photorealistic rendering I've seen in a while. In the meantime, I am getting more and more messages from you Fellow Scholars who tell their stories on how they chose to turn their lives around and started studying science because of this series. Wow. That's incredibly humbling, and I really don't know how to express my joy for this. I always say that it's so great to be a part of the future, and I am delighted to see that some of you want to be a part of the future and not only as an observer, but as a research scientist. This sort of impact is stronger than the absolute best case scenario I have ever dreamed of for the series. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SauCsNkGr-E",
        "paper_link": "https://belcour.github.io/blog/research/2017/05/01/brdf-thin-film.html",
        "paper_title": "A Practical Extension to Microfacet Theory for the Modeling of Varying Iridescence"
    },
    {
        "video_id": "R5t74AC6I0A",
        "video_title": "Simulating Cuts On Virtual Bodies | Two Minute Papers #164",
        "position_in_playlist": 109,
        "description": "The paper \"Robust eXtended Finite Elements for Complex Cutting of Deformables\" is available here:\nhttps://www.animation.rwth-aachen.de/publication/0551/\nhttps://animation.rwth-aachen.de/media/papers/2017-SIGGRAPH-XFEM.pdf\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-185456/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about the absolute favorite thing of computer graphics researchers: destroying virtual objects in the most creative ways. This is the only place on Earth where words like deformable bodies and cutting can be used in the same sentence and be delighted about it. This time around, we are going to cut and dismember every virtual object that stands in our way. And then some more. In these animations, we have complex 3D geometry, and the objective is to change these geometries in a way that remains physically correct even in the presence of complex cut surfaces. When such a cut happens, traditional techniques typically delete and duplicate parts of the geometry close to the cut. This is a heavily simplified solution that leads to inaccurate results. Other techniques try to rebuild parts of the geometry that are affected by the cut. This is what computer graphics researchers like to call remeshing, and it works quite well, but it takes ages to perform. Also, is still has drawbacks, for instance, quantities like temperature and deformations also have to be transferred to the new geometry, which is non-trivial to execute properly. In this work, a new technique is proposed that is able to process really complex cuts without creating new geometry. No remeshing takes place, but the mass and stiffness properties of the materials are retained correctly. Also, the fact that it minimizes the geometric processing overhead leads to a not only simpler, but a more efficient solution. There is so much visual detail in the results that I could watch this video ten times and still find something new in there. There are also some horrifying, Game of Thrones kinda experiments in this footage. Watch out! Ouch! The presentation of the results and the part of the video that compares against a previous technique is absolutely brilliant. You have to see it. The paper is also remarkably well written, make sure to have a look at that too. The link is available in the video description. I am really itching to make some longer videos where we can go into some of these derivations and build a strong intuitive understanding of them. That sounds like a ton of fun, and if this could ever become a full time endeavor, I am more than enthused to start doing more and work on bonus videos like that. If you enjoyed this episode, don't forget to hit the like button and subscribe to the series. Normally, it's up to YouTube to decide whether you get a notification or not, so make sure to click the bell icon as well to never miss a Two Minute Papers episode. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=R5t74AC6I0A",
        "paper_link": "https://www.animation.rwth-aachen.de/publication/0551/",
        "paper_title": "Robust eXtended Finite Elements for Complex Cutting of Deformables"
    },
    {
        "video_id": "9bcbh2hC7Hw",
        "video_title": "DeepMind's AI Creates Images From Your Sentences | Two Minute Papers #163",
        "position_in_playlist": 110,
        "description": "The paper \"Parallel Multiscale Autoregressive Density Estimation\" is available here:\nhttps://arxiv.org/pdf/1703.03664.pdf\n\nOur Patreon page:\nhttps://www.patreon.com/TwoMinutePapers\n\nScott Reed's results:\nhttps://twitter.com/scott_e_reed/status/841099231666544640\nhttps://twitter.com/scott_e_reed/status/841098907887235076\n\nThe older work, PixelCNN is available here:\nhttps://arxiv.org/pdf/1606.05328.pdf\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1208035/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is one of those new, absolutely insane papers from the Google DeepMind guys. You are going to see a followup work to an algorithm that looks at a bunch of images and from that, it automatically learns the concept of birds, human faces or coral reefs, so much so that we're able to write a new sentence, and it will generate a new, close to photorealistic image from this written description. This network is capable of creating images that are significantly different than the ones it has been trained on. This already sounds like science fiction. Completely unreal. This work goes by the name PixelCNN. We'll discuss a followup work to that in a moment. The downside of this method is that these images are generated pixel by pixel, and many of these pixels depend on their neighborhoods. For instance, if I start to draw one pixel of the beak of a bird, the neighboring pixels have to adhere to this constraint and have to be the continuation of the beak. Clearly, these images have a lot of structure. This means that we cannot do this process in parallel, but create these new images one pixel at a time. This is an extremely slow and computationally expensive process, and hence, the original paper showed results with 32x32 and 64x64 images at most. As we process everything sequentially, the execution time of the algorithm scales linearly with the number of pixels we can generate. It is like a factory where there are a ton of assembly lines, but only one person to run around and operate all of them. Here, the goal was to start generating different regions of these images independently, but only in cases when these pixels are not strongly correlated. For instance, doing this with neighbors is a no-go. This is possible, but extremely challenging, and the paper contains details on how to select these pixels and when we can pretend them to be independent. And now, feast your eyes upon these spectacular results. If we're looking for \"A yellow bird with a black head, orange eyes and an orange bill\", we're going to see much more detailed images. The complexity of the new algorithm scales with the number of pixels not linearly, but in a logarithmic manner, which is basically the equivalent of winning the jackpot in terms of parallelization, and it often results in a more than 100 times speedup. This is a factory that's not run by one guy, but one that works properly. The lead author, Scott Reed has also published some more amazing results on twitter as well. In these examples, we can see the evolution of the final image that is generated by the network. It is an amazing feeling to be a part of the future. And note that there is a ton of challenges with the idea, this is one of those typical cases when the idea is only the first step, and execution is king. Make sure to have a look at the paper for more details. According to our regular schedule, we try our best to put out two videos every week. That's eight episodes a month. If you feel that eight of these episodes is worth a dollar for you, please consider supporting us on Patreon. This way, we can create more elaborate episodes for you. The channel is growing at a remarkable rate, and your support has been absolutely amazing. I am honored to have an audience like you Fellow Scholars. We are quite close to hitting our next milestone. And this milestone will be about giving back more to the scientific community and empowering other research projects. I've put a link to our Patreon page with the details in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9bcbh2hC7Hw",
        "paper_link": "https://arxiv.org/pdf/1703.03664.pdf",
        "paper_title": "Parallel Multiscale Autoregressive Density Estimation"
    },
    {
        "video_id": "wlAgyf_e-hA",
        "video_title": "Style Transfer For Fluid Simulations | Two Minute Papers #162",
        "position_in_playlist": 111,
        "description": "The paper \"Stylized Keyframe Animation of Fluid Simulations\" is available here:\nhttp://gfx.cs.princeton.edu/pubs/Browning_2014_SKA/index.php\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1330662/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We've seen a lot of fluid and smoke simulations throughout the series. In each of these cases, the objective was to maximize the realism of these animations, often to the point where they are indistinguishable from reality. However, there are cases where creating photorealistic footage is not the main objective. Artists often seek to imbue these fluid and smoke simulations with their own distinctive style, and this style needs not to be photorealistic. It can be cartoonish, black and white, or take a variety of different color schemes. But unfortunately, to obtain such an effect, we have to sit down, get a bunch of papers, and draw the entirety of the animation frame by frame. And of course, to accomplish this, we also need to be physicists and know the underlying laws of fluid dynamics. That's not only borderline impossible, but extremely laborious as well. It would be really cool to have an algorithm that is somehow able to learn our art style and apply it to a fluid or smoke simulation sequence. But the question is, how do we exactly specify this style? Have a look at this really cool technique, I love the idea behind it: first we compute a classical smoke simulation, then, we freeze a few frames and get the artist to colorize them. After that, the algorithm tries to propagate this artistic style to the entirety of the sequence. Intuitively, this is artistic style transfer for fluid animations, but, without using any machine learning techniques. Here, we are doing patch-based regenerative morphing. This awesome term refers to a technique that is trying to understand the direction of flows and advect the colored regions according to it in a way that is both visually and temporally coherent. Visually coherent means that it looks as close to plausible as we can make it, and temporally coherent means that we're not looking only at one frame, but a sequence of frames, and the movement through these neighboring frames has to be smooth and consistent. These animation sequences were created from 8 to 9 colorized frames, and whatever you see happening in between was filled in by the algorithm. And again, we're talking about the artistic style here, not the simulation itself. A fine, handcrafted work in the world dominated by advanced learning algorithms. This paper is a bit like a beautiful handmade automatic timepiece in the era of quartz watches. If you enjoyed this episode, please make sure to leave a like on the video, and don't forget to subscribe to get a glimpse of the future on the channel, twice a week. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wlAgyf_e-hA",
        "paper_link": "http://gfx.cs.princeton.edu/pubs/Browning_2014_SKA/index.php",
        "paper_title": "Stylized Keyframe Animation of Fluid Simulations"
    },
    {
        "video_id": "Fevg4aowNyc",
        "video_title": "AI Learns To Create User Interfaces (pix2code) | Two Minute Papers #161",
        "position_in_playlist": 112,
        "description": "The paper \"pix2code: Generating Code from a Graphical User Interface Screenshot\" is available here:\nhttps://arxiv.org/abs/1705.07962\nhttps://github.com/tonybeltramelli/pix2code\n\nRecommended for you:\nRecurrent Neural Network Writes Music and Shakespeare Novels -\nhttps://www.youtube.com/watch?v=Jkkjy7dVdaY\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credits: https://pixabay.com/photo-583839/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Creating applications for mobile Android and iOS devices is a laborious endeavor which most of the time, includes creating a graphical user interface. These are the shiny front-end interfaces that enable the user to interact with the back-end of our applications. So what about an algorithm that learns how to create these graphical user interfaces and automates part of this process? This piece of work takes one single input image that we can trivially obtain by making a screenshot of the user interface, and it almost immediately provides us with the code that is required to recreate it. What an amazing idea! The algorithm supports several different target platforms. For instance, it can give us code for iOS and Android devices. This code we can hand over to a compiler which will create an executable application. This technique also supports html as well for creating websites with the desired user interface. Under the hood, a domain specific language is being learned, and using this, it is possible to have a concise text representation of a user interface. Note that's by no means the only use of domain specific languages. The image of the graphical user interface is learned by a classical convolutional neural network, and this text representation is learned by a technique machine learning researchers like to call Long Short Term Memory. LSTM in short. This is a neural network variant that is able to learn sequences of data and is typically used for language translation, music composition, or learning all the novels of Shakespeare and writing new ones in his style. If you were wondering why these examples are suspiciously specific, we've had an earlier episode about this, I've put a link to it in the video description. Make sure to have a look, you are going to love it. Also, this year it will have its twentieth year anniversary. Live long and prosper, little LSTM! Now, I already see the forums go up in flames. Sweeping generalizations, far-reaching statements on front end developers around the world getting fired and all that. I'll start out by saying that I highly doubt that this work would mean the end of front end development jobs in the industry. However, what I do think is that with a few improvements, it can quickly prove its worth by augmenting human labor and cutting down the costs of implementing graphical user interfaces in the future. This is another testament to the variety of tasks modern learning algorithms can take care of. The author also has a GitHub repository with a few more clarifications, stating that the source code of the project and the dataset will be available soon. Tinkerers rejoice! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Fevg4aowNyc",
        "paper_link": "https://arxiv.org/abs/1705.07962",
        "paper_title": "pix2code: Generating Code from a Graphical User Interface Screenshot"
    },
    {
        "video_id": "4Df_BluxwkU",
        "video_title": "Simulating Wet Sand | Two Minute Papers #160",
        "position_in_playlist": 113,
        "description": "The paper \"Multi-species simulation of porous sand and water mixtures\" is available here:\nhttp://web.cs.ucla.edu/~cffjiang/research/wetsand/wetsand_siggraph17.pdf\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nIf you're looking for some additional amusement:\n1. An even slower motion version of the main scene: https://twitter.com/karoly_zsolnai/status/872497135287140353\n2. Watch the citation (\"Source: [...]\") at the bottom left throughout the video.\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-192988/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. After around 160 episodes into Two Minute Papers, I think it is no secret to anyone that I am helplessly addicted to fluid simulations, so you can already guess what this episode will be about. I bet you will be as spellbound by this beautiful footage of wet sand simulations as I was when I've first seen it. Before you ask, yes, I have attempted to prepare some slow-motion action too! As you remember, simulating the motion of fluids involves solving equations that tell us how the velocity and the pressure evolves in time. Now, the 3D world we live in is a continuum, and we cannot solve these quantities everywhere because that would take an infinite amount of time. To alleviate this, we can put a grid in our virtual world and obtain these quantities only in these gridpoints. The higher the resolution the grid is, the more realistic the animations are, but the computation time also scales quite poorly. It is really not a surprise that we have barely seen any wet sand simulations in the visual effects industry so far. Here, we have an efficient algorithm to handle these cases, and as you will see, this is not only extremely expensive to compute, but nasty stability issues also arise. Have a look at this example here. These are sand simulations with different cohesion values. Cohesion means the strength of intermolecular forces that hold the material together. The higher cohesion is, the harder it is to break the sand up, the bigger the clumps are. This is an important quantity for our simulation because the higher the water saturation of this block of sand, the more cohesive it is. Now, if we try to simulate this effect with traditional techniques on a coarse grid, we'll encounter a weird phenomenon: namely, the longer our simulation runs, the larger the volume of the sand becomes. An excellent way to demonstrate this phenomenon is using these hourglasses, where you can clearly see that after only a good couple turns, the amount of sand within is significantly increased. This is particularly interesting, because normally, in classical fluid simulations, if our grid resolution is insufficient, we typically encounter water volume dissipation, which means that the total amount of mass in the simulation decreases over time. Here, we have the exact opposite, like in a magic trick, after every turn, the volume gets inflated. That's a really peculiar and no less challenging problem. This issue can be alleviated by using a finer grid, which is, as we know, extremely costly to compute, or, the authors proposed a volume fixing method to take care of this without significantly increasing the execution time of the algorithm. Make sure to have a look at the paper, which is certainly my kind of paper: lots of beautiful physics and a study on how to solve these equations so that we can obtain an efficient wet sand simulator. And also, don't forget, a fluid paper a day keeps the obsessions away. In the meantime, a word about the Two Minute Papers shirts. I am always delighted to see you Fellow Scholars sending over photos of yourselves proudly posing with your newly obtained shirts for the series. Thanks so much and please, keep them coming! They are available through twominutepapers.com for the US, and the EU and worldwide link is also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4Df_BluxwkU",
        "paper_link": "https://shop.spreadshirt.net/TwoMinutePapers/\n\nIf you're looking for some additional amusement:\n1. An even slower motion version of the main scene:",
        "paper_title": "Multi-species simulation of porous sand and water mixtures"
    },
    {
        "video_id": "UjuBLS15JqM",
        "video_title": "Algorithmic Beautification of Selfies | Two Minute Papers #159",
        "position_in_playlist": 114,
        "description": "The paper \"Perspective-aware Manipulation of Portrait Photos\" and its demo is available here:\nhttp://gfx.cs.princeton.edu/pubs/Fried_2016_PMO/index.php\nhttp://faces.cs.princeton.edu/\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-465563/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we're going to talk about a rigorous scientific topic, none other than the creation of the perfect selfie photo. By definition selfies are made by us, which means that these are typically short-range photos, and due to the perspective distortion of the camera lens, we often experience unpleasant effects like the heavy magnification the nose and the forehead. And get this, this technique enables us to take a photo and after that, edit the perceived camera distance for it without changing anything else. Basically, algorithmic beautification! This technique works the following way: we analyze the photo and try to figure out how distant the camera was when the photo was taken. Then, we create a digital model of the perspective camera and create a 3D model of the face. This is a process that mathematicians like to call fitting. It means that if we know the optics of perspective cameras, we can work backwards from the input photo that we have, and find an appropriate setup that would result in this photo. Then, we will be able to adjust this distance to even out the camera lens distortions. But that's not all, because as we have a digital 3D model of the face, we can do even more. For instance, we can also rotate it around in multiple directions. To build such a 3D model, we typically try to locate several well-recognizable hotspots on the face, such as the chin, eyebrows, nose stem, the region under the nose, eyes and lips. However, as these hotspots lead to a poor 3D representation of the human face, the authors added a few more of these hotspots to the detection process. This still takes less than 5 seconds. Earlier, we also talked about a neural network-based technique that judged our selfie photos by assigning a score to them. I would absolutely love to see how that work would react to a before and after photo that comes from this technique. This way, we could formulate this score as a maximization problem, and as a result, we could have an automated technique that truly creates the perfect selfie photo through these warping operations. The best kind of evaluation is when we let reality be our judge, and use images that were taken closer or farther away and compare the output of this technique against them. These true images bear the ground truth label throughout this video. The differences are often barely perceptible, and to provide a better localization of the error, some difference images are shown in the paper. If you are into stereoscopy, there is also an entire section about that as well. The authors also uploaded an interactive version of their work online that anyone can try, free of charge. So as always, your scholarly before and after selfie experiments are more than welcome in the comments section. Whether you are already subscribed to the series or just subscribing now, which you should absolutely do, make sure to click the bell icon to never miss an episode. We have lots of amazing works coming up in the next few videos. Hope to see you there again. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UjuBLS15JqM",
        "paper_link": "http://gfx.cs.princeton.edu/pubs/Fried_2016_PMO/index.php",
        "paper_title": "Perspective-aware Manipulation of Portrait Photos"
    },
    {
        "video_id": "ZEjUqZU1hNQ",
        "video_title": "Simulating Honey Coiling | Two Minute Papers #158",
        "position_in_playlist": 115,
        "description": "The paper \"Variational Stokes: A Unified Pressure-Viscosity Solver for Accurate Viscous Liquids\" is available here:\nhttps://cs.uwaterloo.ca/~elariono/stokes/index.html\n\nRecommended for you:\nSimulating Viscosity and Melting Fluids - https://www.youtube.com/watch?v=KgIrnR2O8KQ\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1006972/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This episode is about simulating a beautiful phenomenon in nature, the buckling and coiling effect of honey. Mmm! This effect is due to the high viscosity of materials like honey, which means that they are highly resistant against deformation. Water, however, is much less viscous as it is held together by weaker intermolecular forces, therefore it is easier to deform, making it so easy to pour it into a glass. We had an earlier episode on honey buckling, and as every seasoned Fellow Scholar already knows, the link is available in the video description. One key difference of this work is that the older solution was built upon a Lagrangian approach, which means that the simulation consists of computing the velocities and the pressure that acts on these particles. It is a particle-based simulation. Here, a solution is proposed for the Eulerian approach, which means that we do not compute these quantities everywhere in the continuum of space, but we use a fine 3D grid, and we compute these quantities only in these gridpoints. No particles to be seen anywhere. There are mathematical techniques to try to guess what happens between these individual gridpoints, and this process is referred to as interpolation. So normally, in this grid-based approach, if we wish to simulate such a buckling effect, we'll be sorely disappointed because what we will see is that the surface details rapidly disappear due to the inaccuracies in the simulation. The reason for this is that the classical grid-based simulators utilize a technique that mathematicians like to call operator splitting. This means that we solve these fluid equations by taking care of advection, pressure, and viscosity separately. Separate quantities, separate solutions. This is great, because it eases the computational complexity of the problem, however, we have to pay a price for it in the form of newly introduced inaccuracies. For instance, some kinetic and shear forces are significantly dampened, which leads to a loss of detail for buckling effects with traditional techniques. This paper introduces a new way of efficiently solving these operators together in a way that these coupling effects are retained in the simulation. The final solution not only looks stable, but is mathematically proven to work well for a variety of cases, and it also takes into consideration collisions with other solid objects correctly. I absolutely love this, and anyone who is in the middle of creating a new movie with some fluid action going on has to be all over this new technique. And, the paper is absolutely amazing. It contains crystal clear writing, many paragraphs are so tight that I'd find it almost impossible to cut even one word from them, yet it is still digestible and absolutely beautifully written. Make sure to have a look, as always, the link is available in the video description. These amazing papers are stories that need to be told to everyone. Not only to experts. To everyone. And before creating these videos, I always try my best to be in contact with the authors of these works. And nowadays, many of them are telling me that they were really surprised by the influx of views they got after they were showcased in the series. Writing papers that are featured in Two Minute Papers takes a ridiculous amount of hard work, and after that, the researchers make them available for everyone free of charge. And now, I am so glad to see them get more and more recognition for their hard work. Absolutely amazing. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZEjUqZU1hNQ",
        "paper_link": "https://cs.uwaterloo.ca/~elariono/stokes/index.html",
        "paper_title": "Variational Stokes: A Unified Pressure-Viscosity Solver for Accurate Viscous Liquids"
    },
    {
        "video_id": "5vpklJw7uL0",
        "video_title": "Designing Decorative Joinery for Furniture | Two Minute Papers #157",
        "position_in_playlist": 116,
        "description": "The paper \"Interactive Design and Stability Analysis of\nDecorative Joinery for Furniture\" is available here:\nhttps://jiaxianyao.github.io/joinery/\n\nNote: SketchUp is no longer owned by Google and is now called SketchUp 3D.\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about designing and creating furniture with pieces that are geometrically interlocked. Such pieces not only have artistic value, but such structures can also enhance the integrity and sturdiness of a piece of furniture. This piece of work takes a simple 2D drawing of this interlocking structure and assembles the required pieces for us to build a 3D model from them. This drawing can be done with one of the most user friendly modeler program out there, Google SketchUp. This can be used even by novices. From these disassembled parts, it is highly non-trivial to create a 3D printable model. For instance, it is required that these pieces can be put together with one translational motion. Basically, all we need is one nudge to put two of these pieces together. If you ever had a new really simple piece of furniture from Ikea, had a look at the final product at the shop, and thought, \"well, I only have 10 minutes to put this thing together, but anyway, how hard can it be\"? And you know, 3 hours of cursing later, the damn thing is still not completely assembled. If you had any of those experiences before, this one push assembly condition is for you. And the algorithm automatically finds a sequence of motions that assembles our target 3D shape, and because we only have 2D information from the input, it also has to decide how and where to extrude, thicken, or subtract from these volumes. The search space of possible motions is immense, and we have to take into consideration that we don't even know if there is a possible solution for this puzzle at all! If this the case, the algorithm finds out and proposes changes to the model that make the construction feasible. And if this wasn't enough, we can also put this digital furniture model into a virtual world where gravitational forces are simulated to see how stable the final result is. Here, the proposed yellow regions indicate that the stability of this table could be improved via small modifications. It is remarkable to see that a novice user who has never done a minute of 3D modeling can create such a beautiful and resilient piece of furniture. Really, really nice work. Loving it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=5vpklJw7uL0",
        "paper_link": "https://jiaxianyao.github.io/joinery/",
        "paper_title": "Interactive Design and Stability Analysis of\nDecorative Joinery for Furniture"
    },
    {
        "video_id": "jDxsGW5KUP0",
        "video_title": "Self-Illuminating Explosions | Two Minute Papers #156",
        "position_in_playlist": 117,
        "description": "The paper \"Lighting Grid Hierarchy\nfor Self-illuminating Explosions\" is available here:\nhttp://www.cemyuksel.com/research/lgh/\n\nRendering course at the Technical University of Vienna:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nOur light transport-related episodes:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgk1hb8lyu6sTYsLL39r_3bG\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\nIf you don't mind, make sure to send us a picture of yourself with a piece of merch!\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-2262295/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we're going to talk about explosions. To be more precise, imagine that we already have the physics simulation data for an explosion on our computer, but we would like to visualize it on our screen. This requires a light simulation program that is able to create an image of this virtual scene that looks exactly the same as it would in reality. We have had plenty of earlier episodes on light transport, and as you know all too well, it is one of my favorite topics. I just can't get enough of it. I've put a link to these related episodes in the video description. If we wish to render a huge smoke plume, we perform something that computer graphics people call volumetric light transport. This means that a ray of light doesn't necessarily bounce off of the surface of materials, but it can penetrate their surfaces and scatter around inside of them. A technique that can deal with this is called volumetric path tracing, and if we wish to create an image of an explosion using that, well, better pack some fast food because it is likely going to take several hours. The explosion in this image took 13 hours and it is still not rendered perfectly. But this technique is able to solve this problem in 20 minutes, which is almost 40 times quicker. Unbelievable. The key idea is that this super complicated volumetric explosion data can be reimagined as a large batch of point light sources. If we solve this light transport problem between these point light sources, we get a solution that is remarkably similar to the original solution with path tracing, however, solving this new representation is much simpler. But that's only the first step. If we have a bunch of light sources, we can create a grid structure around them, and in these gridpoints, we can compute shadows and illumination in a highly efficient manner. What's more, we can create multiple of these grid representations. They all work on the very same data, but some of them are finer, and some of them are significantly sparser, more coarse. Another smart observation here is that even though sharp, high-frequency illumination details need to be computed on this fine grid, which takes quite a bit of computation time, it is sufficient to solve the coarse, low-frequency details on one of these sparser grids. The results look indistinguishable from the ground truth solutions, but the overall computation time is significantly reduced. The paper contains detailed comparisons against other techniques as well. Most of these scenes are rendered using hundreds of thousands of these point light sources, and as you can see, the results are unbelievable. If you would like to learn even more about light transport, I am holding a Master-level course on this at the Vienna University of Technology in Austria. I thought that the teachings should not only be available for those 30 people who sit in the room, who can afford a university education. It should be available for everyone. So, we made the entirety of the lecture available for everyone, free of charge, and I am so glad to see that thousands of people have watched it, and to this day I get many messages that they enjoyed it and now they see the world differently. It was recorded live with the students in the room, and it doesn't have the audio quality of Two Minute Papers. However, what it does well, is it conjures up the atmosphere of these lectures and you can almost feel like one of the students sitting there. If you're interested, have a look, the link is available in the video description! And make sure to read this paper too, it's incredible. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=jDxsGW5KUP0",
        "paper_link": "http://www.cemyuksel.com/research/lgh/",
        "paper_title": "Lighting Grid Hierarchy\nfor Self-illuminating Explosions"
    },
    {
        "video_id": "ugdciqeOPeM",
        "video_title": "Simulating Liquid-Hair Interactions | Two Minute Papers #155",
        "position_in_playlist": 118,
        "description": "Our Patreon page:\nhttps://www.patreon.com/TwoMinutePapers\n\nThe paper \"A Multi-Scale Model for Simulating Liquid-Hair Interactions\", and its source code is available here:\nhttp://www.cs.columbia.edu/cg/liquidhair/\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-697927/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We already know that by using a computer, we can simulate fluids and we can simulate hair. But what about simulating both of them at the same time? This paper is about liquid-hair interaction and simulating the dynamics of wet hair. Our seasoned Fellow Scholars immediately know that this episode is going to be ample in amazing slow motion footage. I hope I didn't mess up with any of them, you will see soon if this is the case or not! Before we start talking about it, I'd like to note the following remarkable features: the authors uploaded a supplementary video in 4k resolution, executable files for their technique for all 3 major operating systems, data assets, and they also freshly revealed the full source code of the project. Hell yeah! I feel in heaven. A big, Two Minute Papers style hat tip to the authors for this premium quality presentation. If this paper were a car, it would definitely be a Maserati or a Mercedes. This technique solves the equations for liquid motion along every single hair strand, computes the cohesion effects between the hairs, and it can also simulate the effect of water dripping off the hair. Feast your eyes on these absolutely incredible results. The main issue with such an idea is that the theory of large, and small-scale simulations are inherently different, and in this case, we need both. The large-scale simulator would be a standard program that is able to compute how the velocity and pressure of the liquid evolves in time. However, we also wish to model the water droplets contained within one tiny hair strand. With a large-scale simulator, this would take a stupendously large amount of time and resources, so the key observation is that a small-scale fluid simulator program would be introduced take care of this. However, these two simulators cannot simply coexist without side effects. As they are two separate programs that work on the very same scene, we have to make sure that as we pass different quantities between them, they will still remain intact. This means that a drop of water that gets trapped in a hair strand has to disappear from the large-scale simulator, and has to be readded to it when it drips out. This is a remarkably challenging problem. But with this, we only scratched the surface. Make sure to have a look at the paper that has so much more to offer, it is impossible to even enumerate the list of contributions within in such a short video. The quality of this paper simply left me speechless and I would encourage you to take a look as well. And while this amazing footage is rolling, I would like to let you know that Two Minute Papers can exist because of your support through Patreon. Supporters of the series gain really cool perks like watching every single one of these episodes in early access. I am super happy to see how many of you decided to support the series, and in return, we are able to create better and better videos for you. Thank you again, you Fellow Scholars are the most amazing audience. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ugdciqeOPeM",
        "paper_link": "http://www.cs.columbia.edu/cg/liquidhair/",
        "paper_title": "A Multi-Scale Model for Simulating Liquid-Hair Interactions"
    },
    {
        "video_id": "wlndIQHtiFw",
        "video_title": "Real-Time Character Control With Phase-Functioned Neural Networks | Two Minute Papers #154",
        "position_in_playlist": 119,
        "description": "The paper \"Phase-Functioned Neural Networks for Character Control\" is available here:\nhttp://theorangeduck.com/page/phase-functioned-neural-networks-character-control\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1835354/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this piece of work, we seek to control digital characters in real-time. It happens the following way: we specify a target trajectory, and the algorithm has to synthesize a series of motions that follows that path. To make these motions as realistic as possible, this is typically accomplished by unleashing a learning algorithm on a large database that contains a ton of motion information. Previous techniques did not have a good understanding of these databases and they often synthesized motions from pieces that corresponded to different kinds of movements. This lack of understanding results in stiff, unnatural output motion. Intuitively, it is a bit like putting together a sentence from a set of letters that were cut out one by one from different newspaper articles. It is a fully formed sentence, but it lacks the smoothness and the flow of a properly aligned piece of text. This is a neural network based technique that introduces a phase function to the learning process. This phase function augments the learning with the timing information of a given motion. With this phase function, the neural network recognizes that we are not only learning periodic motions, but it knows when these motions start and when they end. The final technique takes very little memory, runs in real time, and it accomplishes smooth walking, running, jumping and climbing motions and so much more over a variety of terrains with flying colors. In a previous episode, we have discussed a different technique that accomplished something similar with a low and high level controller. One of the major selling points of this technique is that this one offers a unified solution for terrain traversal with using only one neural network. This has the potential to make it really big on computer games and real-time animation. It is absolutely amazing to witness this and be a part of the future. Make sure to have a look at the paper, which also contains the details of a terrain fitting step to make this learning algorithm capable of taking into consideration a variety of obstacles. I would also like to thank Claudio Pannacci for his amazing work in translating so many of these episodes to Italian. This makes Two Minute Papers accessible for more people around the globe, and the more people we can reach, the happier I am. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wlndIQHtiFw",
        "paper_link": "http://theorangeduck.com/page/phase-functioned-neural-networks-character-control",
        "paper_title": "Phase-Functioned Neural Networks for Character Control"
    },
    {
        "video_id": "2vnLBb18MuQ",
        "video_title": "Digital Creatures Learn to Navigate in 3D | Two Minute Papers #153",
        "position_in_playlist": 120,
        "description": "The paper \"DeepLoco: Dynamic Locomotion Skills\nUsing Hierarchical Deep Reinforcement Learning\" is available here:\nhttp://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/index.html\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1505714/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Earlier, we have talked about a few amazing algorithms to teach digital creatures to walk. And this time, we're interested in controlling the joints of a digital character to not only walk properly, but take into consideration its surroundings. This new version can navigate in 3D with static and dynamic, moving obstacles, or even dribble a ball toward a target. Loving the execution and the production value of this paper. This is accomplished by an efficient system that consists of two controllers that are represented by learning algorithms. One, the low level controller is about about maintaining balance and proper limb control by manipulating the joint positions and velocities appropriately. This controller operates on a fine time scale, thirty times per second and is trained via a 4-layer neural network. Two, the high level controller can accomplish bigger overarching goals, such as following a path, or avoiding static and dynamic obstacles. We don't need to run this so often, therefore to save resources, this controller operates on a coarse time scale, only twice each second and is trained via a deep convolutional neural network. It also has support for a a small degree of transfer learning. Transfer learning means that after successfully learning to solve a problem, we don't have to start from scratch for the next one, but we can reuse some of that valuable knowledge and get a headstart. This is a heavily researched area and is likely going to be one of the major next frontiers in machine learning research. Now, make no mistake, it is not like transfer learning is suddenly to be considered a solved problem - but in this particular case, it is finally a possibility. Really cool! I hope this brief expos\u00e9 fired you up too. This paper is a bomb, make sure to have a look, as always, the link is available in the video description. And by the way, with your support on Patreon, we will soon be able to spend part of our budget on empowering research projects. How amazing is that? The new Two Minute Papers shirts are also flying off the shelves! Happy to hear you're enjoying them so much. If you're interested, hit up http://twominutepapers.com if you're located in the US. The EU and worldwide store's link is also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2vnLBb18MuQ",
        "paper_link": "http://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/index.html",
        "paper_title": "DeepLoco: Dynamic Locomotion Skills\nUsing Hierarchical Deep Reinforcement Learning"
    },
    {
        "video_id": "D4C1dB9UheQ",
        "video_title": "AI Learns to Synthesize Pictures of Animals | Two Minute Papers #152",
        "position_in_playlist": 121,
        "description": "Our Patreon page is available here. Thanks so much for your generous support!\nhttps://www.patreon.com/TwoMinutePapers\n\nThe paper \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\" and its source code is available here:\nhttps://junyanz.github.io/CycleGAN/\n\nOur earlier episodes on regularization:\nhttps://www.youtube.com/watch?v=6aF9sJrzxaM\nhttps://www.youtube.com/watch?v=HTUxsrO-P_8\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2042765/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I just finished reading this paper and I fell out of the chair. And I can almost guarantee you that the results in this work are so insane, you will have to double, or even triple check to believe what you're going to see here. This one is about image translation, which means that the input is an image, and the output is a different version of this input image that is changed according to our guidelines. Imagine that we have a Monet painting, and we'd like to create a photograph of this beautiful view. There we go. What if we'd like to change this winter landscape to an image created during the summer? There we go. If we are one of those people on the internet forums who just love to compare apples to oranges, this is now also a possibility. And have a look at this - imagine that we like the background of this image, but instead of the zebras, we would like to have a couple of horses. No problem. Coming right up! This algorithm synthesizes them from scratch. The first important thing we should know about this technique, is that it uses generative adversarial networks. This means that we have two neural networks battling each other in an arms race. The generator network tries to create more and more realistic images, and these are passed to the discriminator network which tries to learn the difference between real photographs and fake, forged images. During this process, the two neural networks learn and improve together until they become experts at their own craft. However, this piece of work introduces two novel additions to this process. One, in earlier works, the training samples were typically paired. This means that the photograph of a shoe would be paired to a drawing that depicts it. This additional information helps the training process a great deal and the algorithm would be able to map drawings to photographs. However, a key difference here is that without such pairings, we don't need these labels, we can use significantly more training samples in our datasets which also helps the learning process. If this is executed well, the technique is able to pair anything to anything else, which results in a remarkably powerful algorithm. Key difference number two - a cycle consistency loss function is introduced to the optimization problem. This means that if we convert a summer image to a winter image, and then back to a summer image, we should get the very same input image back. If our learning system obeys to this principle, the output quality of the translation is going to be significantly better. This cycle consistency loss is introduced as a regularization term. Our seasoned Fellow Scholars already know what it means, but in case you don't, I've put a link to our explanation in the video description. The paper contains a ton more results, and fortunately, the source code for this project is also available. Multiple implementations, in fact! Just as a side note, which is jaw dropping, by the way - there is some rudimentary support for video. Amazing piece of work. Bravo! Now you can also see that the rate of progress in machine learning research is completely out of this world! No doubt that it is the best time to be a research scientist it's ever been. If you've liked this episode, make sure to subscribe to the series and have a look at our Patreon page, where you can pick up cool perks, like watching every single one of these episodes in early access. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=D4C1dB9UheQ",
        "paper_link": "https://junyanz.github.io/CycleGAN/",
        "paper_title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"
    },
    {
        "video_id": "oleylS5XGpg",
        "video_title": "An Efficient Scattering Material Representation | Two Minute Papers #151",
        "position_in_playlist": 122,
        "description": "The paper \"Downsampling Scattering Parameters for Rendering Anisotropic Media\" and its source code is available here:\nhttps://shuangz.com/projects/multires-sa16/\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Dennis Abts, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1747666/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Have a look at these beautiful images. Representing these materials that you see here takes more than 25 gigabytes of storage. You could store several feature-length movies on your hard drive using the same amount of storage. And this technique is able to compress all this 25 gigabytes into 45 megabytes without introducing any significant perceptible difference! That is close to a whopping 500 times more efficient representation. This improved representation not only helps easing the storage requirements of these assets, but it is also makes the rendering times, or in other words, the process of creating these images via light simulation programs typically more than twice as fast to process. That is a ton of money and time saved for the artists. An important keyword in this piece of work is anisotropic scattering. So what does that mean exactly? The scattering part means that we have to imagine these materials not as a surface, but as a volume in which rays of light bounce around and get absorbed. If we render a piece cloth made of velvet, twill, or a similar material, there are lots of microscopic differences in the surface, so much so, that it is insufficient to treat them as a solid surface, such as wood or metals. We have to think about them as volumes. This is the scattering part. The anisotropy means that light can scatter unevenly in this medium, these rays don't bounce around in all directions with equal probability. This means that there is significant forward and backward scattering in these media, making it even more difficult to create more optimized algorithms that simplify these scattering equations. If you look below here, you'll see these colorful images that researchers like to call difference images. It basically means that we create one image with the perfectly accurate technique as a reference. As expected, this reference image probably takes forever to compute, but is important to have as a yardstick. Then, we compute one image with the proposed technique that is usually significantly faster. So we have these two images, and sometimes, the differences are so difficult to see, we no way of knowing where the inaccuracies are. So what we do is subtract the two images from each other, and assign a color coding for the differences. As the error may be spatially varying, this is super useful because we can recognize exactly where the information is lost. The angrier the colors are, the higher the error is in a given region. As you can see, the proposed technique is significantly more accurate in representing this medium than a naive method using the same amount of storage. This paper is extraordinarily well written. It is one of the finest pieces of craftsmanship I've come along in long while. And yes, it is a crime not having a look at it. Also, if you liked this episode, make sure to subscribe to the series and check out our other videos. We have more than 150 episodes for you, ready to go right now! You'll love it, and there will be lots of fun to be had. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=oleylS5XGpg",
        "paper_link": "https://shuangz.com/projects/multires-sa16/",
        "paper_title": "Downsampling Scattering Parameters for Rendering Anisotropic Media"
    },
    {
        "video_id": "HTUxsrO-P_8",
        "video_title": "Deep Photo Style Transfer | Two Minute Papers #150",
        "position_in_playlist": 123,
        "description": "The paper \"Deep Photo Style Transfer\" is and its source code is available here:\nhttps://arxiv.org/pdf/1703.07511.pdf\nhttps://github.com/luanfujun/deep-photo-styletransfer\n\nOne more different implementation:\nhttps://github.com/martinbenson/deep-photo-styletransfer\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nDistill:\nhttp://distill.pub/\n\nDistill article on research debt:\nhttp://distill.pub/2017/research-debt/\n\nRecommended for you:\nHow Do Neural Networks See The World? - https://www.youtube.com/watch?v=hBobYd8nNtQ\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, e, Esa Turkulainen, Michael Albrecht, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1598418/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Let's have a look at this majestic technique that is about style transfer for photos. Style transfer is a magical algorithm where we have one photograph with content, and one with an interesting style. And the output is a third image with these two photos fused together. This is typically achieved by a classical machine learning technique that we call a convolutional neural network. The more layers these networks contain, the more powerful they are, and the more capable they are in building an intuitive understanding of an image. We had several earlier episodes on visualizing the inner workings of these neural networks, as always, the links are available in the video description. Don't miss out, I am sure you'll be as amazed by the results as I was when I have first seen them. These previous neural style transfer techniques work amazingly well if we're looking for a painterly result. However, for photo style transfer, the closeups here reveal that they introduce unnecessary distortions to the image. They won't look realistic anymore. But not with this new one. Have a look at these results. This is absolute insanity. They are just right in some sense. There is an elusive quality to them. And this is the challenge! We not only have to put what we're searching for into words, but we have to find a mathematical description of these words to make the computer execute it. So what would this definition be? Just think about this, this is a really challenging question. The authors decided that the photorealism of the output image is to be maximized. Well, this sounds great, but who really knows a rigorous mathematical description of photorealism? One possible solution would be to stipulate that the changes in the output color would have to preserve the ratios and distances of the input style colors. Similar rules are used in linear algebra and computer graphics to make sure shapes don't get distorted as we're tormenting them with rotations, translations and more. We like to call these operations affine transformations. So the fully scientific description would be that we add a regularization term that stipulates, that these colors only undergo affine transformations. But we've used one more new word here - what does this regularization term mean? This means that there are a ton of different possible solutions for transferring the colors, and we're trying to steer the optimizer towards solutions that adhere to some additional criterion, in our case, the affine transformations. In the mathematical description of this problem, these additional stipulations appear in the form of a regularization term. I am so happy that you Fellow Scholars have been watching Two Minute Papers for so long, that we can finally talk about techniques like this. It's fantastic to have an audience that has this level of understanding of these topics. Love it. Just absolutely love it. The source code of this project is also available. Also, make sure to have a look at Distill, an absolutely amazing new science journal from the Google Brain team. But this is no ordinary journal, because what they are looking for is not necessarily novel techniques, but novel and intuitive ways of explaining already existing works. There is also an excellent write-up on research debt that can almost be understood as a manifesto for this journal. A worthy read indeed. They also created a prize for science distillation. I love this new initiative and I am sure we'll hear about this journal a lot in the near future. Make sure to have a look, there is a link to all of these in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HTUxsrO-P_8",
        "paper_link": "https://arxiv.org/pdf/1703.07511.pdf",
        "paper_title": "Deep Photo Style Transfer"
    },
    {
        "video_id": "u9UUWqVquXo",
        "video_title": "AI Creates 3D Models From Faces | Two Minute Papers #149",
        "position_in_playlist": 124,
        "description": "The paper \"Photorealistic Facial Texture Inference Using Deep Neural Networks\" is available here:\nhttp://www.hao-li.com/Hao_Li/Hao_Li_-_publications.html\nhttp://arxiv.org/pdf/1612.00523v1.pdf\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nEarlier episode on texture synthesis: https://www.youtube.com/watch?v=8u3Hkbev2Gg\nPatchMatch: https://www.youtube.com/watch?v=n3aoc36V8LM\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Esa Turkulainen, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1961529/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. How cool would it be to be able to place a character representing us in a digital film or a computer game? Of course, it would clearly be an extremely laborious task to digitize the 3D geometry and the albedo map of our face. This albedo map means a texture, a colored pattern that describes how our skin reflects and absorbs light. Capturing such a representation is clearly a very lengthy and expensive process. So get this completely crazy idea: this technique creates this full digital representation of any face from no more than one simple photograph. We can even get historical figures in our digital universe, all we need is one photograph of them. And now, feast your eyes on these incredible results. After taking a photograph, this technique creates two of these albedo maps: one is a complete, low frequency map, which records the entirety of the face, but only contains the rough details. The other albedo map contains finer details, but in return, is incomplete. Do you remember the texture synthesis methods that we discussed earlier in the series? The input was a tiny patch of image with a repetitive structure, and after learning the statistical properties of these structures, it was possible to continue them indefinitely. The key insight is that we can also do something akin to that here as well - we take this incomplete albedo map, and try to synthesize the missing details. Pretty amazing idea indeed! The authors of the paper invoke a classical learning algorithm, a convolutional neural network to accomplish that. The deeper the neural network we use, the more high-frequency details appear on the outputs, or in other words, the crisper the image we get. In the paper, you will find a detailed description of their crowdsourced user study that was used to validate this technique, including the user interface and the questions being asked. There are also some comparisons against PatchMatch, one of the landmark techniques for texture synthesis that we have also talked about in an earlier episode. It's pretty amazing to see this Two Minute Papers knowledge base grow and get more and more intertwined. I hope you're enjoying the process as much as I do! Also, due to popular request, the Two Minute Papers T-shirts are now available! This time, we are using a different service for printing these shirts, please give us some feedback on how you liked it. I've put my e-mail address in the video description. If you attach a photo of yourself wearing some cool Two Minute Papers merch, we'll be even more delighted! Just open twominutepapers.com and you'll immediately have access to it. This link will bring you to the service that ships to the US. The link for shipping outside the US is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u9UUWqVquXo",
        "paper_link": "http://www.hao-li.com/Hao_Li/Hao_Li_-_publications.html",
        "paper_title": "Photorealistic Facial Texture Inference Using Deep Neural Networks"
    },
    {
        "video_id": "1U3YKnuMS7g",
        "video_title": "AI Learns Geometric Descriptors From Depth Images | Two Minute Papers #148",
        "position_in_playlist": 125,
        "description": "The paper \"3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions\" is available here:\nhttp://3dmatch.cs.princeton.edu/\n\nRecommended for you:\nOur earlier episode on Siamese networks - https://www.youtube.com/watch?v=a3sgFQjEfp4\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Esa Turkulainen, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nAwesome Two Minute Papers merch: http://twominutepapers.com/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1851258/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we're going to discuss a great piece of work that shows us how efficient and versatile neural network-based techniques had become recently. Here, the input is a bunch or RGB-D images, which are photographs endowed with depth information, and the output can be a full 3D reconstruction of a scene, and much, much more, which we'll see in a moment. This task is typically taken care of by handcrafting descriptors. A descriptor is a specialized representation for doing useful tasks on images and other data structures. For instance, if we seek to build an algorithm to recognize black and white images, a useful descriptor would definitely contain the number of colors that are visible in an image, and a list of these colors. Again, these descriptors have been typically handcrafted by scientists for decades. New problem, new descriptors, new papers. But not this time, because here, super effective descriptors are proposed automatically via a learning algorithm, a convolutional neural network and siamese networks. This is incredible! Creating such descriptors took extremely smart researchers and years of work on a specific problem, and were still often not as good as these ones. By the way, we have discussed siamese networks in an earlier episode, as always, the link is available in the video description. And as you can imagine, several really cool applications emerge from this. One, when combined with RANSAC, a technique used to find order in noisy measurement data, it is able to perform 3D scene reconstruction from just a few images. And it completely smokes the competition. Two, pose estimation with bounding boxes. Given a sample of an object, the algorithm is able to recognize not only the shape itself, but also its orientation when given a scene cluttered with other objects. Three, correspondance search is possible. This is really cool! This means that a semantically similar piece of geometry is recognized on different objects. For instance, the algorithm can learn the concept of a handle, and recognize the handles on a variety of objects, such as on motorcycles, carriages, chairs, and more! The source code of this project is also available. Yoohoo! Neural networks are rapidly establishing supremacy in a number of research fields, and I am so happy to be alive in this age of incredible research progress. Make sure to subscribe to the series and click the bell icon, some amazing works are coming up in the next few episodes, and there will be lots of fun to be had. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1U3YKnuMS7g",
        "paper_link": "http://3dmatch.cs.princeton.edu/",
        "paper_title": "3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions"
    },
    {
        "video_id": "8YWgar0uCF8",
        "video_title": "Semantic Scene Completion From One Depth Image | Two Minute Papers #147",
        "position_in_playlist": 126,
        "description": "The paper \"Semantic Scene Completion from a Single Depth Image\" is available here:\nhttp://sscnet.cs.princeton.edu/\n\nRecommended for you:\nHow Does Deep Learning Work? - https://www.youtube.com/watch?v=He4t7Zekob0\nArtificial Neural Networks and Deep Learning - https://www.youtube.com/watch?v=rCWTOOgVXyE\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nAwesome Two Minute Papers merch: http://twominutepapers.com/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2225414/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work is an amazing application of deep neural networks, that performs semantic scene completion from only one depth image. This depth image is the colorful image that you see here, where the colors denote how far away different objects are from our camera. We can create these images inexpensively with commodity hardware, for instance, Microsoft's Kinect has a depth sensor sensor that is suitable for this task. The scene completion part means that from this highly incomplete depth information, the algorithm reconstructs the geometry for the entirety of the room. Even parts, that are completely missing from our images or things that are occluded! The output is what computer graphics researchers like to call a volumetric representation or a voxel array, which is essentially a large collection of tiny Lego pieces that build up the scene. But this is not all because the semantic part means that the algorithm actually understands what we're looking at, and thus, is able to classify different parts of the scene. These classes include walls, windows, floors, sofas, and other furniture. Previous works were able to do scene completion and geometry classification, but the coolest part of this algorithm is that it not only does these steps way better, but it does them both at the very same time. This work uses a 3D convolutional neural network to accomplish this task. The 3D part is required for this learning algorithm to be able to operate on this kind of volumetric data. As you can see, the results are excellent, and are remarkably close to the ground truth data. If you remember, not so long ago, I flipped out when I've seen the first neural network-based techniques that understood 3D geometry from 2D images. That technique used a much more complicated architecture, a generative adversarial network, which also didn't do scene completion and on top of that, the resolution of the output was way lower, which intuitively means that Lego pieces were much larger. This is insanity. The rate of progress in machine learning research is just stunning, probably even for you seasoned Fellow Scholars who watch Two Minute Papers and have high expectations. We've had plenty of previous episodes about the inner workings of different kinds of neural networks. I've put some links to them in the video description, make sure to have a look if you wish to brush up on your machine learning kung fu a bit. The authors also published a new dataset to solve these kind of problems in future research works, and, it is also super useful because the output of their technique can be compared to ground truth data. When new solutions pop up in the future, this dataset can be used as a yardstick to compare results with. The source code for this project is also available. Tinkerers rejoice! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=8YWgar0uCF8",
        "paper_link": "http://sscnet.cs.princeton.edu/",
        "paper_title": "Semantic Scene Completion from a Single Depth Image"
    },
    {
        "video_id": "aAsejHZC5EE",
        "video_title": "Real-Time Modeling and Animation of Climbing Plants | Two Minute Papers #146",
        "position_in_playlist": 127,
        "description": "Two Minute Papers on Patreon + our technical memos:\nhttps://www.patreon.com/TwoMinutePapers\nhttps://www.patreon.com/TwoMinutePapers/posts?tag=what%27s%20new\n\nThe paper \"Interactive Modeling and Authoring of\nClimbing Plants\" is available here:\nhttp://www.pirk.info/projects/climbing_plants/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nAwesome Two Minute Papers merch: http://twominutepapers.com/\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-413686/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about interactively modeling and editing climbing plants. This is one of my favorite kind of works: mundane sounding topic, immaculate execution. There are so many cool things about this paper, I don't even know where to start. But first, let's let's talk about the modeling part. We can, for instance, plant a seed, and we can not only have a look at how it grows as time goes by, but we can also influence the grow variability and shoot growth rates. Branches can be added and removed at will at any point in time. We can also add attractors, regions that are set to be more likely for the plant to grow towards. With these techniques, we can easily create any sort of artistic effect, be it highly artificial looking vines and branches, or some long forgotten object overgrown with climbing plants. However, a model is just 3d geometry. What truly makes these models come alive is animation, which is also executed with flying colors. The animations created with this technique are both biologically and physically plausible. So what do these terms mean exactly? Biologically plausible means that the plants grow according to the laws of nature, and physically plausible means that if we start tugging at it, branches start moving, bending and breaking according to the laws of physics. Due to its responsive and interactive nature, the applications of this technique are typically in the domain of architectural visualization, digital storytelling, or any sort of real-time application. And of course, the usual suspects, animated movies and game developers can use this to create more immersive digital environments with ease. And don't forget about me, K\u00e1roly, who would happily play with this basically all day long. If you are one of our many Fellow Scholars who are completely addicted to Two Minute Papers, make sure to check out our Patreon page where you can grab cool perks, like watching these episodes in early access, or deciding the order of upcoming episodes, and more. Also, your support is extremely helpful, so much so, that even the price of a cup of coffee per month helps us to create better videos for you. We write some reports from time to time to assess the improvements we were able to make with your support. The link is in the video description, or you can just click the letter P on the ending screen in a moment. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=aAsejHZC5EE",
        "paper_link": "http://www.pirk.info/projects/climbing_plants/",
        "paper_title": "Interactive Modeling and Authoring of\nClimbing Plants"
    },
    {
        "video_id": "lxNEWuO6xQk",
        "video_title": "Controllable Fluid and Smoke Simulations | Two Minute Papers #145",
        "position_in_playlist": 128,
        "description": "The paper \"Primal-Dual Optimization for Fluids\" is available here:\nhttp://www.ntoken.com/pubs.html\n\nAn introduction to fluid simulations and fluid control with source code, both CPU and GPU (OpenCL):\n1. https://users.cg.tuwien.ac.at/zsolnai/gfx/fluid_control_msc_thesis/\n2. https://users.cg.tuwien.ac.at/zsolnai/gfx/real_time_fluid_control_eg/\n\nDoyub Kim's book on fluid simulations, with source code:\nhttp://doyub.com/\nhttps://twitter.com/doyub?lang=en\n\nThe first Two Minute Papers episode on Wavelet Turbulence:\nhttps://www.youtube.com/watch?v=5xLSbj5SsSE\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1632785/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we're going to talk some more about fluid simulations and fluid guiding. Hell yeah! As you know all too well, it is possible to simulate the laws of fluid motion on a computer, make a digital scene, and create absolutely beautiful videos such as the ones you see here. Newer and newer research papers show up to both extend the possible scenarios that we can simulate, and there are also other works to speed up already existing solutions. This piece of work introduces a technique that mathematicians like to call the primal-dual optimization method. This helps us accomplish two really cool things. One is fluid guiding. Fluid guiding is a problem where we're looking to exert control over the fluid while keeping the fluid flow as natural as possible. I've written my Master thesis on the very same topic and can confirm that it's one hell of a problem. The core of the problem is that if we use the laws of physics to create a fluid simulation, we get what would happen in reality as a result. However, if we wish to guide this piece of fluid towards a target shape, for instance, to form an image of our choice, we have to both retain natural fluid flows, but still, create something that would be highly unlikely to happen according to the laws of physics. For instance, a splash of water is unlikely to suddenly form a human face of our choice. The proposed technique helps this ambivalent goal of exerting a bit of control over the fluid simulation while keeping the flows as natural as possible. There are already many existing applications of fluids and smoke in movies where an actor fires a gun, and the fire and smoke plumes are added to the footage in post production. However, with a high quality fluid guiding technique, we could choose target shapes for these smoke plumes and explosions that best convey our artistic vision. And number two, it also accomplishes something that we call separating boundary conditions, which prevents imprecisions where small fluid volumes are being stuck to walls. The guiding process is also followed by an upsampling step, where we take a coarse simulation, and artificially synthesize sharp, high-frequency details onto it. Computing the more detailed simulation would often take days without such synthesizing techniques. Kind of like with Wavelet Turbulence, which is an absolutely incredible paper that was showcased in none other than the very first Two Minute Papers episode. Link is in the video description box. Don't watch it. It's quite embarrassing. And all this leads to eye-poppingly beautiful solutions. Wow. I cannot get tired of this. In the paper, you will find much more about breaking dams, tornado simulations, and applications of the primal-dual optimization method. Normally, to remain as authentic to the source materials as possible, I don't do any kind of slow motion and other similar shenanigans, but this time, I just couldn't resist it. Have a look at this, and I hope you'll like the results. Hm hm! If you feel the alluring call of fluids, I've put some resources in the video description, including a gentle description I wrote on the basics of fluid simulation and fluid control with source code both on the CPU and GPU, and a link to Doyub Kim's amazing book that I am currently reading. Highly recommended. If you also have some online tutorials and papers that helped you solidify your understanding of the topic, make sure to leave a link in the comments, I'll include the best ones in the video description. If you would like to see more episodes like this one, make sure to subscribe to Two Minute Papers, we would be more than happy to have you along on our journey of science. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lxNEWuO6xQk",
        "paper_link": "http://www.ntoken.com/pubs.html",
        "paper_title": "Primal-Dual Optimization for Fluids"
    },
    {
        "video_id": "lf3ViWEeKqc",
        "video_title": "On-the-Fly 3D Printing While Modeling | Two Minute Papers #144",
        "position_in_playlist": 129,
        "description": "The paper \"On-the-Fly Print: Incremental Printing While Modeling\" is available here:\nhttp://www.huaishu.me/projects/on-the-fly.html\nhttp://www.cs.cornell.edu/projects/wireprint/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. You are going to love this killer paper. In the classical case of 3D fabrication, we first create a 3D geometry in our modeling software on a computer. Then, after we're done, we send this model to a 3D printer to create a physical copy of it. If we don't like an aspect of the printed model, we have to go back to the computer and adjust accordingly. If there are more fundamental issues, we may even have to start over. And get this, with this piece of work, we can have a low-fidelity wireframe version printed immediately as we make changes within the modeling software. This process we can refer to as real-time or on-the-fly 3D printing. In this work, both the hardware design and the algorithm that runs the printer is described. This approach has a number of benefits, and of course, a huge set of challenges. For instance, we can immediately see the result of our decisions, and can test whether a new piece of equipment would correctly fit into the scene we're designing. Sometimes, depending on the geometry of the final object, different jobs need to be reordered to get a plan that is physically plausible to print. In this example, the bottom branch was designed by the artist first, and the top branch afterwards, but their order has to be changed, otherwise the bottom branch would block the way to the top branch. The algorithm recognizes these cases and reorders the printing jobs correctly. Quite remarkable. And, an alternative solution for rotating the object around for better reachability is also demonstrated. Because of the fact that the algorithm is capable of this sort of decisionmaking, we don't even need to wait for the printer to finish a given step, and can remain focused on the modeling process. Also, the handle of the teapot here collides with the body - because of the limitations of wireframe modeling, such cases have to be detected and omitted. Connecting patches and adding differently sized holes to a model are also highly nontrivial problems that are all addressed in the paper. And this piece of work is also a testament to the potential of solutions where hardware and software is designed with each other in mind. I can only imagine how many work hours were put in this project until the final, working solution was obtained. Incredible work indeed. We really just scratched the surface in this episode, make sure to go to the video description and have a look at the paper for more details. It's definitely worth it. Also, if you enjoyed this Two Minute Papers episode, make sure to subscribe to the series and if you're subscribed, click the bell icon to never miss an episode. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lf3ViWEeKqc",
        "paper_link": "http://www.huaishu.me/projects/on-the-fly.html",
        "paper_title": "On-the-Fly Print: Incremental Printing While Modeling"
    },
    {
        "video_id": "1SHW1-qKKpY",
        "video_title": "Real-Time Oil Painting on Mobile | Two Minute Papers #143",
        "position_in_playlist": 130,
        "description": "The paper \"Real-Time Oil Painting on Mobile Hardware\" is available here:\nhttp://graphics.cs.kuleuven.be/publications/SD2016RTOPOMH/index.html\n\nIn addition:\nIt is mentioned that mobile devices typically have a lower resolution display than desktop computers. While this is true, a more important limiting factor is screen real estate, and the fact that a resolution of the simulation is significantly lower on a phone given the vast differences in processing power. These are challenging limitations that are difficult to overcome.\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credits:\nhttps://pixabay.com/photo-1125445/\nhttps://pixabay.com/photo-1138275/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. It's been quite a while since we've talked about a paper on fluid simulations. Since my withdrawal symptoms are already kicking in, today, I simply have to talk to you about this amazing paint simulator program that runs in real time and on our mobile devices. These handheld devices typically have a lower image resolution compared to desktop computers, therefore it is indeed a challenge to put together a solution that artists can use to create detailed paintings with. And to accomplish this, this piece of work offers several killer features: for instance, the paint pigment concentration can be adjusted. The direction of the brush strokes is also controllable. And third, this technique is powered by a viscoelastic shallow water simulator that also supports simulating multiple layers of paint. This is particularly challenging as the inner paint layers may have already dried when adding a new wet layer on top of them. This all has to be simulated in a way that is physically plausible. But we're not done yet! With many different kinds of paint types that we're using, the overall outlook of our paintings are dramatically different depending on the lighting conditions around them. To take this effect into consideration, this technique also has an intuitive feature where the effect of virtual light sources is also simulated, and the output is changed interactively as we tilt the tablet around. And get this, gravity is also simulated, and the paint trickles down depending on the orientation of our tablet according to the laws of physics. Really cool! The paper also shows visual comparisons against similar algorithms. And clearly, artists who work with these substances all day know exactly how they should behave in reality, so the ultimate challenge is always to give it to them and ask them whether they have enjoyed the workflow and found the simulation faithful to reality. Let the artists be the judge! The user study presented in the paper revealed that the artists loved the user experience and they expressed that it's second to none for testing ideas. I am sure that with a few improvements, this could be the ultimate tool for artists to unleash their creative potential while sitting outside and getting inspired by nature. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1SHW1-qKKpY",
        "paper_link": "http://graphics.cs.kuleuven.be/publications/SD2016RTOPOMH/index.html",
        "paper_title": "Real-Time Oil Painting on Mobile Hardware"
    },
    {
        "video_id": "UBORpapdAfU",
        "video_title": "Instant 3D Floorplans From Your Photos | Two Minute Papers #142",
        "position_in_playlist": 131,
        "description": "The paper \"Rent3D: Floor-Plan Priors for Monocular Layout Estimation\" is available here:\nhttp://www.cs.toronto.edu/~fidler/projects/rent3D.html\nhttp://www.cs.toronto.edu/~urtasun/publications/liu_etal_cvpr15.pdf\n\nFollowup paper - HouseCraft:\nhttp://www.cs.toronto.edu/housecraft/\nhttps://github.com/chuhang/HouseCraft\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-354233/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this piece of work, we are interested in creating a 3D virtual tour for an apartment. However, for this apartment, no 3D information is available - instead, the input for the algorithm is something that we can obtain easily, in this case, a 2D floorplan and a set of images that we shot in the apartment. From this information, we would create a 3D floorplan that is not only faithful to the real one in terms of geometry, but the photos with the correct viewpoints are also to be assigned to the correct walls. In order to accomplish this, one has to overcome a series of challenging problems. For instance, we have to estimate the layout of each room and find the location of the camera in each of these images. Also, to obtain high-quality solutions, the goal is to extract as much information from the inputs as possible. The authors recognized that the floorplans provide way more information than we take for granted. For instance, beyond showing the geometric relation of the rooms, it can also be used to find out the aspect ratios of the floor for each room. The window-to-wall ratios can also be approximated and matched between the photos and the floorplan. This additional information is super useful when trying to find out which room is to be assigned to which part of the 3D floorplan. Beyond just looking at the photos, we also have access to a large swath of learning algorithms that can reliably classify whether we're looking at a bathroom or a living room. There are even more constraints to adhere to in order to aggressively reduce the number of physical configurations, make sure to have a look at the paper for details, there are lots of cool tricks described there. As always, there is a link to it in the video description. For instance, since the space of possible solutions is still too vast, a branch and bound type algorithm is proposed to further decimate the number of potential solutions to evaluate. And as you can see here, the comparisons against ground truth floorplans reveal that these solutions are indeed quite faithful to reality. The authors also kindly provided a dataset with more than 200 full apartments with well over a thousand photos and annotations for future use in followup research works. Creating such a dataset and publishing it is incredibly laborious, and could easily be a paper on its own, and here, we also get an excellent solution for this problem as well. In a separate work, the authors also published a different version of this problem formulation that reconstructs the exteriors of buildings in a similar manner. There is so much to explore, the links are available in the video description, make sure to have a look! In case you're wondering, it's still considered a crime not doing that. I hope you have enjoyed this episode, and I find it so delightful to see this unbelievably rapid growth on the channel. Earlier I thought that even 2 would be amazing, but now, we have exactly 8 times as many subscribers as one year ago. Words fail me to describe the joy of showing these amazing works to such a rapidly growing audience. This is why I always say at the end of every episode... Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UBORpapdAfU",
        "paper_link": "http://www.cs.toronto.edu/~fidler/projects/rent3D.html",
        "paper_title": "Rent3D: Floor-Plan Priors for Monocular Layout Estimation"
    },
    {
        "video_id": "wz9cUncBdxw",
        "video_title": "Geometric Detail Transfer | Two Minute Papers #141",
        "position_in_playlist": 132,
        "description": "The paper \"Learning Detail Transfer based on Geometric Features\" is available here:\nhttp://www.chongyangma.com/publications/ld/index.html\n\nThe story of our recent software and hardware overhaul: https://www.patreon.com/posts/software-and-8622149\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Christian Lawson, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1853203/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the world of digital 3D modeling, it often occurs that we are looking for surfaces that are not perfectly smooth, but have some sort of surface detail. Wrinkles, engravings, grain on a wooden table are excellent examples of details that we can add to our models, and computer graphics people like to collectively call these things displacement maps. Artists often encounter cases where they like the displacements on one object, but the object itself is not really interesting. However, it could be that there is a different piece of geometry these details would look great on. Consider this problem solved, because in this piece of work, the input is two 3D models: one with interesting geometric details, and the other is the model onto which we transfer these surface details. The output will be our 3D geometric shape with two of these models fused together. The results look absolutely amazing. I would love to use this right away in several projects. The first key part is the usage of metric learning. Wait, technical term, so what does this mean exactly? Metric learning is a classical technique in the field of machine learning, where we're trying to learn distances between things where distance is mathematically ill-defined. Let's make it even simpler and go with an example: for instance, we have a database of human faces, and we would like to search for faces that are similar to a given input. To do this, we specify a few distances by hand, for instance, we could say that a person with a beard is a short distance from one with a moustache, and a larger distance from one with no facial hair. If we hand many examples of these distances to a learning algorithm, it will be able to find people with similar beards. And in this work, this metric learning is used to learn the relationship between objects with and without these rich surface details. This helps in the transferring process. As to creating the new displacements on the new model, there are several hurdles to overcome. One, we cannot just grab the displacements and shove them onto a different model, because it can potentially look different, have different curvatures and sizes. The solution to this would be capturing the statistical properties of the surface details and use this information to synthesize new ones on the target model. Note that we cannot just perform this texture synthesis in 2D like we do for images, because as we project the result to a 3D model, it introduces severe distortions to the displacement patterns. It is a bit like putting a rubber blanket onto a complicated object. Different regions of the blanket will be distorted differently. Make sure to have a look at the paper where the authors present quite a few more results and of course, the intricacies of this technique are also described in detail. I hope some public implementations of this method will appear soon, I would be quite excited to use this right away, and I am sure there are many artists who would love to create these wonderfully detailed models for the animated films and computer games of the future. In the meantime, we have a completely overhauled software and hardware pipeline to create these videos. We have written down our joyful and perilous story of it on Patreon - if you're interested in looking a bit behind the curtain as to how these episodes are made, make sure to have a look, it is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wz9cUncBdxw",
        "paper_link": "http://www.chongyangma.com/publications/ld/index.html",
        "paper_title": "Learning Detail Transfer based on Geometric Features"
    },
    {
        "video_id": "UEPbzj-ekAI",
        "video_title": "Modeling Knitted Clothing | Two Minute Papers #140",
        "position_in_playlist": 133,
        "description": "The paper \"Stitch Meshes for Modeling Knitted Clothing with Yarn-level Detail\" is available here:\nhttp://www.cs.cornell.edu/projects/stitchmeshes/\nhttp://www.cemyuksel.com/research/stitchmeshes/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Claudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-2042186/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Not so long ago, we talked about a technique that enabled us to render stunningly high quality cloth models in real-time. It supported level of detail, self shadowing, and lots of other goodies that make computer game developers, and of course, my humble self super happy. And today, we're going to talk about a technique that is able to create these highly detailed cloth geometries for our digital characters. I have really fond memories of attending to the talk of the Oscar award winner Steve Marschner on this paper a few years ago in Switzerland, and I remember being so spellbound by it that I knew this was a day I will never forget. I am sure you'll love it too. In this piece of work, the goal is to create a digital garment model that is as detailed and realistic as possible. We start out with an input 3D geometry that shows the rough shape of the model, then, we pick a knitting pattern of our choice. After that, the points of this knitting pattern are moved so that they correctly fit this 3D geometry that we specified. And now comes the coolest part! What we created so far is an ad-hoc model that doesn't really look and behave like a real piece of cloth. To remedy this, a physics-based simulation is run that takes this ad-hoc model and the output of this process will be a realistic rest shape for these yarn curves. And here you can witness how the simulated forces pull the entire piece of garment together. We start out with dreaming up a piece of cloth geometry, and this simulator gradually transforms it into a real-world version of that. This is a step that we call yarn-level relaxation. Wow. These final results not only look magnificent, but in a physical simulation, they also behave like real garments. It's such a joy to look at results like this. Loving it. Again, I would like to note that we're not talking about the visualization of the garment, but creating a realistic piece of geometry. The most obvious drawback of this technique is its computation time - it was run on a very expensive system and still took several hours of number crunching to get this done. However, I haven't seen an implementation of this on the graphics card yet, so if someone can come up with an efficient way to do it, in an ideal case, we may be able to do this in several minutes. I also have to notify you about the fact that it is considered a crime not having a look at the paper in the video description. It does not suffice to say that it is well written, it is so brilliantly presented, it's truly a one of kind work that everyone has to see. If you enjoyed this episode, make sure to subscribe to Two Minute Papers, we'd be happy to have you in our growing club of Fellow Scholars. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UEPbzj-ekAI",
        "paper_link": "http://www.cs.cornell.edu/projects/stitchmeshes/",
        "paper_title": "Stitch Meshes for Modeling Knitted Clothing with Yarn-level Detail"
    },
    {
        "video_id": "n3aoc36V8LM",
        "video_title": "Structural Image Editing With PatchMatch | Two Minute Papers #139",
        "position_in_playlist": 134,
        "description": "The paper \"PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing\" and part of its source code is available here:\nhttp://gfx.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php\n\nAdditional, unofficial implementations:\nhttps://github.com/ikuwow/PatchMatch\nhttps://github.com/rcrandall/PatchMatch\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Claudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image source: https://pixabay.com/photo-1594689/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We're currently more than 130 episodes into the series, and we still haven't talked about this algorithm. How could we go on for so long without PatchMatch? So, let's do this right now! You'll love this one. This technique helps us to make absolutely crazy modifications to previously existing photographs and it is one of the landmark papers for all kinds of photo manipulation which is still widely used to this day. Consider the following workflow: we have this image as an input. Let's mark the roofline for hole filling, or image inpainting as the literature refers to it. And the hole is now filled with quite sensible information. Now we mark some of the pillars to reshape the object. And then, we pull the roof upward. The output is a completely redesigned version of the input photograph. Wow, absolutely incredible. And the whole thing happens interactively, almost in real time, but if we consider the hardware improvement since this paper was published, it is safe to say that today it runs in real time even on a mediocre computer. And in this piece of work, the image completion part works by adding additional hints to the algorithm, for instance, marking the expected shape of an object that we wish to cut out and have it filled in with new data. Or, showing the shape of the building that we wish to edit also helps the technique considerably. These results are so stunning, I remember that when I had first seen them, I had to recheck over and over again because I could hardly believe my eyes. This technique offers not only these high quality results, but it is considerably quicker than its competitors. To accomplish image inpainting, most algorithms look for regions in the image that are similar to the one that is being removed, and borrow some information from there for the filling process. Here, one of the key ideas that speed up the process is that when good correspondances are found, if we are doing another lookup, we shouldn't restart the patch matching process, but we should try to search nearby, because that's where we are most likely to find useful information. You know what the best part is? What you see here is just the start: this technique does not use any of the modern machine learning techniques, so in the era of these incredibly powerful deep neural networks, I can only imagine the quality of solutions we'll be able to obtain in the near future. We are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=n3aoc36V8LM",
        "paper_link": "http://gfx.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php",
        "paper_title": "PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing"
    },
    {
        "video_id": "bB54Wz4kq0E",
        "video_title": "Shape2vec: Understanding 3D Shapes With AI | Two Minute Papers #138",
        "position_in_playlist": 135,
        "description": "The paper \"Shape2Vec: semantic-based descriptors for 3D shapes, sketches and images\" is available here:\nhttp://www.cl.cam.ac.uk/research/rainbow/projects/shape2vec/\n\nCode (coming soon according to the authors): https://github.com/ftasse/Shape2Vec\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Claudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1828007/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This one is going to be absolutely amazing. This piece of work is aimed to help a machine build a better understanding of images and 3D geometry. Imagine that we have a large database with these geometries and images, and we can search and compare them with arbitrary inputs and outputs. What does this mean exactly? For instance, it can handle a text input, such as school bus and automatically retrieve 3D models, sketches and images that depict these kinds of objects. This is great, but we said that it supports arbitrary inputs and outputs, which means that we can use the 3D geometry of a chair as an input, and obtain other, similar looking chairs from the database. This technique is so crazy, it can even take a sketch as an input and provide excellent quality outputs. We can even give it a heatmap of the input and expect quite reasonable results. Typically, these images and 3D geometries contain a lot of information, and to be able to compare which is similar to which, we have to compress this information into a more concise description. This description offers a common ground for comparisons. We like to call these embedding techniques. Here, you can see an example of a 2D visualization of such an embedding of word classes. The retrieval from the database happens by compressing the user-provided input and putting it into this space, and fetching the results that are the closest to it in this embedding. Before the emergence of powerful learning algorithms, these embeddings were typically done by hand. But now, we have these deep neural networks that are able to automatically create solutions for us, that are in some sense, optimal, meaning that according to a set of rules, it will always do better than we would by hand. We get better results by going to sleep and leaving the computer on overnight than we would have working all night using the finest algorithms from ten years ago. Isn't this incredible? The interesting thing is that here, we are able to do this for several different representations: for instance, a piece of 3D geometry, or 2D color image, or a simple word, is being embedded into the very same vector space, opening up the possibility of doing these amazing comparisons between completely different representations. The results speak for themselves. This is another great testament to the power of convolutional neural networks and as you can see, the rate of progress in AI and machine learning research is absolutely stunning. Also big thumbs up for the observant Fellow Scholars out there who noticed the new outro music, and, some other minor changes in the series. If you are among those people, you can consider yourself a hardcore Two Minute Papers Scholar! High five! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bB54Wz4kq0E",
        "paper_link": "http://www.cl.cam.ac.uk/research/rainbow/projects/shape2vec/",
        "paper_title": "Shape2Vec: semantic-based descriptors for 3D shapes, sketches and images"
    },
    {
        "video_id": "YWK-bnyXvbg",
        "video_title": "Space-Time Video Completion | Two Minute Papers #137",
        "position_in_playlist": 136,
        "description": "The paper \"Space-Time Video Completion\" is available here:\nhttp://www.wisdom.weizmann.ac.il/~vision/VideoCompletion.html\n\nUnofficial implementation:\nhttp://www2.mta.ac.il/~tal/ImageCompletion/\n\nDisclaimer: as the website mentioned the source code, I incorrectly assumed that it also contains that. Unfortunately, this is not the case. Please have a look at this followup paper with source code, hopefully this will be of help - http://perso.telecom-paristech.fr/~gousseau/video_inpainting/\n\nIn the meantime, if you find an implementation of this technique, please let me know and I'll add a link to it here.\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Claudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-790220/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we're going to talk about an algorithm that is capable of filling holes in space and time. Classical image inpainting, or in other words, filling holes in images is something that we've explored in earlier episodes - there are several incredible techniques to take care of that. But with this piece of work, it is possible to generalize such a technique for video, and fill holes for not only one image, but a series of images. Like removing a umbrella to create an unoccluded view to the beach. Or removing a waving person from a video of us jogging. These results are truly incredible, and even though this method was published long long ago, it still enjoys a great deal of reverence among computer graphics practitioners. Not only that, but this algorithm also serves the basis of the awesome content aware fill feature introduced in Adobe Photoshop CS5. In this problem formulation, we have to make sure that our solution has spatio-temporal consistency. What does this mean exactly? This means that holes can exist through space and time, so multiple frames of a video may be missing, or there may be regions that we wish to cut out not only for one image, but for the entirety of the video. The filled in regions have to be consistent with their surroundings if they are looked at as an image, but there also has to be a consistency across the time domain, otherwise we would see a disturbing flickering effect in the results. It is a really challenging problem indeed because there are more constraints that we have to adhere to, however, a key observation is that it is also easier because in return, we have access to more information that comes from the previous and the next frames in the video. For instance, here, you can see an example of retouching old footage by removing a huge pesky artifact. And clearly, we know the fact that Charlie Chaplin is supposed to be in the middle of the image only because we have this information from the previous and next frames. All this is achieved by an optimization algorithm that takes into consideration that consistency has to be enforced through the spatial and the time domain at the same time. It can also be used to fill in completely missing frames of a video. Or, it also helps where we have parts of an image missing after being removed by an image stabilizer algorithm. Video editors do this all the time, so such a restoration technique is super useful. The source code of this technique is available, I've put a link to it in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=YWK-bnyXvbg",
        "paper_link": "http://www.wisdom.weizmann.ac.il/~vision/VideoCompletion.html",
        "paper_title": "Space-Time Video Completion"
    },
    {
        "video_id": "8u3Hkbev2Gg",
        "video_title": "Stable Neural Style Transfer | Two Minute Papers #136",
        "position_in_playlist": 137,
        "description": "The paper \"Stable and Controllable Neural Texture Synthesis and\nStyle Transfer Using Histogram Losses\" is available here:\nhttps://arxiv.org/abs/1701.08893\n\nTexture synthesis survey: http://www-sop.inria.fr/reves/Basilic/2009/WLKT09/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nAndrew Melnychuk, Claudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1138294/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Neural style transfer is an incredible technique where we have two input photographs, and the output would be a combination of these two, namely, the content of one and the artistic style of the other fused together. When the first paper appeared on this topic, the news took the world by storm, and lots of speculative discussions emerged as to what this could be used for and how it would change digital arts and the video game industry. It is great fun to use these algorithms and we have also witnessed a recent proliferation of phone apps that are able to accomplish this, which is super cool because of two reasons: one, the amount of time to go from a published research paper to industry-wide application has never been so small, and, two, the first work required a powerful computer to accomplish this, and took several minutes of strenuous computation, and now, less than two years later, it's right in your pocket and can be done instantly. Talk about exponential progress in science and research, absolutely amazing. And now, while we feast our eyes upon these beautiful results, let's talk about the selling points of this extension of the original technique. The paper contains a nice formal explanation of the weak points of the existing style transfer algorithms. The intuition behind the explanation is that the neural networks think in terms of neuron activations, which may not be proportional to the color intensities in the source image styles, therefore their behavior often becomes inconsistent or different than expected. The authors propose thinking in terms of histograms, which means that the output image should rely on statistical similarities with the source images. And as we can see, the results look outstanding even when compared to the original method. It is also important to point out that this proposed technique is also more art directable, make sure to have a look at the paper for more details on that. As always, I've put a link in the video description. This extension is also capable of texture synthesis, which means that we give it a small image patch that shows some sort of repetition, and it tries to continue it indefinitely in a way that seems completely seamless. However, we have to be acutely aware of the fact that in the computer graphics community, texture synthesis is considered a subfield of its own with hundreds of papers, and one has to be extremely sure to have a clear cut selling point over the state of the art. For the more interested Fellow Scholars out there, I've put a survey paper on this in the video description, make sure to have a look! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=8u3Hkbev2Gg",
        "paper_link": "https://arxiv.org/abs/1701.08893",
        "paper_title": "Stable and Controllable Neural Texture Synthesis and\nStyle Transfer Using Histogram Losses"
    },
    {
        "video_id": "QFu0vZgMcqk",
        "video_title": "Breaking DeepMind's Game AI System | Two Minute Papers #135",
        "position_in_playlist": 138,
        "description": "Our Patreon page is available here: https://www.patreon.com/TwoMinutePapers\n\nThe paper \"Adversarial Attacks on Neural Network Policies\" is available here:\nhttp://rll.berkeley.edu/adversarial/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nClaudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim, VR Wizard.\nhttps://www.patreon.com/TwoMinutePapers\n\nRecommended for you:\nBreaking Deep Learning Systems With Adversarial Examples - https://www.youtube.com/watch?v=j9FLOinaG94\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Antarctica by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/ \n\nThumbnail background image credit: https://pixabay.com/photo-1837125/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Not so long ago, Google DeepMind introduced a novel learning algorithm that was able to reach superhuman levels in playing many Atari games. It was a spectacular milestone in AI research. Interestingly, while these learning algorithms are being improved at a staggering pace, there is a parallel subfield where researchers endeavor to break these learning systems by slightly changing the information they are presented with. Fraudulent tampering with images or video feeds, if you will. Imagine a system that is designed to identify what is seen in an image. In an earlier episode, we discussed an adversarial algorithm, where in an amusing example, they added a tiny bit of barely perceptible noise to this image, to make the deep neural network misidentify a bus for an ostrich. Machine learning researchers like to call these evil forged images adversarial samples. And now, this time around, OpenAI published a super fun piece of work to fool these game learning algorithms by changing some of their input visual information. As you will see in a moment, it is so effective that by only using a tiny bit of information, it can turn a powerful learning algorithm into a blabbering idiot. The first method adds a tiny bit of noise to a large portion of the video input, where the difference is barely perceptible, but it forces the learning algorithm to choose a different action that it would have chosen otherwise. In the other one, a different modification was used, that has a smaller footprint, but is more visible. For instance, in pong, adding a tiny fake ball to the game to coerce the learner into going down when it was originally planning to go up. The algorithm is able to learn game-specific knowledge for almost any other game to fool the player. Despite the huge difference in the results, I loved the elegant mathematical formulation of the two noise types, because despite the fact that they do something radically different, their mathematical formulation is quite similar, mathematicians like to say that we're solving the same problem, while optimizing for different target norms. Beyond DeepMind's Deep Q-Learning, two other high-quality learning algorithms are also fooled by this technique. In the white box formulation, we have access to the inner workings of the algorithm. But interestingly, a black box formulation is also proposed, where we know much less about the target system, but we know the game itself, and we train our own system and look for weaknesses in that. When we've found these weak points, we use this knowledge to break other systems. I can only imagine how much fun there was to be had for the authors when they were developing these techniques. Super excited to see how this arms race of creating more powerful learning algorithms, and in response, more powerful adversarial techniques to break them develops. In the future, I feel that the robustness of a learning algorithm, or in other words, its resilience against adversarial attacks will be just as important of a design factor as how powerful it is. There are a ton of videos published on the authors' website, make sure to have a look! And also, if you wish to support the series, make sure to have a look at our Patreon page. We kindly thank you for your contribution, it definitely helps keeping the series running. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=QFu0vZgMcqk",
        "paper_link": "http://rll.berkeley.edu/adversarial/",
        "paper_title": "Adversarial Attacks on Neural Network Policies"
    },
    {
        "video_id": "brs1qCDzRdk",
        "video_title": "Automatic Creation of Sketch Tutorials | Two Minute Papers #134",
        "position_in_playlist": 139,
        "description": "The paper \"How2Sketch: Generating Easy-To-Follow Tutorials for Sketching 3D Objects\" is available here:\nhttp://geometry.cs.ucl.ac.uk/projects/2017/how2sketch/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nClaudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-1582108/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Have a look at this magnificent idea: the input is a digital 3D model of an object and a viewpoint of our choice, and the output is an easy to follow, step by step breakdown on how to draw it. Automated drawing tutorials! I wish tools like this were available back when I was a child! Awesome! This technique offers a way to create the scaffoldings to help achieving the correct perspective and positioning for the individual elements of the 3D model, something that novice artists often struggle with. This problem is particularly challenging, because we have a bunch of competing solutions and we have to decide which one should be presented to the user. To achieve this, we have to include a sound mathematical description of how easy a drawing process is. The algorithm also makes adjustments to the individual parts of the model to make them easier to draw without introducing severe distortions to the shapes. The proposed technique uses graph theory to find a suitable ordering of the drawing steps. Beyond the scientific parts, there are a lot of usability issues to be taken into consideration: for instance, the algorithm should notify the user when a given guide is not to be used anymore and can be safely erased. Novice, apprentice, and adept users are also to be handled differently. To show the validity of this solution, the authors made a user study where they tested this new tutorial type against the most common existing solution, and found that the users were not only able to create more accurate drawings with it, but they were also enjoying the process more. I commend the authors for taking into consideration the overall experience of the drawing process, which is an incredibly important factor - if the user enjoyed the process, he'll surely come back for more, and of course, the more we show up, the more we learn. Some of these tutorials are available on the website of the authors, as always, I've linked it in the video description. If you are in the mood to draw, make sure to give it a go and let us know how it went in the comments section! Hell, even I am now in the mood to give this a try! If I disappear for a while, you know where I am. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=brs1qCDzRdk",
        "paper_link": "http://geometry.cs.ucl.ac.uk/projects/2017/how2sketch/",
        "paper_title": "How2Sketch: Generating Easy-To-Follow Tutorials for Sketching 3D Objects"
    },
    {
        "video_id": "u7kQ5lNfUfg",
        "video_title": "AI Makes Stunning Photos From Your Drawings (pix2pix) | Two Minute Papers #133",
        "position_in_playlist": 140,
        "description": "Online demo of pix2pix (try drawing there!): https://affinelayer.com/pixsrv/\n\nThe paper \"Image-to-Image Translation with Conditional Adversarial Nets\" and its source code is available here:\nhttps://phillipi.github.io/pix2pix/\n\nTwitter: https://twitter.com/search?vertical=default&q=pix2pix&src=typd\n\nMore amusing results:\nhttp://www.neogaf.com/forum/showthread.php?t=1346254&page=1\nhttp://thechive.com/2017/02/22/this-drawing-to-image-machine-is-made-of-nightmares-17-photos/\n\nRecommended for you:\nImage Editing with Generative Adversarial Networks - https://www.youtube.com/watch?v=pqkpIfu36Os\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nClaudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-408746/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In an earlier work, we were able to change a photo of an already existing design according to our taste. That was absolutely amazing. But now, hold onto your papers and have a look at this! Because here, we can create something out of thin air! The input in this problem formulation is an image, and the output is an image of a different kind. Let's call this process image translation. It is translation in a sense, that for instance, we can add an aerial view of a city as an input, and get the map of this city as an output. Or, we can draw the silhouette of a handbag, and have it translated to an actual, real-looking object. And we can go even crazier, for instance, day to night conversion of a photograph is also possible. What an incredible idea, and look at the quality of the execution. Ice cream for my eyes. And, as always, please don't think of this algorithm as the end of the road - like all papers, this is a stepping stone, and a few more works down the line, the kinks will be fixed, and the output quality is going to be vastly improved. The technique uses a conditional adversarial network to accomplish this. This works the following way: there is a generative neural network that creates new images all day, and a discriminator network is also available all day to judge whether these images look natural or not. During this process, the generator network learns to draw more realistic images, and the discriminator network learns to tell fake images from real ones. If they train together for long enough, they will be able to reliably create these image translations for a large set of different scenarios. There are two key differences that make this piece of work stand out from the classical generative adversarial networks: One - both neural networks have the opportunity to look at the before and after images. Normally we restrict the problem to only looking at the after images, the final results. And two - instead of only positive, both positive and negative examples are generated. This means that the generator network is also asked to create really bad images on purpose so that the discriminator network can more reliably learn the distinction between flippant attempts and quality craftsmanship. Another great selling point here is that we don't need several different algorithms for each of the cases, the same generic approach is used for all the maps and photographs, the only thing that is different is the training data. Twitter has blown up with fun experiments, most of them include cute drawings ending up as horrifying looking cats. As the title of the video says, the results are always going to be stunning, but sometimes, a different kind of stunning than we'd expect. It's so delightful to see that people are having a great time with this technique and it is always a great choice to put out such a work for a wide audience to play with. And if you got excited for this project, there are tons, and I mean tons of links in the video description, including one to the source code of the project, so make sure to have a look and read up some more on the topic, there's going to be lots of fun to be had! You can also try it for yourself, there is a link to an online demo in the description and if you post your results in the comments section, I guarantee there will be some amusing discussions. I feel that soon, a new era of video games and movies will dawn where most of the digital models are drawn by computers. As automation and mass-producing is a standard in many industries nowadays, we'll surely be hearing people going: \"Do you remember the good old times when video games were handcrafted? Man, those were the days!\". If you enjoyed this episode, make sure to subscribe to the series, we try our best to put out two of these videos per week. We would be happy to have have join our growing club of Fellow Scholars and be a part of our journey to the world of incredible research works such as this one. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u7kQ5lNfUfg",
        "paper_link": "https://phillipi.github.io/pix2pix/",
        "paper_title": "Image-to-Image Translation with Conditional Adversarial Nets"
    },
    {
        "video_id": "JzOc_NNY_zY",
        "video_title": "Real-Time Fiber-Level Cloth Rendering | Two Minute Papers #132",
        "position_in_playlist": 141,
        "description": "The paper \"Real-time Fiber-level Cloth Rendering\" is available here:\nhttp://www.cs.utah.edu/~kwu/rtfr.html\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nClaudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credits: https://pixabay.com/photo-1856679/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work shows us how to render a piece of cloth down to the level of fibers. This is a difficult problem, because we need to be able to handle models that are built from potentially over a hundred million fiber curves. This technique supports a variety of goodies. One - level of detail is possible. This means that the closer we get to the cloth, the more details appear, and that it is possible to create a highly optimized algorithm that doesn't render these details when they are not visible. This means a huge performance boost if we are zoomed out. Two - optimizations are introduced so that fiber-level self-shadows are computed in real time, which would normally be an extremely long process. Note that we're talking millions of fibers here! And three - the graphical card in your computer is amazingly effective at computing hundreds of things in parallel. However, its weak point is data transfer, at which it is woefully slow, to the point that it is often worth recomputing multiple gigabytes of data right on it just to avoid uploading it to its memory again. This algorithm generates the fiber curves directly on the graphical card to minimize such data transfers and hence, it maps really effectively to the graphical card. And the result is a remarkable technique that can render a piece of cloth down to the tiniest details, with multiple different kinds of yarn models, and in real-time. What I really like about this piece of work is that this is not a stepping stone, this could be used in many state of the art systems as-is, right now. The authors also made the cloth models available for easier comparisons in followup research works. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=JzOc_NNY_zY",
        "paper_link": "http://www.cs.utah.edu/~kwu/rtfr.html",
        "paper_title": "Real-time Fiber-level Cloth Rendering"
    },
    {
        "video_id": "ZUa5sNVSjGw",
        "video_title": "Shape and Material from Video | Two Minute Papers #131",
        "position_in_playlist": 142,
        "description": "The paper \"Recovering Shape and Spatially-Varying Surface Reflectance under Unknown Illumination\" is available here:\nhttp://www.cs.wm.edu/~ppeers/showPublication.php?id=Xia:2016:RSS\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nClaudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://www.flickr.com/photos/8143264@N08/4946656511/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Imagine the following: we put an object on a robot arm, and the input is a recorded video of it. And the output would be the digital geometry and a material model for this object. This geometry and material model we can plug into a photorealistic light simulation program to have a digital copy of our real-world object. First, we have to be wary of the fact that normally, solving such a problem sounds completely hopeless. We have three variables that we have to take into consideration: the lighting in the room, the geometry of the object, and the material properties of the object. If any two of the three variables is known, the problem is relatively simple and there are already existing works to address these combinations. For instance, if we create a studio lighting setup and know the geometry of the object as well, it is not that difficult to capture the material properties. Also, if the material properties and lighting is known, there are methods to extract the geometry of the objects. However, this is a way more difficult formulation of the problem, because out of the three variables, not two, not one, but zero are known. We don't have control over the lighting, the material properties can be arbitrary, and the geometry can also be anything. Several very sensible assumptions are being made, such as that our camera has to be stationary and the rotation directions of the object should be known, and some more of these are discussed in the paper in detail. All of them are quite sensible and they really don't feel limiting. The algorithm works the following way: in the first step, we estimate the lighting and leaning on this estimation, we build a rough initial surface model. And in the second step, using this surface model, we see a bit clearer, and therefore we can refine our initial guess for the lighting and material model. However, now that we know the lighting a bit better, we can again, get back to the surface reconstruction and improve our solution there. This entire process happens iteratively, which means, that first, we obtain a very rough initial guess for the surface, and we constantly refine this piece of surface to get closer and closer to the final solution. And now, feast your eyes on these incredible results, and marvel at the fact that we know next to nothing about the input, and the geometry and material properties almost magically appear on the screen. This is an amazing piece of work, and lots and lots of details and results are discussed the paper, which is quite well written and was a joy to read. Make sure to have a look at it, the link is available in the video description. Also, thanks for all the kind comments, I've experienced a recent influx of e-mails from people all around the world expressing how they are enjoying the series and many of them telling their personal stories and their relation to science, and how these new inventions are being talked about over dinner with the family and relatives. It has been such a delight to read these messages. Thank you! And also, thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZUa5sNVSjGw",
        "paper_link": "http://www.cs.wm.edu/~ppeers/showPublication.php?id=Xia:2016:RSS",
        "paper_title": "Recovering Shape and Spatially-Varying Surface Reflectance under Unknown Illumination"
    },
    {
        "video_id": "psOPu3TldgY",
        "video_title": "Learning to Fill Holes in Images | Two Minute Papers #130",
        "position_in_playlist": 143,
        "description": "The paper \"Scene Completion Using Millions of Photographs\" is available here:\nhttp://graphics.cs.cmu.edu/projects/scene-completion/scene-completion.pdf\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nClaudio Fernandes, Daniel John Benton, Dave Rushton-Smith, Sunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-1984308/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is from 2007, from ten years ago and I am sure you'll be surprised by how well it is still holding up to today's standards. For me, it was one of the works that foreshadowed the incredible power of data-driven learning algorithms. So, let's grab an image, and cut a sizeable part out of it, and try to algorithmically fill it with data that makes sense. Removing a drunk photobombing friend from your wedding picture, or a building blocking a beautiful view to the sea are excellent, and honestly, painfully real examples of this. This problem we like to call image completion or image inpainting. But mathematically, this may sound like crazy talk, who really knows what information should be there in these holes, let alone a computer? The first question is, why would we have to synthesize all these missing details from scratch? Why not start looking around in an enormous database of photographs and look for something similar? For instance, let's unleash a learning algorithm on one million images. And if we do so, we could find that there may be photographs in the database that are from the same place. But then, what about the illumination? The lighting may be different! Well, this is an enormous database, so then, we pick a photo that was taken at a similar time of the day and use that information! And as we can see in the results, the technique works like magic! Awesome! It doesn't require user made annotations or any sort of manual labor. These results were way, way ahead of the competition. And sometimes, the algorithm proposes a set of solutions that we can choose from. The main challenge of this solution is finding similar images within the database, and fortunately, even a trivial technique that we call nearest neighbor search can rapidly eliminate 99.99% of the dissimilar images. The paper also discusses some of the failure cases, which arise mostly from the lack of high-level semantic information, for instance, when we have to finish people, which is clearly not what this technique is meant to do, unless it's a statue of a famous person with many photographs taken in the database. Good that we're in 2017, and we know that plenty of research groups are already working on this, and I wouldn't be surprised to see a generative adversarial network-based technique to pop up for this in the very near future. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=psOPu3TldgY",
        "paper_link": "http://graphics.cs.cmu.edu/projects/scene-completion/scene-completion.pdf",
        "paper_title": "Scene Completion Using Millions of Photographs"
    },
    {
        "video_id": "kf-KViOuktc",
        "video_title": "AI Builds 3D Models From Images With a Twist | Two Minute Papers #129",
        "position_in_playlist": 144,
        "description": "The paper \"IM2CAD\" is available here:\nhttp://homes.cs.washington.edu/~izadinia/im2cad.html\n\nLSUN Challenge datasets: http://lsun.cs.princeton.edu/2016/\n\nMore related papers are available here:\nhttp://www.cs.toronto.edu/~fidler/projects/rent3D.html\nhttp://web.engr.illinois.edu/~slazebni/publications/iccv15_informative.pdf\nhttp://ieeexplore.ieee.org/document/6619238/?reload=true\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-389254/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. CAD stands for computer aided design, basically a digital 3D model of a scene. Here is an incredibly difficult problem: what if we give the computer a photograph of a living room, and the output would be a digital, fully modeled 3D scene. A CAD model. This is a remarkably difficult problem: just think about it! The algorithm would have to have an understanding of perspective, illumination, occlusions, and geometry. And with that in mind, have a look at these incredible results. Now, this clearly sounds impossible. Not so long ago we talked about a neural network-based technique that tried to achieve something similar and it was remarkable, but the output was a low resolution voxel array, which is kind of like an approximate model built by children from a few large lego pieces. The link to this work is available in the video description. But clearly, we can do better, so how could this be possible? Well, the most important observation is that if we take a photograph of a room, there is a high chance that the furniture within are not custom built, but mostly commercially available pieces. So who said that we have to build these models from scratch? Let's look into a database that contains the geometry for publicly available furniture pieces and find which ones are seen in the image! So here's what we do: given a large amount of training samples, neural networks are adept at recognizing objects on a photograph. That would be step number one. After the identification, the algorithm knows where the object is, now we're interested in what it looks like and how it is aligned. And then, we start to look up public furniture databases for objects that are as similar to the ones presented in the photo as possible. Finally, we put everything in its appropriate place, and create a new digital image with a light simulation program. This is an iterative algorithm, which means that it starts out with a coarse initial guess that is being refined many many times until some sort of convergence is reached. Convergence means that no matter how hard we try, only minor improvements can be made to this solution. Then, we can stop. And here, the dissimilarity between the photograph and the digitally rendered image was subject to minimization. This entire process of creating the 3D geometry of the scene takes around 5 minutes. And this technique can also estimate the layout of a room from this one photograph. Now, this algorithm is absolutely amazing, but of course, the limitations are also to be candidly discussed. While some failure cases arise from misjudging the alignment of the objects, the technique is generally quite robust. Non-cubic room shapes are also likely to introduce issues such as the omission or misplacement of an object. Also, kitchens and bathrooms are not yet supported. Note that this is not the only paper solving this problem, I've made sure to link some more related papers in the video description for your enjoyment. If you have found this interesting, make sure to subscribe and stay tuned for more Two Minute Papers episodes! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kf-KViOuktc",
        "paper_link": "http://homes.cs.washington.edu/~izadinia/im2cad.html",
        "paper_title": "IM2CAD"
    },
    {
        "video_id": "LmYKfU5O_NA",
        "video_title": "Digital Creatures Learn to Cooperate | Two Minute Papers #128",
        "position_in_playlist": 145,
        "description": "The paper \"Discovery of complex behaviors through contact-invariant optimization\" is available here:\nhttp://homes.cs.washington.edu/~todorov/papers/MordatchSIGGRAPH12.pdf\nhttp://homes.cs.washington.edu/~todorov/papers.html\n\nOur earlier episode on optimization: https://www.youtube.com/watch?v=1ypV5ZiIbdA\n\nOur technical write-up on our video rendering pipeline changes is available here:\nhttps://www.patreon.com/posts/improvements-for-7607896\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-768641/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Here's a really cool computer animation problem, you'll love this one: we take a digital character, specify an initial pose and a target objective - for instance, a position somewhere in space. And the algorithm has to come up with a series of smooth movements and contact interactions to obtain this goal. But these movements have to be physically meaningful, such as that self-intersection and non-natural contortions have to be avoided throughout this process. Keep an eye out for the white cross to see where the target positions are. Now, for starters, it's cool to see that humanoids are handled quite well, but here comes the super fun part: the mathematical formulation of this optimization problem does not depend on the body type at all, therefore, both humanoids and almost arbitrarily crazy, non-humanoid creatures are also supported. If we make this problem a bit more interesting, and make changes to the terrain to torment these creatures, we'll notice that they come up with sensible movements to overcome these challenges. These results are absolutely amazing. And this includes obtaining highly non-trivial target poses, such as handstands. The goal does not necessarily have to be a position, but it can be an orientation or a given pose as well. We can even add multiple characters to the environment and ask them to join forces to accomplish a task together. And here you can see that both characters take into consideration the actions of the other one and not only compensate accordingly, but they make sure that this happens in a way that brings them closer to their objective. It is truly incredible to see how these digital characters can learn such complex animations in a matter of minutes. A true testament to the power of mathematical optimization algorithms. If you wish to hear more about how optimization works, we've had a previous episode on this topic, make sure to check it out, it includes a rigorous mathematical study on how to make the perfect vegetable stew. The link is available in the video description. And, if you feel a bit addicted to Two Minute Papers, please note that these episodes are available in early access through Patreon, click on the icon with the \"p\" at the ending screen if you're interested. It also helps us a great deal in improving the quality of the series. We try to be as transparent as possible, and every now and then we write a technical memo to summarize the recent improvements we were able to make, and this is all thanks to you. If you're interested, I've put a link to the latest post in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=LmYKfU5O_NA",
        "paper_link": "http://homes.cs.washington.edu/~todorov/papers/MordatchSIGGRAPH12.pdf",
        "paper_title": "Discovery of complex behaviors through contact-invariant optimization"
    },
    {
        "video_id": "XbuEYcFfl6s",
        "video_title": "How Do Hollywood Movies Render Smoke? | Two Minute Papers #127",
        "position_in_playlist": 146,
        "description": "The paper \"Importance Sampling Techniques for Path Tracing in\nParticipating Media\" is available here:\nhttps://www.solidangle.com/research/egsr2012_volume.pdf\n\nImplementation in 2k (binary + video without code):\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/volumetric-path-tracing-with-equiangular-sampling-in-2k/\n\nSolid Angle (Arnold renderer) webpage + Oscar award headline:\nhttps://www.solidangle.com/\nhttps://www.solidangle.com/news/2017-scitech-award/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nRendered scene credits:\nBedroom - http://www.blendswap.com/blends/view/17385\nSkin - http://www.blendswap.com/blends/view/84082\nShadertoy - https://www.shadertoy.com/view/lsV3zV\n\nThumbnail background image credit: https://pixabay.com/photo-690293/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. How do people create these beautiful computer generated images and videos that we see in Hollywood blockbuster movies? In the world of light simulation programs, to obtain a photorealistic image, we create a digital copy of a scene, add a camera and a light source, and simulate the paths of millions of light rays between the camera and the light sources. This technique we like to call path tracing and it may take several minutes to obtain only one image on a powerful computer. However, in these simulations, the rays of light are allowed to bounce off of the surface of objects. In reality, many objects are volumes, where the rays of light can penetrate their surface and scatter around before exiting or being absorbed. Examples include not only rendering amazingly huge smoke plumes and haze, but all kinds of translucent objects, like our skin, marble, wax, and many others. Such an extended simulation program we call not path tracing but volumetric path tracing, and we can create even more beautiful images with it, however, this comes at a steep price: if the classical path tracing took several minutes per image, this addition of complexity often bumps up the execution time to several hours. In order to save time, we have to realize that not all light paths contribute equally to our image. Many of them carry barely any contributions, and only a tiny tiny fraction of these paths carry the majority of the information that we see in these images. So what if we could create algorithms that know exactly where to look for these high value light paths and systematically focus on them? This family of techniques we call importance sampling methods. These help us finding the regions where light is concentrated, if you will. This piece of work is an excellent new way of doing importance sampling for volumetric path tracing, and it works by identifying and focusing on regions that are the most likely to scatter light. And, it beats the already existing importance sampling techniques with ease. Now, to demonstrate how simple this technique is, a few years ago, during a discussion with one of the authors, Marcos Fajado, I told him that I would implement their method in real time on the graphical card in a smaller than four kilobyte program. So we made a bet. Four kilobytes is so little, we can store only a fraction of a second of mp3 music in it. Also, this is an empty file generated with Windows Word. Apparently, in some software systems, the definition of \"nothing\" takes several times more than 4 kilobytes. And after a bit of experimentation, I was quite stunned by the results, because the final result was less than 2 kilobytes, even if support for some rudimentary animations is added. The whole computer program that executes volumetric path tracing with this equiangular importance sampling technique fits on your business card... twice. Absolute insanity. I've put a link discussing some details in the video description. Now, don't be fooled by the simplicity of the presentation here, the heart and soul of the algorithm that created the rocket launch scene in Men in Black 3 is the same as this one. Due to legal reasons it is not advisable to show it to you in this video, but this is fortunately one more excellent reason for you to have a look at the paper! As always, the link is available in the video description. This is my favorite kind of paper, where there are remarkably large gains to be had, and it can be easily added to pretty much any light simulation program out there. I often like to say that the value/complexity ratio is tending towards infinity. This work is a prime example of that. By the way, Marcos and his team recently won a technical Oscar award, not only for this, but for their decades of hard work on their Arnold renderer, which is behind many many Hollywood productions. I've put a link to their website in the video description as well, have a look! Congrats guys! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XbuEYcFfl6s",
        "paper_link": "https://www.solidangle.com/research/egsr2012_volume.pdf",
        "paper_title": "Importance Sampling Techniques for Path Tracing in\nParticipating Media"
    },
    {
        "video_id": "-all65C-dh0",
        "video_title": "Fast Photorealistic Fur and Hair With Cone Tracing | Two Minute Papers #126",
        "position_in_playlist": 147,
        "description": "Our Twitter feed is available here:\nhttps://twitter.com/karoly_zsolnai\n\nThe paper \"Cone Tracing for Furry Object Rendering\" is available here:\nhttp://gaps-zju.org/mlchai/resources/qin2014cone.pdf\nhttp://gaps-zju.org/mlchai/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-640498/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a cone-based ray tracing algorithm for rendering photorealistic images of hair and furry objects, where the number of hair strands is typically over a hundred thousand. Okay, wait. What do these term mean exactly? Ray tracing means a bona fide light simulation program where we follow the path of many millions of light rays between the light sources and our camera. This usually means that light reflections and refractions, lens blur and defocus are taken into consideration. This feature is often referred to as depth of field, or DoF in short as you can see in the video. A fully ray traced system like this for hair and fur leads to absolutely beautiful images that you also see throughout this footage. So what about the cone-based part? Earlier, we had an episode about Voxel Cone Tracing, which is an absolutely amazing technique to perform ray tracing in real time. It works by replacing these infinitely thin light rays with thicker, cone-shaped rays which reduces the execution time of the algorithm significantly, at the cost of mostly a minor, sometimes even imperceptible degradation in image quality. Since the hair strands that we're trying to hit with the rays are extremely thin, and cone tracing makes the rays thicker, extending this concept to rendering fur without nontrivial extensions is going to be a fruitless endeavor. The paper contains techniques to overcome this issue, and an efficient data structure is proposed to store and find the individual hair strands, and a way to intersect these cones with the fibers. The algorithm is also able to adapt the cone sizes to the scene we have at hand. The previous techniques typically took at least 20 to 30 minutes to render one image, and with this efficient solution, we'll be greeted by a photorealistic image at least four to six times quicker, in less than 5 minutes, while some examples were completed in less than a minute. I cannot get tired of seeing these tiny photorealistic furry animals in Pixar movies and I am super happy to see there's likely going to be much, much more of these. By the way, if you're subscribed to the channel, please click the little bell next to the subscription icon to make sure that you never miss an episode. Also, you can also follow us on twitter for updates. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-all65C-dh0",
        "paper_link": "http://gaps-zju.org/mlchai/resources/qin2014cone.pdf",
        "paper_title": "Cone Tracing for Furry Object Rendering"
    },
    {
        "video_id": "vaFhLAbPi8w",
        "video_title": "Game AI Development With OpenAI Universe | Two Minute Papers #125",
        "position_in_playlist": 148,
        "description": "OpenAI Universe + blog post:\nhttps://openai.com/blog/universe/\nhttps://universe.openai.com/\n\nAlso, make sure to check out Google DeepMind's lab:\nhttps://github.com/deepmind/lab\nhttps://deepmind.com/blog/open-sourcing-deepmind-lab/\n\nFor the record: no, I am not an Edge user. :)\n\nTerrain learning footage credit: http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-821568/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. OpenAI's Gym was a selection of gaming environments for reinforcement learning algorithms. This is a class of techniques that are able to learn and perform an optimal chain of actions in an environment. This environment could be playing video games, navigating a drone around, or teaching digital creatures to walk. In this system, people could create new reinforcement learning programs and decide whose AI is the best. Gym was a ton of fun, ... but have a look this one! How is this different from Gym? This new software platform, Universe works not only for reinforcement learning algorithms, but for arbitrary programs. Like a freestyle wrestling competition for AI researchers! The list of games include GTA V, Mirror's Edge, Starcraft 2, Civilization 5, Minecraft, Portal and a lot more this time around. Super exciting! You can download this framework right now and proceed testing. One can also perform different browser tasks, such as booking a plane ticket and other endeavors that require navigating around in a web browser interface. Given the current software architecture for Universe, practically any task where automation makes sense can be included in the future. And, we don't need to make any intrusive changes to the game itself, in fact, we don't even have to have access to the source code. This is huge, especially given that many of these games are proprietary software, so to make this happen, individual deals had taken place between the game development companies and OpenAI. When the company was founded by Elon Musk and Sam Altman, they have picked up so many of the most talented AI researchers around the globe. And I am so happy to see that it really, really shows. There is an excellent blog post describing the details of the system, make sure to have a look! Also, I reckon that our comments section is the absolute best I've seen on YouTube. Feel free to participate or start a discussion, there are always plenty of amazing ideas in the comments section. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=vaFhLAbPi8w"
    },
    {
        "video_id": "WovbLx8C0yA",
        "video_title": "Enhance! Super Resolution From Google | Two Minute Papers #124",
        "position_in_playlist": 149,
        "description": "The paper \"RAISR: Rapid and Accurate Image Super Resolution\" is available here:\nhttps://arxiv.org/abs/1606.01299\n\nAdditional supplementary materials: https://drive.google.com/file/d/0BzCe024Ewz8ab2RKUFVFZGJ4OWc/view\n\nBlog posts:\nhttps://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html\nhttps://www.blog.google/products/google-plus/saving-you-bandwidth-through-machine-learning/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nImage credits:\nSuper resolution -\nhttps://en.wikipedia.org/wiki/Super-resolution_imaging\nhttps://commons.wikimedia.org/wiki/File:An_example_of_super_resolution_with_still_RAW_photo..jpg\nImage inpainting - http://www.cs.toronto.edu/~mangas/teaching/320/assignments/a2/\nhttp://cimg.eu/greycstoration/demonstration.shtml\n\nThumbnail background image credit: https://pixabay.com/photo-1844081/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What is super resolution? Super resolution is process where our input is a coarse, low resolution image, and the output is the same image, but now with more details and in high resolution. We'll also refer to this process as image upscaling. And in this piece of work, we are interested in performing single image super resolution, which means that no additional data is presented to the algorithm that could help the process. Despite the incredible results seen in practically any of the crime solving television shows out there, our intuition would perhaps say that this problem, for the first sight, sounds impossible. How could one mathematically fill in the details when these details are completely unknown? Well, that's only kind of true. Let's not confuse super resolution with image inpainting, where we essentially cut an entire part out of an image and try to replace it leaning on our knowledge of the surroundings of the missing part. That's a different problem. Here, the entirety of the image is known, and the details require some enhancing. This particular method is not based on neural networks, but is still a learning-based technique. The cool thing here, is that we can use a training dataset, that is, for all intents and purposes, arbitrarily large. We can just grab a high resolution image, convert it to a lower resolution and we immediately have our hands on a training example for the learning algorithm. These would be the before and after images, if you will. And here, during learning, the image is subdivided into small image patches, and buckets are created to aggregate the information between patches that share similar features. These features include brightness, textures, and the orientation of the edges. The technique looks at how the small and large resolution images relate to each other when viewed through the lens of these features. Two remarkably interesting things arose from this experiment: - one, it outperforms existing neural network-based techniques, - two, it only uses 10 thousand images, and one hour of training time, which is in the world of deep neural networks, is so little, it's completely unheard of. Insanity. Really, really well done. Some tricks are involved to keep the memory consumption low, the paper discusses how it is done, and there are also plenty of other details within, make sure to have a look, as always, it is linked in the video description. It can either be run directly on the low resolution image, or alternatively we can first run a cheap and naive decade-old upscaling algorithm, and run this technique on this upscaled output to improve it. Note that super resolution is a remarkably competitive field of research, there are hundreds and hundreds of papers appearing on this every year, and almost every single one of them seems to be miles ahead of the previous ones. Where in reality, the truth is that most of these methods have different weaknesses and strengths, and so far I haven't seen any technique that would be viable for universal use. To make sure that a large number of cases is covered, the authors posted a sizeable supplementary document with comparisons. This gives so much more credence to the results. I am hoping to see a more widespread adoption of this in future papers in this area. For now, when viewing websites, I feel that we are close to the point where we could choose to transmit only the lower resolution images through the network and perform super resolution on them locally on our phones and computers. This will lead to significant savings on network bandwidth. We are living amazing times indeed. If you are enjoying the series, make sure to subscribe to the channel, or you can also pick up really cool perks on our Patreon page through this icon here with the letter P. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=WovbLx8C0yA",
        "paper_link": "https://arxiv.org/abs/1606.01299",
        "paper_title": "RAISR: Rapid and Accurate Image Super Resolution"
    },
    {
        "video_id": "Yd4blFeRTEw",
        "video_title": "Large-Scale Fluid Simulations On Your Graphics Card | Two Minute Papers #123",
        "position_in_playlist": 150,
        "description": "The paper \"A scalable Schur-complement fluids solver for heterogeneous compute platforms\" is available here:\nhttp://graphics.cs.wisc.edu/Papers/2016/LMAS16/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit: https://pixabay.com/photo-1913559/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The better fluid simulation techniques out there typically run on our graphical cards, which, if we formulate the problem in a way that the many compute units within can do in parallel, if they row in unison, if you will, we'll be greeted by an incredible bump in the speed of the simulation. This leads to amazingly detailed simulations, many of which you'll see in this footage. It's going to be really good! However, sometimes we have a simulation domain that is so large, it simply cannot be loaded into the memory of our graphical card. What about those problems? Well, the solution could be subdividing the problem into independent subdomains and solving them separately on multiple devices. Slice the problem up into smaller, more manageable pieces. Divide and conquer. But wait, we would just be pretending that these subdomains are independent, because in reality, they are clearly not, because there is a large amount of fluid flowing between them, and it takes quite a bit of algebraic wizardry to make sure that the information exchange between these devices happens correctly, and on time. But if we do it correctly, we can see our reward on the screen. Let's marvel at it together! Oh, hohoho, yeah! I cannot get tired of this. Typically, the simulation in the individual subdomains are computed on one or more separate graphical cards, and the administration on the intersecting interface takes place on the processor. The challenge of such a solution is that one has to be able to show that the solution of this problem formulation is equivalent to solving the huge original problem. And, it also has to be significantly more efficient to be useful for projects in the industry. The paper is one of the finest pieces of craftsmanship I've seen lately, the link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Yd4blFeRTEw",
        "paper_link": "http://graphics.cs.wisc.edu/Papers/2016/LMAS16/",
        "paper_title": "A scalable Schur-complement fluids solver for heterogeneous compute platforms"
    },
    {
        "video_id": "HO1LYJb818Q",
        "video_title": "AI Makes 3D Models From Photos | Two Minute Papers #122",
        "position_in_playlist": 151,
        "description": "The paper \"Learning a Probabilistic Latent Space of Object Shapes \nvia 3D Generative-Adversarial Modeling\" and its source code is available here:\nhttp://3dgan.csail.mit.edu/\nhttps://arxiv.org/pdf/1610.07584v2.pdf\n\nMore about generative adversarial networks (and some explanations):\nImage Editing with Generative Adversarial Networks - https://www.youtube.com/watch?v=pqkpIfu36Os\nImage Synthesis From Text With Deep Learning  - https://www.youtube.com/watch?v=rAbhypxs1qQ\n\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail image background credit: https://pixabay.com/photo-881120/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What if we tried to build a generative adversarial network for 3D data? This means that this network would work not on the usual 2 dimensional images, but instead, on 3 dimensional shapes. So, the generator network generates a bunch of different 3 dimensional shapes, and the basic question for the discriminator network would be - are these 3D shapes real or synthetic? The main use case of this technique can be, and now watch closely, taking a photograph from a piece of furniture, and automatically getting a digital 3D model of it. Now it is clear for both of us that this is still a coarse, low resolution model, but it is incredible to see how a machine can get a rudimentary understanding of 3D geometry in the presence of occlusions, lighting, and different camera angles. That's a stunning milestone indeed! It also supports interpolation between two shapes, which means that we consider the presumably empty space between the shapes as a continuum, and imagine new shapes that are closer to either one or the other. We can do this kind of interpolation, for instance between two chair models. But the exciting thing is that no one said it has to be two objects of the same class. So we can go even crazier, and interpolate between a car and a boat. Since the technique works on a low-dimensional representation of these shapes, we can also perform these crazy algebraic operations between them that follow some sort of intuition. We can add two chairs together or subtract different kinds of tables from each other. Absolute madness. And one of the most remarkable things about the paper is that the learning took place on a very limited amount of data, not more than 25 training examples per class. One class we can imagine as one object type, such as, chairs, tables or cars. The authors made the the source code and a pretrained network available on their website, the link is in the video description, make sure to have a look! I am so happy to see breakthroughs like this in machine learning research. One after another in quick succession. This work is surely going to spark a lot of followup papers, and we'll soon find ourselves getting extremely high quality 3D models from photographs. Also, imagine combining this with a 3D printer! You take a photograph of something, run this algorithm on it, and then print a copy of that furniture or appliance for yourself. We are living amazing times indeed! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HO1LYJb818Q",
        "paper_link": "http://3dgan.csail.mit.edu/",
        "paper_title": "Learning a Probabilistic Latent Space of Object Shapes \nvia 3D Generative-Adversarial Modeling"
    },
    {
        "video_id": "MtWtY4DdiWs",
        "video_title": "Text Style Transfer | Two Minute Papers #121",
        "position_in_playlist": 152,
        "description": "The paper \"Awesome Typography: Statistics-Based Text Effects Transfer\" is available here:\nhttps://arxiv.org/abs/1611.09026\n\nRecommended for you:\nArtistic Style Transfer For Videos - https://www.youtube.com/watch?v=Uxax5EKg0zA\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail image credit: https://pixabay.com/photo-851328/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Before we start, it is important to emphasize that this paper is not using neural networks. Not so long ago, in 2015, the news took the world by storm: researchers were able to create a novel neural network-based technique for artistic style transfer, which had quickly become a small subfield of its own within machine learning. The problem definition was the following: we provide an input image and a source photograph, and the goal is to extract the artistic style of this photo and apply it to our image. The results were absolutely stunning, but at the same time, it was difficult to control the outcome. Later, this technique was generalized for higher resolution images, and instead of waiting for hours, it now works in almost real time and is used in several commercial products. Wow, it is rare to see a new piece of technology introduced to the markets so quickly! Really cool! However, this piece of work showcases a handcrafted algorithm that only works on a specialized case of inputs, text-based effects, but in this domain, it smokes the competition. And here, the style transfer happens not with any kind of neural network or other popular learning algorithm, but in terms of statistics. In this formulation, we know about the source text as well, and because of that we know exactly the kind of effects that are applied to it. Kind of like a before and after image for some beauty product, if you will. This opens up the possibility of analyzing its statistical properties and applying a similar effect to practically any kind of text input. The term statistically means that we are not interested in one isolated case, but we describe general rules, namely, in what distance from the text, what is likely to happen to it. The resulting technique is remarkably robust and works on a variety of input output pairs, and is head and shoulders beyond the competition, including the state of the art neural network-based techniques. That is indeed quite remarkable. I expect graphic designers to be all over this technique in the very near future. This is an excellent, really well-written paper and the evaluation is also of high quality. If you wish to see how one can do this kind of magic by hand without resorting to neural networks, don't miss out on this one and make sure to have a look! There is also a possibility of having a small degree of artistic control over the outputs, and who knows, some variant of this could open up the possibility of a fully animated style transfer from one image. Wow! And before we go, we'd like to send a huge shoutout to our Fellow Scholars who contributed translations of our episodes for a variety of languages. Please note that the names of the contributors are always available in the video description. It is really great to see how the series is becoming more and more available for people around the globe. If you wish to contribute, click on the cogwheel icon in the lower right and the substitles/cc text. Thank you so much! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MtWtY4DdiWs",
        "paper_link": "https://arxiv.org/abs/1611.09026",
        "paper_title": "Awesome Typography: Statistics-Based Text Effects Transfer"
    },
    {
        "video_id": "oitGRdHFNWw",
        "video_title": "Deep Learning Program Hallucinates Videos | Two Minute Papers #120",
        "position_in_playlist": 153,
        "description": "The paper \"Generating Videos with Scene Dynamics\" and its source code, and a pre-trained network is available here:\nhttp://web.mit.edu/vondrick/tinyvideo/\n\nRecommended for you:\nImage Synthesis From Text With Deep Learning  - https://www.youtube.com/watch?v=rAbhypxs1qQ\nWhat is an Autoencoder? - https://www.youtube.com/watch?v=Rdpbnd0pCiI\nHallucinating Images With Deep Learning - https://www.youtube.com/watch?v=hnT-P3aALVE\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail image credit: https://pixabay.com/photo-1751455/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Ever thought about the fact that we have a stupendously large amount of unlabeled videos on the internet? And, of course, with the ascendancy of machine learning algorithms that can learn by themselves, it would be a huge missed opportunity to not make any use of all this free data. This is a crazy piece of work where the idea is to unleash a neural network on a large number of publicly uploaded videos on the internet, and see how well it does if we ask it to generate new videos from scratch. Here, unlabeled means there is no information as to what we see in these videos, they are just provided as-is. Machine learning methods that work on this kind of data, we like to call unsupervised learning techniques. This work is based on a generative adversarial network. Wait, what does this mean exactly? This means that we have two neural networks that race each other, where one tries to generate more and more real-looking animations, and passes it over to the other that learns to tell real footage from fake ones. The first we call the generator network, and the second is the discriminator network. They try to outperform each other, and this rivalry goes on for quite a while and improves the quality of output for both neural networks, hence the name, generative adversarial networks. At first, we have covered this concept that was used to generate images from written text descriptions. The shortcoming of this approach was the slow training time that led to extremely tiny, low resolution output images. This was remedied by a followup work which proposed a two-stage version of this architecture. We have covered this in an earlier Two Minute Papers episode, as always, the link is available in the video description. It would not be an understatement to say that I nearly fell off the chair when seeing these incredible results. So, where do we go from here? What shall be the next step? Well, of course, video! However, the implementation of such a technique is far from trivial. In this piece of work, the generator network learns not on the original representation of the videos, but on the foreground and background video streams separately, and it also has to learn what combination of these yields realistic footage. This two-stream architecture is particularly useful in modeling real-world videos where the background is mostly stationary and there is an animated movement in the foreground. A train passing the station or people playing golf on the field are excellent examples of this kind of separation. We definitely need a high quality discriminator network as well, as in the final synthesized footage, not only the foreground and background must go well together, but the synthesized animations also have to be believable for human beings. This human being is, in our case, is represented by the discriminator network. Needless to say, this problem is extremely difficult and the quality of the discriminator network makes or breaks this magic trick. And of course, the all important question immediately arises: if there are multiple algorithms performing this action, how do we decide which one is the best? Generally, we get a few people, and show them a piece of synthesized footage with this algorithm and previous works, and have them decide which they deem more realistic. This is still the first step - I expect these techniques to improve so rapidly that we'll soon find ourselves testing against real-world footage. And who knows, sometimes perhaps failing to recognize which is which. The results in the paper show that this new technique beats the previous techniques by a significant margin, and that users have a strong preference towards the two-stream architecture. The previous technique they compare against is an autoencoder, which we have discussed in a previous Two Minute Papers episode, check it out, it is available in the video description! The disadvantages of this approach are quite easy to identify this time around: we have a very limited resolution for these output video streams, that is, 64x64 pixels for 32 frames, which, even at modest framerate, is just slightly over one second of footage. The synthesized results vary greatly in quality, but it is remarkable to see that the machine can have a rough understanding of the concept of a large variety of movement and animation types. It is really incredible to see that the neural network learns about the representations of these objects and how they move, even when it wasn't explicitly instructed to do so. We can also visualize what the neural network has learned. This is done by finding different image inputs that make a particular neuron extremely excited. Here, we see a collection of inputs including these activations for images of people and trains. The authors' website is definitely worthy of checking out as some of the submenus are quite ample in results. Some amazing, some, well, a bit horrifying, but what is sure is that all of them are quite interesting. And before we go, a huge shoutout to L\u00e1szl\u00f3 Cs\u00f6ndes, who helped us quite a bit in sorting out a number of technical issues with the series. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=oitGRdHFNWw",
        "paper_link": "http://web.mit.edu/vondrick/tinyvideo/",
        "paper_title": "Generating Videos with Scene Dynamics"
    },
    {
        "video_id": "7aLda2E0Yyg",
        "video_title": "Amazing Slow Motion Videos With Optical Flow | Two Minute Papers #119",
        "position_in_playlist": 154,
        "description": "The paper \"An Iterative Image Registration Technique\nwith an Application to Stereo Vision\" is available here:\nhttp://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf\n\nOur earlier episode on extrapolation:\nhttps://www.youtube.com/watch?v=AHl2JjGsu0s\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nOther video credits:\nSimulating Viscosity and Melting Fluids - https://www.youtube.com/watch?v=KgIrnR2O8KQ&list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI&index=2\nModeling Colliding and Merging Fluids - https://www.youtube.com/watch?v=uj8b5mu0P7Y&list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI&index=8\nMultiphase Fluid Simulations - https://www.youtube.com/watch?v=cUWDeDRet4c&list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI&index=11\n\nThumbnail background image credit: https://pixabay.com/photo-1032741/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I am really excited to show this to you as I was looking to make this episode for quite a while. You'll see lots of beautiful slow-motion footage during the narration. And at first, it may seem disconnected from the narrative, but by the end of video, you'll understand why they look the way they do. Now, before we proceed, let's talk about the difference between interpolation and extrapolation. Interpolation means that we have measurement points for a given quantity, and we'd like to know what happened between these points. For instance, we have two samples of a person's location at four and at five o'clock, and we'd like to know where the guy was at four thirty. However, if we're doing extrapolation, we're interested in guessing a quantity beyond the reach of our sample points. For instance, extrapolation would be predicting what happens after the very last frame of the video. In our earlier episode, we talked about financial extrapolation, make sure to have a look, it was super fun. The link is in the video description. Optical flow is really useful because it can do this kind of interpolation and extrapolation for images. So let's do one of them right now. You'll see which it will be in a second. So this is a classical scenario that we often encounter when producing a new Two Minute Papers episode - here, we have a 25 or 30 frames per second video on a 60 frames per second timeline. This means that roughly only every other frame is duplicated, and offers no new information. You can see this as I step through these individual frames. The more astute Fellow Scholars immediately would point out that wait, we have a lot of before and after image pairs, so we could do a lot better! Why don't we try to estimate what happened between these images? And that is exactly what we call frame interpolation. Interpolation because it is something between two known measurement points. And if we run the optical flow algorithm that can accomplish this, we can fill in these doubled frames with new ones that actually carry new information. So the ratio here was roughly two to one. Roughly every other frame provides new information. Super cool! So, what are the limits of this technique? What if we artificially slow the video down, so that it's much longer, so not only every other, but most of the frames are just duplicates? This results in a boring and choppy animation. Can we fill those in too? Note that the basic optical flow equations are written for tiny changes in position, so we shouldn't expect it to be able to extrapolate or interpolate any quantity over a longer period of time. But of course, it always depends on the type of motion we have at hand, so let's give it a try! As you can see, with optical flow, the algorithm has an understanding of the motions that take place in the footage, and because of that, we can get some smooth, buttery, slow motion footage that is absolutely mesmerizing. Almost like shooting with a slow motion camera. And note that the majority of these frames were not containing any information, and this motion was synthesized from these distant sample points that are miles and miles away from each other. However, it is also important to point out that optical flow is not a silver bullet and it should be used with moderation and special care as it can also introduce nasty artifacts like the one that you see here. This is due to an abrupt, high frequency change that is more difficult to predict than a slow and steady translation or rotation motion. To avoid these cases, we can use a much simpler frame interpolation technique that we call frame blending. This is a more naive technique that doesn't do any meaningful guesswork and computes the average of the two results. Why don't we give this one a try too? Or even better, let's have a look at the difference between the original choppy footage, and the interpolated versions with frame blending and optical flow. If we do that, we see that frame blending is unlikely to give us nasty artifacts, but in return, the results are significantly more limited compared to optical flow because it doesn't have an understanding of the motion taking place in the footage. So, the question is, when to use which? Well, until we get an algorithm that is able to adaptively decide when to use what, it still comes down to individual judgement and sometimes quite a bit of trial and error. I'd like to make it extremely sure that you don't leave this video thinking that this is the only application of optical flows. It's just one of the coolest ones! But this motion estimation technique also has many other uses. For instance, if we have an unmanned aerial vehicle, it's really great if we can endow it with an optical flow sensor, because then, it will be able to know in which direction it needs to rotate to avoid a tree, or whether it is stable or not at a given point in time. And, with your support on Patreon, we were not only able to bump up the resolution of future Two Minute Papers episodes to 4K, but we're also running them at true 60 frames per second, which means that every footage can undergo either a frame blending or optical flow step to make the animations smoother and more enjoyable for you. This takes a bit of human labor and is computationally expensive, but our new Two Minute Papers rig is now capable of handling this. It is fantastic to see that you Fellow Scholars are willing to support the series, and through this, we can introduce highly desirable improvements to the production pipeline. This is why we thank you at the end of every episode for your generous support. You Fellow Scholars are the best YouTube audience anywhere. And who knows, maybe one day, we'll be at a point where Two Minute Papers can be a full time endeavor, and we'll be able to make even more elaborate episodes. As I am tremendously enjoying making these videos, that would be absolutely amazing. Have you found any of these disturbing optical flow artifacts during this episode? Have you spotted some of these in other videos on YouTube? Let us know in the comments section so we can learn from each other. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7aLda2E0Yyg",
        "paper_link": "http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf",
        "paper_title": "An Iterative Image Registration Technique\nwith an Application to Stereo Vision"
    },
    {
        "video_id": "iOWamCtnwTc",
        "video_title": "Neural Network Learns The Physics of Fluids and Smoke | Two Minute Papers #118",
        "position_in_playlist": 155,
        "description": "The paper \"Accelerating Eulerian Fluid Simulation With Convolutional Networks\" and its source code is available here:\nhttp://cims.nyu.edu/~schlacht/CNNFluids.htm\nhttps://users.cg.tuwien.ac.at/zsolnai/accelerating-eulerian-fluid-simulation-convolutional-networks/\nhttps://github.com/google/FluidNet\n\nThe mentioned previous work has used an SPH-based Lagrangian simulation, performed the regression with regression forests, and the process also has included a fair amount of feature engineering. It is an excellent piece of work by the name \"Data-driven Fluid Simulations using Regression Forests\" and is a highly recommended read:\nhttps://www.inf.ethz.ch/personal/ladickyl/fluid_sigasia15.pdf\nhttps://www.youtube.com/watch?v=kGB7Wd9CudA\n\nVideo credits:\nSurface-Only Liquids - https://www.youtube.com/watch?v=-rf_MDh-FiE&list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI&index=6\nSchr\u00f6dinger's Smoke - https://www.youtube.com/watch?v=heY2gfXSHBo&list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI&index=5\n\nThumbnail image background credit: https://pixabay.com/photo-889131/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work is still in progress done by one of the members of the Google Brain research team and several researchers from the amazing New York University. The goal was to show a neural network video footage of lots and lots of fluid and smoke simulations, and have it learn how the dynamics work, to the point that it can continue and guess how the behavior of a smoke puff would change in time. We stop the video and it would learn how to continue it, if you will. Now that is a tall order if I've ever seen one. Most of this episode will not be about the technical details of this method, but about the importance and ramifications of such a technique. And since almost all the time, our episodes are about already published works, it also makes a great case study on how to evaluate and think about the merits and shortcomings of a research project that is still in the works. This definitely is an interesting take as normally, we use neural networks to solve problems that are otherwise close to impossible to tackle. Here, the neural networks are applied to solve something that we already know how to solve. And the question immediately comes to mind, why would anyone bother to do that? We've had the very least 20 episodes on different kinds of incredible fluid simulation techniques, so it is abundantly clear that this is a problem that we can solve. However, the neural networks does not only solve it correctly in a sense that the results are easily confused with real footage, but what's more, the execution time of the algorithm is in the order of a few milliseconds for a reasonably sized simulation. This normally takes several minutes with traditional techniques. It does something that we already know quite well how to do, but it does it better in many regards. Loving the idea behind this work. Training is a pre-processing step that is a long and arduous process that only has to be done once, and afterwards, querying the neural network, that is, predicting what happens next in the simulation runs almost immediately. In any case, in way less time than calculating all the forces and pressures in the simulation while retaining high quality results. It is like the preparation for an exam that may take weeks, but when we're finally there in the examination room, if we're well prepared, we make short work of the puny questions the professor has presented us with. I am quietly noting that during my college years, I was also studying the beautiful Navier-Stokes equations and even as a highly motivated student, it took several months to understand the theory and write my first fluid simulator. This neural network can learn something very similar in a matter of days. What a stunning and, may I say humiliating revelation. Note that this piece of work has not yet been peer-reviewed, there are some side by side comparisons with real simulations to validate the accuracy of the algorithm, but more rigorous analysis is required before publishing. The failure cases for classical hand-crafted techniques are easier to identify because of the fact that their mathematical description is available for scrutiny. In the case of a neural network, this piece of mathematics is also there, but it's not intuitive for human beings, therefore it is harder to assess when it works well and when it is expected to break down. We should be particularly vigilant about this fact when evaluating a task performed by any kind of neural network-based learning algorithm. For now, the results look quite reassuring, even the phenomenon of a smoke puff bouncing back from an object is modeled with high fidelity. There was a loosely related work from the ETH Z\u00fcrich and Disney Research in Switzerland, and enumerating the differences is a bit too technical for such a short video, but I have included it in the video description box for the more curious Fellow Scholars out there. Now you might have noticed the lack of the usual disclaimer in the thumbnail image, stating that I did not take any part in the project, which was not the case this time. I feel that it is important to mention my affiliation, even though my role in this project has been extremely tiny. You can read about this in the acknowledgements section of the paper. Needless to say, all the credit goes to the authors of the paper for this amazing idea. I envision all kinds of interactive digital media, including the video games of the future being infused with such neural networks for real-time fluid and smoke simulations. And let's not forget that this is only the first step: we haven't even talked about other kinds of perhaps learnable physical simulations with collision detection, shattering glassy objects, and gooey soft body simulations. And we also have seen the very first results with light simulation pipelines that are augmented with neural networks. I think it is now a thinly veiled fact that I am extremely excited for this. And this piece of work is not the destination, but a stepping stone towards something truly remarkable. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iOWamCtnwTc",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/accelerating-eulerian-fluid-simulation-convolutional-networks/",
        "paper_title": "Accelerating Eulerian Fluid Simulation With Convolutional Networks"
    },
    {
        "video_id": "dQSzmngTbtw",
        "video_title": "Stunning Video Game Graphics With Voxel Cone Tracing (VXGI) | Two Minute Papers #117",
        "position_in_playlist": 156,
        "description": "The paper \"Interactive Indirect Illumination Using Voxel Cone Tracing\" is available here:\nhttps://research.nvidia.com/publication/interactive-indirect-illumination-using-voxel-cone-tracing\n\nImplementations (without highlighting a particular one): https://goo.gl/AZeWAU\n\nOur post on Patreon on improvements you can expect from Two Minute Papers in 2017:\nhttps://www.patreon.com/posts/improvements-for-7607896\n\nRendering course at the TU Wien:\nhttps://www.youtube.com/watch?v=pjc1QAI6zS0&index=1&list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail image credit:\nhttps://pixabay.com/photo-1872196/\n\nVideo credits:\nBR34K - https://www.youtube.com/watch?v=h1hdAQQ3-Ck\nNVIDIA, Byzantos - https://www.youtube.com/watch?v=cH2_RkfStSk\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\n\n\n#rtx #rtxon",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I consider this one to be one of the most influential papers in the field of light transport. Normally, to create a photorealistic image, we have to create a digital copy of a scene, and simulate the paths of millions of light rays between the camera and the light sources. This is a very laborious task that usually takes from minutes to hours on a complex scene, noting that there are many well-known corner cases that can take up to days as well. As the rays of light can bounce around potentially indefinitely, and if we add that realistic materials and detailed scene geometry descriptions are not easy to handle mathematically, it is easy to see why this is a notoriously difficult problem. Simulating light transport in real time has been an enduring problem and is still not solved completely for every possible material model and light transport effect. However, Voxel Cone Tracing is as good of a solution as one can wish for at the moment. The original formulation of the problem is continuous, which means that rays of light can bounce around in infinitely many directions and the entirety of this digital world is considered to be a continuum. If we look at the mathematical formulation, we see infinities everywhere we look. If you would like to learn more about this, I am holding a Master-level course at the Technical University of Vienna, the entirety of which is available on YouTube. As always, the link is available in the video description for the more curious Fellow Scholars out there. If we try to approximate this continuous representation with tiny tiny cubes, we get a version of the problem that is much less complex, and easier to tackle. If we do this well, we can make it adaptive, which means that these cubes are smaller where there is a lot of information so we don't lose out on many details. This data structure we call a sparse voxel octree. For such a solution, mathematicians like to say that this technique works on a discretized version of the continuous problem. And since we're solving a vastly simplified version of the problem, the question is always whether this way we can remain true to the original solution. And the results show beauty unlike anything we've seen in computer game graphics. Just look at this absolutely amazing footage. Ice cream for my eyes. And all this runs in real time on your consumer graphics card. Imagine this in the virtual reality applications of the future. My goodness. I've chosen the absolute best profession. Also, this technique maps really well to the graphical card and is already implemented in Unreal Engine 4 and NVIDIA has a framework, GameWorks, where they are experimenting with this in their project by the name VXGI. I had a very pleasant visit at NVIDIA's GameWorks lab in Switzerland not so long ago, friendly greetings to all the great and fun people in the team! Some kinks still have to be worked out. For instance, there are still issues with light leaking through thin objects. Beyond that, the implementation of this algorithm contains a multitude of tiny little distinct elements. It is indeed true that many of the elements are puzzle pieces that are interchangeable and can be implemented in a number of different ways, and that's likely one of the reasons why NVIDIA and others are still preparing their implementation for widespread industry use. Soon, we'll be able to solidify the details some more and see what the best practices are. I cannot wait to see this technique appear in the video games of the future. Note that this, and future episodes will be available in 4K resolution for a significant bump in the visual quality of the series. It takes a ton of resources to produce these videos, but it's now possible through the support of you Fellow Scholars on Patreon. There is a more detailed write-up on that, I've included it in the video description. Thank you so much for supporting the show throughout 2016, and looking forward to continuing our journey together in 2017! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dQSzmngTbtw",
        "paper_link": "https://research.nvidia.com/publication/interactive-indirect-illumination-using-voxel-cone-tracing",
        "paper_title": "Interactive Indirect Illumination Using Voxel Cone Tracing"
    },
    {
        "video_id": "rAbhypxs1qQ",
        "video_title": "Image Synthesis From Text With Deep Learning | Two Minute Papers #116",
        "position_in_playlist": 157,
        "description": "The paper \"StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks\" is available here:\nhttps://arxiv.org/abs/1612.03242\n\nSource code for this project is also available here:\nhttps://github.com/hanzhanggit/StackGAN\n\nWe have a Patreon post on the improvements you can expect from Two Minute Papers in 2017. Lots of goodies behind the link, have a look! https://www.patreon.com/posts/7607896\n\nOur previous episode on Recurrent Neural Networks:\nhttps://www.youtube.com/watch?v=Jkkjy7dVdaY\n\nRecurrent Neural Network Writes Sentences About Images:\nhttps://www.youtube.com/watch?v=e-WB4lfg30M\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credit - https://pixabay.com/photo-1616713/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is what we have been waiting for. Earlier, we talked about a neural network that was able to describe in a full sentence what we can see on an image. And it had done a damn good job at that. Then, we have talked about a technique that did something really crazy, the exact opposite: we wrote a sentence, and it created new images according to that. This is already incredible. And we can create an algorithm like this by training not one, but two neural networks: The first is the generative network, that creates millions of new images, and the discriminator network judges whether these are real or fake images. The generative network can improve its game based on the feedback and will create more and more realistic looking images, while the discriminator network gets better and better at telling real images from fake ones. Like humans, this rivalry drives both neural networks towards perfecting their crafts. This architecture is called a generative adversarial network. It is also like the classical, evergoing arms race between criminals who create counterfeit money and the government, which seeks to implement newer and newer measures to tell a real hundred dollar bill from a fake one. The previous generative adversarial networks were adept at creating new images, but due to their limitations, their image outputs were the size of a stamp at best. And we were wondering, how long until we get much higher resolution images from such a system? Well, I am delighted to say that apparently, within the same year. In this work, a two-stage version of this architecture is proposed. The stage 1 network is close to the generative adversarial network we described. And most of the fun happens in the stage 2 network, that takes this rough, low resolution image and the text description and is told to correct the defects of the previous output and create a higher resolution version of it. In the video, the input text description and the stage-1 results are shown, and building on that, the higher resolution stage-2 images are presented. And the results are... unreal. There was a previous article and Two Minute Papers episode on the unreasonable effectiveness of recurrent neural networks. If that is unreasonable effectiveness, then what is this? The rate of progress in machine learning research is unlike any other field I have ever seen. I honestly can't believe what I am seeing here. Dear Fellow Scholars, what you see might very well be history in the making. Are there still faults in the results? Of course there are. Are they perfect? No, they certainly aren't. However, research is all about progress and it's almost never possible to go from 0 to a 100% with one new revolutionary idea. However, I am sure that in 2017, researchers will start working on generating full HD animations with an improved version of this architecture. Make sure to have a look at the paper, where the ideas, challenges, and possible solutions are very clearly presented. And for now, I need some time to digest these results. Currently I feel like being dropped into the middle of a science-fiction movie. And, this one will be our last video for this year. We have had an amazing year with some incredible growth on the channel, way more of you Fellow Scholars decided to come with us on our journey than I would have imagined. Thank you so much for being a part of Two Minute Papers, we'll be continuing full steam ahead next year, and for now, I wish you a Merry Christmas and happy holidays. 2016 was an amazing year for research, and 2017 will be even better. Stay tuned! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rAbhypxs1qQ",
        "paper_link": "https://arxiv.org/abs/1612.03242",
        "paper_title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks"
    },
    {
        "video_id": "PMSV7CjBuZI",
        "video_title": "Crumpling Sound Synthesis | Two Minute Papers #115",
        "position_in_playlist": 158,
        "description": "The paper \"Crumpling Sound Synthesis\" is available here:\nhttp://www.cs.columbia.edu/cg/crumpling/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we're going to crush some soda cans. In the footage that you see here, the animations are performed by an already existing algorithm for thin shell deformations, and for a complete sensorial experience, this piece of work aims synthesize sound for these phenomena. Sounds for crumpling up all kinds of candy wraps, foils, and plastic bags. A lofty, noble goal, loving it. However, this problem is extraordinarily difficult. The reason is that these crumpling simulations are amazingly detailed, and even if we knew all the physical laws for the sound synthesis, which is already pretty crazy, it would still be a fruitless endeavor to take into consideration every single thing that takes place in the simulation. We have to come up with ways to cut corners to decrease the execution time of our algorithm. Running a naive, exhaustive search would take tens of hours for only several seconds of footage. And the big question is, of course, what can we do about it? And before we proceed, just a quick reminder that the geometry of these models are given by a lot of connected points that people in computer graphics like to call vertices. The sound synthesis takes place by observing the changes in the stiffness of these models, which is the source of the crumpling noise. Normally, our sound simulation scales with the number of vertices, and it is abundantly clear that there are simply too many of them to go through one by one. To this end, we should strive to reduce the complexity of this problem. First, we start with identifying and discarding the less significant vibration modes. Beyond that if in one of these vertices, we observe that a similar kind of buckling behavior is present in its neighborhood, we group up these vertices into a patch, and we then forget about the vertices and run the sound synthesis on these patches. And of course, the number of patches is significantly less than the number of vertices in the original model. In this footage, you can see some of these patches. And it turns out that the execution time can be significantly decreased by these optimizations. With these techniques, we can expect results in at least 5 times quicker, but if we're willing to introduce slight degradations to the quality of the sounds, we can even go 10 times quicker with barely perceptible changes. To evaluate the quality of the solutions, there is a user study presented in the paper. And the pinnacle of all tests is of course, when we let reality be our judge. Everything so far sounds great on paper, but how does it compare to what we experience in reality? Wow. Truly excellent results. Suffice to say, they are absolutely crushing it. And we haven't even talked about stochastic enrichment and how one of these problems can be solved optimally via dynamic programming. If you're interested, make sure to have a look at the paper! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=PMSV7CjBuZI",
        "paper_link": "http://www.cs.columbia.edu/cg/crumpling/",
        "paper_title": "Crumpling Sound Synthesis"
    },
    {
        "video_id": "j7XWCCvBrwU",
        "video_title": "3D Printing Flexible Shells For Molding | Two Minute Papers #114",
        "position_in_playlist": 159,
        "description": "The paper \"FlexMolds: Automatic Design of Flexible Shells for Molding\" is available here:\nhttp://vcg.isti.cnr.it/Publications/2016/MPBC16/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about 3D printing flexible molds for objects with detailed geometry. The main observation is that the final object not only has to be cast, but also has to be removed conveniently from the mold. Finding an appropriate layout for the cuts is a non-trivial problem. The technique endeavors to have the least amount of cuts, and the length of the cuts is also subject to minimization. I see the light bulb lighting up in the heads of our seasoned Fellow Scholars, immediately noticing that this sounds like an optimization problem. And in this problem, we start out from a dense cut layout, and iteratively remove as many of these cuts as possible until some prescribed threshold is met. However, we have to be vigilant about the fact that these cuts will result in deformations during the removal process. We mentioned before that we're interested in shapes that have geometry that is rich in details, therefore this distortion effect is to be minimized aggressively. Also, we cannot remove these cuts indefinitely, because sometimes, more cuts have to be added to reduce the stress induced by the removal process. This is a cunning plan, however, a plan that only works if we can predict where and how these deformations will happen, therefore we have to simulate this process on our computer. During removal, forces are applied to the mold, which we also have to take into consideration. To this end, there is an actual simulation of the entirety of the extraction process to make sure that the material can be removed from the mold in a non-destructive manner. Wow! The paper discusses tons of issues that arise from this problem formulation, for instance, what one should do with the tiny air bubbles stuck in the resin. Or, the optimization part is also non-trivial, to which a highly effective homebrew solution is presented. And there's a lot more, make sure to have a look! Of course, as always, we would love to hear your ideas about possible applications of this technique. Leave your thoughts in the comments section! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=j7XWCCvBrwU",
        "paper_link": "http://vcg.isti.cnr.it/Publications/2016/MPBC16/",
        "paper_title": "FlexMolds: Automatic Design of Flexible Shells for Molding"
    },
    {
        "video_id": "cUWDeDRet4c",
        "video_title": "Multiphase Fluid Simulations | Two Minute Papers #113",
        "position_in_playlist": 160,
        "description": "The paper \"Multiphase SPH Simulation for Interactive Fluids and Solids\" is available here:\nhttp://cg.cs.tsinghua.edu.cn/papers/SIG_2016_Multiphase.pdf\nhttp://cg.cs.tsinghua.edu.cn/research.htm\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credits - https://pixabay.com/photo-165192/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What a wonderful day to talk about fluid simulations! This technique is an extension to Smoothed Particle Hydrodynamics, or SPH in short, which is a widely used particle-based simulation technique where the visual quality scales with the number of simulated particles. The more particles we use in the simulation, the more eye candy we can expect. And the goal here is to create an extension of these SPH-based simulations to include deformable bodies and granular materials to the computations. This way, it is possible to create a scene where we have instant coffee and soft candy dissolving in water. Which not only looks beautiful, but sounds like a great way to get your day started. Normally, we have to solve a separate set of equations for each of the phases or material types present, but because this proposed method is able to put them in one unified equation, it scales well with the number of materials within the simulation. This is not only convenient from a theoretical standpoint, but it also maps well to parallel architectures and the results shown in the video were run on a relatively high-end consumer NVIDIA card. This is remarkable, as it should not be taken for granted that a new fluid simulation technique runs well on the GPU. The results indeed indicate that the number of phases only have a mild effect on the execution time of the algorithm. A nice and general framework for fluid-solid interactions, dissolution, elastoplastic solids, and deformable bodies. What a fantastic value proposition. I could watch and play with these all day. I'll try my best to resist, but in case the next episode is coming late, you know where I am. The quality of the paper is absolutely top tier and if you like physics, you're going to have lots of fun reading it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cUWDeDRet4c",
        "paper_link": "http://cg.cs.tsinghua.edu.cn/papers/SIG_2016_Multiphase.pdf",
        "paper_title": "Multiphase SPH Simulation for Interactive Fluids and Solids"
    },
    {
        "video_id": "tB0AVkPDDJU",
        "video_title": "Precomputed Deformation Simulations | Two Minute Papers #112",
        "position_in_playlist": 161,
        "description": "The paper \"Expediting Precomputation for Reduced Deformable Simulation \" is available here:\nhttp://www.cs.columbia.edu/cg/fastprecomp/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work is about reducing the time needed to simulate elastic deformations by means of precomputation. Okay, so what does the term precomputation mean? If we are at an open book exam and we are short on time, which is, basically, every time, it would be much better to do a precomputation step, namely studying at home for a few days before, and then, when we are there, we are endowed with quite a bit of knowledge, and are guaranteed to do much better than trying to grasp the simplest concepts on the spot. This precomputation step we only have to do once, and it almost doesn't matter how lengthy it is, because after that, we can answer any question in this topic in the future. Well, sometimes passing exams is not as easy as described here, but a fair bit of precomputation often goes a long way. Just saying. The authors have identified 3 major bottlenecks in already existing precomputation techniques, and proposed optimizations to speed them up considerably at the cost of higher memory consumption. For instance, the algorithm is trained on a relatively large set of training pose examples. If we don't have enough of these training examples, the quality of the animations will be unsatisfactory, but if we use too many, that's too resource intensive. We have to choose just the right amount, and the right kinds of poses, which is a highly non-trivial process. Note that this training is not the same kind of training we are used to see with neural networks. This work doesn't have anything to do with neural networks at all! The results of the new technique are clearly very close to the results we would obtain with standard methods, however, the computation time is 20 to 2000 times less. In the more favorable cases, computing deformations that would take several hours can take less than a second. That is one jaw-dropping result and a hefty value proposition indeed. This example shows that after a short precomputation step, we can start torturing this poor armadillo and expect high-quality elastic deformations. And there is a lot of other things to be learned from the paper. Gram-Schmidt orthogonalization, augmented Krylov iterations, Newton-PCG solvers. Essentially, if you pick up a dry textbook on linear algebra, and for every technique you see there, you ask what on Earth this is useful for, you wouldn't have to go through hundreds of works, you would find a ton of answers in just this one absolutely amazing paper. Also, please don't forget that you Fellow Scholars make Two Minute Papers happen. If you wish to support the show and get access to cool perks, like an exclusive early access program where you can watch these episodes 16-24 hours in advance, check out our page on Patreon. Just click on the icon with the letter P at the end of this video or just have a look at the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=tB0AVkPDDJU",
        "paper_link": "http://www.cs.columbia.edu/cg/fastprecomp/",
        "paper_title": "Expediting Precomputation for Reduced Deformable Simulation "
    },
    {
        "video_id": "DzsZ2qMtEUE",
        "video_title": "Sound Propagation With Bidirectional Path Tracing | Two Minute Papers #111",
        "position_in_playlist": 162,
        "description": "The paper \"Interactive Sound Propagation with Bidirectional Path Tracing\" is available here:\nhttp://gaps-zju.org/bst/\n\nVeach's paper on Multiple Importance Sampling:\nhttp://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Veach95.pdf\nhttp://dl.acm.org/citation.cfm?id=218498\n\nI am also holding a full course on light transport simulations at the Technical University of Vienna. There is plenty of discussion on path tracing and bidirectional path tracing therein:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image is courtesy of Wikimedia Commons - https://commons.wikimedia.org/wiki/File:Dubrovnik,_palazzo_sponza,_cortile_02.JPG\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Imagine if we had an accurate algorithm to simulate how different sound effects would propagate in a virtual world. We would find computer games exhibiting gunfire in open areas, or a pianist inside a castle courtyard to be way more immersive, and we've been waiting for efficient techniques for this for quite a while now. This is a research field where convolutions enjoy quite a bit of attention due to the fact that they are a reliable and efficient way to approximate how a given signal would sound in a room with given geometry and material properties. However, the keyword is approximate - this, however, is one of those path sampling techniques that gives us the real deal, so quite excited for that! So what about this path sampling thing? This means an actual simulation of sound waves. We have a vast literature and decades of experience in simulating how rays of light bounce and reflect around in a scene, and leaning on this knowledge, we can create beautiful photorealistic images. The first idea is to adapt the mathematical framework of light simulations to be able to do the very same with sound waves. Path tracing is a technique where we build light paths from the camera, bounce them around in a scene, and hope that we hit a light source with these rays. If this happens, then we compute the amount of energy that is transferred from the light source to the camera. Note that energy is a more popular and journalistic term here, what researchers actually measure here is a quantity called radiance. The main contribution of this work is adapting bidirectional path tracing to sound. This is a technique originally designed for light simulations that builds light paths from both the light source and the camera at the same time, and it is significantly more efficient than the classical path tracer on difficult indoors scenes. And of course, the main issue with these methods is that they have to simulate a large number of rays to obtain a satisfactory result, and many of these rays don't really contribute anything to the final result, only a small subset of them are responsible for most of the image we see or sound we hear. It is a bit like the Pareto principle or the 80/20 rule  on steroids. This is ice cream for my ears. Love it! This work also introduces a metric to not only be able to compare similar sound synthesis techniques in the future, but the proposed technique is built around minimizing this metric, which leads us to an idea on which rays carry important information and which ones we are better off discarding. I also like this minimap on the upper left that actually shows what we hear in this footage, exactly where the sound sources are and how they change their positions. Looking forward to seeing and listening to similar presentations in future papers in this area! A typical number for the execution time of the algorithm is between 15-20 milliseconds per frame on a consumer-grade processor. That is about 50-65 frames per second. The position of the sound sources makes a great deal of difference for the classical path tracer. The bidirectional path tracer, however, is not only more effective, but offers significantly more consistent results as well. This new method is especially useful in these cases. There are way more details explained in the paper, for instance, it also supports path caching and also borrows the all-powerful multiple importance sampling from photorealistic rendering research. Have a look! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=DzsZ2qMtEUE",
        "paper_link": "http://gaps-zju.org/bst/",
        "paper_title": "Interactive Sound Propagation with Bidirectional Path Tracing"
    },
    {
        "video_id": "FeMSEaHR8aw",
        "video_title": "Water Wave Simulation with Dispersion Kernels | Two Minute Papers #110",
        "position_in_playlist": 163,
        "description": "The paper \"Dispersion Kernels for Water Wave Simulation\" is available here:\nhttp://www.gmrv.es/Publications/2016/CMTKPO16/\n\nRecommended for you:\nRocking Out With Convolutions - https://www.youtube.com/watch?v=JKYQOAZRZu4\nSeparable Subsurface Scattering - https://www.youtube.com/watch?v=72_iAlYwl0c&t=1s\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image is taken from the corresponding paper (link available above).\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this piece of work, we are interested in simulating the dynamics of water waves. There are quite a few forces acting on a bucket of water, such as surface tension, internal pressure, external force fields (such as wind, for instance), and gravity. Therefore, it is not a surprise that these waves can become quite complex with a lot of high-frequency details that are difficult to simulate. Accurately modeling wave reflections after colliding with solids is also an important and highly sought-after detail to capture. This piece of work simulates Sir George Biddell Airy's dispersion model. Now what does this mean exactly? The Airy model describes many common wave phenomena accurately, such as how longer waves are dominated by gravitational forces and how shorter waves dance mostly according the will of surface tension. However, as amazing this theory is, it does not formulate these quantities in a way that would be directly applicable to a computer simulation. The main contribution of this paper is a new convolution formulation of this model and some more optimizations that can be directly added into a simulation, and not only that, but the resulting algorithm parallelizes and maps well to the graphical card in our computers. We have earlier discussed what a convolution is. Essentially, it is a mathematical operation that can add reverberation to the sound of our guitar, or accurately simulate how light bounces around under our skin. Links to these episodes are available in the video description and at the end of the video, check them out, I am sure you'll have a lot of fun with them! Regarding applications - as the technique obeys Airy's classical dispersion model, I expect and hope this to be useful for ocean and coastal engineering and in simulating huge tidal waves. Note that limitations apply, for instance, the original linear theory is mostly good for shallow water simulations and larger waves in deeper waters. The proposed approximation itself also has inherent limitations, such as the possibility of waves going through thinner objects. The resulting algorithm is, however, very accurate and honestly, a joy to watch. It is also shown to support larger-scale scenes - here you see how beautifully it can simulate the capillary waves produced by these rain drops and of course, the waves around the swans in the pond. This example took roughly one and a half second per frame to compute. You know the drill - a couple more followup papers down the line, and it will surely run in real time. Can't wait! Also, please let me know in the comments section whether you have found this episode understandable. Was it easy to follow? Too much? Your feedback is, as always, highly appreciated. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=FeMSEaHR8aw",
        "paper_link": "http://www.gmrv.es/Publications/2016/CMTKPO16/",
        "paper_title": "Dispersion Kernels for Water Wave Simulation"
    },
    {
        "video_id": "7JbN9vXxGYE",
        "video_title": "3D Printing Acoustic Filters | Two Minute Papers #109",
        "position_in_playlist": 164,
        "description": "The paper \"Acoustic Voxels: Computational Optimization of Modular Acoustic Filters\" is available here:\nhttp://www.cs.columbia.edu/cg/lego/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What is an acoustic filter? Well, it is an arbitrarily shaped object that takes a sound as an input, and outputs a different sound. These filters have some really amazing applications that you'll hear about in a minute. In this work, a novel technique is proposed to automatically design such filters. It works by building an arbitrarily shaped object as a set of connected tiny resonators, and chooses appropriate sizes and setups for each of these elements to satisfy a prescribed set of acoustic properties. Instead of resorting to a lengthy and flimsy trial and error phase, we can use physics to simulate what would happen if we were to use a given arrangement in reality. Again, one of those works that have a deep connection to the real world around us. Absolutely amazing. The goal can be to eliminate the peaks of the sound of a car horn or an airplane engine, and we can achieve this objective by means of optimization. The proposed applications we can divide into three main categories: The first is identifying and filtering noise attenuation components for a prescribed application, to which we can also refer to as muffler design. In simpler words, we are interested in filtering or muffling the peaks of a known signal. Designing such objects is typically up to trial and error and in this case, it is even harder because we're interested in a wider variety of shape choices other than exhaust pipes and tubes that are typically used in the industry. Second, designing musical instruments is hard, and unless we design them around achieving a given acoustic response, we'll likely end up with inharmonious gibberish. And this method also supports designing musical instruments with, hmm, well, non-conventional shapes. Well, this is as non-conventional as it gets I'm afraid. And also, that is about the most harmonious sound I've heard coming out of the rear end of a hippo. And third, this work opens up the possibility of making hollow objects that are easy to identify by means of acoustic tagging. Check out this awesome example that involves smacking these 3d printed piggies. If you feel like improving your kung fu in math, there are tons of goodies such as transmission matrices, the Helmholtz equation, oh my! The paper and the talk slides are amazingly well written, and yes, you should definitely have a look at them. Let us know in the comments section if you have some ideas for possible applications beyond these ones, we love to read your take on these works. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7JbN9vXxGYE",
        "paper_link": "http://www.cs.columbia.edu/cg/lego/",
        "paper_title": "Acoustic Voxels: Computational Optimization of Modular Acoustic Filters"
    },
    {
        "video_id": "aMo7pkkaZ9o",
        "video_title": "Synchronizing Animations To Sound | Two Minute Papers #108",
        "position_in_playlist": 165,
        "description": "The paper \"Inverse-Foley Animation: Synchronizing rigid-body motions to sound\" is available here:\nhttp://www.cs.cornell.edu/projects/Sound/ifa/\n\nRecommended for you:\nSound Synthesis for Fluids With Bubbles - https://www.youtube.com/watch?v=kwqme8mEgz4\nSynthesizing Sound From Collisions - https://www.youtube.com/watch?v=rskdLEl05KI\nVisually Indicated Sounds - https://www.youtube.com/watch?v=flOevlA9RyQ\nWhat Do Virtual Objects Sound Like? - https://www.youtube.com/watch?v=ZaFqvM1IsP8\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credits: https://commons.wikimedia.org/wiki/File:Spinning_top_(5448672388).jpg\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is going to be absolutely amazing. Earlier, we had some delightful discussions on synthesizing sound from animations. The input would be a sequence, for instance, a video depicting the complete and utter destruction of plates, wooden bunnies, or footage of bubbling water. And the output should be a physical simulation that yields appropriate sound effects for the observed phenomenon. In short: input, animation, output, synthesized sound effects for this animation. And, get this! What if we would turn the problem around, where we have sound as an input, and we try to synthesize an animation that could create such a sound. Mmm, I like it, a very spicy project indeed. And however crazy this idea may sound, given the richness of sound effects in nature, it may actually be easier to generate a believable animation than a perfect sound effect. It is extremely difficult to be able to match the amazingly detailed real-world sound of, for instance a sliding, rolling or bouncing bolt with a simulation. The more I think about this, the more I realize that this direction actually makes perfect sense. And, no machine learning is used here, if we look under the hood, we'll see a pre-generated database of rigid body simulations with dozens of different objects, and a big graph that tries to group different events and motions together and encode the order of execution of these events and motions. Now hold on to your papers, and let's check out the first round of results together. Wow. I think it would be an understatement to say that they nailed it. And what's more, we can also add additional constraints, like a prescribed landing location to the object to make sure that the animations are not too arbitrary, but are more in line with our artistic vision. Crazy. As I am looking through the results, I am still in complete disbelief. This shouldn't be possible. Also, please don't get the impression that this is all there is to this technique. There are a lot more important details that we haven't discussed here that the more curious Fellow Scholars could be interested in... discrete and continuous time contact events, time warping and motion connections. There are tons of goodies like these in the paper, please have a look to be able to better gauge and appreciate the merits of this work. Some limitations apply, such as the environment is constrained to be this plane that we've seen in these animations, and as always with works that are inventing something completely new - it currently takes several minutes, which is not too bad, but of course, there is plenty of room to accelerate the execution times. And some bonus footage! This will be just spectacular for creating music videos, animated movies, and I am convinced that professional artists will be able to do incredible things with such a tool. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=aMo7pkkaZ9o",
        "paper_link": "http://www.cs.cornell.edu/projects/Sound/ifa/",
        "paper_title": "Inverse-Foley Animation: Synchronizing rigid-body motions to sound"
    },
    {
        "video_id": "4MfG9CDufPA",
        "video_title": "Deep Learning Program Simplifies Your Drawings | Two Minute Papers #107",
        "position_in_playlist": 166,
        "description": "The Ishikawa Watanabe Laboratory, the University of Tokyo laboratory has all rights to the materials shown in the video.\n\nThe paper \"Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup\" and its online demo is available here:\nhttp://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/\nhttp://hi.cs.waseda.ac.jp:8081/\n\nRecommended for you:\nRocking Out With Convolutions - https://www.youtube.com/watch?v=JKYQOAZRZu4\nSeparable Subsurface Scattering - https://www.youtube.com/watch?v=72_iAlYwl0c\nWaveNet by Google DeepMind - https://www.youtube.com/watch?v=CqFIVCD1WWo\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nImage credits:\nBitmap and vector images (two of them): Wikipedia - https://en.wikipedia.org/wiki/Vector_graphics and https://en.wikipedia.org/wiki/Image_tracing\nImage resolution: Wikipedia - https://en.wikipedia.org/wiki/Image_resolution\nVectorization: Wikipedia - https://en.wikipedia.org/wiki/Image_tracing\nThumbnail background - https://pixabay.com/photo-1281718/\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. First, let's talk about the raster and vector graphics. What do these terms mean exactly? A raster image is a grid made up of pixels, and for each of these pixels, and for each of the pixels, we specify a color. That's all there is in an image - it is nothing but a collection of pixels. All photographs on your phone, and generally most images you encounter are raster images. It is easy to see that the quality of such images greatly depends on the resolution of this grid - of course, the more grid points, the finer the grid is, the more details we can see. However, in return, if we disregard compression techniques, the file size grows proportionally to the number of pixels, and if we zoom in too close, we shall witness these classic staircase effects that we like to call aliasing. However, if we are designing a website, or a logo for a company, which should look sharp on all possible devices and zoom levels, vector graphics is a useful alternative. Vector images are inherently different from raster images, as the base elements of the image are not pixels, but vectors and control points. The difference is like storing the shape of a circle on a lot of pixels point by point, which would be a raster image, or just saying that I want a circle on these coorindates with a given radius. And as you can see in this example, the point of this is to have razor sharp images at higher zoom levels as well. Unless we go too crazy with fine details, file sizes are also often remarkably small for vector images, because we're not storing the colors of millions of pixels. We are only storing shapes. If we want to sound a bit more journalistic we can kind of say that vector images have infinite resolution. We can zoom in as much as we wish, and we won't lose any detail during this process. Vectorization is the process where we try to convert a raster image to a vector image. Some also like to call this process image tracing. The immediate question arises - why are we not using vector graphics everywhere? Well, one, the smoother the color transitions and the more detail we have in our images, the quicker the advantage of vectorization evaporates. And two, also note that this procedure is not trivial and we are also often at the mercy of the vectorization algorithm in terms of output quality. It is often unclear in advance whether it will work well on a given input. So now we know everything we need to know to be able to understand and appreciate this amazing piece of work. The input is a rough sketch, that is a raster image, and the output is a simplified, cleaned-up and vectorized version of it. We're not only doing vectorization, but simplification as well. This is a game changer, because this way, we can lean on the additional knowledge that these input raster images are sketches, hand-drawn images, therefore there is a lot of extra fluff in them that would be undesirable to retain in the vectorized output, therefore the name, sketch simplification. In each of these cases, it is absolute insanity how well it works. Just look at these results! The next question is obviously, how does this wizardry happen? It happens by using a classic deep learning technique, a convolutional neural network, of course, that was trained on a large number of input and output pairs. However, this is no ordinary convolutional neural network! This particular variant differs from the standard well-known architecture as it is augmented with a series of upsampling convolution steps. Intuitively, the algorithm learns a sparse and concise representation of these input sketches, this means that it focuses on the most defining features and throws away all the unneeded fluff. And the upsampling convolution steps make it able to not only understand, but synthesize new, simplified, and high-resolution images that we can easily vectorize using standard algorithms. It is fully automatic and requires no user intervention. In case you are scratching your head about these convolutions, we have had plenty of discussions about this peculiar term before, I have linked the appropriate episodes in the video description box. I think you'll find them a lot of fun - in one of them, I pulled out a guitar and added reverberation to it using convolution. It is clear that there is a ton of untapped potential in using different convolution variations in deep neural networks. We've seen in a DeepMind paper earlier that used dilated convolutions for state of the art speech synthesis, that is a novel convolution variant and this piece of work is no exception either. There is also a cool online demo of this technique that anyone can try. Make sure to post your results in the comments section! We'd love to have a look at your findings. Also, have a look at this Two Minute Papers fan art. A nice little logo one of our kind Fellow Scholars sent in. It's really great you see that you've taken your time to help out the series, that's very kind of you. Thank you! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4MfG9CDufPA",
        "paper_link": "http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/",
        "paper_title": "Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup"
    },
    {
        "video_id": "NnzzSkKKoa8",
        "video_title": "Human Pose Estimation With Deep Learning | Two Minute Papers #106",
        "position_in_playlist": 167,
        "description": "The paper \"Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image\" is available here:\n http://files.is.tue.mpg.de/black/papers/BogoECCV2016.pdf\n\nWelch Labs:\nNeural Networks Demystified - https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\nLearning to See - https://www.youtube.com/playlist?list=PLiaHhY2iBX9ihLasvE8BKnS2Xg8AhY6iV\n\nOur earlier episode on optimization - https://www.youtube.com/watch?v=1ypV5ZiIbdA\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail image background credits: https://pixabay.com/photo-1725207/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Pose estimation is an interesting area of research where we typically have a few images or video footage of humans, and we try to automatically extract the pose this person was taking. In short, the input is mostly a 2D image, and the output is typically a skeleton of the person. Applications of pose estimation include automatic creation of assets for computer games and digital media, analyzing and coaching the techniques of athletes, or helping computers understand what they see for the betterment of robotics and machine learning techniques. And this is just a taste, the list was by no means exhaustive. Beyond the obvious challenge of trying to reconstruct 3D information from a simple 2D image, this problem is fraught with difficulties as one has to be able to overcome the ambiguity of lighting, occlusions, and clothing covering the body. A tough problem, no question about that. An ideal technique would do this automatically without any user intervention, which sounds like wishful thinking. Or does it? In this paper, a previously proposed convolutional neural network is used to predict the position of the individual joints, and curiously, it turns out that we can create a faithful representation of the 3D human body from that by means of optimization. We have had a previous episode on mathematical optimization, you know the drill, the link is available in the video description box. What is remarkable here is that not only the pose, but the body type is also inferred, therefore the output of the process is not just a skeleton, but full 3D geometry. It is coarse geometry, so don't expect a ton of details, but it's 3D geometry, more than what most other competing techniques can offer. To ease the computational burden of this problem, in this optimization formulation, healthy constraints are assumed that apply to the human body, such as avoiding unnatural knee and elbow bends, and self-intersections. If we use these constraints, the space in which we have to look for possible solutions shrinks considerably. The results show that this algorithm outperforms several other state of the art techniques by a significant margin. It is an auspicious opportunity to preserve and recreate a lot of historic events in digital form, maybe even use them in computer games, and I am sure that artists will make great use of such techniques. Really well done, the paper is extremely well written, the mathematics and the optimization formulations are beautiful, it was such a joy to read. Regarding the future, I am pretty sure we're soon going to see some pose and skeleton transfer applications via machine learning. The input would be a real-world video with a person doing something, and we could essentially edit the video and bend these characters to our will. There are some exploratory works in this area already, the Disney guys for instance are doing quite well. There will be lots of fun to be had indeed! Also, make sure to check out the YouTube channel of Welch Labs, who has a great introductory series for neural networks, which is in my opinion, second to none. He also has a new series called \"Learning to see\", where he codes up a machine learning technique for a computer vision application. It is about counting the number of fingers on an image. Really cool, right? The quality of these videos is through the roof, the link for both of these series are available in the description box, make sure to check them out! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=NnzzSkKKoa8",
        "paper_link": "http://files.is.tue.mpg.de/black/papers/BogoECCV2016.pdf",
        "paper_title": "Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image"
    },
    {
        "video_id": "QkqNzrsaxYc",
        "video_title": "Computer Games Empower Deep Learning Research | Two Minute Papers #105",
        "position_in_playlist": 168,
        "description": "The paper \"Playing for Data: Ground Truth from Computer Games\" is available here:\nhttp://download.visinf.tu-darmstadt.de/data/from_games/\n\nComputer graphics / VR challenge grant at Experiment:\nhttps://experiment.com/grants/graphics-and-virtualreality\n\nOther popular datasets:\n- CamVid - http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/\n- CityScapes - https://www.cityscapes-dataset.com/dataset-overview/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What are datasets? A dataset is typically a big bunch of data, for instance, a database of written letters, digits, images of human faces, stock market data that scientists can use to test their algorithms on. If two research groups wish to find out whose algorithm performs better at recognizing traffic signs, they run their techniques on one of these datasets and test their methods on equal footings. For instance, the CamVid dataset stands for Cambridge-driving Labeled Video Database, and it offers several hundreds of images depicting a variety of driving scenarios. It is meant to be used to test classification techniques: the input is an image, and the question is for each of the pixels, which one of them belongs to what class. Classes include roads, vegetation, vehicles, pedestrians, buildings, trees and more. These regions are labeled with all the different colors that you see on these images. To have a usable dataset, we have to label tens of thousands of these images, and as you may imagine, creating such labeled images requires a ton of human labor. The first guy has to accurately trace the edges of each of the individual objects seen on every image, and there should be a second guy to cross-check and make sure everything is in order. That's quite a chore. And we haven't even talked about all the other problems that arise from processing footage created with handheld cameras, so this takes quite a bit of time and effort with stabilization and calibration as well. So how do we create huge and accurate datasets without investing a remarkable amount of human labor? Well, hear out this incredible idea. What if we would record a video of us wandering about in an open-world computer game, and annotate those images. This way, we enjoy several advantages: - 1. Since we have recorded continuous videos, after annotating the very first image, we will have information from the next frames, therefore if we do it well, we can propagate a labeling from one image to the next one. That's a huge time saver. - 2. In a computer game, one can stage and record animations of important, but rare situations that would otherwise be extremely difficult to film. Adding rain or day and night cycles to a set of images is also trivial, because we simply can query the game engine to do this for us. - 3. Not only that, but the algorithm also has some knowledge about the rendering process itself. This means that it looks at how the game communicates with the software drivers and the video card, tracks when the geometry and textures for a given type of car are being loaded or discarded, and uses this information to further help the label propagation process. - 4. We don't have any of the problems that stem from using handheld cameras. Noise, blurriness, problems with the lens, and so on are all non-issues. Using this previous CamVid dataset, the annotation of one image takes around 60 minutes, while with this dataset, 7 seconds. Thus, the authors have published almost 25000 high-quality images and their annotations to aid computer vision and machine learning research in the future. That's a lot of images, but of course, the ultimate question arises: how do we know if these are really high-quality training samples? They were only taken from a computer game after all! Well, the results show that using this dataset, we can achieve an equivalent quality of learning compared to the CamVid dataset by using one third as many images. Excellent piece of work, absolutely loving the idea of using video game footage as a surrogate for real-world data. Fantastic. And in the meantime, while we're discussing computer graphics, here's a nice computer graphics challenge grant from Experiment. Basically, if you start a new research project through their crowdfunded system, you may win additional funding that comes straight for them. Free money. If you are interested in doing any kind of research in this area, or if you are a long-time practitioner, make sure to have a look. The link is available in the video description box. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=QkqNzrsaxYc",
        "paper_link": "http://download.visinf.tu-darmstadt.de/data/from_games/",
        "paper_title": "Playing for Data: Ground Truth from Computer Games"
    },
    {
        "video_id": "sWZQxB2es88",
        "video_title": "Building a Community Around Two Minute Papers",
        "position_in_playlist": 169,
        "description": "The Two Minute Papers Data project:\nhttps://www.reddit.com/r/twominutepapers/comments/58qa8p/github_repository_for_video_data/\nhttps://www.reddit.com/r/twominutepapers/comments/5a6jes/suggestions_and_help_on_twominutepapersdata/\n\nA nice writeup about the Starcraft 2 panel az Blizzcon:\nhttps://www.reddit.com/r/starcraft/comments/5bb6y0/notes_from_the_ai_panel/\n\nRecommended for you:\nStyLit, Illumination-Guided Artistic Style Transfer - https://www.youtube.com/watch?v=ksCSL6Ql0Yg\nReal-Time Shading With Area Light Sources - https://www.youtube.com/watch?v=SC0D7aJOySY\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credits: https://flic.kr/p/J5Ys9N\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a quick update on our plans with Two Minute Papers. The more I look at our incredibly high quality comments sections on YouTube or the community efforts that started out on our subreddit, the more I get the feeling that we should strive to create a community platform around the series. A platform for people who wish to learn, exchange ideas, collaborate, and help each other. For instance, the Two Minute Papers Data project has recently started. The initial idea is that there would be a public crowdsourced database with episode-related metadata that anyone can add information to. This would be useful for several different reasons. We can, for instance, include implementation links for each of the papers, for the ones that have source code either from the authors or some independent implementations from the community. We can also include keywords on the topic of the episode, such as machine learning, recurrent neural networks, cloth simulations for an easy way to search for a topic of interest. If we are interested in any of these, we can just easily search for a keyword and immediately know what episode it was referenced in. The very same could be done with some of the technical words used in the series, such as Metropolis sampling, backpropagation and overfitting. This way, we could maybe build an intuitive technical dictionary with definitions and links pointing to the appropriate episodes where they were explained. I think that would be the ultimate learning resource and having such a dictionary would be super useful - I was playing with the thought of having this for quite a while now. So this data project has just started out on the Two Minute Papers subreddit, but it can only happen with the help of the community - and this means you. If you're interested in helping, please drop by and leave us a note, I've put a link in the description box. Looking forward to seeing you there! It seems that there is interest in doing such collaborations and experiments together between you Fellow Scholars. Imagine how cool it would be to see students and researchers forming an open and helpful community, brainstorming, running the implementations, and sharing their ideas and findings with each other. Who knows, maybe one day, a science paper will be written as a result of such endeavors. Please let us know in the comments section what you think about these things. Would you participate in such an endeavor? As always, we love reading your awesome feedback! Also, in the meantime, a huge thank you for the following Fellow Scholars who translated our episodes to several languages! If you wish to contribute too, click the cogwheel button in any video on the lower right, click subtitles/CC, then add subtitles/CC. I am trying my very best to credit every contributor, if I have forgotten anyone, that's not intended, please let me know and I'll fix it. Meanwhile, on Patreon, we are currently oscillating around our current milestone. Reaching this one means that all of our software and hardware costs are covered, which is quite amazing. Because creating YouTube videos with high information density and short duration is the very definition of financial suicide, it's very helpful for us to have such a safety net with Patreon. Also, there was an episode not so long ago about the new Two Minute Papers machine which was bought solely from your support, and I am still stunned by this. I tried to explain this to close members of the family that we were able to buy this new machine with the support of complete strangers from the internet whom I've never met. They said that this is clearly impossible. Apparently, it's not impossible - and so many kind people are watching the series, really, thank you so much! When we reach our milestone after that, we will be able to spend 1% of these funds to directly help other research projects and conferences. Please remember that your support makes Two Minute Papers possible, and you can cancel these pledges at any time. Also, if you don't feel like using Patreon or don't have any disposable income, that is completely fine. I'd like to emphasize that none of this is required, it's just an option to help, and we completely understand that it's not easy to make ends meet for many many of you, even with lots of overtime at a tiring and difficult job. Two Minute Papers is always going to be here and will always be free for everyone. We would like to spread the word so even more of us can marvel at the wonders of research. Just watching the series and sharing these episodes is also a great deal of help and we are super grateful for it! It is really incredible to see how the series has grown and how many of you Fellow Scholars are interested in research, and the inventions of the future. Let's continue our journey of science together! In the meantime, Google DeepMind and Blizzard has announced a joint effort to make it possible for computer programs to play Starcraft 2, a famous real-time strategy game from the start of 2017. Whoa! There was a question about the first thing I will do when this project takes off. Well, of course, I'll take one week of vacation and write an AI that can play against itself and watch with tremendous enjoyment as they beat the living hell out of each other and other real players. If I heard it correctly, in one of the upcoming championships, computer algorithms will also be allowed to play. And who knows, maybe defeat the reigning player champions, you know, like in Chess and Go. We're living amazing times indeed. I am counting the days. Super excited. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=sWZQxB2es88"
    },
    {
        "video_id": "u9kvJbWb_1U",
        "video_title": "How To Steal a Lost Election With Gerrymandering | Two Minute Papers #104",
        "position_in_playlist": 170,
        "description": "Gerrymandering, is the process of manipulating electoral district boundaries to turn the tide of an election. There are efficient mathematical techniques to optimally solve such a problem and we'll see how the different electoral districts in the US evolved over the last 20 years as a result of gerrymandering.\n\n___________________\n\nAn article on the evolution of a district over 20 years:\nhttps://www.theguardian.com/us-news/2016/oct/19/gerrymandering-supreme-court-us-election-north-carolina\n\nRecommended for you:\nMetropolis Light Transport - https://www.youtube.com/watch?v=f0Uzit_-h3M\nAutomatic Parameter Control for Metropolis Light Transport - https://www.youtube.com/watch?v=9wOBkJJ-w2s\n\nThe redistricting game seen in the video:\nhttp://www.redistrictinggame.org/game/launchgame.php\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image credits: https://pixabay.com/photo-1594962/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Let's talk about the mathematical intricacies of the elections! Here you can see the shape of the twelfth congressional district in North Carolina in the 90's. This is not a naturally shaped electoral district, is it? One might say this is more of an abomination. If we try to understand why it has this peculiar shape, we shall find a remarkable mathematical mischief. Have a look at this example of 50 electoral precincts. The distribution is 60% percent blue, and 40% red. So this means that the blue party should win the elections and gain seats with the ratio of 60 to 40, right? Well, this is not exactly how it works. There is a majority decision district by district, regardless of the vote ratios. If the electoral districts are shaped like this, then the blue party wins 5 seats to zero. However, if they are, instead, shaped like this, the red party wins 3 to 2. Which is kind of mind blowing, because the votes are the very same. And this is known as the wasted vote effect. This term doesn't refer to someone who enters the voting booth intoxicated, this means that one can think of pretty much every vote beyond 50% + 1 to a party in a district, to be irrelevant. It doesn't matter if the district is won by 99% of the votes or just by 50% + 1 vote. So, the cunning plan is now laid out. What if, instead, we could regroup all these extra votes to win in a different district where we were losing? And now, we have ceremoniously arrived to the definition of Gerrymandering, which is the process of manipulating electoral district boundaries to turn the tide of an election. The term originates from one of the elections in the USA in the 1800s, where Governor Elbridge Gerry signed a bill to reshape the districts of Massachusetts in order to favor his party. And at that time, understanably, all the papers and comic artists were up in arms about this bill. So how does one perform gerrymandering? Gerrymandering is actually a mathematical problem of the purest form where we are trying to maximize the number of seats that we can win by manipulating the district boundaries appropriately. It is important to note that the entire process relies on a relatively faithful prediction of the vote distributions per region, which in many countries, is not really changing all that much in time. This is a problem that we can solve via standard optimization techniques. Now, hold on to your papers, and get this: for instance, we can use Metropolis sampling to solve this problem, which is, absolutely stunning. So far, in an earlier episode, we have used Metropolis sampling to develop a super efficient light simulation program to create beautiful images of virtual scenes, and the very same technique can also be used to steal an election. In fact, Metropolis sampling was developed and used during the Manhattan project, where the first atomic bomb was created in Los Alamos. I think it is completely understandable that the power of mathematics and research still give many of us sleepless nights, sometimes delightful, sometimes perilous. It is also important to note that in order to retain the fairness of the elections in a district-based system, it is of utmost importance that these district boundaries are drawn by an independent organization and that the process is as transparent as possible. I decided not to cite a concrete paper in this episode. If you would like to read up on this topic, I recommend searching for keywords, like redistricting and gerrymandering on Google Scholar. Please feel free to post the more interesting finding of yours in the comments section, we always have excellent discussions therein. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u9kvJbWb_1U"
    },
    {
        "video_id": "fl-7e8yBUic",
        "video_title": "Real-Time Soft Body Dynamics for Video Games | Two Minute Papers #103",
        "position_in_playlist": 171,
        "description": "We have had plenty of episodes about fluid simulations, so how about some tasty soft body dynamics for today? Soft body dynamics basically means computing what happens when we smash together different deformable objects. Examples include folding sheets, playing around with noodles, or torturing armadillos. I think this is a nice and representative showcase of the immense joys of computer graphics research!\n\nClarification: the 15 ms per frame execution time is a nice ballpark number, but it depends on the scene.\n\n____________________\n\nThe paper \"Vivace: a Practical Gauss-Seidel Method for Stable Soft Body Dynamics\" is available here:\nhttp://pellacini.di.uniroma1.it/publications/vivace16/vivace16.html\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nImage credits:\nThumbnail background: https://pixabay.com/photo-1747663/\nGraph coloring: https://commons.wikimedia.org/wiki/File:Complete_coloring_clebsch_graph.svg\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have had plenty of episodes about fluid simulations, so how about some tasty soft body dynamics for today? Soft body dynamics basically means computing what happens when we smash together different deformable objects. Examples include folding sheets, playing around with noodles, or torturing armadillos. I think this is a nice and representative showcase of the immense joys of computer graphics research! The key to real-time physically based simulations is parallelism. Parallelism means that we have many of the same units working together in harmony. Imagine if we had to assign 50 people to work together to make a coffee in the same kitchen. As you may imagine, they would trip over each other, and the result would be chaos, not productivity. Such a process would not scale favorably, because as we would add more people after around 3 or 4, the productivity would not increase, but drop significantly. You can often hear a similar example of 9 pregnant women not being able to give birth to a baby in one month. For better scaling, we have to subdivide a bigger task into small tasks in a way that these people can work independently. The more independently they can work, the better the productivity will scale as we add more people. In software engineering, these virtual people we like call threads, or compute units. As of 2016, mid-tier processors are equipped with 4-8 logical cores, and for a video card, we typically have compute units in the order of hundreds. So if we wish to develop efficient algorithms, we have to make sure that these big simulation tasks are subdivided in a way so that these threads are not tripping over each other. And the big contribution of this piece of work is a technique to distribute the computation tasks to these compute units in a way that they are working on independent chunks of the problem. This is achieved via using graph coloring, which is a technique typically used for designing seating plans, exam timetabling, solving sudoku puzzles and similar assignment tasks. It not only works in an absolutely spectacular manner, but graph theory is an immensely beautiful subfield of mathematics, so additional style points to the authors! The technique produces remarkably realistic animations and requires only 15 milliseconds per frame, which means that this technique can render over 60 frames per second comfortably. And the other most important factor is that this technique is also stable, meaning that it offers an appropriate solution, even when many other techniques fail to deliver. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fl-7e8yBUic",
        "paper_link": "http://pellacini.di.uniroma1.it/publications/vivace16/vivace16.html",
        "paper_title": "Vivace: a Practical Gauss-Seidel Method for Stable Soft Body Dynamics"
    },
    {
        "video_id": "nK3giIsNAHg",
        "video_title": "Generating Tangle Patterns With Grammars | Two Minute Papers #102",
        "position_in_playlist": 172,
        "description": "A tangle pattern is a beautiful, intervowen tapestry of basic stroke patterns, like dots, straight lines, and simple curves. If we look at some of these works, we see that many of these are highly structured, and maybe, we could automatically create such beautiful structures with a computer. \n\n______________\n\nThe paper \"gTangle: a Grammar for the Procedural Generation of Tangle Patterns\" is available here:\nhttp://pellacini.di.uniroma1.it/publications/gtangle16/gtangle16.html\n\nThe paper \"Layer-Based Procedural Design of Facades\" is available here:\nhttps://www.cg.tuwien.ac.at/research/publications/2015/Ilcik_2015_LAY/\nhttps://vimeo.com/118400233\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nImage credits:\nWikipedia: https://en.wikipedia.org/wiki/Context-free_grammar\nThe thumbnail background image was taken from the corresponding paper (link above).\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A tangle pattern is a beautiful, intervowen tapestry of basic stroke patterns, like dots, straight lines, and simple curves. If we look at some of these works, we see that many of these are highly structured, and maybe, we could automatically create such beautiful structures with a computer. And, now, hold on to your papers, because this piece of work is about generating tangle patterns with grammars. Okay, now, stop right there. How on Earth do grammars have anything to do with computer graphics or tangle patterns? The idea of this sounds as outlandish as it gets. Grammars are a set of rules that tell us how to build up a structure, such as a sentence properly from small elements, like nouns, adjectives, pronouns, and so on. Math nerds also study grammars extensively and set up rules that enforce that every mathematical expression satisfies a number of desirable constraints. It's not a surprise that when mathematicians talk about grammars, they will use these mathematical hieroglyphs like the ones you see on the screen. It is a beautiful subfield of mathematics that I have studied myself before, and am still hooked. Especially given the fact that from grammars, we can build not only sentences, but buildings. For instance, a shape grammar for buildings can describe rules like a wall can contain several windows, below a window goes a window sill, one wall may have at most two doors attached, and so on. My friend Martin Ilcik is working on defining such shape grammars for buildings, and using these grammars, he can generate a huge amount of different skyscrapers, facades, and all kinds of cool buildings. In this piece of work, we start out with an input shape, subdivide it into multiple other shapes, assign these smaller shapes into groups. And the final tangle is obtained by choosing patterns and assigning them to all of these groups. This yields a very expressive, powerful, tool that anyone can use to create beautiful tangle patterns. And all this, through the power of grammars. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=nK3giIsNAHg",
        "paper_link": "https://www.cg.tuwien.ac.at/research/publications/2015/Ilcik_2015_LAY/",
        "paper_title": "gTangle: a Grammar for the Procedural Generation of Tangle Patterns"
    },
    {
        "video_id": "w2D5JR83pFI",
        "video_title": "3D Printing Materials With Subsurface Scattering | Two Minute Papers #98",
        "position_in_playlist": 176,
        "description": "Better Explained tutorials:\nhttps://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/\nhttps://betterexplained.com/cheatsheet/\n\nToday, our main question is whether we can reproduce the effect of subsurface scattering with 3D printed materials. The input would be a real material, and the output would be an arbitrary shaped 3d printed material with similar scattering properties. Something that looks similar.\n\n___________________________\n\nThe paper \"Physical Reproduction of Materials with Specified Subsurface Scattering\" is available here:\nhttp://people.csail.mit.edu/wojciech/PRO/index.html\n\nRecommended for you:\nSeparable Subsurface Scattering (more on diffusion profiles therein) - https://www.youtube.com/watch?v=72_iAlYwl0c\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nImage credits:\nThumbnail background image: C\u00e1ssia Afini - https://flic.kr/p/6mPh2m\nEar and skin subsurface scattering images: Wikipedia\nPlant leaf: Dan Markeye - https://flic.kr/p/fGie2L\nMarble: Koen Beets and Erik Hubo - http://research.edm.uhasselt.be/thaber/subsurface.php\nMarble dragon: Rui Wang - http://www.cs.virginia.edu/~rw2p/cs647/project.htm\nOrange: PieDog Media - https://flic.kr/p/83CZc1\nVoxel Lambo: Philippe Put - https://flic.kr/p/E4WLJQ\nVoxel Castle: post-apocalyptic research institute - https://flic.kr/p/b9xtJa\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Subsurface scattering means that not every ray of light is reflected or absorbed on the surface of a material, but some of it may get inside somewhere, and come out somewhere else. For instance, our skin is a great and fairly unknown example of that. We can witness this beautiful effect if we place a strong light source behind our ears. Note that many other materials, such as plant leaves, many fruits such as apples and oranges, wax, marble also have subsurface scattering. The more we look at objects like these, the more we recognize how beautiful and ubiquitous subsurface scattering and translucency is in mother nature. And today, our main question is whether we can reproduce this kind of effect with 3D printed materials. The input would be a real material, such as these slabs, and the output would be an arbitrary shaped 3d printed material with similar scattering properties. Something that looks similar. What you see here is already the result of the 3D printing process, and wow, they look very tasty indeed. The process starts with a measurement apparatus where we grab a real material, and create a diffusion profile from it that describes how light scatters inside of this material. We have talked quite a bit about diffusion profiles before, I've put some links to earlier episodes in the video description box. If you check it out, you'll see how we can add subsurface scattering to an already existing image by \"kind of\" multiplying it with an other image. This is another one of those amazing inventions of mankind. Now, onto 3D printing. When we would like to 3d print something, we basically have a few different materials to work with, and we have to specify a shape. This shape is approximated with a three-dimensional grid. Each of these tiny grid elements typically have the thickness of several microns, which basically means a tiny fraction of the diameter of one hair strand, and we like to call these elements voxels. Now, before printing, we have to specify what kind of material we'd like to fill each of these voxels with. This is the general workflow for most 3D printers. What is specific to this work is that, after that, we have to take one column of this material, and look at the scattering properties of it. Let's call this column one stacking. We could measure that stacking by hand and see how it relates to the original target material, and we are trying to minimize the difference between the two. However, it would take millions of tries and would likely take a lifetime to print just one high-quality reproduction. So basically, we have an optimization problem where we're looking for a stacking that will appear similar to the chosen diffusion profiles. The difference between the appearance of the two is to be minimized. However, we have to realize, that in physics, the laws of light scattering are well understood, and the wonderful thing is that instead of printing a real object, we could just use a light simulation program to tell us how close the results should be. Now, this would work great, but it would still take an eternity because simulating light scattering through a stack of materials would take the very least, several seconds. And we have to try up to millions of stackings for each column, and there is a lot of columns to compute. Why a lot of different columns? Well, it's because we have a heterogeneous problem, which means that the whole material can contain variations in color and scattering properties. The geometry may also be uneven, so this is a vastly more difficult formulation of the initial problem. A classical light simulation program would be able to solve this, well, in a matter of years. However, there is a wonderful tool that is able to almost immediately tell us how much light is scattering inside of a stack of a ton of different materials. An almost instant multi-layer scattering tool, if you will. It really is a miracle that we can get the results for something so quickly that would otherwise require following the paths of millions of light rays. We call this technique the Hankel transform. The mathematical description of it is absolutely beautiful, but I personally think the best way of motivating these techniques is through application. Like this one. Imagine that many mathematicians have to study this transform without ever hearing what it can be used for. These are not some dry and tedious materials that one has to memorize - we can do miracles with these inventions, and I feel that people need to know about that! With the use of the Hankel transform and some additional optimizations, one can efficiently find solutions that lead to high-quality reproductions of the input material. Excellent piece of work, definitely one of my favorites in 3D fabrication. As always, we'd love to read your feedback on this episode, let us know whether you have found it understandable! I hope you did! Also, a quick shoutout to BetterExplained.com. Please note that this is not a sponsored message. It has multiple slogans, such as \"math lessons for lasting insight\" or \"math without endless memorization\". This webpage is run by Kalid Azad, and contains tons of intuitive math lessons I wish I had access to during my years at the university. For instance, here is his guide on Fourier Transforms, which is a staple technique in every mathematician's and engineer's skill set, and is a prerequisite to understanding the Hankel transform. If you wish to learn mathematics, definitely check this website out, and if you don't wish to learn mathematics, then also definitely check this website out. Thanks for watching, and for your generous  support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=w2D5JR83pFI",
        "paper_link": "http://people.csail.mit.edu/wojciech/PRO/index.html",
        "paper_title": "Physical Reproduction of Materials with Specified Subsurface Scattering"
    },
    {
        "video_id": "kwqme8mEgz4",
        "video_title": "Sound Synthesis for Fluids With Bubbles | Two Minute Papers #97",
        "position_in_playlist": 177,
        "description": "In this work, the authors created a simulator, that shows us not only the motion of a piece of fluid, but the physics of bubbles within as well. This sounds great, but there are two huge problems: one, there are a lot of them, and two, they can undergo all kinds of deformations and topology changes.\n\n________________________\n\nThe paper \"Toward Animating Water with Complex Acoustic Bubbles\" is available here:\nhttp://www.cs.cornell.edu/projects/Sound/bubbles/\n\nRecommended for you:\nAll previous episodes on fluid simulations (and more!) - https://www.youtube.com/playlist?list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe source of the thumbnail background image: https://pixabay.com/photo-83758/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have had quite a few episodes on simulating the motion of fluids and creating beautiful footages from the results. In this work, the authors created a simulator, that shows us not only the motion of a piece of fluid, but the physics of bubbles within as well. This sounds great, but there are two huge problems: one, there are a lot of them, and two, they can undergo all kinds of deformations and topology changes. To conjure up video footage that is realistic, and relates to the real world, several bubble-related effects, such as entrainment, splitting, merging, advection and collapsing, all have to be simulated faithfully. However, there is a large body of research out there to simulate bubbles, and here, we are not only interested in the footage of this piece of fluid, but also what kind of sounds it would emit when we interact with it. The result is something like this. The vibrations of a bubble is simulated by borrowing the equations that govern the movement of springs in physics. However, this, by itself would only be a forlorn attempt at creating a faithful sound simulation, as there are other important factors to take into consideration. For instance, the position of the bubble matters a great deal. This example shows that the pitch of the sound is expected to be lower near solid walls, as you can see it marked with dark blue on the left, right side and below, and have a higher pitch near the surface, which is marked with red. You can also see that there are significant differences in the frequencies depending on the position, the highest frequency being twice as high as the lowest. So this is definitely an important part of the simulation. Furthermore, taking into consideration the shape of the bubbles is also of utmost importance. As the shape of the bubble goes from an ellipsoid to something close to a sphere, the emitted sound frequency can drop by as much as 30%. Beyond these effects, there were still blind spots even in state of the art simulations. With previous techniques, a chirp-like sound was missing, which is now possible to simulate with a novel frequency extension model. Additional extensions include a technique that models the phenomenon of the bubbles popping at the surface. The paper discusses what cases are likely to emphasize which of these extension's effects. Putting it all together, it sounds magnificent, check it out! But still, however great these sounds are, without proper validation, these are still just numbers on a paper. And of course, as always, the best way of testing these kinds of works if we let reality be our judge, and compare the results to real world footage. So I think you can guess what the next test is going to be about! The  authors also put up a clinic on physics and math, and the entirety of the paper is absolutely beautifully written. I definitely recommend having a look, as always, the link is available in the description box. Also, you'll find one more link to a playlist with all of our previous episodes on fluid simulations. Lots of goodies there. As always, we'd love to read your feedback on this episode, let us know whether you have found it understandable! I hope you did. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kwqme8mEgz4",
        "paper_link": "http://www.cs.cornell.edu/projects/Sound/bubbles/",
        "paper_title": "Toward Animating Water with Complex Acoustic Bubbles"
    },
    {
        "video_id": "5-xMV3sT3Tw",
        "video_title": "3D Printing Auxetic Materials | Two Minute Papers #96",
        "position_in_playlist": 178,
        "description": "In this episode, we shall talk about auxetic materials. Auxetic materials are materials that when stretched, thicken perpendicular to the direction we're stretching them. In other words, instead of thinning, they get fatter when stretched.\n\n_____________________________________\n\nThe paper \"Beyond Developable: Computational Design and Fabrication with Auxetic Materials\" is available here:\nhttp://lgg.epfl.ch/publications/2016/BeyondDevelopable/index.php\n\nThe tendon paper, \"Negative Poisson\u2019s ratios in tendons: An unexpected mechanical response\" is available here:\nhttp://www.sciencedirect.com/science/article/pii/S1742706115002871\n\nOur previous episode about optimization is available here:\nhttps://www.youtube.com/watch?v=1ypV5ZiIbdA\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was taken from the corresponding paper listed above.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We are back! And in this episode, we shall talk about auxetic materials. Auxetic materials are materials that when stretched, thicken perpendicular to the direction we're stretching them. In other words, instead of thinning, they get fatter when stretched. Really boggles the mind, right? They are excellent at energy absorption and resisting fracture, and are therefore widely used in body armor design, and I've read a research paper stating that even our tendons also show auxetic behavior. These auxetic patterns can be cut out from a number of different materials, and are also used in footwear design and actuated electronic materials. However, all of these applications are restricted to rather limited shapes. Furthermore, even the simplest objects, like this sphere cannot be always approximated by inextensible materials. However, if we remove parts of this surface in a smart way, this inextensible material becomes auxetic, and can approximate not only these rudimentary objects, but much more complicated shapes as well. However, achieving this is not trivial. If we try the simplest possible solution, which would basically be shoving the material onto a human head like a paperbag, but as it is aptly demonstrated in these images, it would be a fruitless endeavor. This method tries to solve this problem by flattening the target surface with an operation that mathematicians like to call a conformal mapping. For instance, the world map in our geography textbooks is also a very astutely designed conformal mapping from a geoid object, the Earth, to a 2D plane which can be shown on a sheet of paper. However, this mapping has to make sense so that the information seen on this sheet of paper actually makes sense in the original 3D domain as well. This is not trivial to do. After this mapping, our question is where the individual points would have to be located so that they satisfy three conditions: one: the resulting shape has to approximate the target shape, for instance, the human head, as faithfully as possible two: the construction has to be rigid three: when we stretch the material, the triangle cuts have to make sense and not intersect each other, so huge chasms and degenerate shapes are to be avoided. This work is using optimization to obtain a formidable solution that satisfies these constraints. If you remember our earlier episode about optimization, I said there will be a ton of examples of that in the series. This is one fine example of that! And the results are absolutely amazing - the possibility of creating a much richer set of auxetic material designs is now within the realm of possibility, and I expect that it will have applications from designing microscopic materials, to designing better footwear and leather garments. And we are definitely just scratching the surface! The method supports copper, aluminum, plastic and leather designs, and I am sure there will be mind blowing applications that we cannot even fathom so early in the process. As an additional selling point, the materials are also reconfigurable, meaning that from the same piece of material, we can create a number of different shapes. Even non-trivial shapes with holes, such as a torus, can be created. Note that in mathematics, the torus is basically a fancy name for a donut. A truly fantastic piece of work, definitely have a look at the paper, it has a lot of topological calculations, which is an awesome subfield of mathematics. And, the authors' presentation video is excellent, make sure to have a look at that. Let me know if you have found this episode understandable, we always get a lot of awesome feedback and we love reading your comments. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=5-xMV3sT3Tw",
        "paper_link": "http://lgg.epfl.ch/publications/2016/BeyondDevelopable/index.php\n\nThe tendon paper, \"Negative Poisson\u2019s ratios in tendons: An unexpected mechanical response\" is available here:",
        "paper_title": "Beyond Developable: Computational Design and Fabrication with Auxetic Materials"
    },
    {
        "video_id": "ZaAUFqcfDJg",
        "video_title": "Patreon Update - New Machine!",
        "position_in_playlist": 179,
        "description": "Our Patreon page is available here:\nhttps://www.patreon.com/TwoMinutePapers\n\nThe new PC configuration is the following:\nMotherboard: Asrock H170A-X1\nCPU: Intel Core i5-6600 3.3GHz LGA1151 BOX\nRAM: 16GB 2400MHz Kingston DDRIV HyperX Fury Black Kit RAM HX424C15FB\nVGA: Gigabyte PCI-E NVIDIA GTX1060 WF2 OC (6144MB, DDR5, 192bit, 1556/800)\nSSD: Samsung 250GB SSD 2,5\" SATA3 MZ-75E250B 850 EVO\nCase: Sharkoon VG5-W\nPSU: Chieftec 700W GPS-700-A8\n\n_______________\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Dana Mattocks - https://flic.kr/p/66dtrw\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I apologize for the delays during last week, I always put out notifications about such events on Twitter and Facebook, make sure to follow us there so you Fellow Scholars know about these well in advance. And I have to say, during this time, I really missed you and making videos so much! This video is a quick update on what has happened since, and I'd also like to assure you that the next Two Minute Papers episode is already in the works and is going to arrive soon. Very soon. Patreon is a platform where you can support your favorite creators with monthly recurring tips and get cool perks in return. We have quite a few supporters who are really passionate about the show, and during the making of the last few episodes, we have encountered severe hardware issues. There were freezes, random restarts, blue screens of death constantly, and the computer was just too unstable to record and edit Two Minute Papers. The technicians checked it out and found that quite a few parts would have to be replaced, and I figured that this would be a great time to replace this old configuration. So, we have ordered the new Two Minute Papers rig, and the entire purchase happened with the help of you, our Patreon supporters! We were able to replace it effortlessly, which is just amazing. Words fail me to describe how grateful I am for your generous support, and I am still stunned by this. It is just unfathomable to me that I am just sitting here in a room with a microphone, having way too much fun with research papers, and many of you enjoy this series enough to support it, and it had come to this. You Fellow Scholars make Two Minute Papers happen. Thanks so much. I'll try to squeeze in a bit of footage of the new rig, and transparency above all, will post the configuration in the video description box for the more curious minds out there. What I can say at this point, is that this new rig renders videos three times as quickly as the previous one. And even though these are just short videos, the rendering times for something in full HD and 60 fps are surprisingly long even when run on the graphical card. Well, not anymore. So, one more time, thank you so much for supporting the series. This is absolutely amazing, it really is. This was a quick update video, the next episode is coming soon, and I'll try my very best so we can be back at our regular schedule of two videos per week. Lots of spectacular works are on our list, stay tuned! Oh, and by the way, we have Fellow Scholars watching from all around the world. And in the meantime, some of them have started translating our episodes to German, Portuguese, Spanish and Italian. For some reason, I cannot see the names of the kind people who took their time to contribute, so I'd like to kindly thank you for your work, it makes Two Minute Papers accessible to even more people, which is amazing. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZaAUFqcfDJg"
    },
    {
        "video_id": "Mx8viOFKiIs",
        "video_title": "Sound Propagation With Adaptive Impulse Responses | Two Minute Papers #95",
        "position_in_playlist": 180,
        "description": "A realistic simulation of sounds within virtual environments dramatically improves the immersion of the user in computer games and virtual reality applications. To be able to simulate these effects, we need to compute the interaction between sound waves and the geometry and materials within the scene. Let's see how this work accomplishes it!\n\n______________________________\n\nThe paper \"Adaptive Impulse Response Modeling for Interactive Sound Propagation\" is available here:\nhttp://gamma.cs.unc.edu/ADAPTIVEIR/\n\nRecommended for you:\nRocking Out With Convolutions - https://www.youtube.com/watch?v=JKYQOAZRZu4\nAll light-transport related episodes: https://www.youtube.com/playlist?list=PLujxSBD-JXgk1hb8lyu6sTYsLL39r_3bG\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Robert - https://flic.kr/p/9YViuT\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Have you ever wondered how your voice, or your guitar would sound in the middle of a space station? A realistic simulation of sounds within virtual environments dramatically improves the immersion of the user in computer games and virtual reality applications. To be able to simulate these effects, we need to compute the interaction between sound waves and the geometry and materials within the scene. If you remember, we also had quite a few episodes about light simulations, where we simulated the interaction of light rays or waves and the scene we have at hand. Sounds quite similar, right? Well, kind of, and the great thing is that we can reuse quite a bit of this knowledge and some of the equations for light transport for sound. This technique we call path tracing, and it is one of the many well-known techniques used for sound simulation. We can use path tracing to simulate the path of many waves to obtain an impulse response, which is a simple mathematical function that describes the reverberation that we hear if we shoot a gun in a given scene, such as a space station or a church. After we obtained these impulse reponses, we can use an operation called the convolution with our input signal, like our voice to get a really convincing result. We have talked about this in more detail in earlier episodes, I've put a link for them in the video description box. It is important to know that the impulse reponse depends on the scene and where we, the listeners are exactly in the scene. In pretty much every concert ever, we find that sound reverberations are quite different in the middle of the arena versus standing at the back. One of the main contributions of this work is that it exploits temporal coherence. This means that even though the impulse response is different if we stand at different places, but these locations don't change arbitrarily and we can reuse a lot of information from the previous few impulse reponses that we worked so hard to compute. This way, we can get away with tracing much fewer rays and still get high-quality results. In the best cases, the algorithm executes five times as quickly as previous techniques, and the memory requirements are significantly more favorable. The paper also contains a user study. Limitations include a bit overly smooth audio signals and some fidelity loss in the lower frequency domains. Some of these scenes in the footage showcase up to 24 distinct sound sources, and all of them are simulated against the geometry and the materials found in the scene. So let's listen together and delight in these magnificent results. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Mx8viOFKiIs",
        "paper_link": "http://gamma.cs.unc.edu/ADAPTIVEIR/",
        "paper_title": "Adaptive Impulse Response Modeling for Interactive Sound Propagation"
    },
    {
        "video_id": "bLFISzfQCDQ",
        "video_title": "Estimating Matrix Rank With Neural Networks | Two Minute Papers #94",
        "position_in_playlist": 181,
        "description": "This tongue in cheek work is about identifying matrix ranks from images, plugging in a convolutional neural network where it is absolutely inaproppriate to use.\n\nThe paper \"Visually Identifying Rank\" is available here:\nhttp://www.oneweirdkerneltrick.com/rank.pdf\n\nDavid Fouhey's website is available here:\nhttp://www.cs.cmu.edu/~dfouhey/\n\nThe machine learning calculator is available here:\nhttp://armlessjohn404.github.io/calcuMLator/\n\nThe paper \"Separable Subsurface Scattering\" is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n\n__________________________\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Comfreak - https://pixabay.com/photo-356024/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work is not meant to be a highly useful application, only a tongue in cheek jab at the rising trend of trying to solve simple problems using deep learning without carefully examining the problem at hand. As always, we note that all intuitive explanations are wrong, but some are helpful, and the most precise way to express these thoughts can be done by using mathematics. However, we shall leave that to the textbooks and will try to understand these concepts by floating about on the wings of intuition. In mathematics, a matrix is a rectangular array in which we can store numbers and symbols. Matrices can be interpreted in many ways, for instance, we can think of them as transformations. Multiplying a matrix with a vector means applying this transform to the vector, such as scaling, rotation or shearing. The rank of a matrix can be intuitively explained in many ways. My favorite intuition is that the rank encodes the information content of the matrix. For instance, in an earlier work on Separable Subsurface Scattering, we recognized that many of these matrices that encode light scattering inside translucent materials, are of relatively low rank. This means that the information within is highly structured and it is not random noise. And from this low rank property follows that we can compress and represent this phenomenon using simpler data structures, leading to an extremely efficient algorithm to simulate light scattering within our skin. However, the main point is that finding out the rank of a large matrix is an expensive operation. It is also important to note that we can also visualize these matrices by mapping the numbers within to different colors. As a fun sidenote, the paper finds, that the uglier the colorscheme is, the better suited it is for learning. This way, after computing the ranks of many matrices, we can create a lot of input images and output ranks for the neural network to learn on. After that, the goal is that we feed in an unknown matrix in the form of an image, and the network would have to guess what the rank is. It is almost like having an expert scientist unleash his intuition on such a matrix, much like a fun guessing game for intoxicated mathematicians. And the ultimate question, as always is, how does this knowledge learned by the neural network generalize? The results are decent, but not spectacular, but they also offer some insights as to which matrices have surprising ranks. We can also try computing the products of matrices, which intuitively translates to guessing the result after we have done one transformation after the other. Like the output of scaling after a rotation operation. They also tried to compute the inverse of matrices, for which the intuition can be undoing the transformation. If it is a rotation to a given direction, the inverse would be rotating back the exact same amount, or if we scaled something up, then scaling it back down would be its inverse. Of course, these are not the only operations that we can do with matrices, we only used these for the sake of demonstration. The lead author states on his website that this paper shows that \"linear algebra can be replaced with machine learning\". Talk about being funny and tongue in cheek. Also, I have linked the website of David in the description box, he has a lot of great works and I am surely not doing him justice by of all those great works, covering this one. Rufus von Woofles, graduate of the prestigious Muddy Paws University was the third author of the paper, overlooking the entirety of the work and making sure that the quality of the results is impeccable. As future work, I would propose replacing the basic mathematical operators such as addition and multiplication by machine learning. Except that it is already done and is hilariously fun, and it even supports division by zero. Talk about the almighty powers of deep learning. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bLFISzfQCDQ",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/",
        "paper_title": "Visually Identifying Rank"
    },
    {
        "video_id": "CqFIVCD1WWo",
        "video_title": "WaveNet by Google DeepMind | Two Minute Papers #93",
        "position_in_playlist": 182,
        "description": "Let's talk about Google DeepMind's Wavenet! This piece of work is about generating audio waveforms for Text To Speech and more. Text To Speech basically means that we have a voice reading whatever we have written down. The difference in this work, is, however that it can synthesize these samples in someone's voice provided that we have training samples of this person speaking.\n\n__________________________\n\nThe paper \"WaveNet: A Generative Model for Raw Audio\" is available here:\nhttps://arxiv.org/abs/1609.03499\n\nThe blog post about this with the sound samples is available here:\nhttps://deepmind.com/blog/wavenet-generative-model-raw-audio/\n\nThe machine learning reddit thread about this paper is available here:\nhttps://www.reddit.com/r/MachineLearning/comments/51sr9t/deepmind_wavenet_a_generative_model_for_raw_audio/?ref=search_posts\n\nRecommended for you:\nEvery Two Minute Papers episode on deep learning: https://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nThanks so much to JulioC EA for the Spanish captions! :)\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was found on Pixabay - https://pixabay.com/hu/spektrum-hangsz%C3%ADnszab%C3%A1lyz%C3%B3-h%C3%A1tt%C3%A9r-545827/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When I opened my inbox today, I was greeted by a huge deluge of messages about WaveNet. Well, first, it's great to see that so many people are excited about these inventions, and second, may all your wishes come true as quickly as this one! So here we go. This piece of work is about generating audio waveforms for Text To Speech and more. Text To Speech basically means that we have a voice reading whatever we have written down. The difference in this work, is, however that it can synthesize these samples in someone's voice provided that we have training samples of this person speaking. It also generates waveforms sample by sample, which is particularly perilous because we typically need to produce these at the rate of 16 or 24 thousand samples per second, and as we listen to the TV, radio and talk to each other several hours a day, the human ear and brain is particularly suited to processing this kind of signal. If the result is off by only the slightest amount, we immediately recognize it. It is not using a recurrent neural network, which is typically suited to learn sequences of things, and is widely used for sound synthesis. It is using a convolutional neural network, which is quite surprising because it is not meant to process sequences of data that change in time. However, this variant contains an extension that is able to do that. They call this extension dilated convolutions and they open up the possibility of making large skips in the input data so we have a better global view of it. If we were working in computer vision, it would be like increasing the receptive field of the eye so we can see the entire landscape, and not only a tree on a photograph. It is also a bit like the temporal coherence problem we've talked about earlier. Taking all this into consideration results in more consistent outputs over larger time scales, so the technique knows what it had done several seconds ago. Also, training a convolutional neural network is a walk in the park compared to a recurrent neural network. Really cool! And the results beat all existing widely used techniques by a large margin. One of these is the concatenative technique, which builds sentences from a huge amount of small speech fragments. These have seen a ton of improvements during the years, but the outputs are still robotic and it is noticeable that we're not listening to a human but a computer. The DeepMind guys also report that: \"Notice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model.\" At the same time, I'd like to note that in the next few episodes, it may be that my voice is a bit different, but don't worry about that. It may also happen that I am on a vacation but new episodes and voice samples pop up on the channel, please don't worry about that either. Everything is working as intended! They also experimented with music generation, and the results are just stunning. I don't know what to say. These difficult problems, these impenetrable walls crumble one after another as DeepMind takes on them. Insanity. Their blog post and the paper are both really well written, make sure to check them out, they are both linked in the video description box. I wager that artistic style transfer for sound and instruments is not only coming, but it'll be here soon. I imagine that we'll play a guitar and it will sound like a harp, and we'll be able to sing something in Lady Gaga's voice and intonation. I've also seen someone pitching the idea of creating audiobooks automatically with such a technique. Wow. I travel a lot and am almost always on the go, so I personally would love to have such audiobooks! I have linked the mentioned machine learning reddit thread in the description box, as always, there's lots of great discussion and ideas there. It was also reported that the algorithm currently takes 90 minutes to synthesize one second of sound waveforms. You know the drill, one followup paper down the line, it will take only a few minutes, a few more papers down the line, it'll be real time. Just think about all these advancements. What a time we're living in! And I am extremely excited to present them all to you Fellow Scholars in Two Minute Papers. Make sure to leave your thoughts and ideas in the comments section, we love reading them! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=CqFIVCD1WWo",
        "paper_link": "https://arxiv.org/abs/1609.03499",
        "paper_title": "WaveNet: A Generative Model for Raw Audio"
    },
    {
        "video_id": "XmM1tF7AxdA",
        "video_title": "Automatic Hair Modeling from One Image | Two Minute Papers #92",
        "position_in_playlist": 183,
        "description": "This time, we are going to talk about hair modeling - obtaining hair geometry information from a photograph. This geometry information we can use in our movies and computer games. We can also run simulations on them and see how they look on a digital character. This is a remarkably difficult problem and you'll see a great solution to it in this episode.\n\n____________________________________\n\nThe paper \"AutoHair: Fully Automatic Hair Modeling from A Single Image\" is available here:\nhttp://gaps-zju.org/mlchai/resources/chai2016autohair.pdf\nhttp://gaps-zju.org/mlchai/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Betsy Jons - https://flic.kr/p/oye7FJ\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A couple episodes ago, we finally set sail in the wonderful world of hair simulations. And today we shall continue our journey in this domain. But this time, we are going to talk about hair modeling. So first, what is the difference between hair simulation and modeling? Well, simulation is about trying to compute the physical forces that act on hair strands, thereby showing to the user, how they would move about in reality. Modeling, however, is about obtaining geometry information from a photograph. This geometry information we can use in our movies and computer games. We can also run simulations on them and see how they look on a digital character. Just think about it, the input is one photograph and the output is a digital 3D model. This sounds like a remarkably difficult problem. Typically something that a human would do quite well at, but it would be too labor-intensive to do so for a large number of hairstyles. Therefore, as usual, neural networks enter the fray by looking at the photograph, and trying to estimate the densities and distributions of the hair strands. The predicted results are then matched with the hairstyles found in public data repositories, and the closest match is presented to the user. You can see some possible distribution classes here. Not only that, but the method is fully automatic, which means that unlike most previous works, it doesn't need any guidance from the user to accomplish this task. As a result, the authors created an enormous dataset with 50 thousand photographs and their reconstructions that they made freely available for everyone to use. The output results are so spectacular, it's almost as if we were seeing magic unfold before our eyes. The fact that we have so many hairstyles in this dataset also opens up the possibility of editing, which is always quite a treat for artists working in the industry. The main limitation is a poorer reconstruction of regions that are not visible in the input photograph, but I think that goes without saying. It is slightly ameliorated by the fact that the public repositories contain hairstyles that make sense, so we can expect results of reasonable quality even for the regions we haven't seen in the input photograph. As always, please let me know below in the comments section whether you have found everything understandable in this episode. Was it easy to digest? Was there something that was not easy to follow? Your feedback, as always, is greatly appreciated. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XmM1tF7AxdA",
        "paper_link": "http://gaps-zju.org/mlchai/resources/chai2016autohair.pdf",
        "paper_title": "AutoHair: Fully Automatic Hair Modeling from A Single Image"
    },
    {
        "video_id": "ksCSL6Ql0Yg",
        "video_title": "StyLit, Illumination-Guided Artistic Style Transfer | Two Minute Papers #91",
        "position_in_playlist": 184,
        "description": "Earlier, we have talked quite a bit about a fantastic new tool that we called artistic style transfer. This means that we have an input photograph that we'd like to modify, and another image from which we'd like to extract the artistic style. This way, we can, for instance, change our photo to look in the style of famous artists. Today, we're going to talk about a flamboyant little technique that is able to perform artistic style transfer in a way that preserves the illumination of the scene.\n\n______________________________\n\nThe paper \"StyLit: Illumination-Guided Example-Based Stylization of 3D Renderings\" is available here:\nhttp://dcgi.fel.cvut.cz/home/sykorad/stylit\n\nRecommended for you:\nArtistic Style Transfer For Videos - https://www.youtube.com/watch?v=Uxax5EKg0zA\nDeep Neural Network Learns Van Gogh's Art - https://www.youtube.com/watch?v=-R9bJGNHltQ\nNeural Material Synthesis (this contains discussions on diffuse and specular materials)- https://www.youtube.com/watch?v=XpwW3glj2T8\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Irina - https://flic.kr/p/nWXd5H\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Earlier, we have talked quite a bit about a fantastic new tool that we called artistic style transfer. This means that we have an input photograph that we'd like to modify, and another image from which we'd like to extract the artistic style. This way, we can, for instance, change our photo to look in the style of famous artists. Now, artists in the visual effects industry spend a lot of time designing the lighting and the illumination of their scenes, which is a long and arduous process. This is typically done in some kind of light simulation program, and if anyone thinks this is an easy and straightforward thing to do, I would definitely recommend trying it. After this lighting and illumination step is done, we can apply some kind of artistic style transfer, but we shall quickly see that there is an insidious side effect to this process: it disregards, or even worse, destroys our illumination setup, leading to results that look physically incorrect. Today, we're going to talk about a flamboyant little technique that is able to perform artistic style transfer in a way that preserves the illumination of the scene. These kinds of works are super important, because they enable us to take the wheel from the hands of the neural networks that perform these operations, and force our will on them. This way, we can have a greater control over what these neural networks do. Previous techniques take into consideration mostly color and normal information. Normals basically encode the shape of an object. However, these techniques don't really have a notion of illumination. They don't know that a reflection on an object should remain intact, and they have no idea about the existence of shadows either. For instance, we have recently talked about diffuse and specular material models, and setting up this kind of illumination is something that artists in the industry are quite familiar with. The goal is that we can retain these features throughout the process of style transfer. In this work, the artist is given a printed image of a simple object, like a sphere. This is no ordinary printed image, because this image comes from a photorealistic rendering program, which is augmented by additional information, like what part of the image is a shadowed region, and where the reflections are. And then, when the artist starts to add her own style to it, we know exactly what has been changed and how. This leads to a much more elaborate style transfer pipeline where the illumination stays intact. And the results are phenomenal. What is even more important, the usability of the solution is also beyond amazing. For instance, here, the artist can do the stylization on a simple sphere and get the artistic style to carry over to a complicated piece of geometry almost immediately. Temporal coherence is still to be improved, which means that if we try this on an animated sequence, it will be contaminated with flickering noise. We have talked about a work that does something similar for the old kind of style transfer, I've put a link in the video description box for that. I am sure that this kink will be worked out in no time. It's also interesting to note that the first style transfer paper was also published just a few months ago this year, and we're already lavishing in excellent followup papers. I think this demonstrates the excitement of research quite aptly - the rate of progress in technology and algorithms are completely unmatched. Fresh, new ideas pop up every day, and we can only frown and wonder at their ingenuity. As usual, please let me know in the comments section whether you have found this episode interesting and understandable. If you felt that everything is fine here, that is also valuable feedback. Thank you! And by the way, if you wish to express your scholarly wisdom, our store is open with some amazing quality merch. Have a look! We also have a huge influx of people who became Patrons recently, welcome, thank you so much for supporting Two Minute Papers. We love you too. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ksCSL6Ql0Yg",
        "paper_link": "http://dcgi.fel.cvut.cz/home/sykorad/stylit",
        "paper_title": "StyLit: Illumination-Guided Example-Based Stylization of 3D Renderings"
    },
    {
        "video_id": "HvHZXPd0Bjs",
        "video_title": "Interactive Hair-Solid Simulations | Two Minute Papers #90",
        "position_in_playlist": 185,
        "description": "We have talked about fluid and cloth simulations earlier, but we never really set foot in the domain of hair simulations in the series. \n\nTo obtain some footage of virtual hair movement, simulating the dynamics of hundreds of thousands of hair strands is clearly too time consuming and would be a flippant attempt to do so. In this episode, we discuss a technique to faithfully simulate 150 thousand hair strands by using only 400 guide hairs.\n\n____________________________\n\nThe paper \"Adaptive Skinning for Interactive Hair-Solid Simulation\" is available here:\nhttp://gaps-zju.org/mlchai/resources/chai2016adaptive.pdf\nhttp://gaps-zju.org/mlchai/\n\nTwo Minute Papers offers great perks to supporters on Patreon:\nhttps://www.patreon.com/TwoMinutePapers\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Faylyne (the image has been flipped and edited) - https://flic.kr/p/dayCjn\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have talked about fluid and cloth simulations earlier, but we never really set foot in the domain of hair simulations in the series. To obtain some footage of virtual hair movement, simulating the dynamics of hundreds of thousands of hair strands is clearly too time consuming and would be a flippant attempt to do so. If we do not wish to watch a simulation unfold with increasing dismay as it would take hours of computation time to obtain just one second of footage, we have to come up with a cunning plan. A popular method to obtain detailed real-time hair simulations is not to compute the trajectory of every single hair strand, but to have a small set of strands that we call guide hairs. For these guide hairs, we compute everything. However, since this is a sparse set of elements, we have to fill the gaps with a large number of hair strands, and we essentially try to guess how these should move based on the behavior of guide hairs near them. Essentially, one guide hair is responsible in guiding an entire batch, or an entire braid of hair, if you will. This technique we like to call a reduced hair simulation, and the guessing part is often referred to as interpolation. And the question immediately arises: how many guide hairs do we use and how many total hair strands can we simulate with them without our customers finding out that we're essentially cheating? The selling point of this piece of work, is that it uses only 400 guide hairs, and leaning on them, and it can simulate up to a total number of 150 thousand strands in real time. This leads to amazingly detailed hair simulations. My goodness, look at how beautiful these results are! Not only that, but as it is demonstrated quite aptly here, it can also faithfully handle rich interactions and collisions with other solid objects. For instance, we can simulate all kinds of combing, or pulling our hair out, which is what most researchers do in their moments of great peril just before finding the solution to a difficult problem. Not only hair, but a researcher simulator, if you will. The results are compared to a full space simulation, which means simulating every single hair strand, and that is exactly as time consuming as it sounds. The results are very close to being indistinguishable, which was not the case for previous works that created false intersections where hair strands would erroneously go through solid objects. We can also stroke bunnies with our hair models in this truly amazing piece of work. These episodes are also available in early access for our Patreon supporters. We also have plenty of other really neat perks, I've put a link in the description box, make sure to have a look! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HvHZXPd0Bjs",
        "paper_link": "http://gaps-zju.org/mlchai/resources/chai2016adaptive.pdf",
        "paper_title": "Adaptive Skinning for Interactive Hair-Solid Simulation"
    },
    {
        "video_id": "cVZzkSaxKmY",
        "video_title": "3D Printing With Filigree Patterns | Two Minute Papers #89",
        "position_in_playlist": 186,
        "description": "Filigrees are detailed, thin patterns typically found in jewelry, fabrics and ornaments, and as you may imagine, crafting such motifs on objects is incredibly laborious. This project is about leaving out the craftsmen from the equation by choosing a set of target filigree patterns and creating a complex shape out of them that can be easily 3D printed. The challenge lies in grouping and packing up these patterns to fill a surface evenly. Let's see what this piece of work has to say about the problem!\n\n____________________\n\nThe paper \"Synthesis of Filigrees for Digital Fabrication\" is available here:\nhttp://i.cs.hku.hk/~wkchen/projects/proj_sig16.html\n\nRecommended for you:\nOur earlier video on optimization - https://www.youtube.com/watch?v=1ypV5ZiIbdA\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was taken from the corresponding paper linked above.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Filigrees are detailed, thin patterns typically found in jewelry, fabrics and ornaments, and as you may imagine, crafting such motifs on objects is incredibly laborious. This project is about leaving out the craftsmen from the equation by choosing a set of target filigree patterns and creating a complex shape out of them that can be easily 3D printed. The challenge lies in grouping and packing up these patterns to fill a surface evenly. We start out with a base model with a poor structure, which is not completely random, but as you can see, is quite a forlorn effort. In several subsequent steps, we try to adjust the positions and shapes of the filigree elements to achieve more pleasing results. The more pleasing results we define as one that minimizes the amount of overlapping, and maximizes the connectivity of the final shape. Sounds like an optimization problem from earlier. And, that is exactly what it is. Really cool, right? The optimization procedure itself is far from trivial, and the paper discusses possible challenges and their solutions in detail. For instance, the fact that we can also add control fields to describe our vision regarding the size and orientation of the filigree patterns is an additional burden that the optimizer has to deal with. We can also specify the ratio of the different input filigree elements that we'd like to see added to the model. The results are compared to previous work, and the difference speaks for itself. However, it's important to point out that even this thing that we call previous work was still published this year. Talk about rapid progress in research! Absolutely phenomenal work. The evaluation and the execution of the solution, as described in the paper is also second to none. Make sure to have a look. And thank so much for taking the time to comment on our earlier video about the complexity of the series. I'd like to assure you we read every single comment and found a ton of super helpful feedback there. It seems to me that a vast majority of you agree that a simple overlay text does the job, and while it is there, it's even better to make it clickable so it leads to a video that explains the concept in a bit more detail for the more curious minds out there. I'll try to make sure that everything is available in mobile as well. You Fellow Scholars are the best and thanks so much for everyone for leaving a comment. Also, please let me know in the comments section if you have found this episode to be understandable or if there were any terms that you've never heard of. If everything was in order, that's also valuable information, so make sure to leave a comment. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cVZzkSaxKmY",
        "paper_link": "http://i.cs.hku.hk/~wkchen/projects/proj_sig16.html",
        "paper_title": "Synthesis of Filigrees for Digital Fabrication"
    },
    {
        "video_id": "XpwW3glj2T8",
        "video_title": "Neural Material Synthesis | Two Minute Papers #88",
        "position_in_playlist": 187,
        "description": "We are going to talk about techniques that create physically based material models from photographs that we can use in our light simulation programs. In an earlier work, two photographs are required for high-quality reconstruction. It seems that working from only one photograph doesn't seem possible at all. However, with the power of deep learning...\n\n___________________________________\n\nThe paper \"Two-Shot SVBRDF Capture for Stationary Materials\" is available here:\nhttps://mediatech.aalto.fi/publications/graphics/TwoShotSVBRDF/\n\nThe paper \"Reflectance Modeling by Neural Texture Synthesis\" is available here:\nhttps://mediatech.aalto.fi/publications/graphics/NeuralSVBRDF/\n\nNVIDIA has implemented the two-shot model! Have a look:\nhttps://twitter.com/karoly_zsolnai/status/839570124017438726\n\nOur earlier episode on Gradient Domain Light Transport is available here:\nhttps://www.youtube.com/watch?v=sSnDTPjfBYU\n\nThe light transport course at the Technical University of Vienna is available here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nImage credits:\nSherrie Thai - https://flic.kr/p/7pNLqB\nGiulio Bernardi - https://flic.kr/p/b2GMJ\nAndy Beatty - https://flic.kr/p/7j3dUX\nOpen Grid Scheduler - https://flic.kr/p/Gdg3JC\nliz west - https://flic.kr/p/rAbED\nhttp://collagefactory.blogspot.hu/2010/04/brdf-for-diffuseglossyspecular.html\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If you are new here, this is a series about research with the name Two Minute Papers, but let's be honest here. It's never two minutes. We are going to talk about two really cool papers that help us create physically based material models from photographs that we can use in our light simulation programs. Just as a note, these authors, Miika and Jaakko have been on a rampage for years now and have popped so many fantastic papers each of which I was blown away by. For instance, earlier, we talked about their work on Gradient Domain Light Transport, brilliant piece of work, I've put a link in the description box, make sure to check it out! So the problem we're trying to solve is very simple to understand: the input is a photograph of a given material somewhere in our vicinity, and the output is a bona fide physical material model that we can use in our photorealistic rendering program. We can import real world materials in our virtual worlds, if you will. Before we proceed, let's define a few mandatory terms: A material is diffuse if incoming light from one direction is reflected equally in all directions. This means that they look the same from all directions. White walls and matte surfaces are excellent examples of that. A material we shall consider specular if incoming light from one direction is reflected back to one direction. This means that if we turn our head a bit, we will see something different. For instance, the windshield of a car, water and reflections in a mirror can be visualized with a specular material model. Of course, materials can also be a combination of both. For instance, car paint, our hair and skin are all combinations of these material models. Glossy materials are midway between the two where the incoming light from one direction is reflected to not everywhere equally, and not in one direction, but a small selected set of directions. They change a bit when we move our head, but not that much. In the Two-Shot Capture paper, a material model is given by how much light is reflected and absorbed by the diffuse and specular components of the material, and something that we call a normal map, which captures the bumpiness of the material. Other factors like glossiness and anisotropy are also recorded, but we shall focus on the diffuse and the specular parts. The authors ask us to grab our phone for two photographs of a material to ensure a high-quality reconstruction procedure: one with flash, and one without. And the question immediately arises: why two images? Well, the image without flash can capture the component that looks the same from all directions, this is the diffuse component, and the photograph with flash can capture the specular component because we can see how the material handles specular reflections. And it is needless to say, the presented results are absolutely fantastic. So, first paper, two images, one material model. And therein lies the problem, which they tried to address in the second paper. If a computer looks at such an image, it doesn't know which part of one photograph is the diffuse and which is the specular reflection. However, I remember sitting in the waiting room of a hospital while reading the first paper, and this waiting room had a tiled glossy wall, and I was thinking that one image should be enough, because if I look at something, I can easily discern what the diffuse colors are, and which part is the specular reflection of something else. I don't need multiple photographs for that. I can also immediately see how bumpy it is, even from one photograph, I don't need to turn my head around. This is because we, humans have not a mathematical, but an intuitive understanding of the materials we see around us. So can we explain the same kind of understanding of materials to a computer somehow? Can we do it with only one image? And the answer is, yes we can, and, hopefully, we already feel the alluring call of neural networks. We can get a neural network that was trained on a lot of different images to try to guess what these material reflectance parameters should look like. However, the output should not be one image, but multiple images with the diffuse and specular reflectance informations, and a normal map to describe the bumpiness of this surface. Merely throwing a neural network at this problem, is however, not sufficient. There needs to be some kind of conspiracy between these images, because real materials are not arbitrarily put together. If one of these images is smooth, or has interesting features somewhere, the others have to follow it in some way. This \"some way\" is mathematically quite challenging to formulate, which is a really cool part of the paper. This conspiracy part is a bit like if we had 4 criminals testifying at a trial, where they try to sell their lies, and to maintain the credibility of their made up story, they have previously had to synchronize their lies so they line up correctly. The paper contains neat tricks to control the output of the neural network and create these conspiracies across these multiple image outputs that yield a valid and believable material model. And the results, are again, just fantastic. Second paper, one image, one material model. It doesn't get any better than that. Spectacular, not specular...spectacular piece of work. The first paper is great, but the second is smoking hot, by all that is holy, I am getting goosebumps. If you are interested in hearing a bit more about light transport and are not afraid of some mathematics, we recently recorded my full course on this at the Technical University of Vienna, the entirety of which is freely available for everyone. There is a link for it in the video description box, make sure to check it out! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XpwW3glj2T8",
        "paper_link": "https://mediatech.aalto.fi/publications/graphics/TwoShotSVBRDF/\n\nThe paper \"Reflectance Modeling by Neural Texture Synthesis\" is available here:",
        "paper_title": "Two-Shot SVBRDF Capture for Stationary Materials"
    },
    {
        "video_id": "heB2tD0-r-c",
        "video_title": "On the Complexity of Two Minute Papers | Two Minute Papers #87",
        "position_in_playlist": 188,
        "description": "There are some minor changes coming to Two Minute Papers, and I am trying my very best to make it as enjoyable as possible to you, so I would really like to hear your opinion on an issue.\n\nThe earlier episode showcased in the video:\nSchr\u00f6dinger's Smoke - https://www.youtube.com/watch?v=heY2gfXSHBo\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThe thumbnail background image was created by Tulip Vorlax - https://flic.kr/p/84QwGn\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is not an episode about a paper, but it's about the series itself. There are some minor changes coming, and I am trying my very best to make it as enjoyable as possible to you, so I would really like to hear your opinion on an issue. Many of our episodes are on new topics where I am trying my best to cover the basics so that the scope of a new research work can be understood clearly. However, as we are continuing our journey deeper into the depths of state of the art research, it inevitably happens that we have to build on already existing knowledge from earlier episodes. The big question is, how we should handle such cases. For instance, in the case of a neural network paper, the solution we went for so far was having a quick recap for what a neural network is. We can either have this recap in every episode about for instance, neural networks, fluid simulations or photorealistic rendering and be insidiously annoying to our seasoned Fellow Scholars who know it all. Or, we don't talk about the preliminaries to cater to the more seasoned Fellow Scholars out there, at the expense new people who are locked out of the conversation, as they may be watching their very first episode of Two Minute Papers. So the goal is clear, I'd like the episodes to be as easily understandable as possible, but while keeping the narrative intact so that every term I use is explained in the episode. First, I was thinking about handing out a so called \"dictionary\" in the video description box where all of these terms would be explained briefly. At first, this sounded like a good idea, but most people new to the series would likely not know about it, and for them, the fact that these episodes are not self-contained anymore would perhaps be confusing, or even worse, repulsive. The next idea was that, perhaps, instead of re-explaining these terms over and over again, we could add an overlay text in the video for them. The more seasoned Fellow Scholars won't be held up because they know what a Lagrangian fluid simulation is, but someone new to the series could also catch up easily just by reading a line of text that pops up. I think this one would be a formidable solution. I would love to know your opinion on these possible solutions, I personally think that the overlay text is the best, but who knows, maybe a better idea gets raised. Please make sure to let me know below in the comments section whether you have started watching Two Minute Papers recently or maybe you're a seasoned Fellow Scholar, and how you feel about the issue. Have you ever encountered terms that you didn't understand? Or was it the opposite, am I beating a dead horse with re-explaining all this simple stuff? I'd like to make these episodes the best I possibly can so that seasoned Fellow Scholars and people new to the show alike can marvel at the wonders of research. All feedback is welcome and please make sure to leave a comment so I can better understand how you feel about this issue and what would make you happier. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=heB2tD0-r-c"
    },
    {
        "video_id": "Rdpbnd0pCiI",
        "video_title": "What is an Autoencoder? | Two Minute Papers #86",
        "position_in_playlist": 189,
        "description": "Autoencoders are neural networks that are capable of creating sparse representations of the input data and can therefore be used for image compression. There are denoising autoencoders that after learning these sparse representations, can be presented with noisy images. What is even better is a variant that is called the variational autoencoder that not only learns these sparse representations, but can also draw new images as well. We can, for instance, ask it to create new handwritten digits and we can actually expect the results to make sense!\n\n_____________________________\n\nThe paper \"Auto-Encoding Variational Bayes\" is available here:\nhttp://arxiv.org/pdf/1312.6114.pdf\n\nRecommended for you:\nRecurrent Neural Network Writes Sentences About Images - https://www.youtube.com/watch?v=e-WB4lfg30M\n\nAndrej Karpathy's convolutional neural network that you can train in your browser:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nSentdex's Youtube channel is available here:\nhttps://www.youtube.com/user/sentdex\n\nFrancois Chollet's blog post on autoencoders:\nhttps://blog.keras.io/building-autoencoders-in-keras.html\n\nMore reading on autoencoders:\nhttps://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nThumbnail background image source (we have edited the colors and edited it some more): https://pixabay.com/hu/fizet-sz%C3%A1mok-sz%C3%A1mjegyek-kit%C3%B6lt%C3%A9s-937882/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As we have seen in earlier episodes of the series, neural networks are remarkably efficient tools to solve a number of really difficult problems. The first applications of neural networks usually revolved around classification problems. Classification means that we have an image as an input, and the output is, let's say a simple decision whether it depicts a cat or a dog. The input will have as many nodes as there are pixels in the input image, and the output will have 2 units, and we look at the one of these two that fires the most to decide whether it thinks it is a dog or a cat. Between these two, there are hidden layers where the neural network is asked to build an inner representation of the problem that is efficient at recognizing these animals. So what is an autoencoder? An autoencoder is an interesting variant with two important changes: first, the number of neurons is the same in the input and the output, therefore we can expect that the output is an image that is not only the same size as the input, but actually is the same image. Now, this normally wouldn't make any sense, why would we want to invent a neural network to do the job of a copying machine? So here goes the second part: we have a bottleneck in one of these layers. This means that the number of neurons in that layer is much less than we would normally see, therefore it has to find a way to represent this kind of data the best it can with a much smaller number of neurons. If you have a smaller budget, you have to let go of all the fluff and concentrate on the bare essentials, therefore we can't expect the image to be the same, but they are hopefully quite close. These autoencoders are capable of creating sparse representations of the input data and can therefore be used for image compression. I consciously avoid saying \"they are useful for image compression\". Autoencoders, offer no tangible advantage over classical image compression algorithms like JPEG. However, as a crumb of comfort, many different variants exist that are useful for different tasks other than compression. There are denoising autoencoders that after learning these sparse representations, can be presented with noisy images. As they more or less know how this kind of data should look like, they can help in denoising these images. That's pretty cool for starters! What is even better is a variant that is called the variational autoencoder that not only learns these sparse representations, but can also draw new images as well. We can, for instance, ask it to create new handwritten digits and we can actually expect the results to make sense! There is an excellent blog post from Francois Cholle, the creator of the amazing Keras library for building and training neural networks, make sure to have a look! With these examples, we were really only scratching the surface, and I expect quite a few exciting autoencoder applications to pop up in the near future as well. I cannot wait to get my paws on those papers. Hopefully you Fellow Scholars are also excited! If you are interested in programming, especially in python, make sure to check out the channel of Sentdex for tons of machine learning programming videos and more. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Rdpbnd0pCiI",
        "paper_link": "http://arxiv.org/pdf/1312.6114.pdf",
        "paper_title": "Auto-Encoding Variational Bayes"
    },
    {
        "video_id": "gHMY40kEXzs",
        "video_title": "The Science of Medal Predictions (2016 Rio Olympics Edition) | Two Minute Papers #85",
        "position_in_playlist": 190,
        "description": "The 2016 Rio Olympic Games is right around the corner, so it is the perfect time to talk a bit about how we can use science to predict the results.  Daniel Johnson, a professor of microeconomics at the Colorado College created a simple prediction model, that, over the past 5 Olympic Games, was able to achieve 94% agreement between the predicted and actual medal counts per nation. What is even more amazing is that the model doesn't even take into consideration the athletic abilities of any of these contenders. \n\n____________________\n\nThe paper \"A Tale of Two Seasons: Participation and Medal Counts\nat the Summer and Winter Olympic Games\" is available here:\nhttps://www.researchgate.net/profile/Daniel_Johnson7/publication/4920482_A_Tale_of_Two_Seasons_Participation_and_Medal_Counts_at_the_Summer_and_Winter_Olympic_Games/links/0c9605229d43e35dbf000000.pdf\n\nA media article about this on Forbes:\nhttp://www.forbes.com/2010/01/19/olympic-medal-predictions-business-sports-medals.html\n\nThe Olympics subreddit is available here:\nhttps://www.reddit.com/r/olympics/\n\nFrom an earlier episode:\nTwo Minute Papers - Narrow Band Liquid Simulations - https://www.youtube.com/watch?v=nfPBT71xYVQ\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by Rob124 - https://flic.kr/p/efKqca\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The 2016 Rio Olympic Games is right around the corner, so it is the perfect time to talk a bit about how we can use science to predict the results. Before we start, I'd like to mention that we won't be showcasing any predictions for this year's Olympics, instead, we are going to talk about a model that was used to predict the results of previous Olympic Games events. So, the following, very simple question arises: can we predict the future? The answer is very simple: no, we can't. End of video, thanks for watching! Well, jokes aside, we cannot predict the future itself, but we can predict what is likely to happen based on our experience of what happened so far. In mathematics, this is what we call extrapolation. There is also a big difference between trying to extrapolate the results of one athlete, or the aggregated number of medals for many athletes, usually an entire nation. To bring up an example about traffic, if we were to predict where one individual car is heading, we would obviously fail most of the time. However, whichever city we live in, we know exactly the hotspots where there are traffic jams every single morning of the year. We cannot accurately predict the behavior of one individual, but if we increase the size of the problem and predict for a group of people, it suddenly gets easier. Going back to the Olympics, will Usain Bolt win the gold on the 100 meter sprint this year? Predicting the results of one athlete is usually hopeless, and we bravely call such endeavors to be mere speculation. The guy whose results we're trying to predict may not even show up this year - as many of you have heard, many of the Russian athletes have been banned from the Olympic Games. Our model would sure as hell not be able to predict this. Or would it? We'll see in a second, but hopefully it is easy to see that macro-level predictions are much more feasible than predicting on an individual level. In fact, to demonstrate how much of an understatement it is to say feasible, hold onto your seatbelts, because Daniel Johnson, a professor of microeconomics at the Colorado College created a simple prediction model, that, over the past 5 Olympic Games, was able to achieve 94% agreement between the predicted and actual medal counts per nation. What is even more amazing is that the model doesn't even take into consideration the athletic abilities of any of these contenders. Wow! Media articles report that his model uses only 5 simple variables: a country's per-capita income, population, political structure, climate, and host-nation advantage. Now first, I'd first like to mention that GDP per capita means the Gross Domestic Product of one person in a given country, therefore it is independent of the population of the country. If we sit down and read the paper, which is a great and very easy read and you should definitely have a look, it's in the video description box. So, upon reading the paper, we realize there are more variables that are subject to scrutiny: for instance, a proximity factor, which encodes the distance from the hosting nation. Not only the hosting nation itself, but its neighbors are also enjoying significant advantages in the form of lower transportation costs and being used to the climate of the venue. Unfortunately I haven't found his predictions for this year's Olympics, but based on the simplicity of the model, it should be quite easy to run the predictions provided that the sufficient data is available. The take home message is that usually the bigger the group we're trying to predict results for, the lesser the number of variables that are enough to explain their behavior. If we are talking about the Olympics, 5 or 6 variables are enough to faithfully predict nationwide medal counts. These are amazing results that are also a nice testament to the power of mathematics. I also really like how the citation count of the paper gets a big bump every four years. I wonder why? If you are interested in how the Olympic Games unfold, make sure to have a look at the Olympics reddit, I found it to be second to none. As always, the link is available in the description box. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=gHMY40kEXzs",
        "paper_link": "https://www.researchgate.net/profile/Daniel_Johnson7/publication/4920482_A_Tale_of_Two_Seasons_Participation_and_Medal_Counts_at_the_Summer_and_Winter_Olympic_Games/links/0c9605229d43e35dbf000000.pdf",
        "paper_title": "A Tale of Two Seasons: Participation and Medal Counts\nat the Summer and Winter Olympic Games"
    },
    {
        "video_id": "a1z6GXj8QK8",
        "video_title": "Peer Review and the NeurIPS Experiment | Two Minute Papers #84",
        "position_in_playlist": 191,
        "description": "What is peer review and how is it done? How can we check the validity of a paper? And more importantly, how can we be sure that the peer review process is fair and consistent? We'll talk about these things and how the NIPS experiment addresses them.\n\n____________________\n\nThe NIPS experiment:\nhttp://blog.mrtz.org/2014/12/15/the-nips-experiment.html\nhttp://www.kdnuggets.com/2016/05/embrace-random-acceptance-borderline-papers.html\n\nThe showcased earlier episode video:\nArtistic Manipulation of Caustics - https://www.youtube.com/watch?v=K-0KJtk07YU\n\nA New Publishing Model in Computer Science by Yann LeCun:\nhttp://yann.lecun.com/ex/pamphlets/publishing-models.html\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nThe thumbnail background image was created by Quinn Dombrowski (the color of the pen was changed) - https://flic.kr/p/8HRJoc\nThe blind man icon was created by Scott de Jonge - http://www.flaticon.com/free-icon/blind-man-silhouette_8711\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We are here to answer a simple question: what is peer review? Well, in science, making sure that the validity of published results is beyond doubt is of utmost importance. To this end, many scientific journals and conferences exist where researchers can submit their findings in the form of a science paper. As a condition of acceptance, these papers shall undergo extensive scrutiny by typically 2 to 5 other scientists. This refereeing process we call peer review. Single blind reviewing means that the names of the reviewers are shrouded in mystery, but the authors of the paper are known to them. In double blind reviews, however, the papers are anonymized, and none of the parties know the names of each other. These different kinds of blind reviews were made to eliminate possible people-related biases. There is a lot of discussion whether they do a good job at that or not, but this is what they are for. After the review, if the results are found to be correct, and the reviews are favorable enough, the paper is accepted and subsequently published in a journal and/or presented at a conference. Usually, the higher the prestige of a publication venue is, the higher the likelihood of rejection, which inevitably raises a big question: how to choose the papers that are to be accepted? As we are scientists, we have to try to ensure that the peer review is a fair and consistent process. To measure if this is the case, the NIPS experiment was born. NIPS is one of the highest quality conferences in machine learning with a remarkably low acceptance ratio, which typically hovers below 25%. This is indeed remarkably low considering the fact that many of the best research groups in the world submit their finest works here. So here is the astute idea behind the NIPS experiment: a large amount of papers would be secretly disseminated to multiple committees, they would review it without knowing about each other, and we would have a look whether they would accept or reject the same papers. Re-reviewing papers and see if the results is the same, if you will. At a given prescribed acceptance ratio, there was a disagreement for 57% of the papers. This means that one of the committees would accept the paper and the other wouldn't, and vice versa. Now, to put this number into perspective, the mathematical model of a random committee was put together. This means that the members of this committee have no idea what they are doing, and as a review, they basically toss up a coin and accept or reject the paper based on the result. The calculations conclude that this random committee would have this disagreement ratio of about 77%. This is hardly something to be proud of: the consistency of expert reviewers is significantly closer to a coinflip than to a hypothetical perfect review process. So, experts, 57% disagreement, Coinflip Committee, 77% disagreement. It is not as bad as the Coinflip Committee, so the question naturally arises: where are the differences? Well, it seems that the top 10% of the papers are clearly accepted by both committees, the bottom 25% of the papers are clearly rejected, this is the good news, and the bad news is that anything between might as well be decided with a cointoss. If the consistency of peer review is subject to maximization, we clearly have to do something different. Huge respect for the NIPS organizers for doing this laborious experiment, for the reviewers who did a ton of extra work, and kudos for the fact that the organizers were willing to release such uncomfortable results. This is very important, and is the only way of improving our processes. Hopefully, someday we shall have our revenge over the Coinflip Committee. Can we do something about this? What is a possible solution? Well, of course, this is a large and difficult problem for which I don't pretend to have any perfect solutions, but there is a really interesting idea by a renowned professor about crowdsourcing reviews that I found to be spectacular. I'll leave the blog post in the comments section both for this and the NIPS experiment, and we shall have an entire episode about this soon. Stay tuned! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=a1z6GXj8QK8"
    },
    {
        "video_id": "ZHoNpxUHewQ",
        "video_title": "Task-based Animation of Virtual Characters | Two Minute Papers #83",
        "position_in_playlist": 192,
        "description": "This piece of work is about synthesizing believable footstep animations for virtual characters.\n\nThe paper \"Task-based Locomotion\" is available here:\nhttp://www.cs.ubc.ca/~van/papers/2016-TOG-taskBasedLocomotion/index.html\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this piece of work, you'll see wondrous little sequences of animations where a virtual character is asked to write on a whiteboard, move boxes, and perform different kinds of sitting behaviors. The emphasis is on synthesizing believable footstep patterns for this character. This sounds a bit mundane, but we'll quickly realize that the combination and blending of different footstep styles is absolutely essential for a realistic animation of these tasks. Beyond simple locomotion (walking if you will), for instance, sidestepping, using toe and heel pivots, or partial turns and steps every now and then are essential in obtaining a proper posture for a number of different tasks. A rich vocabulary of these movement types and proper transitions between them lead to really amazing animation sequences that you can see in the video. For instance, one of the most heinous examples of the lack of animating proper locomotion we can witness in older computer games, and sometimes even today, is when a character is writing on a whiteboard, who suddenly runs out of space, turns away from it, walks a bit, turns back towards the whiteboard and continues writing there. Even if we have impeccable looking photorealistically rendered characters, such robotic behaviors really ruin the immersion. In reality, a simple sidestepping would do the job, and this is exactly what the algorithm tells the character to perform. Very simple and smooth. This technique works by decomposing a given task to several subtasks, like starting to sit on a box, or getting up, and choosing the appropriate footstep types and transitions for them. One can also mark different tasks as being low or high-effort that are marked with green and blue. A low effort task could mean fixing a minor error on the whiteboard nearby without moving there, and a high effort task that we see marked with blue would be continuing our writing on a different part of the whiteboard. For these tasks, the footsteps are planned accordingly. Really cool. This piece of work is a fine example of the depth and complexity of computer graphics and animation research, and how even the slightest failure in capturing fine scale details is enough to break the immersion of reality. It is also really amazing that we have so many people who are interested in watching these videos about research, and quite a few of you decided to also support us on Patreon. I feel really privileged to have such amazing supporters like you Fellow Scholars. As always, I kindly thank you for this at the end of these videos, so here goes... Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZHoNpxUHewQ",
        "paper_link": "http://www.cs.ubc.ca/~van/papers/2016-TOG-taskBasedLocomotion/index.html",
        "paper_title": "Task-based Locomotion"
    },
    {
        "video_id": "1ypV5ZiIbdA",
        "video_title": "What is Optimization? + Learning Gradient Descent | Two Minute Papers #82",
        "position_in_playlist": 193,
        "description": "Let's talk about what mathematical optimization is, how gradient descent can solve simpler optimization problems, and Google DeepMind's proposed algorithm that automatically learn optimization algorithms.\n\nThe paper \"Learning to learn by gradient descent\nby gradient descent\" is available here:\nhttp://arxiv.org/pdf/1606.04474v1.pdf\n\nSource code:\nhttps://github.com/deepmind/learning-to-learn\n\n______________________________\n\nRecommended for you:\nGradients, Poisson's Equation and Light Transport - https://www.youtube.com/watch?v=sSnDTPjfBYU\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe chihuahua vs muffin image is a courtesy of teenybiscuit - https://twitter.com/teenybiscuit\nMore fun stuff here: http://twistedsifter.com/2016/03/puppy-or-bagel-meme-gallery/\n\n\nThe thumbnail background image was created by Alan Levine - https://flic.kr/p/vbEd1W\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we're not going to have the usual visual fireworks that we had with most topics in computer graphics, but I really hope you'll still find this episode enjoyable and stimulating. This episode is also going to be a bit heavy on what optimization is and we'll talk a little bit at the end about the intuition of the paper itself. We are going to talk about mathematical optimization. This term is not to be confused with the word \"optimization\" that we use in our everyday lives for, for instance, improving the efficiency of a computer code or a workflow. This kind of optimization means finding one, hopefully optimal solution from a set of possible candidate solutions. An optimization problem is given the following way: one, there is a set of variables we can play with, and two, there is an objective function that we wish to minimize or maximize. Well, this probably sounds great for mathematicians, but for everyone else, maybe this is a bit confusing. Let's build a better understanding of this concept through an example! For instance, let's imagine that we have to cook a meal for our friends from a given set of ingredients. The question is, how much salt, vegetables and meat goes into the pan. These are our variables that we can play with, and the goal is to choose the optimal amount of these ingredients to maximize the tastiness of the meal. Tastiness will be our objective function, and for a moment, we shall pretend that tastiness is an objective measure of a meal. This was just one toy example, but the list of applications is endless. In fact, optimization is so incredibly ubiquitous, there is hardly any field of science where some form of it is not used to solve difficult problems. For instance, if we have the plan of a bridge, we can ask it to tell us the minimal amount of building materials we need to build it in a way that it remains stable. We can also optimize the layout of the bridge itself to make sure the inner tension and compression forces line up well. A big part of deep learning is actually also an optimization problem. There are a given set of neurons, and the variables are when they should be activated, and we're fiddling with these variables to minimize the output error, which can be, for instance, our accuracy in guessing whether a picture depicts a muffin or a chihuahua. The question for almost any problem is usually not whether it can be formulated as an optimization problem, but whether it is worth it. And by worth it I mean the question whether we can solve it quickly and reliably. An optimizer is a technique that is able to solve these optimization problems and offer us a hopefully satisfactory solution to them. There are many algorithms that excel at solving problems of different complexities, but what ties them together is that they are usually handcrafted techniques written by really smart mathematicians. Gradient descent is one of the simplest optimization algorithms where we change each of the variables around a bit, and as a result, see if the objective function changes favorably. After finding a direction that leads to the most favorable changes, we shall continue our journey in that direction. What does this mean in practice? Intuitively, in our cooking example, after making several meals, we would ask our guests about the tastiness of these meals. From their responses, we would recognize that adding a bit more salt led to very favorable results, and since these people are notorious meat eaters, decreasing the amount of vegetables and increasing the meat content also led to favorable reviews. And we, of course, on the back of this newfound knowledge, will cook more with these variable changes in pursuit of the best possible meal in the history of mankind. This is something that is reasonably close to what gradient descent is in mathematics. A slightly more sophisticated version of gradient descent is also a very popular way of training neural networks. If you have any questions regarding the gradient part, we had an extended Two Minute Papers episode on what gradients are and how to use them to build an awesome algorithm for light transport. It is available, where? Well, of course, in the video description box, K\u00e1roly, why are you even asking. So what about the paper part? This incredible new work of Google DeepMind shows that an optimization algorithm itself can emerge as a result of learning. An algorithm itself is not considered the same one thing as deciding what an image depicts or how we should grade a student essay, it is an algorithm, a sequence of steps we have to take. If we're talking about outputting sequences, we'll definitely need to use a recurrent neural network for that. Their proposed learning algorithm can create new optimization techniques that outperform previously existing methods not everywhere, but on a set of specialized problems. I hope you've enjoyed the journey, we'll talk quite a bit about optimization in the future, you'll love it. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1ypV5ZiIbdA",
        "paper_link": "http://arxiv.org/pdf/1606.04474v1.pdf",
        "paper_title": "Learning to learn by gradient descent\nby gradient descent"
    },
    {
        "video_id": "zLzhsyeAie4",
        "video_title": "Bundlefusion: 3D Scenes from 2D Videos | Two Minute Papers #81",
        "position_in_playlist": 194,
        "description": "This piece of work enables us to walk around in a room with a camera, and create a complete 3D computer model from the video footage. Note that the title says \"2D\", but since RGB-D cameras are relatively new, they are both referred to as 2D and 3D (I've heard 2.5D as well before). We went with the 2D for now and I hope it won't raise any confusion! :)\n\n____________________________\n\nThe paper \"BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration\" is available here:\nhttp://graphics.stanford.edu/projects/bundlefusion/\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nThe thumbnail background image was created by gregzaal - http://www.blendswap.com/blends/view/74382\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work enables us to walk around in a room with a camera, and create a complete 3D computer model from the video footage. The technique has a really cool effect where the 3D model is continuously refined as we obtain more and more data by walking around with our camera. This is a very difficult problem, and a good solution to this offers a set of cool potential applications. If we have a 3D model of a scene, what can we do with it? Well, of course, assign different materials to them and run a light simulation program for architectural visualization applications, animation movies, and so on. We can also easily scan a lot of different furnitures and create a useful database out of them. There are tons of more applications, but I think these should do for starters. Normally, if one has to create a 3D model of a room or a building, the bottom line is that it requires several days or weeks of labor. Fortunately, with this technique, we'll obtain a 3D model in real time and we won't have to go through these tribulations. However, I'd like to note that the models are still by far not perfect, if we are interested in the many small, intricate details, we have add them back by hand. Previous methods were able to achieve similar results, but they suffer from a number of different drawbacks, for instance, most of them don't support traditional consumer cameras or take minutes to hours to perform the reconstruction. To produce the results presented in the paper, an NVIDIA TITAN X video card was used, which is currently one of the pricier pieces of equipment for consumers, but not so much for companies who are typically interested in these applications. If we take into consideration the rate at which graphical hardware is improving, anyone will be able to run this at home in real time in a few years time. The comparisons to previous works reveal that this technique is not only real time, but the quality of the results is mostly comparable, and in some cases, it surpasses previous methods. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=zLzhsyeAie4",
        "paper_link": "http://graphics.stanford.edu/projects/bundlefusion/",
        "paper_title": "BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration"
    },
    {
        "video_id": "a3sgFQjEfp4",
        "video_title": "Photorealistic Images from Drawings | Two Minute Papers #80",
        "position_in_playlist": 195,
        "description": "The Two Minute Papers subreddit is available here: \nhttps://www.reddit.com/r/twominutepapers/\n\nBy using a convolutional neural networks (a powerful deep learning technique), it is now possible to build an application that takes a rough sketch as an input, and fetches photorealistic images from a database.\n\n___________________________________\n\nThe paper \"The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies\" and the online demo is available here:\nhttp://sketchy.eye.gatech.edu/\n\nThe paper \"Signature verification using a Siamese time delay neural network\" is available here:\nhttps://scholar.google.hu/scholar?cluster=4400768003729787411&hl=en&as_sdt=0,5\n\nThe paper \"Learning Fine-grained Image Similarity with Deep Ranking\" is available here:\nhttps://arxiv.org/abs/1404.4661\n\nOur deep learning-related videos are available here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was drawn by Fel\u00edcia Feh\u00e9r.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When we were children, every single one of us dreamed about having a magic pencil that would make our adorable little drawings come true. With the power of machine learning, the authors of this paper just made our dreams come true. Here's the workflow: we provide a crude drawing of something, and the algorithm fetches a photograph from a database that depicts something similar to it. It's not synthesizing new images from scratch from a written description like one of the previous works, it fetches an already existing image from a database. The learning happens by showing a deep convolutional neural network pairs of photographs and sketches. If you are not familiar with these networks, we have some links for you in the video description box! It is also important to note that this piece of work does not showcase a new learning technique, it is using existing techniques on a newly created database that the authors kindly provided free of charge to encourage future research in this area. What we need to teach these networks is the relation of a photograph and a sketch. For instance, in an earlier work by the name Siamese networks, the photo and the sketch would be fed to two convolutional neural networks with the additional information whether this pair is considered similar or dissimilar. This idea of Siamese networks was initially applied to signature verification more than 20 years ago. Later, Triplet networks were used provide the relation of multiple pairs, like \"this sketch is closer to this photo than this other one\". There is one more technique referred to in the paper that they used, which is quite a delightful read, make sure to have a look! We need lots and lots of these pairs so the learning algorithm can learn what it means that a sketch is similar to a photo, and as a result, fetch meaningful images for us. So, if we train these networks on this new database, this magic pencil dream of ours can come true. What's even better, anyone can try it online! This is going to be a very rigorous and scholarly scientific experiment - I don't know what this should be, but I hope the algorithm does. Well, that kinda makes sense. Thanks, algorithm! For those Fellow Scholars out there who are endowed with better drawing skills than I am, well, basically all of you - if you have tried it and got some amazing, or maybe not so amazing results, please post them in the comments section! Or, as we now have our very own subreddit, make sure to drop by and post some of your results there so we can marvel at them, or have a good laugh at possible failure cases. I am looking forward to meeting you Fellow Scholars at the subreddit. Flairs are also available. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=a3sgFQjEfp4",
        "paper_link": "https://scholar.google.hu/scholar?cluster=4400768003729787411&hl=en&as_sdt=0,5\n\nThe paper \"Learning Fine-grained Image Similarity with Deep Ranking\" is available here:",
        "paper_title": "The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies"
    },
    {
        "video_id": "flOevlA9RyQ",
        "video_title": "Visually Indicated Sounds | Two Minute Papers #79",
        "position_in_playlist": 196,
        "description": "The Scholarly Store is available here: https://shop.spreadshirt.net/TwoMinutePapers\n\nUsing the power of deep learning, it is now possible to create a technique that looks at a silent video and synthesize appropriate sound effects for it. The usage is at the moment, limited to hitting these objects with a drumstick.\n\nNote: The authors seem to lean on a database of sounds, i.e., the synthesis does not happen from scratch, but they are not merely fetching the database entry for a given sound, but performing example-based synthesis (Section 5.2 in the paper below). In the video and the paper, they both use the words \"synthesized sound\" and \"predicted sound\", and it may be a bit unclear what degree of synthesis qualifies as a \"synthesized sound\". I think this is definitely worthy of further scrutiny.\n\n_____________________________________\n\nThe paper \"Visually Indicated Sounds\" is available here:\nhttps://arxiv.org/abs/1512.08512\n\nRecommended for you:\nWhat Do Virtual Objects Sound Like? - https://www.youtube.com/watch?v=ZaFqvM1IsP8&index=37&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\nSynthesizing Sound From Collisions - https://www.youtube.com/watch?v=rskdLEl05KI&index=51&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\nReconstructing Sound From Vibrations - https://www.youtube.com/watch?v=2i1hrywDwPo&index=83&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nOur deep learning-related videos are available here (if you are looking for convolutional neural networks, recurrent neural networks):\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by slgckgc - https://flic.kr/p/9x93qE\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This name is not getting any easier, is it? It used to be K\u00e1roly Zsolnai, which was hard enough, and now this... haha. Anyway, let's get started. This technique simulates how different objects in a video sound when struck. We have showcased some marvelous previous techniques that were mostly limited to wooden and plastic materials. Needless to say, there are links to these episodes in the video description box. A convolutional neural network takes care of understanding what is seen in the video. This technique is known to be particularly suited to processing image and video content. And it works by looking at the silent video directly and trying to understand what is going on, just like a human would. We train these networks with input and output pairs - the input is a video of us beating the hell out of some object with a drumstick. The joys of research! And the output is the sound this object emits. However, the output sound is something that changes in time. It is a sequence, therefore it cannot be handled by a simple classical neural network. It is learned by a recurrent neural network that can take care of learning such sequences. If you haven't heard these terms before, no worries, we have previous episodes on all of them in the video description box, make sure to check them out! This piece of work is a nice showcase of combining two quite powerful techniques: the convolutional neural network tries to understand what happens in the input video, and the recurrent neural network seals the deal by learning and guessing the correct sound that the objects shown in the video would emit when struck. The synthesized outputs were compared to the real world results both mathematically and by asking humans to try to tell from the two samples which one the real deal is. These people were fooled by the algorithm around 40% of the time, which I find to be a really amazing result, considering two things: first, the baseline is not 50%, but 0% because people don't pick choices at random - we cannot reasonably expect a synthesized sound to fool humans at any time. Like nice little neural networks, we've been trained to recognize these sounds all our lives, after all. And second, this is one of the first papers from a machine learning angle on sound synthesis. Before reading the paper, I expected at most 10 or 20 percent, if that. The tidal wave of machine learning runs through a number of different scientific fields. Will deep learning techniques establish supremacy in these areas? Hard to say yet, but what we know for sure is that great strides are made literally every week. There are so many works out there, sometimes I don't even know where to start. Good times indeed! Before we go, some delightful news for you Fellow Scholars! The Scholarly Two Minute Papers store is now open! There are two different kinds of men's T-shirts available, and a nice sleek design version that we made for the Fellow Scholar ladies out there! We also have The Scholarly Mug to get your day started in the most scientific way possible. We have tested the quality of these products and were really happy with what we got. If you ordered anything, please provide us feedback on how you liked the quality of the delivery and the products themselves. If you can send us an image of yourself wearing or using any of these, we'd love to have a look. Just leave them in the comments section or tweet at us! If you don't like what you got, within 30 days, you can exchange it or get your product cost refunded. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=flOevlA9RyQ",
        "paper_link": "https://arxiv.org/abs/1512.08512",
        "paper_title": "Visually Indicated Sounds"
    },
    {
        "video_id": "FMHGS8jWtzM",
        "video_title": "Time Varying Textures | Two Minute Papers #78",
        "position_in_playlist": 197,
        "description": "This work simulates how textures evolve and wear over time by taking only one image as an input sample.\n\n_______________________________\n\n\nThe paper \"Time-varying Weathering in Texture Space\" is available here:\nhttp://www.math.tau.ac.il/~dcor/articles/2016/TW.pdf\nhttp://www.math.tau.ac.il/~dcor/pubs.html\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by David Flores (we have applied a blur effect to it) - https://flic.kr/p/9eaVRJ\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This research group is known for their extraordinary ideas, and this piece of work is, of course, no exception. This paper is about time varying textures. Have a look at these photographs that were taken at a different time. And the million dollar question is, can we simulate how this texture would look if we were to go forward in time? A texture weathering simulation, if you will. The immediate answer is that of course not. However, in this piece of work, a single input image is taken, and without any user interaction, the algorithm attempts to understand how this texture might have looked in the past. Now, let's start out by addressing the elephant in the room: this problem can obviously not be solved in the general case for any image. However, if we restrict our assumptions to textures that contain a repetitive pattern, then it is much more feasible to identify the weathering patterns. To achieve this, an age map is built where the red regions show the parts that are assumed to be weathered. You can see on the image how these weathering patterns break up the regularity. Leaning on the assumption that if we go back in time, the regions marked with red will recede, and if we go forward in time, they will grow, we can write a really cool weathering simulator that creates results that look like wizardry. Broken glass, cracks, age rings on a wooden surface, you name it. But we can also use this technique to transfer weathering patterns from one image onto another. Textures with multiple layers are also supported, which means that it can handle images that are given as a sum of a regular and irregular patterns. The blue background is regular and quite symmetric, but the \"no parking\" text is lacking these regularities. And the amazing thing is that the technique still works on such cases. The results are also demonstrated by putting these weathered textures on 3D models so we can see them all in their glory in our own application. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=FMHGS8jWtzM",
        "paper_link": "http://www.math.tau.ac.il/~dcor/articles/2016/TW.pdf",
        "paper_title": "Time-varying Weathering in Texture Space"
    },
    {
        "video_id": "6rNcAVr-U4s",
        "video_title": "Fermat Spirals for Layered 3D Printing | Two Minute Papers #77",
        "position_in_playlist": 198,
        "description": "The paper \"Connected Fermat Spirals for Layered Fabrication\" is available here:\nhttp://irc.cs.sdu.edu.cn/html/2016/2016_0519/222.html\n\nThe ThatsMaths article on sunflowers + paper \"Fibonacci patterns: common or rare?\" is available here:\nhttps://thatsmaths.com/2014/06/05/sunflowers-and-fibonacci-models-of-efficiency/\nhttp://www.sciencedirect.com/science/article/pii/S2210983813001314\n\nAnother nice application of Hilbert curves for spatial indexing (thanks for the link TheJonManley!):\nhttp://blog.notdot.net/2009/11/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs, Daniel John Benton.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by Jos\u00e9 Carlos Cortizo P\u00e9rez - https://flic.kr/p/5bXvB9\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What are Hilbert curves? Hilbert curves are repeating lines that are used to fill a square. Such curves, so far, have enjoyed applications like drawing zigzag patterns to prevent biting in our tail in a snake game. Or, jokes aside, it is also useful in, for instance, choosing the right pixels to start tracing rays of light in light simulations, or to create good strategies in assigning numbers to different computers in a network. These numbers, by the way, we call IP addresses. These are just a few examples, and they show quite well how a seemingly innocuous mathematical structure can see applications in the most mind bending ways imaginable. So here is one more. Actually, two more. Fermat's spiral is essentially a long line as a collection of low curvature spirals. These are generated by a remarkably simple mathematical expression and we can also observe such shapes in mother nature, for instance, in a sunflower. And the most natural question emerges in the head of every seasoned Fellow Scholar. Why is that? Why would nature be following mathematics, or anything to do with what Fermat wrote on a piece of paper once? It has only been relatively recently shown that as the seeds are growing in the sunflower, they exert forces on each other, therefore they cannot be arranged in an arbitrary way. We can write up the mathematical equations to look for a way to maximize the concentration of growth hormones within the plant to make it as resilient as possible. In the meantime, this force exertion constraint has to be taken into consideration. If we solve this equation with blood sweat and tears, we may experience some moments of great peril, but it will be all washed away by the beautiful sight of this arrangement. This is exactly what we see in nature. And, which happens to be almost exactly the same as a mind-bendingly simple Fermat spiral pattern. Words fail me to describe how amazing it is that mother nature is essentially able to find these solutions by herself. Really cool, isn't it? If our mind wasn't blown enough yet, Fermat spirals can also be used to approximate a number of different shapes with the added constraint that we start from a given point, take an enormously long journey of low curvature shapes, and get back to almost exactly where we started. This, again, sounds like an innocuous little game evoking ill-concealed laughter in the audience as it is presented by as excited as underpaid mathematicians. However, as always, this is not the case at all. Researchers have found that if we get a 3D printing machine and create a layered material exactly like this, the surface will have a higher degree of fairness, be quicker to print, and will be generally of higher quality than other possible shapes. If we think about it, if we wish to print a prescribed object, like this cat, there is a stupendously large number of ways to fill this space with curves that eventually form a cat. And if we do it with Fermat spirals, it will yield the highest quality print one can do at this point in time. In the paper, this is demonstrated for a number of shapes of varying complexities. And this is what research is all about - finding interesting connections between different fields that are not only beautiful, but also enrich our everyday lives with useful inventions. In the meantime, we have reached our first milestone on Patreon, and I am really grateful to you Fellow Scholars who are really passionate about supporting the show. We are growing at an extremely rapid pace and I am really excited to make even more episodes about these amazing research works. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6rNcAVr-U4s",
        "paper_link": "https://thatsmaths.com/2014/06/05/sunflowers-and-fibonacci-models-of-efficiency/",
        "paper_title": "Connected Fermat Spirals for Layered Fabrication"
    },
    {
        "video_id": "iTRnr6p7iYo",
        "video_title": "Procedural Yarn Models for Cloth Rendering | Two Minute Papers #76",
        "position_in_playlist": 199,
        "description": "The paper \"Fitting Procedural Yarn Models for Realistic Cloth Rendering\" is available here:\nhttps://shuangz.com/publications.htm\nhttp://www.cs.cornell.edu/~kb/publications/SIG16ProceduralYarn.pdf\n\nVideo credits (in order):\nBandyte - https://www.youtube.com/watch?v=e4BhdrFDHkQ\nTheJamsh - https://www.youtube.com/watch?v=oSYjg9W4Nrk\nGamasutra - http://www.gamasutra.com/blogs/AAdonaac/20150903/252889/Procedural_Dungeon_Generation_Algorithm.php\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by Ny - https://flic.kr/p/gqShF5\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we're going to talk about a procedural algorithm. But first of all, what does procedural mean? Procedural graphics is an exciting subfield of computer graphics where instead of storing a lot of stuff, information is generated on the fly. For instance, in photorealistic rendering, we're trying to simulate how digital objects would look like in real life. We usually seek to involve some scratches on our digital models, and perhaps add some pieces of bump or dirt on the surface of the model. To obtain this, we can just ask the computer to not only generate them on the fly, but we can also edit them as we desire. We can also generate cloudy skies and many other things where only some statistical properties have to be satisfied, like how many clouds we wish to see and how puffy they should be, which would otherwise be too laborious to draw by hand. We are scholars after all, we don't have time for that. There are also computer games where the levels we can play through are not predetermined, but also generated on the fly according to some given logical constraints. This can mean that a labyrinth should be solvable, or the level shouldn't contain too many enemies that would be impossible to defeat. The main selling point is that such a computer game has potentially an infinite amount of levels. In this paper, a technique is proposed to automatically generate procedural yarn geometry. A yarn is a piece of thread from which we can sew garments. The authors extensively studied parameters in physical pieces of yarns such as twisting and hairyness and tried to match them with a procedural technique. So, for instance, if in a sudden trepidation we wish to obtain a realistic looking piece of cotton, rayon or silk in our light simulation programs, we can easily get a unique sample of a chosen material, which will be very close to the real deal in terms of these intuitive parameters like hairyness. And we can not only get as long or as many of these as we desire, but can also edit them according to our artistic vision. The solutions are validated against photographs and even CT scans. I always emphasize that I really like these papers where the solutions have some connection to real world around us. This one is super fun indeed! The paper is a majestic combination of beautifully written mathematics and amazing looking results. Make sure to have a look! And you know, we always hear these news where other YouTubers have problems with what is going on in their comments section. Well, not here with our Fellow Scholars. Have a look at the comments section of our previous episode. Just absolutely beautiful. I don't even know what to say, it feels like a secret hideout of respectful and scholarly conversations. It's really amazing that we are building a community of Fellow Scholars, humble people who wish nothing else than to learn more. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iTRnr6p7iYo",
        "paper_link": "https://shuangz.com/publications.htm",
        "paper_title": "Fitting Procedural Yarn Models for Realistic Cloth Rendering"
    },
    {
        "video_id": "ZBWTD2aNb_o",
        "video_title": "What Can We Learn From Deep Learning Programs? | Two Minute Papers #75",
        "position_in_playlist": 200,
        "description": "The paper \"Model Compression\" is available here:\nhttps://www.cs.cornell.edu/~caruana/compression.kdd06.pdf\n\nThere is also a talk on it here:\nhttp://research.microsoft.com/apps/video/default.aspx?id=103668&r=1\n\nDiscussions on this issue:\n1. https://www.linkedin.com/pulse/computer-vision-research-my-deep-depression-nikos-paragios\n2. https://www.reddit.com/r/MachineLearning/comments/4lq701/yann_lecuns_letter_to_cvpr_chair_after_bad/\n\nRecommended for you:\nNeural Programmer Interpreters - https://www.youtube.com/watch?v=B70tT4WMyJk\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by John Lord - https://flic.kr/p/nVUaB\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I have recently been witnessing a few heated conversations regarding the submission of deep learning papers to computer vision conferences. The forums are up in arms about the fact that despite some of these papers showcased remarkably good results, they were rejected on the basis of, from what I have heard, not adding too much to the tree of knowledge. They argue that we don't understand what is going on in these neural networks and cannot really learn anything new from them. I'll try to understand and rephrase their argument differently. We know exactly how to train a neural network, it's just that as an output of this process, we get a model of something that resembles a brain as a collection of neurons and circumstances under which these neurons are activated. We store these in a file that can take up to several gigabytes, and the best solutions are often not intuitively understandable for us. For instance, in this video, we're training a neural network to classify these points correctly, but what exactly can we learn if we look into these neurons? Now imagine that in practice, we don't have a handful of these boxes, but millions of them, and more complex than the ones you see here. Let's start with a simple example that hopefully helps getting a better grip of this argument. Now, I'll be damned if this video won't be more than a couple minutes, so this is going to be one of those slightly extended Two Minute Paper episodes. I hope you don't mind! The grammatical rules of my native language, a lot of them, are contained in enormous tomes that everyone has to go through during their school years. Rules are important. They give the scaffoldings for constructing sentences that are grammatically correct. Can we explain or even enumerate these rules? Well, unless you are a linguist, the answer is no. Almost no one really remembers more than a few rules, but every native speaker knows how their language should be spoken. And it is because we've heard a lot of sentences that are correct and learned it by heart what makes a proper sentence and what is gibberish. This is exactly what neural networks do - they are trained in a very similar way. In fact, they are so effective at it that if we you try to forcefully insert some of our knowledge in there, the solutions are going to get worse. It is therefore an appropriate time to ask questions like what merits a paper and what do we define as scientific progress. What if we have extremely accurate algorithms where we don't know what is going on under the hood, or simpler, more intuitive algorithms that may be subpar in accuracy. If we have a top tier scientific conference where only a very limited number of papers get accepted, which ones shall we accept? I hope that this question will spark a productive discussion, and hopefully scientific research venues will be more vigilant about this question in the future. Okay, so the question is crystal clear: knowledge or efficiency? How about possible solutions? Can we extract scientific insights out of these neural networks? Model compression is a way to essentially compress the information in this brain-ish thing, this collection of neurons we described earlier. To demonstrate why this is such a cool idea, let's quickly jump to this program by DeepMind that plays Atari games at an amazingly high level. In breakout, the solution program that you see here is essentially an enormous table that describes what the program should do when it sees different inputs. It is so enormous that it has many millions of records in there. A manual of many thousand pages, if you will. It is easy to execute for a computer, but completely impossible for us to understand why and how it works. However, if we intuitively think about the game itself, we could actually write a super simple program in one line of code that would almost be as good as this. All we need to do is try to follow the ball with the paddle. One line of code, and pretty decent results. Not optimal, but quite decent. From such a program, we can actually learn something about the game. Essentially what we could do with these enormous tables, is compressing them into much much smaller ones. Ones that are so tiny, that we can actually build an intuition from them. This way, the output of a machine learning technique wouldn't only be an extremely efficient program, but finally, the output of the procedure would be knowledge. Insight. If you think about it, such an algorithm would essentially do research by itself. At first, it would randomly try experimenting, and after a large amount of observations are collected, these observations would be explained by a small number of rules. That is exactly the definition of research. And perhaps, this is one of the more interesting future frontiers of machine learning research. And by the way, earlier we have talked about a fantastic paper on Neural Programmer Interpreters that also aimed to output complete algorithms that can be directly used and understood. The link is available in the description box. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZBWTD2aNb_o",
        "paper_link": "https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf",
        "paper_title": "Model Compression"
    },
    {
        "video_id": "hnT-P3aALVE",
        "video_title": "Hallucinating Images With Deep Learning | Two Minute Papers #74",
        "position_in_playlist": 201,
        "description": "During our journeys in deep learning, we have seen techniques that can summarize photographs in entire sentences that actually make sense. This time, we are going to turn this process around and ask a deep learning system to \"hallucinate\", i.e., generate images according to sentences that we add as an input. The results are nothing short of insane!\n\n_____________________________\n\nThe paper \"Generative Adversarial Text to Image Synthesis\" is available here:\nhttp://arxiv.org/abs/1605.05396\n\nRecommended for you:\nRecurrent Neural Network Writes Sentences About Images - https://www.youtube.com/watch?v=e-WB4lfg30M\nDeep Learning related Two Minute Papers episodes - \nhttps://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by C. P. Ewing - https://flic.kr/p/GDm4Jd\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In an earlier episode, we showcased a technique for summarizing images not in a word, but an entire sentence that actually makes sense. If you were spellbound by those results, you'll be out of your mind when you hear this one: let's turn it around, and ask the neural network to have a sentence as an input, and ask it to generate images according to it. Not fetching already existing images from somewhere, generating new images according to these sentences. Create new images according to sentences. Is this for real? This is an idea, that is completely out of this world. A few years ago, if someone proposed such an idea and hoped that any useful result can come out of this, that person would have immediately been transported to an asylum. An important keyword here is \"zero shot\" recognition. Before we go to the zero part, let's talk about one shot learning. One shot learning means a class of techniques that can learn something from one, or at most a handful of examples. Deep neural networks typically require to see hundreds of thousands of mugs before they can learn the concept of a mug. However, if I show one mug to any of you Fellow Scholars, you will, of course, immediately get the concept of a mug. At this point, it is amazing what these deep neural networks can do, but with the current progress in this area, I am convinced that in a few years, feeding millions of examples to a deep neural network to learn such a simple concept will be considered a crime. Onto zero shot recognition! The zero shot is pretty simple - it means zero training samples. But this sounds preposterous! What it actually means is that we can train our network to recognize birds, tiny things, what the concept of blue is, what a crown is, but then we ask it to show us an image of \"a tiny bird with a blue crown\". Essentially, the neural network learns to combine these concepts together and generate new images leaning on these learned concepts. I think this paper is a wonderful testament as to why Two Minute Papers is such a strident advocate of deep learning and why more people should know about these extraordinary works. About the paper - it is really well written, there are quite a few treats in there for scientists: game theory and minimax optimization, among other things. Cupcakes for my brain. We will definitely talk about these topics in later Two Minute Papers episodes, stay tuned! But for now, you shouldn't only read the paper - you should devour it. And before we go, let's address the elephant in the room: the output images are tiny because this technique is very expensive to compute. Prediction: two papers down the line, it will be done in a matter of seconds, two even more papers down the line, it will do animations in full HD. Until then, I'll sit here stunned by the results, and just frown and wonder. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hnT-P3aALVE",
        "paper_link": "http://arxiv.org/abs/1605.05396",
        "paper_title": "Generative Adversarial Text to Image Synthesis"
    },
    {
        "video_id": "JKYQOAZRZu4",
        "video_title": "Rocking Out With Convolutions | Two Minute Papers #73",
        "position_in_playlist": 202,
        "description": "A lot of university students tend to have a lot of problems understanding convolutions. Today, we're going to talk about both many cool useful applications of convolutions and there will be a bit of intuition on how the computation is done. Among other cool applications, it turns out we can add very convincing reverberation effects to our guitars by computing convolutions.\n\n__________________________\n\nImmersive Math:\nhttp://immersivemath.com/ila/index.html\n\nSeparable Subsurface Scattering:\nhttps://www.youtube.com/watch?v=72_iAlYwl0c\n\nConvolutions and Gaussian blur image source - Wikipedia\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by Dustin Gaffke - https://flic.kr/p/nKy4EK\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. And today we're here to answer one question: what is a convolution? I have heard many university students crying in despair over their perilous journeys of understanding what convolutions are, and why they are useful. Let me give a helping hand here! A convolution is a mathematical technique to mix together two signals. A lot of really useful tasks can be accomplished through this operation. For instance, convolutions can be used to add reverberation to a recorded instrument. So I play my guitar here in my room, and it can sound like as if it were recorded in a large concert hall. Now, dear Fellow Scholars, put on a pair of headphones, and let me rock out on my guitar to show it to you! First, you'll hear the dry guitar signal. And this is the same signal with the added reverberation. It sounds much more convincing, right? In simple words, a convolution is a bit like saying guitar plus concert hall equals a guitar sound that was played in a concert hall. The only difference is that we don't say guitar \"plus\" concert hall, we say guitar \"convolved\" with the concert hall. If we want to be a bit more accurate, we could say the impulse response of the hall, which records how this place reacts to a person who starts to play the guitar in there. People use the living hell out of convolution reverberation plugins in the music industry. Convolutions can also be used to blur or sharpen an image. We also had many examples of convolutional neural networks that provide efficient means to, for instance, get machines to recognize traffic signs. We can also use them to add sophisticated light transport effects, such as subsurface scattering to images. This way, we can conjure up digital characters with stunningly high quality skin and other translucent materials in our animations and computer games. We have had a previous episode on this, and it is available in the video description box, make sure to have a look! As we said before, computing a convolution is not at all like addition. Not even close. For instance, the convolution of two boxes is ... a triangle. Wow, what? What kind of witchcraft is this? It doesn't sound intuitive at all! The computation of the convolution means that we start to push this box over the other one and at every point in time, we take a look at the intersection between the two signals. As you can see, at first, they don't touch at all. Then, as they start to overlap, we have highlighted the intersected area with green, and as they get closer to each other, this area increases. When they are completely overlapped, we get the maximum intersection area, which then starts to dwindle as they separate. So there you have it. It is the miracle of mathematics that by computing things like this, we can rock out in a virtual chuch or a stadium which sounds very close to the real deal. And before we go, a quick shoutout to immersive math, a really intuitive resource for learning linear algebra. If you are into math, you simply have to check this one out, it's really cool. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=JKYQOAZRZu4"
    },
    {
        "video_id": "1PNhuHa7lS0",
        "video_title": "Reinforcement Learning with OpenAI's Gym | Two Minute Papers #72",
        "position_in_playlist": 203,
        "description": "OpenAI's Gym is available here:\nhttps://gym.openai.com/\n\nOpenAI - Non-profit AI company by Elon Musk and Sam Altman\nhttps://www.youtube.com/watch?v=AbcRlDBnwjM\n\nGoogle DeepMind's paper \"Unifying Count-Based Exploration and Intrinsic Motivation\" and video on reniforcement learning and curiosity:\nhttps://arxiv.org/pdf/1606.01868v1.pdf\nhttps://www.youtube.com/watch?v=0yI2wJ6F8r0\n\nLink to the mentioned research project at Experiment:\n1. https://experiment.com/projects/opening-your-mind-s-eye-collaborating-with-a-computer-to-reveal-visual-imagination?s=discover\n2. https://experiment.com/projects/yvgjmnuxsnavvjuhxzwf\n\nWE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nWe also thank Experiment for sponsoring our series. - https://experiment.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image is licensed under CC0 and is available here: https://pixabay.com/en/dumbbell-training-fitness-room-940375/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a technique in the field of machine learning to learn how to navigate in a labyrinth, play a video game, or to teach a digital creature to walk. Usually, we are interested in a series of actions that are in some sense, optimal in a given environment. Despite the fact that many enormous tomes exist to discuss the mathematical details, the intuition behind the algorithm itself is incredibly simple. Choose an action, and if you get rewarded for it, keep doing it. If the rewards are not coming, try something else. The reward can be, for instance, our score in a computer game or how far our digital creature could walk. It is usually quite difficult to learn things where the reward comes long after our action because we don't know when exactly the point was when we did something well. This is one of the reasons why Google DeepMind will try to conquer strategy games in the future, because this is a genre where good plays usually include long-term planning that reinforcement learning techniques don't really excel at. By the way, this just in: they the have just published an excellent paper on including curiosity in this equation in a way that helps long term planning remarkably. As more techniques pop up in this direction, it is getting abundantly clear that we need a framework where they can undergo stringent testing. This means that the amount of collected rewards and scores should be computed the same way, and in the same physical framework. OpenAI is a non-profit company boasting an impressive roster of top-tier researchers who embarked on the quest to develop open and ethical artificial intelligence techniques. We've had a previous episode on this when the company was freshly founded, and as you might have guessed, the link is available in the description box. They have recently published their first major project that goes by the name Gym. Gym is a unified framework that puts reinforcement learning techniques on an equal footing. Anyone can submit their solutions which are run on the same problems, and as a nice bit of gamification, leaderboards are established to see which technique emerges victorious. These environments range from a variety of computer games to different balancing tasks. Some simpler reference solutions are also provided for many of them as a starting point. This place is like Disneyworld for someone who is excited about the the field of reinforcement learning. With more and more techniques, the subfield gets more saturated, it gets more and more difficult to be the first at something. That's a great challenge for researchers. From a consumer point of view, this means that better techniques will pop up day by day. And, as I like to say quite often, we have really exciting times ahead of us. A quick shoutout to Experiment, a startup to help research projects come to fruition by crowdsourcing them. Current experiments include really cool projects like how we could implement better anti-doping policies for professional sports, or to show on a computer screen how our visual imagination works. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1PNhuHa7lS0"
    },
    {
        "video_id": "MfaTOXxA8dM",
        "video_title": "Image Colorization With Deep Learning and Classification | Two Minute Papers #71",
        "position_in_playlist": 204,
        "description": "The paper \"Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification\" and its implementation are available here:\nhttp://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/\nhttps://github.com/satoshiiizuka/siggraph2016_colorization\n\nThe video classification paper by Karpathy et al.:\nhttp://cs.stanford.edu/people/karpathy/deepvideo/\n\nRecommended for you: \nArtistic Style Transfer For Videos - https://www.youtube.com/watch?v=Uxax5EKg0zA\nDeep Learning related Two Minute Papers videos - https://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about adding color to black and white images. There were some previous works that tackled this problem, and many of them worked quite well, but there were cases when the results simply didn't make too much sense. For instance, the algorithm often didn't guess what color the fur of a dog should be. If we would give the same task to a human, we could usually expect better results because the human knows what breed the dog is, and what colors are appropriate for that breed. In short, we know what is actually seen on the image, but the algorithm doesn't - it just trains on black and white and colored image pairs and learns how it is usually done without any concept of what is seen on the image. So here is the idea - let's try to get the neural network not only to colorize the image, but classify what is seen on the image before doing that. If we see a dog in an image, it is not that likely to be pink, is it? If we know that we have to deal with a golf course, we immediately know to reach out for those green crayons. This is a novel fusion-based technique. This means that we have a separate neural network for classifying the images and one for colorizing them. The fusion part is when we unify the information in these neural networks so we can create an output that aggregates all this information. And the results, are just spectacular, the additional information on what these images are about really make a huge impact on the quality of the results. Please note that this is by far not the first work on fusion, I've also linked an earlier paper for recognizing objects in videos, but I think this is a really creative application of the same train of thought that is really worthy of our attention. To delight the fellow tinkerers out there, the source code of the project is also available. The supplementary video reveals that temporal coherence is still a problem. This means that every image is colorized separately with no communication. It is a bit like giving the images to colorize one by one to different people, with no overarching artistic direction. The result we'll get this way is a flickery animation. This problem has been solved for artistic style transfer, which we have discussed in an earlier episode, the link is in the description box. There was one future episode planned about plastic deformations. I have read the paper several times, and it is excellent, but I felt that the quality of my presentation was not up there to put it in front of you Fellow Scholars. It may happen in the future, but I had to shelf this one for now. Please accept my apologies for that. In the next episode, we'll continue with OpenAI's great new invention for reinforcement learning. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MfaTOXxA8dM",
        "paper_link": "http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/",
        "paper_title": "Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification"
    },
    {
        "video_id": "heY2gfXSHBo",
        "video_title": "Schr\u00f6dinger's Smoke | Two Minute Papers #70",
        "position_in_playlist": 205,
        "description": "Today we will talk about Eulerian and Lagrangian smoke and fluid simulations and how this excellent technique can incorporate a variant of Schr\u00f6dinger's equation and make an excellent fluid simulator out of it. :)\n\n_________________\n\nThe paper \"Schr\u00f6dinger's Smoke\" and its implementation is available here:\nhttp://multires.caltech.edu/pubs/SchrodingersSmoke.pdf\nhttp://multires.caltech.edu/pubs/SchrodingersSmokeCode.zip\n\nThe publisher's version is expected to show up here soon:\nhttp://www.multires.caltech.edu/pubs/pubs.htm\n\nThe Short Science website is available here:\nhttp://www.shortscience.org/\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nDavid Jaenisch, Sunil Kim, Julian Josephs.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. There are two main branches of efficient smoke and fluid simulator programs: Eulerian and Lagrangian techniques. Before we dive into what these terms mean, I'd like to note that we have closed captions available for this series that you can turn on by clicking the cc button at the bottom of the player. With that out of the way, the Eulerian technique means that we have a fixed grid, and the measurement happens in the gridpoints only. We have no idea what happens between these gridpoints. It may sound counterintuitive at first because it has no notion of particles at all. With the Lagrangian technique, we have particles that move around in space, and we measure important quantities like velocity and pressure with these particles. In short, Eulerian - grids, Lagrangian - particles. Normally, the problem with Eulerian simulations is that we don't know what exactly happens between the gridpoints, causing information to disappear in these regions. To alleviate this, they are usually combined with Lagrangian techniques, because if we also track all these particles individually, we cannot lose any of them. The drawback is, of course, that we need to simulate millions of particles, which will take at least a few minutes for every frame we wish to compute. By formulating his famous equation, the Austrian physicist Erwin Schr\u00f6dinger won the Nobel prize in 1933. In case you're wondering, yes, this is the guy who forgot to feed his cat. There is two important things you should know about the Schr\u00f6dinger equation: one is that it is used to describe how subatomic particles behave in time, and two, it has absolutely nothing to do with large-scale fluid simulations whatsoever. The point of this work is to reformulate Sch\u00f6dinger's equation in a way that it tracks the density and the velocity of the fluid in time. This way, it can be integrated in a purely grid-based, Eulerian fluid simulator, and we don't need to track all these individual particles one by one, but we can still keep these fine, small-scale details in a way that rivals Lagrangian simulations, but without the huge additional costs. So, the idea is absolutely bonkers, just the thought of doing this sounds so outlandish to me. And it works! Obstacles are also supported by this technique. Many questions still remain, such as how to mix different fluid interfaces together, how to model the forces between them. I do not have the prescience to see the limits of the approach, but I am quite convinced that this direction holds a lot of promise for the future. I cannot wait to play with the code and see some followup works on this! As always, everything is linked in the video description box. The paper is not only absolutely beautifully written, but it is also a really fun paper to read. As I read it, I really loved how a jolt of epiphany ran through me. It is a fantastic feeling when a lightbulb lights up in my mind as I suddenly get to understand something. I think it is the scientist's equivalent of obtaining englightenment. May it happen to you Fellow Scholars often during your journeys! And I get to spend quite a bit of time every day reading fine works like this. It's a good life. I'd like to give a quick shoutout to this really cool website called Short Science, which is a collection of crowdsourced short summaries for scientific papers. Really cool stuff, make sure to have a look! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=heY2gfXSHBo",
        "paper_link": "http://multires.caltech.edu/pubs/SchrodingersSmoke.pdf",
        "paper_title": "Schr\u00f6dinger's Smoke"
    },
    {
        "video_id": "7ymM4cG1zfQ",
        "video_title": "Storytime & Reading Comments | Two Minute Papers",
        "position_in_playlist": 206,
        "description": "We have reached one million views on the channel, and are nearing ten thousand Fellow Scholars as well! In this video, we celebrate this milestone with some storytime and I'll read some of your kind comments from my mailbox. Thanks so much everyone!\n\nMy apologies, the second half of the closed captions are missing for this episode.\n\n________________________\n\nVideo credits:\nSimulating Viscosity and Melting Fluids - https://www.youtube.com/watch?v=KgIrnR2O8KQ\nCapturing Waves of Light With Femto-photography - https://www.youtube.com/watch?v=TRNUTN01SEg\nHow Do Genetic Algorithms Work? - https://www.youtube.com/watch?v=ziMHaGQJuSI \nNarrow Band Liquid Simulations - https://www.youtube.com/watch?v=nfPBT71xYVQ\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by ChristianSchd CC BY-SA 3.0 - https://en.wikipedia.org/wiki/Lecture_hall#/media/File:Hanover_Institute_Inorganic_Chemisty_Lecture_Hall.jpg\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have reached a lot of milestones lately. For instance, we now have over one million views on the channel. In the next few days we're also hoping to hit ten thousand subscribers. To put it in perspective, in 2015 August 20th, we have had a 250 subscriber special episode where my mind was pretty much blown. That was only about 9 months ago. Whoa. For the record, here is an image that supposedly contains ten thousand people. Just imagine that they all have doctoral hats and you immediately have a mental image of ten thousand of our Fellow Scholars. It's insanity. And I must say that I am completely blown away by your loyal support. Thanks so much for everyone for the many kind messages, comments and e-mails, of which I'll read quite a few in a second. It just boggles the mind to see that so many people are interested in learning more about awesome new research inventions, and I hope that it is as addictive to you as it was for me when I first got to see some of these results. A huge thank you also to our generous supporters on Patreon, I find it really amazing that we have quite a few Fellow Scholars out there who love the series so much that they are willing to financially help our cause. Just think about it. Especially given that fact that it's not easy to make ends meet these days, I know this all too well being a full time researcher, doing Two Minute Papers and having a baby is extremely taxing, but I love it. I really do. And I know there are people who have it way worse, and here we have these Fellow Scholars who believe in this cause and are willing to help out. Thanks so much for each one of you, I am honored to have loyal supporters like you. We are more than halfway there towards our first milestone on Patreon, which means that our hardware and software costs can be completely covered with your help. As you know, we have really cool perks for our Patrons. One of these perks is that, for instance, our Professors can decide the order of the next episodes, and I was really surprised to see that this episode won by a record number of points. Looks like you Fellow Scholars are really yearning for some story time, so let's bring it on! The focus of Two Minute Papers has always been what a work is about and why it is important. It has always been more about intuition than specific details. That is the most important reason why Two Minute Papers exists. But this sounds a bit cryptic, so let me explain myself. Most materials on any scientific topic on YouTube are about details. When this big deep learning rage started, and it was quite fresh, I wanted to find out what deep learning was. I haven't found a single video that was not at least 30-90 minutes long, and all of them talked about partial derivatives and the chain rule, but never explained what we're exactly doing and why we are doing it. It took me four days to get a good grasp, a good high-level intuition of what is going on. I would have loved to have a resource that explains this four days worth of research work in just two minutes in a way that anyone can understand. Anyone. I haven't found anything, and this was just one example of many. Gandhi said \"Be the change that you wish to see in the world\", and this is how Two Minute Papers was born. It has never been about the details, it has always been about intuition",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7ymM4cG1zfQ"
    },
    {
        "video_id": "-rf_MDh-FiE",
        "video_title": "Surface-Only Liquids | Two Minute Papers #69",
        "position_in_playlist": 207,
        "description": "The paper \"Surface-Only Liquids\" is available here:\nhttp://www.cs.columbia.edu/cg/surfaceliquids/\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by J. Frog - https://flic.kr/p/9Ruz12\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Most of the techniques we've seen in previous fluid papers run the simulation inside the entire volume of the fluids. These traditional techniques scale poorly with the size of our simulation. But wait, as we haven't talked about scaling before, what does this scaling thing really mean? Favorable scaling means that if we have a bigger simulation, we don't have to wait longer for it. Our scaling is fairly normal if we have a simulation twice as big and we need to wait about twice as much. Poor scaling can give us extraordinarily bad deals, such as waiting ten or more times as much for a simulation that is only twice as big. Fortunately, a new class of algorithms is slowly emerging that try to focus more resources on computing what happens near the surface of the liquid, and try to get away with as little as possible inside of the volume. This piece of work shows that most of the time, we can get away with not doing computations inside the volume of the fluid, but only on the surface. This surface-only technique scales extremely well compared to traditional techniques that simulate the entire volume. If a piece of fluid were an apple, we'd only have to eat the peel, and not the whole apple. It's a lot less chewing, right? As a result, the chewing, or the computation, if you will, typically takes seconds per image instead of minutes. A previous technique on narrow band fluid simulations computed the important physical properties near the surface, but in this case, we compute not near the surface, but only on the surface. The difference sounds subtle, but it makes a completely different mathematical background. To make such a technique work, we have to make simplifications to the problem. For instance, one of the simplifications is to make the fluids incompressible. This means that the density of the fluid is not allowed to change. The resulting technique supports simulating a variety of cases such as dripping water, droplet and crown splashes. fluid chains and sheet flapping. I was spellbound by the mathematics written in the paper that is both crystal clear and beautiful in its flamboyancy. This one is such a spectacular paper. It is so good, I had it on my tablet and couldn't wait to get on the train so I could finally read it. The main limitation of the technique is that it is not that useful if we have a large surface to volume ratio, simply because the peel is still a large amount compared to the volume of our apple. We need it the other way around for this technique to be useful, which is true in many cases. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-rf_MDh-FiE",
        "paper_link": "http://www.cs.columbia.edu/cg/surfaceliquids/",
        "paper_title": "Surface-Only Liquids"
    },
    {
        "video_id": "Uxax5EKg0zA",
        "video_title": "Artistic Style Transfer For Videos | Two Minute Papers #68",
        "position_in_playlist": 208,
        "description": "Artificial neural networks were inspired by the human brain and simulate how neurons behave when they are shown a sensory input (e.g., images, sounds, etc). They are known to be excellent tools for image recognition, any many other problems beyond that - they also excel at weather predictions, breast cancer cell mitosis detection, brain image segmentation and toxicity prediction among many others. Deep learning means that we use an artificial neural network with multiple layers, making it even more powerful for more difficult tasks. \n\nThis time they have been shown to be apt at reproducing the artistic style of many famous painters, such as Vincent Van Gogh and Pablo Picasso among many others. All the user needs to do is provide an input photograph and a target image from which the artistic style will be learned.\n\nAnd now, onto the next frontier: transferring artistic style to videos!\n\n_________\n\nThe paper \"Artistic style transfer for videos\" is available here:\nhttp://arxiv.org/abs/1604.08610\n\nThe implementation of this technique is also available:\nhttps://github.com/manuelruder/artistic-videos\n\nRecommended for you:\nDeep Neural Network Learns Van Gogh's Art - https://www.youtube.com/watch?v=-R9bJGNHltQ\nDeep Learning Program Learns to Paint - https://www.youtube.com/watch?v=UGAzi1QBVEg\nFrom Doodles To Paintings With Deep Learning   - https://www.youtube.com/watch?v=jMZqxfTls-0\n\nSintel Movie copyright: Blender Foundation\nhttps://durian.blender.org/sharing/\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was taken from the corresponding paper.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We have previously talked about a technique that used a deep neural network to transfer the artistic style of a painting to an arbitrary image, for instance, to a photograph. As always, if you're not familiar with some of these terms, we have discussed them in previous episodes, and links are available in the description box, make sure to check them out! Style transfer is possible on still images. As there is currently no technique to apply this to videos, it is hopefully abundantly clear that a lot of potential still lies dormant inside. But can we apply this artistic style transfer to videos? Would it work if we would simply try? For an experienced researcher, it is flagrantly obvious that it's an understatement to say that that it wouldn't work. It would fail in a spectacular manner, as you can see here. But with this technique, it apparently works quite well. To be frank, the results look gorgeous. So how does it work? Now, don't be afraid, you'll be presented with a concise, but deliberately obscure statement: This technique preserves temporal coherence when applying the artistic style by incorporating the optical flow of the input video. Now, the only question is what temporal coherence and optical flow means. Temporal coherence is a term that was used by physicists to describe, for instance, how the behavior of a wave of light changes, or stays the same if we observe it at different times. In computer graphics, it is also an important term because oftentimes, we have techniques that we can apply to one image, but not necessarily to a video, because the behavior of the technique changes drastically from frame to frame, introducing a disturbing flickering effect that you can see in this video here. We have the same if we do the artistic style transfer, because there is no communication between the individual images of the video. The technique has no idea that most of the time we're looking at the same things, and if so, the artistic style would have to be applied the same way over and over to these regions. We are clearly lacking temporal coherence. Now, onto optical flows. Imagine a flying drone that takes a series of photographs while hovering and looking around above us. To write sophisticated navigation algorithms, the drone would have to know which object is which across many of these photographs. If we have slightly turned, most of what we see is the same, and only a small part of the new image is new information. But the computer doesn't know that, as all it sees is a bunch of pixels. Optical flow algorithms help us achieving this by describing the possible motions that give us photograph B from photograph A. In this application, what this means is that there is some inter-frame communication, the algorithm will know that if I colored this person this way a moment ago, I cannot drastically change the style of that region on a whim. It is now easy to see why naively applying such techniques to many individual frames would be a flippant attempt to create beautiful, smooth looking videos. So now, it hopefully makes a bit more sense: This technique preserves temporal coherence when applying the artistic style by incorporating the optical flow of the input video. Such great progress in so little time. Loving it. Last time I've mentioned Kram from the comments section, and this time, I'd like to commend Relatedgiraffe for his insightful comments. Thanks for being around and I've definitely learned from you Fellow Scholars! I am really loving the respectful and quality discussions that take place in the comments section, and it is really cool that we can both learn from each other. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Uxax5EKg0zA",
        "paper_link": "http://arxiv.org/abs/1604.08610",
        "paper_title": "Artistic style transfer for videos"
    },
    {
        "video_id": "wBrwN4dS-DA",
        "video_title": "Deep Reinforcement Terrain Learning | Two Minute Papers #67",
        "position_in_playlist": 209,
        "description": "In this piece of work, a combination of deep learning and reinforcement learning is presented which has proven to be useful in solving many extremely difficult tasks. Google DeepMind built a system that can play Atari games at a superhuman level using this technique that is also referred to as Deep Q-Learning. This time, it was used to teach digital creatures to walk and overcome challenging terrain arrangements.\n\n__________________________\n\nThe paper \"Terrain-Adaptive Locomotion Skills\nUsing Deep Reinforcement Learning \" is available here:\nhttp://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html\n\nThe implementation of the paper is also available here:\nhttps://github.com/xbpeng/DeepTerrainRL\n\nOpenAI's Gym project:\nhttps://gym.openai.com/\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by Fulvio Spada - https://flic.kr/p/o7z8o1\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a followup work to a technique we have talked about earlier. We have seen how different creatures learned to walk, and their movement patterns happened to be robust to slight variations in the terrain. In this work, we imagine these creatures as a collection of joints and links, typically around 20 links. Depending on what actions we choose for these individual body parts in time, we can construct movements such as walking, or leaping forward. However, this time, these creatures not only learn to walk, but they also monitor their surroundings and are also taught to cope with immense difficulties that arise from larger terrain differences. This means that they learn both on character features, like where the center of mass is and what the velocity of different body parts are, and terrain features, such as, what the displacement of the slope we're walking up on is or if there's a wall ahead of us. The used machinery to achieve this is deep reinforcement learning. It is therefore a combination of a deep neural network and a reinforcement learning algorithm. The neural network learns the correspondence between these states and output actions, and the reinforcement learner tries to guess which action will lead to a positive reward, which is typically measured as our progress on how far we got through the level. In this footage we can witness how a simple learning algorithm built from these two puzzle pieces can teach these creatures to modify their center of mass and adapt their movement to overcome more sophisticated obstacles, and, other kinds of advertisites. And please note that the technique still supports a variety of different creature setups. One important limitation of this technique is that it is restricted to 2D. This means that the characters can walk around not in a 3D world, but on a plane. A question whether we're shackled by the 2D-ness of the technique or if the results can be applied to 3D remains to be seen. I'd like to note that candidly discussing limitations is immensely important in research, and the most important thing is often not we can do at this moment, but the long-term potential of the technique, which, I think this work has in abundance. It's very clear that in this research area, enormous leaps are made year by year, and there's lots to be excited about. As more papers are published on this locomotion problem, the authors also discuss that it would be great to have a unified physics system and some error metrics so that we can measure these techniques against each other on equal footings. I feel that such a work would provide fertile grounds for more exploration in this area, and if I see more papers akin to this one, I'll be a happy man. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wBrwN4dS-DA",
        "paper_link": "http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html",
        "paper_title": "Terrain-Adaptive Locomotion Skills\nUsing Deep Reinforcement Learning "
    },
    {
        "video_id": "72_iAlYwl0c",
        "video_title": "Separable Subsurface Scattering | Two Minute Papers #66",
        "position_in_playlist": 210,
        "description": "Separable Subsurface Scattering is a novel technique to add real-time subsurface light transport calculations for computer games and other real-time applications.\n\n____________________________\n\nThe paper \"Separable Subsurface Scattering\" and its implementation is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\nhttp://www.iryoku.com/separable-sss/\n\nRecommended for you:\nRay Tracing / Subsurface Scattering @ Function 2015 - https://www.youtube.com/watch?v=qyDUvatu5M8\nSeparable Subsurface Scattering Unofficial Talk - https://www.youtube.com/watch?v=mU-5CsaPfsE\n\nSeparable Subsurface Scattering Implementation in Blender (thank you Lubos Lenco!):\nhttp://www.blendernation.com/2016/05/02/separable-subsurface-scattering-game-engine-cycles/\nhttp://luboslenco.com/notes/ssss/\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nImage credits:\nLeaves - https://flic.kr/p/fGie2L\nSnail - https://flic.kr/p/8wXFiC\nSkin: Wikipedia\n\nExtended credits (copied from the Acknowledgements section of the mentioned paper):\nThe authors want to thank the reviewers for their insightful comments; Infinity Realities, in particular Lee Perry-Smith, for his head model and for the Lauren model; the Institute of Creative Technologies at USC, in particular Paul Debevec, for the Ari and Bernardo models; and Bernardo Antoniazzi for letting us use his likeness. Furthermore, we want to thank the Stanford University Computer Graphics Laboratory for the Dragon model, and the following contributors from Blend Swap under CC-BY licence: longrender for the Dish model, metalix for the Green apple model, betomo16 for the Plant model, and PickleJones for the Grapes model. We also thank Fel\u00edcia Feh\u00e9r for editing the figures. This research has been partially funded by the European Commission, 7th Framework Programme, through projects GOLEM and VERVE, the Spanish Ministry of Economy and Competitiveness through project LIGHTSLICE, and project TAMA, and the Austrian Science Fund (FWF) through project no. P23700-N23.\n\nThe thumbnail background image was taken from the corresponding paper linked above.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Subsurface scattering means that a portion of incoming light penetrates the surface of a material. Our skin is a little known, but nonetheless great example of that, but so are plant leaves, marble, milk, or snails, to have a wackier example. Subsurface scattering looks unbelievably beautiful, but at the same time, it is very expensive to compute because we have to simulate up to thousands and thousands of light scattering events for every ray of light. And we have to do this for millions of rays. It really takes forever. The lack of subsurface scattering is the reason why we've seen so many lifeless, rubber-looking human characters in video games and animated movies for decades now. This technique is a collaboration between the Activision Blizzard game development company, the University of Zaragoza in Spain, and the Technical University of Vienna in Austria. And, it can simulate this kind of subsurface light transport in half a millisecond per image. Let's stop for a minute and think about this. Earlier, we talked about subsurface scattering techniques that were really awesome, but still took at least let's say four hours on a scene before they became useful. This one is half a millisecond per image. Almost nothing. In one second, it can do this calculation two thousand times. Now, this has to be a completely different approach than just simulating many millions of rays of light, right? We can't take a four hour long algorithm, do some magic and get something like this. The first key thought is that we can set up some cool experiment where we play around with light sources and big blocks of translucent materials, and record how light bounces off of these materials. Cool thing number one: we only need to do it once per material. Number two: the results can be stored in an image. This is what we call a diffusion profile and this is how it looks like. So we have an image of the diffusion profile, and one image of the material that we would like to add subsurface scattering to. This is a convolution-based technique, which means that it enables us not to add these two images together, but to mix them together in a way that the optical properties of the diffusion profiles are carried to the image. If we add the optical properties of an apple to a human face, it will look more like a face that has been carved out of a giant apple. A less asinine application is, of course, if we mix it with the appropriate skin profile image, then we'll get photorealistic looking faces, as it is demonstrated quite aptly by this animation. This apple to skin example, by the way, you can actually try for yourself, as the source code and an executable demo is also freely available for everyone to experiment with. Convolutions have so many cool applications, I don't even know where to start. In fact, I think we should have an episode solely on that. Can't wait, it's going to be a lot of fun! These convolution computations are great, but they are still too expensive for real-time video games. What this work gives us, is a set of techniques that are able to compute this convolution not on these original images, but much smaller, tiny-tiny strips which are much cheaper, but the result of the computations look barely distinguishable. Another cool thing is that the quality of the results is not only scientifically provable, but this technique also opens up the possibility of artistic manipulation. It is done in a way that we can start out with a physically plausible result and tailor it to our liking. You can see some exaggerated examples of that. The entire technique is so simple, a computer program that executes it can fit on your business card. It also seems to have appeared in Blender recently. Also, a big hello and a shoutout for the awesome people at Intel who recently invited my humble self to chat a bit about this technique. If you would like to hear more about the details on how this algorithm works, I have put some videos in the description box. The most important take home message from this project, at least for me, is that it is possible to conduct academic research projects together with companies, and create results that can make it to multi-million dollar computer games, but also having proven results that are useful for the scientific community. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=72_iAlYwl0c",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/",
        "paper_title": "Separable Subsurface Scattering"
    },
    {
        "video_id": "SC0D7aJOySY",
        "video_title": "Real-Time Shading With Area Light Sources | Two Minute Papers #65",
        "position_in_playlist": 211,
        "description": "The paper \"Real-Time Polygonal-Light Shading with Linearly Transformed Cosines\" is available here:\nhttps://eheitzresearch.wordpress.com/415-2/\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background was taken from the paper mentioned above.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In computer graphics, we use the term shading to describe the process of calculating the appearance of a material. This gives the heart and soul of most graphical systems that visualize something on our screen. Let the blue sphere be the object to be shaded and the red patch be the light source illuminating it. The question is, in this configuration, how should the blue sphere look in reality? In order to obtain high-quality images, we need to calculate how much of the red patch is visible from the blue sphere. This describes the object's relation to the light source. It it close or is it nearby? Is it facing the object or not? What shape is the light source? These factors determine how much light will arrive to the surface of the blue sphere. This is what mathematicians like to call an integration problem. However, beyond this calculation we also have to take into consideration the reflectance of the material that the blue sphere is made of. Whether we have a white wall surface or an orange makes a great deal of difference and throws a wrench in our already complex calculations. The final shading is the product of this visibility situation and the material properties of the sphere. Needless to say that the mathematical description of many materials can get extremely complex, which makes our calculations really time consuming. In this piece of work, a technique is proposed that can approximate these two factors in real time. The paper contains a very detailed demonstration of the difference between this and the analytical computations that give us the perfect results but take extremely long. In short, this technique is very closely matching the analytic results, but it is doing it in real time. I really don't know what to say. We're used to wait for hours to obtain images like this, and now, 15 milliseconds per frame. What a hefty value proposition for a paper. Absolutely spectacular. Some of the results really remind me of topological calculations. Topology is a subfield of mathematics that studies what properties of different shapes are preserved when these shapes are undergoing deformations. It's super useful because, for instance, if we can prove that light behaves in some way when the light source has the shape of a disk, then if we're interested in other shapes, topology can help us determine whether all these enormous books full of theorems on other shapes are going to apply to this shape or not. It may be that we don't need to invent anything and can just use this vast existing knowledge base. Some of the authors of this paper work at Unity, which means that we can expect these awesome results to appear in the video games of the future. Some code and demos are also available on their website which I've linked in the description box, make sure to check them out! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SC0D7aJOySY",
        "paper_link": "https://eheitzresearch.wordpress.com/415-2/",
        "paper_title": "Real-Time Polygonal-Light Shading with Linearly Transformed Cosines"
    },
    {
        "video_id": "5PSWr2ovBvU",
        "video_title": "Deep Learning and Cancer Research | Two Minute Papers #64",
        "position_in_playlist": 212,
        "description": "A few quite exciting applications of deep learning in cancer research have appeared recently. This new algorithm can recognize cancer cells by looking at blood samples without introducing any intrusive chemicals in the process. Amazing results ahead. :)\n\n_________________________\n\nThe paper \"Deep Learning in Label-free Cell Classification\" is available here:\nhttp://www.nature.com/articles/srep21471\n\nThe link from Healthline:\nhttp://www.healthline.com/health/cancer/ovarian-cancer-facts-statistics-infographic#10\n\nRecommended for you:\nTwo+ Minute Papers - Overfitting and Regularization For Deep Learning - https://www.youtube.com/watch?v=6aF9sJrzxaM\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by zhouxuan12345678 (CC BY-SA 2.0). Some blood cells were removed. - https://flic.kr/p/9ATvC1\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Let's try to assess the workflow of this piece of work in the shortest possible form. The input is images of cells, and the output of the algorithm is a decision that tells us which one of these are cancer cells. As the pipeline of the entire experiment is quite elaborate, we'll confine ourselves to discuss the deep learning-related step at the very end. Techniques prior to this one involved adding chemicals to blood samples. The problem is that these techniques were not so reliable, and that they also destroyed the cells, so it was not possible to check the samples later. As the title of the paper says, it is a label-free technique, therefore it can recognize cancer cells without any intrusive changes to the samples. The analysis happens by simply looking at them. To even have a chance at saying anything about these cells, domain experts have designed a number of features that help us making an educated decision. For instance, they like to look at refractive indices, that tell us how much light slows down when passing through cells. Light absorption and scattering properties are also recognized by the algorithm. Morphological features are also quite important as they describe the shape of the cells and they are among the most useful features for the detection procedure. So, the input is an image, then come the high level features, and the neural networks help locating the cancer cells by learning the relation of exactly what values for these high-level features lead to cancer cells. The proposed technique is significantly more accurate and consistent in the detection than previous techniques. It is of utmost importance that we are able to do something like this on a mass scale because the probability of curing cancer depends greatly on which phase we can identify it. One of the most important factors is early detection and this is exactly how deep learning can aid us. To demonstrate how important early detection is, have a look at this chart of the ovarian cancer survival rates as a function of how early the detection takes place. I think the numbers speak for themselves, but let's bluntly state the obvious: it goes from almost surely surviving to almost surely dying. By the way, they were using L2 regularization to prevent overfitting in the network. We have talked about what each of these terms mean in a previous episode, I've put a link for that in the description box. 95% success rate with the throughput of millions of cells per second. Wow, bravo. A real, Two Minute Papers style hat tip to the authors of the paper. It is really amazing to see different people from so many areas working together to defeat this terrible disease. Engineers create instruments to be able to analyze blood samples, doctors choose the most important features, and computer scientists try to find out the relation between the features and illnesses. Great strides have been made in the last few years, and I am super happy to see that even if you're not a doctor and you haven't studied medicine, you can still help in this process. That's quite amazing. A big shoutout to Kram who has been watching Two Minute Papers since the very first episodes and his presence has always been ample with insightful comments. Thanks for being around! And also, thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=5PSWr2ovBvU",
        "paper_link": "http://www.nature.com/articles/srep21471",
        "paper_title": "Deep Learning in Label-free Cell Classification"
    },
    {
        "video_id": "_S1lyQbbJM4",
        "video_title": "Face2Face: Real-Time Facial Reenactment",
        "position_in_playlist": 213,
        "description": "In computer animation, animating human faces is an art itself, but transferring expressions from one human to someone else is an even more complex task. One has to take into consideration the geometry, the reflectance properties, pose, and the illumination of both faces, and make sure that mouth movements and wrinkles are transferred properly. The fact that the human eye is very keen on catching artificial changes makes the problem even more difficult. This paper describes a real-time solution to this animation problem.\n\n______________________\n\nThe paper \"Face2Face: Real-time Face Capture and Reenactment of RGB Videos\" is available here:\nhttp://www.graphics.stanford.edu/~niessner/thies2016face.html\n\nRecommended for you:\nReal-Time Facial Expression Transfer (previous work) - https://www.youtube.com/watch?v=mkI6qfpEJmI\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe background of the thumbnail image was created by DonkeyHotey (CC BY 2.0) - https://flic.kr/p/aPiKLe\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\n\n#Deepfake #Face2Face",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. There was a previous episode on a technique where the inputs were a source video of ourselves, and a target actor. And the output was a video of this target actor with our facial gestures. With such an algorithm, one can edit pre-recorded videos in real time, and the current version only needs a consumer webcam to do that. This new version addresses two major shortcomings: One: the previous work relied on depth information, which means that we needed to know how far different parts of the image were from the camera. This newer version only relies on color information and does not need anything beyond that. Whoa! Two: Previous techniques often resorted to copying the footage from the mouth and adding synthetic proxies for teeth. Not anymore with this one! I tip my hat to the authors, who came up with a vastly improved version of their previous method so quickly, and it is probably needless to say that the ramifications of such an existing technique are far reaching, and are hopefully pointed in a positive direction. However, we should bear in mind that from now on, we may be one step closer to an era where a video of something happening won't be taken as proper evidence. I wonder how this will affect legal decision-making in the future. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_S1lyQbbJM4",
        "paper_link": "http://www.graphics.stanford.edu/~niessner/thies2016face.html",
        "paper_title": "Face2Face: Real-time Face Capture and Reenactment of RGB Videos"
    },
    {
        "video_id": "LhhEv1dMpKE",
        "video_title": "Training Deep Neural Networks With Dropout | Two Minute Papers #62",
        "position_in_playlist": 214,
        "description": "In this episode, we discuss the bane of many machine learning algorithms - overfitting. It is also explained why it is an undesirable way to learn and how to combat it via dropout.\n\n_____________________\n\nThe paper \"Dropout: A Simple Way to Prevent Neural Networks from\nOvertting\" is available here:\nhttps://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n\nAndrej Karpathy's autoencoder is available here:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html\n\nRecommended for you:\nOverfitting and Regularization For Deep Learning - https://www.youtube.com/watch?v=6aF9sJrzxaM\nDecision Trees and Boosting, XGBoost -https://www.youtube.com/watch?v=0Xc9LIb_HTw\nA full playlist with machine learning and deep learning-related Two Minute Papers videos - https://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by Norma (CC BY 2.0) - https://flic.kr/p/ejXPXt\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A quick recap for the Fellow Scholars out there who missed some of our earlier episodes. A neural network is a machine learning technique that was inspired by the human brain. It is not a brain simulation by any stretch of the imagination, but it was inspired by the inner workings of the human brain. We can train it on input and output pairs like images, and descriptions, whether the images depict a mug or a bus. The goal is that after training, we would give unknown images to the network and expect it to recognize whether there is a mug or a bus on them. It may happen that during training, it seems that the neural network is doing quite well, but when we provide the unknown images, it falters and almost never gets the answer right. This is the problem of overfitting, and intuitively, it is a bit like students who are not preparing for an exam by obtaining useful knowledge, but students who prepare by memorizing answers from the textbook instead. No wonder their results will be rubbish on a real exam! But no worries, because we have dropout, which is a spectacular way of creating diligent students. This is a technique where we create a network where each of the neurons have a chance to be activated or disabled. A network that is filled with unrealiable units. And I really want you to think about this. If we could have a system with perfectly reliable units, we should probably never go for one that is built from less reliable units instead. What is even more, this piece of work proposes that we should cripple our systems, and seemingly make them worse on purpose. This sounds like a travesty. Why would anyone want to try anything like this? And what is really amazing is that these unreliable units can potentially build a much more useful system that is less prone to overfitting. If we want to win competitions, we have to train many models and average them, as we have seen with the Netflix prize winning algorithm in an earlier episode. It also relates back to the committee of doctors example that is usually more useful than just asking one doctor. And the absolutely amazing thing is that this is exactly what dropout gives us. It gives the average of a very large number of possible neural networks, and we only have to train one network that we cripple here and there to obtain that. This procedure, without dropout, would normally take years and such exorbitant timeframes to compute, and would also raise all kinds of pesky problems we really don't want to deal with. To engage in modesty, let's say that if we are struggling with overfitting, we could do a lot worse than using dropout. It indeed teaches slacking students how to do their homework properly. Please keep in mind using dropout also leads to longer training times, my experience has been between 2 to 10x, but of course, it heavily depends on other external factors. So it is indeed true that dropout is slow compared to training one network, but it is blazing fast at what it actually approximates, which is training an exponential number of models. I think dropout is one of the greatest examples of the beauty and the perils of research, where sometimes the most counterintuitive ideas give us the best solutions. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=LhhEv1dMpKE",
        "paper_link": "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf",
        "paper_title": "Dropout: A Simple Way to Prevent Neural Networks from\nOvertting"
    },
    {
        "video_id": "nfPBT71xYVQ",
        "video_title": "Narrow Band Liquid Simulations | Two Minute Papers #61",
        "position_in_playlist": 215,
        "description": "We continue our journey in the land of fluid simulations and discuss a really cool FLIP-based technique that uses both particles and grids to create very high quality footage at a much more reasonable cost than previous works.\n\n____________________\n\nThe paper \"Narrow Band FLIP for Liquid Simulations\" is available here:\nhttps://wwwcg.in.tum.de/research/research/publications/2016/narrow-band-flip-for-liquid-simulations.html\n\nYearning for more fluids? :) A Two Minute Papers playlist of fluid and cloth simulation-related topics is available here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnnd16wIjedAcvfQcLw0IJI\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was taken from the corresponding paper.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Our endeavors in creating amazingly detailed fluid simulations is often hamstrung by the fact that we need to simulate the motion of tens of millions of particles. Needless to say this means excruciatingly long computation times and large memory consumption. This piece of work tries to alleviate the problem by confining the usage of particles to a narrow band close to the liquid surface and thus, decimating the number of particles used in the simulation. The rest of the simulation is computed on a very coarse grid, where we compute quantities of the fluid like velocity and pressure in gridpoints, and instead of computing them everywhere, we try to guess what is happening between these gridpoints. The drawback of this is that we may miss a lot of details because of that. And the brilliant part of this new technique is that we only use a cheap, sparse grid where there is not a lot of things happening, and use the expensive particles only near the surface, where there are a lot of details we can capture well. The FLIP term that you see in the video means Fluid Implicit Particle, a popular way of simulating fluids that uses both grids and particles. In this scene, the old method uses 24 million particles, while the new technique uses only one million, and creates closely matching results. You can see a lot of excess particles in the footage with the classical simulation technique, and the foamish-looking version is the proposed new, more efficient algorithm. Creating such a technique is anything but trivial. Unless special measures are taken, the simulation may have robustness issues, which means that there are situations where it does not produce a sensible result. This is demonstrated in a few examples where with the naive version of the technique, a piece of fluid never ever comes to rest, or it may exhibit behaviors that are clearly unstable. It also takes approximately half as much time to run the simulation, and uses half as much memory, which is such a huge relief for visual  effects artists. I don't know about you Fellow Scholars, but I see a flood of amazing fluid papers coming in the near future and I'm having quite a bit of trouble containing my excitement. Exciting times are ahead indeed. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=nfPBT71xYVQ",
        "paper_link": "https://wwwcg.in.tum.de/research/research/publications/2016/narrow-band-flip-for-liquid-simulations.html",
        "paper_title": "Narrow Band FLIP for Liquid Simulations"
    },
    {
        "video_id": "OV3Xcv42JSw",
        "video_title": "No Such Thing As Artificial Intelligence | Two Minute Papers #60",
        "position_in_playlist": 216,
        "description": "What is happening with the neural networks in this video? You'll find the answers in these videos:\n1. How Does Deep Learning Work? - https://www.youtube.com/watch?v=He4t7Zekob0&index=5&list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n2. Overfitting and Regularization For Deep Learning - https://www.youtube.com/watch?v=6aF9sJrzxaM&index=18&list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nIn this episode, we discuss the perils of debating whether different existing techniques can be deemed artificial intelligence or not.\n\n____________________\n\nAnd here is a full playlist of our videos related to machine learning and deep learning:\nhttps://www.youtube.com/watch?v=V1eYniJ0Rnk&list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nTensorflow playground:\nhttp://playground.tensorflow.org/\n\nThe A* algorithm: https://en.wikipedia.org/wiki/A*_search_algorithm\nA neat demo application that runs in your browser: http://www.briangrinstead.com/blog/astar-search-algorithm-in-javascript\n\nLink to the experiment designed by arthomas:\nhttps://www.reddit.com/r/MachineLearning/comments/4eila2/tensorflow_playground/d20noqu\n\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by jjmusgrove (CC BY 2.0) - https://flic.kr/p/ewPWSk\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Whether a technique can be deemed as artificial intelligence or not, is a question that I would like to see exiled from future debates and argumentations. Of course, anyone may take part in any debate of their liking, I would, however, like to point out the futility of such endeavors. Let me explain why. Ever heard a parent and a son having an argument whether the son is an adult or not? \"You are not an adult, because adults don't behave like this!\" And arguments like that. The argument is not really about whether a person is an adult, but it is about the very definition of an adult. Do we define an adult as someone who has common sense and behaves responsibly? Or is it enough to be of 18 or 21 years old to be an adult? If we decide which definition we go for, the scaffolding for the entire argument crumbles, because it is built upon a term for which the definition is not agreed upon. I feel that we have it the same with artificial intelligence in many debates. The definition of artificial intelligence, or at least one possible definition, is the following: Artificial intelligence (AI) is the intelligence exhibited by machines or software. It is a bit of a copout, so we have to go and check the definition of intelligence. There are multiple definitions, but for the sake of argument, we are going to let this one slip. One possible definition for intelligence is \"the ability to learn or understand things or to deal with new or difficult situations\". Now, this sentence is teeming with ill-defined terms, such as learn, understand things, deal with new situations, difficult situations. So, if we have a shaky definition of artificial intelligence, it is quite possibly pointless to argue whether self driving cars can be deemed artificially intelligent or not. Imagine two physicists arguing whether a material is ferromagnetic, but none of them has the slightest idea what magnetism means. If we look at it like this, it is very easy to see the futility of such arguments. If we had as poorly crafted definitions in physics as we have for intelligence, magnetism would be defined as \"stuff pulling on other stuff\". This is the first part of the argument. The second part is that artificial intelligence is imagined to be a mystical thing that only exists in the future, or it may exist in the present, but it has to be shrouded in mystery. Let me give you an example. The A* algorithm used to be called AI and was (and still is) widely taught in AI courses at many universities. A* is used in many pathfinding situations where we seek to go from A to B on a map in the presence of possible obstacles. It is widely used in robotics and computer games. Nowadays, calling a pathfinding algorithm AI is simply preposterous. It is a simple, well-understood technique that does something we are used to. Imagine someone waving their GPS device claiming that there is AI in there. But back then, when it was new, hazy, and poorly understood, we put it in a drawer with the label \"AI\" on it. As soon as people start to understand it, they pull it out from this drawer, and disgustedly claim, \"Well, this is not AI, it's just a graph algorithm. Graphs are not AI, that's just mathematics.\" It is important to note that none of the techniques that we see today are mysterious in any sense, the entirety of deep learning and everything else is a series of carefully prescribed mathematical operations. I will try to briefly assess the two arguments: - Arguments about AI are not about the algorithms they seem to be discussing, but about the very definition of AI, which is ill-defined at best. - AI is imagined to be a mystical thing that only exists in the future, or it may exist in the present, but it has to be, in some way, shrouded in mystery. The good news is that using this knowledge, we can easily defuse such futile arguments. If someone says that deep learning is not artificial intelligence because all it does is matrix algebra, we can ask: \"okay, what is your definition of artificial intelligence?\" If this person defines AI as being a sentient learning being akin to humans, then we have immediately arrived to a conclusion that deep learning is not AI. Let us not fool ourselves by thinking that we are arguing about things when we are simply arguing about definitions. As soon as the definition is agreed upon, the conclusion emerges effortlessly. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=OV3Xcv42JSw"
    },
    {
        "video_id": "aKSILzbAqJs",
        "video_title": "10 Even Cooler Deep Learning Applications | Two Minute Papers #59",
        "position_in_playlist": 217,
        "description": "For the third time, we present another round of incredible deep learning applications!\n___________________\n\n1. Geolocation - http://arxiv.org/abs/1602.05314\n2. Super-resolution - http://arxiv.org/pdf/1511.04491v1.pdf\n3. Neural Network visualizer - http://experiments.mostafa.io/public/ffbpann/\n4. Recurrent neural network for sentence completion: http://www.cs.toronto.edu/~ilya/fourth.cgi\n5. Human-in-the-loop and Doctor-in-the-loop: http://link.springer.com/article/10.1007/s40708-016-0036-4\n6. Emoji suggestions for images - https://emojini.curalate.com/\n7. MNIST handwritten numbers in HD - http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/\n8. Deep Learning solution to the Netflix prize -https://karthkk.wordpress.com/2016/03/22/deep-learning-solution-for-netflix-prize/\n9. Curating works of art - http://cs231n.stanford.edu/reports2016/210_Report.pdf\n10. More robust neural networks against adversarial examples - http://cs231n.stanford.edu/reports2016/103_Report.pdf\n\nThe Keras library:\nhttp://keras.io/\nhttps://github.com/fchollet/keras\n\nRecommended for you:\nTwo Minute Papers Machine Learning Playlist - https://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by Steven S. (CC BY 2.0) - https://flic.kr/p/sdUQ7\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is the third episode in our series of Deep Learning applications. I have mixed in some recurrent neural networks for your, and honestly, my own enjoyment. I think this series of applications shows what an amazingly versatile tool we have been blessed with with deep learning. And I know you Fellow Scholars have been quite excited for this one! Let's get started! This piece of work accomplishes geolocation for photographs. This means that we toss in a photograph, and it tells us exactly where it was made. Super resolution is a hot topic where we show a coarse, heavily pixelated image to a system, and it tries to guess what it depicts and increase the resolution of it. If we have a tool that accomplishes this, we can zoom into images way more than the number of megapixels of our camera would allow. It is really cool to see that deep learning has also made an appearance in this subfield. This handy little tool visualizes the learning process in a neural network with the classical forward and backward propagation steps. This recurrent neural network continues our sentences in a way that kind of makes sense. Well, kind of. Human in the loop techniques seek to create a bidirectional connection between humans and machine learning techniques so they can both learn from each other. I think it definitely is an interesting direction - at first, DeepMind's AlphaGo also learned the basics of Go from amateurs and then took off like a hermit to learn on its own and came back with guns blazing. We usually have at least one remarkably rigorous and scientific application of deep learning in every collection episode. This time, I'd like to show you this marvelous little program that suggests emojis for your images. It does so well, that nowadays, even computer algorithms are more hip than I am. This application is akin to the previous one we have seen about super resolution - here, we see beautiful, high resolution images of digits created from these tiny, extremely pixelated inputs. Netflix is an online video streaming service. The Netflix Prize was a competition where participants wrote programs to estimate how a user would enjoy a given set of movies based on this user's previous preferences. The competition was won by an ensemble algorithm, which is essentially a mixture of many existing techniques. And by many, I mean 107. It is not a surprise that some contemptuously use the term 'abomination' instead of 'ensemble' because of their egregious complexity. In this blog post, a simple neural network implementation is described that achieves quite decent results and the core of the solution fits in no more than 20 lines of code. The code has been written using Keras, which also happens to be one of my favorite deep learning libraries. Wholeheartedly recommended for everyone who likes to code, and a big shoutout to Francois, the developer of the mentioned library. Amazing feat. Convolutional Neural Networks have also started curating works of art by assigning a score to how aesthetic they are. Oh, sorry Leonardo! Earlier we talked about adversarial techniques that add a very specific type of noise to images to completely destroy the accuracy of previously existing image classification programs. The arms race has officially started, and new techniques are popping up to prevent this behavior. If you find some novel applications of deep learning, just send a link my way in the comments section. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=aKSILzbAqJs"
    },
    {
        "video_id": "4Y7RIAgOpn0",
        "video_title": "The Dunning-Kruger Effect | Two Minute Papers #58",
        "position_in_playlist": 218,
        "description": "The Dunning-Kruger effect describes a phenomenon where incompetent people assess their skills way higher than it is. We will talk about this phenomenon, its connection to impostor syndrome, and most importantly, why we should not use this knowledge to condemn others but to improve ourselves.\n\n__________________________\n\nThe paper \"Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-Assessments\" is available here. It is a really easy and enjoyable read, make sure you give it a shot!\nhttp://www.nottingham.ac.uk/~ntzcl1/literature/metacognition/kruger.pdf\n\nRecommended for you:\nWhat Is Impostor Syndrome? - https://www.youtube.com/watch?v=YPpIWQnufu8\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe background of the thumbnail image was created by samuelrodgers752 (CC BY 2.0) - https://flic.kr/p/rjdQyY\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This episode is about a classic, the Dunning-Kruger effect. I wonder how we could go on for almost 60 Two Minute Papers episodes without the Dunning-Kruger effect? Here is the experiment: participants were tested in different subjects, their test scores were computed, and at the same time, without the scores, they were asked to assess their perceived performance. The test subjects were humor, grammar, and logic. Things, of course, everyone excels at ... or do they? And here is the historic plot with the results. Such a simple plot, yet it tells us so much about people. From left to right, people were ordered by their score as you see with the dotted line. And the other line with the squares shows their perceived score, what they thought their scores would be. People from the bottom 10 percent, the absolute worst performers are convinced that they were well above the average. Competent people, on the other hand, seemed to underestimate their skills. Because the test was easy for them, they assumed that it was easy for everyone else. The extreme to the left is often referred to as the Dunning-Kruger effect, and the extreme to the right, maybe if you imagine the lines extending way-way further, is a common example of impostor syndrome. By the way, everyone thinks they are above average, which is neat mathematical anomaly. We would expect that people who perform poorly should know that they perform poorly, and people who're doing great should know they're doing great. One of the conclusions is that this is not the case, not the case at all. The fact that incompetent people are completely ignorant about their own inadequacy, at first, sounds like such a surprising conclusion. But if we think about it, we find there's nothing surprising about this. The more skilled we are, the more adept we are at estimating our skill level. By gaining more competence, incompetent people also obtained the skill to recognize their own shortcomings. A fish, in the world of Poker, means an inadequate player who is to be extorted by the more experienced. Someone asked how to recognize who the fish is at a poker table. The answer is a classic: if you don't know who the fish is at the table, it is you. The knowledge of the Dunning-Kruger effect is such a tempting tool to condemn other people for their inadequacy. But please, try to resist the temptation, remember, it doesn't help, that's the point of the paper! It is a much more effective tool for our own development if we attempt to use it on ourselves. Does it hurt a bit more? Oh yes, it does! The results of this paper solidify the argument that we need to be very vigilant about our own shortcomings. This knowledge endows you with a shield against ignorance. Use it wisely. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4Y7RIAgOpn0",
        "paper_link": "http://www.nottingham.ac.uk/~ntzcl1/literature/metacognition/kruger.pdf",
        "paper_title": "Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-Assessments"
    },
    {
        "video_id": "jMZqxfTls-0",
        "video_title": "From Doodles To Paintings With Deep Learning | Two Minute Papers #57",
        "position_in_playlist": 219,
        "description": "This technique uses deep learning to create beautiful paintings from terribly drawn sketches. The results look so great that many people called this work out to be an April Fools' day joke!\n\n_________________________________\n\nThe paper 'Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork' and its implementation is available here:\nhttps://github.com/alexjc/neural-doodle\nhttp://arxiv.org/pdf/1603.01768v1.pdf\n\nA playlist with out neural network and deep learning-related videos:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\nhttps://www.patreon.com/TwoMinutePapers\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nMusic:\nJohn Stockton Slow Drag by Chris Zabriskie is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nSource: http://chriszabriskie.com/uvp/\nArtist: http://chriszabriskie.com/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is the first paper in Two Minute Papers that showcased so stunning results that people called it out to be an April Fools' day joke. It is based on a deep neural network, and the concept is very simple: you choose an artistic style, you make a terrible drawing, and it creates a beautiful painting out of it. If you would like to know more about deep neural networks, we've had a ton of fun with them in previous episodes, I've put a link to them in the description box! I expect an onslaught of magnificent results with this technique to appear very soon. It is important to note that one needs to create a semantic map for each artistic style so that the algorithm learns the correspondence between the painting and the semantics. However, these maps have to be created only once and can be used forever, so I expect quite a few of them to show up in the near future, which greatly simplifies the workflow. After that, these annotations can be changed at will, you press a button, and the rest is history. Whoa! Wicked results. Some of these neural art results are so good that we should be creating a new class of Turing tests for paintings. This means that we are presented with two images, one of them is painted by a human, and one by a computer. We need to click the ones that we think were painted by a human. Damn, curses! As always, these techniques are new and heavily experimental, and this usually means that they take quite a bit of time to compute. The presentation video you have seen was sped up considerably. If these works are worthy of further attention, and I definitely think they are, then we can expect great strides towards interactivity in followup papers very soon. I am really looking forward to it and we Fellow Scholars will have a ton of fun with these tools in the future. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=jMZqxfTls-0"
    },
    {
        "video_id": "6aF9sJrzxaM",
        "video_title": "Overfitting and Regularization For Deep Learning | Two Minute Papers #56",
        "position_in_playlist": 220,
        "description": "In this episode, we discuss the bane of many machine learning algorithms - overfitting. It is also explained why it is an undesirable way to learn and how to combat it via L1 and L2 regularization.\n\n_____________________________\n\nThe paper \"Regression Shrinkage and Selection via the Lasso\" is available here:\nhttp://statweb.stanford.edu/~tibs/lasso/lasso.pdf\n\nAndrej Karpathy's excellent lecture notes on neural networks and regularization:\nhttp://cs231n.github.io/neural-networks-1/\n\nThe neural network demo is available here:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\n\nA playlist with out neural network and deep learning-related videos:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by Tony Hisgett (CC BY 2.0). It has undergone recoloring. - https://flic.kr/p/5dkbNV\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In machine learning, we often encounter classification problems where we have to decide whether an image depicts a dog or a cat. We'll have an intuitive, but simplified example where we imagine that the red dots represent dogs, and the green ones are the cats. We first start learning on a training set, which means that we get a bunch of images that are points on this plane, and from these points we try to paint the parts of the plane red and green. This way, we can specify which regions correspond to the concept of dogs and cats. After that, we'll get new points that we don't know anything about, and we'll ask the algorithm, for instance, a neural network to classify these unknown images, so it tells us whether it thinks that it is a dog or a cat. This is what we call a test set. We have had a lots of fun with neural networks and deep learning in previous Two Minute Papers episodes, I've put some links in the description box, check them out! In this example, it is reasonably easy to tell that the reds roughly correspond to the left, and the greens to the right. However, if we just jumped on the deep learning hype train, and don't know much about a neural networks, we may get extremely poor results like this. What we see here is the problem of overfitting. Overfitting means that our beloved neural network does not learn the concept of dogs or cats, it just tries to adapt as much as possible to the training set. As an intuition, think of poorly made real-life exams. We have a textbook where we can practice with exercises, so this textbook is our training set. Our test set is the exam. The goal is to learn from the textbook and obtain knowledge that proves to be useful at the exam. Overfitting means that we simply memorize parts of the textbook instead of obtaining real knowledge. If you're on page 5, and you see a bus, then the right answer is B. Memorizing patterns like this, is not real learning. The worst case is if the exam questions are also from the textbook, because you can get a great grade just by overfitting. So, this kind of overfitting has been a big looming problem in many education systems. Now the question is, which kind of neural network do we want? Something that works like a lazy student, or one that can learn many complicated concepts. If we're aiming for the latter, we have to combat overfitting, which is the bane of so many machine learning techniques. Now, there's several ways of doing that, but today we're going to talk about one possible solution by the name L1 and L2 regularization. The intuition of our problem is that the deeper and bigger neural networks we train, the more potent they are, but at the same time, they get more prone to overfitting. The smarter the student is, the more patterns he can memorize. One solution is to hurl a smaller neural network at the problem. If this smaller version is powerful enough to take on the problem, we're good. A student who cannot afford to memorize all the examples is forced to learn the actual underlying concepts. However, it is very possible that this smaller neural network is not powerful enough to solve the problem. So we need to use a bigger one. But, bigger network, more overfitting. Damn. So what do we do? Here is where L1 and L2 regularization comes to save the day. It is a tool to favor simpler models instead of complicated ones. The idea is that the simpler the model is, the better it transfers the textbook knowledge to the exam, and that's exactly what we're looking for. Here you see images of the same network with different regularization strengths. The first one barely helps anything and as you can see, overfitting is still rampant. With a stronger L2 regularization, you see that the model is simplified substantially, and is likely to perform better on the exam. However, if we add more regularization, it might be that we simplified the model too much, and it is almost the same as a smaller neural network that is not powerful enough to grasp the underlying concepts of the exam. Keep your neural network as simple as possible, but not simpler. One has to find the right balance which is an art by itself, and it shows that training deep neural networks takes a bit of expertise. It is more than just a plug and play tool that solves every problem by magic. If you want to play with the neural networks you've seen in this video, just click on the link in the description box. I hope you'll have at least as much fun with it as I had! Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6aF9sJrzxaM",
        "paper_link": "http://statweb.stanford.edu/~tibs/lasso/lasso.pdf",
        "paper_title": "Regression Shrinkage and Selection via the Lasso"
    },
    {
        "video_id": "0Xc9LIb_HTw",
        "video_title": "Decision Trees and Boosting, XGBoost | Two Minute Papers #55",
        "position_in_playlist": 221,
        "description": "A decision tree is a great tool to help making good decisions from a huge bunch of data. In this episode, we talk about boosting, a technique to combine a lot of weak decision trees into a strong learning algorithm. \n\nPlease note that gradient boosting is a broad concept and this is only one possible application of it!\n\n__________________________________\n\nOur Patreon page is available here:\nhttps://www.patreon.com/TwoMinutePapers\nIf you don't want to spend a dime or you can't afford it, it's completely okay, I'm very happy to have you around! And please, stay with us and let's continue our journey of science together!\n\nThe paper \"Experiments with a new boosting algorithm\" is available here:\nhttp://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/boosting-icml.pdf\n\nAnother great introduction to tree boosting:\nhttp://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nThe thumbnail image background was created by John Voo (CC BY 2.0), content-aware filling has been applied - https://flic.kr/p/BLphju\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePap...\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A decision tree is a great tool to help making good decisions from a huge bunch of data. The classical example is when we have a bunch of information about people and would like to find out whether they like computer games or not. Note that this is a toy example for educational purposes. We can build the following tree: if the person's age in question is over 15, the person is less likely to like computer games. If the subject is under 15 and is a male, he is quite likely to like video games, if she's female, then less likely. Note that the output of the tree can be a decision, like yes or no, but in our case, we will assign positive and negative scores instead. You'll see in a minute why that's beneficial. But this tree wa s just one possible way of approaching the problem, and admittedly, not a spectacular one - a different decision tree could be simply asking whether this person uses a computer daily or not. Individually, these trees are quite shallow and we call them weak learners. This term means that individually, they are quite inaccurate, but slightly better than random guessing. And now comes the cool part. The concept of tree boosting means that we take many weak learners and combine them into a strong learner. Using the mentioned scoring system instead of decisions also makes this process easy and straightforward to implement. Boosting is similar to what we do with illnesses. If a doctor says that I have a rare condition, I will make sure and ask at least a few more doctors to make a more educated decision about my health. The cool thing is that the individual trees don't have to be great, if they give you decisions that are just a bit better than random guessing, using a lot of them will produce strong learning results. If we go back to the analogy with doctors, then if the individual doctors know just enough not to kill the patient, a well-chosen committee will be able to put together an accurate diagnosis for the patient. An even cooler, adaptive version of this technique brings in new doctors to the committee according to the deficiencies of the existing members. One other huge advantage of boosted trees over neural networks is that we actually see why and how the computer arrives to a decision. This is a remarkably simple method that leads to results of very respectable accuracy. A well-known software library called XGBoost has been responsible for winning a staggering amount of machine learning competitions in Kaggle. I'd like to take a second to thank you Fellow Scholars for your amazing support on Patreon and making Two Minute Papers possible. Creating these episodes is a lot of hard work and your support has been invaluable so far, thank you so much! We used to have three categories for supporters. Undergrad students get access to a Patron-only activity feed and get to know well in advance the topics of the new episodes. PhD students who are addicted to Two Minute Papers get a chance to see every episode up to 24 hours in advance. Talking about committees in this episode, Full Professors form a Committee to decide the order of the next few episodes. And now, we introduce a new category, the Nobel Laureate. Supporters in this category can literally become a part of Two Minute Papers and will be listed in the video description box in the upcoming episodes. Plus all of the above. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=0Xc9LIb_HTw",
        "paper_link": "http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/boosting-icml.pdf",
        "paper_title": "Experiments with a new boosting algorithm"
    },
    {
        "video_id": "ZolWxY4f9wc",
        "video_title": "3D Depth From a Single Photograph | Two Minute Papers #54",
        "position_in_playlist": 222,
        "description": "This piece of work tries to estimate depth information from an input photogaph. This means that it looks at the photo and tries to tell how far away parts of the image are from the camera, and the final goal is that we provide a photograph for which the depth information is completely unknown and we ask the algorithm to provide it for us.\n\n_______________________________\n\nThe paper \"3-D Depth Reconstruction from a Single Still Image\" is available here:\nhttp://www.cs.cornell.edu/~asaxena/learningdepth/saxena_ijcv07_learningdepth.pdf\n\nThe source of the shown video at the end:\nhttps://www.youtube.com/watch?v=GWWIn29ZV4Q\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by Willy Verhulst (CC BY 2.0) - https://flic.kr/p/pZj8KD\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This piece of work tries to estimate depth information from an input photograph. This means that it looks at the photo and tries to tell how far away parts of the image are from the camera. An example output looks like this: on the left, there is an input photograph, and on the right, you see a heatmap with true distance information. This is what we're trying to approximate. This means that we collect a lot of indoor and outdoor images with their true depth information, and we try to learn the correspondence, how they relate to each other. Sidewalks, forests, buildings, you name it. These image and depth pairs can be captured by mounting 3D scanners on this awesome custom-built vehicle. And, gentlemen, that is one heck of a way of spending research funds. The final goal is that we provide a photograph for which the depth information is completely unknown and we ask the algorithm to provide it for us. Here you can see some results: the first image is the input photograph, the second shows the true depth information. The third image is the depth information that was created by this technique. And here is a bunch of results for images downloaded from the internet. It probably does at least as good as a human would. Spectacular. This sounds like a sensorial problem for humans, and a perilous journey for computers to say the least. What is quite remarkable is that these relations can be learned by a computer algorithm. What can we use this for? Well, a number of different things, one of which is to create multiple views of this 2D photograph using the guessed depth information. It can also be super helpful in building robots that can wander about reliably with inexpensive consumer cameras mounted on them. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZolWxY4f9wc",
        "paper_link": "http://www.cs.cornell.edu/~asaxena/learningdepth/saxena_ijcv07_learningdepth.pdf",
        "paper_title": "3-D Depth Reconstruction from a Single Still Image"
    },
    {
        "video_id": "a-ovvd_ZrmA",
        "video_title": "How DeepMind's AlphaGo Defeated Lee Sedol | Two Minute Papers #53",
        "position_in_playlist": 223,
        "description": "This time around, Google DeepMind embarked on a journey to write an algorithm that plays Go. Go is an ancient chinese board game where the opposing players try to capture each other's stones on the board. Behind the veil of this deceptively simple ruleset, lies an enormous layer of depth and complexity. As scientists like to say, the search space of this problem is significantly larger than that of chess. So large, that one often has to rely on human intuition to find a suitable next move, therefore it is not surprising that playing Go on a high level is, or maybe was widely believed to be intractable for machines. The result is Google DeepMind's AlphaGo, the deep learning technique that defeated a professional player and world champion, Lee Sedol.\n\nWhat it also important to note is that the techniques used in this algorithm are general, and can be used for a large number of different tasks. By this, I mean not AlphaGo specifically, but the Monte Carlo Tree Search, the value network and deep neural networks.\n\n______________________\n\nThe paper \"Mastering the Game of Go with Deep Neural Networks and Tree Search\" is available here:\nhttps://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf\nhttp://www.nature.com/nature/journal/v529/n7587/full/nature16961.html\n\nA great Go analysis video by Brady Daniels. Make sure to check it out and subscribe if you like what you see there!\nhttps://www.youtube.com/watch?v=dOQsYWxMNJQ\n\nThe mentioned post on the Go reddit:\nhttps://www.reddit.com/r/baduk/comments/49y17z/the_true_strength_of_alphago/\n\nSome clarification on what part of the algorithm is specific to Go and how:\nhttps://news.ycombinator.com/item?id=11280744\n\nGo board image credits (all CC BY 2.0):\nRenato Ganoza - https://flic.kr/p/7nX4kK\nJaro Larnos - https://flic.kr/p/dDeQU9\nLuis de Bethencourt - https://flic.kr/p/4c5RaR\n\nWE WOULD LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe background of the thumbnail image is the property of Google DeepMind.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A few months ago, AlphaGo played and defeated Fan Hui, a 2 dan master and European champion player in the game of Go. However, the next opponent, Lee Sedol is a 9 dan master and world champion player. Just to give an intuition of the difference, Lee Sedol is expected to beat Fan Hui 97 times out of 100 games. Google DeepMind had 6 months of preparation for this bout Five matches were played over five days. In my timezone, the matches started around 4 am, and the results would usually pop up exactly a few minutes after I woke up. It was amazing. I could barely fall asleep I was so excited for the results, and when I woke up, I kissed my daughter and immediately ran to my computer to see what was going on. Most people were convinced that Lee Sedol was going to beat the machine 5-0, and I was stunned to see AlphaGo triumphed over Lee Sedol in the first match, and then the second, and then the third. Huge respect for both Google DeepMind for putting together such a spectacular algorithm and for Lee Sedol who played extremely well under enormous pressure. He is indeed a true champion. The game of Go has a stupendously large search space that makes it completely impossible to check every move and choose the best. What is also not often talked about is that processing through many moves is one thing, but judging which move is advantageous and which is not, it just as difficult as the search itself. The definition of the best move is not clear-cut by any stretch of the imagination. We also have to look into the future and simulate the moves of the opponent. I think it is easy to see that the difficulty of this problem is completely out of this world. A neural network is a crude approximation of the human brain, just like a stick figure is a crude approximation of a human being. In this work, neural networks are used to reduce the size of the search space, and value networks are used to predict the expected outcome of a move. This value network basically tries to determine who will win if a sequence of moves is made. To defeat AlphaGo, or any computer opponent, playing non-traditional moves that it surely hasn't practiced sounds like a great idea. However, there is no database involed per se, this technique is simulating the moves until the very end of the game, so non-traditional \"weird\" moves won't throw it off. It is also very important to know that the structure of AlphaGo is not like Deep Blue for chess. Deep Blue was specifically designed to maximize metrics that are likely to lead to victory, such as pawn advantage, king safety, tempo and more. AlphaGo doesn't do any of that. It is a general technique that can learn to solve a large number of different problems. I cannot overstate the significance of this. Almost the entirety of computer science research revolves around creating algorithms that are specifically tailored to one task. Different task, different research projects, different algorithm. Imagine how empowering it would be to have a general algorithm that can solve a large amount of problems. It's incredible! Just as people who don't speak a word of Chinese can write an artificial intelligence program to recognize handwritten Chinese text, someone who hasn't played more than a few games can write a chess or Go program that is beyond the skill of most professional players. This is a wonderful testament of the power of mathematics and science. It was quite surprising to see that AlphaGo played seemingly suboptimal moves when it was ahead to reduce variance and maximize its chance of victory. Take a look at at DeepMind's other technique by the name Deep Q-Learning that plays space invaders on a superhuman level. This shot, at first, looks like a blunder, but if you wait it out, you'll see how brilliant it really is. A move that seems like a blunder at a time may be the optimal move in the grand scheme of things. It not a blunder. It is a move from someone whose brilliance is way beyond the capabilities of even the best human players. There is an excellent analysis of this phenomenon on the Go reddit, I've put a link in the description box, check it out. I'd like to emphasize that the technique learns at first, by looking at a large number of games by amateurs. But the question is, how can it get beyond the level of amateurs? After looking at these games, it will learn the basics and will play millions of games against itself and learn from them. And, to be emphasized: nothing in this algorithm is specific to Go. Nothing. It can be used to solve a number of different problems without significant changes. It would be immensely difficult to overstate the significance of that. Shoutout to Brady Daniels who has an excellent Go educational channel. He has very fluid, enjoyable and understandable explanations, highly recommended, check it out. There is a link to one of his videos in the description box. It is a possibility that the first Go grandmaster to reach 10 dans may not be a human, but a computer. My mind is officially blown. Insanity. One more cobblestone has been laid on the path to artificial general intelligence. This achievement I find to be of equivalent magnitude to landing on the Moon. And this is just the beginning. I can't wait to see this technique being used for research in medicine. Huge respect for Demis Hassabis and Lee Sedol, who were both respectful and humble both in victory, and in defeat. They are true champions of their craft. Thanks so much for DeepMind for creating this rivetingly awesome event. My daughter, J\u00e1zmin was born one day before this glorious day. What an exciting time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=a-ovvd_ZrmA",
        "paper_link": "https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf",
        "paper_title": "Mastering the Game of Go with Deep Neural Networks and Tree Search"
    },
    {
        "video_id": "hPKJBXkyTKM",
        "video_title": "10 More Cool Deep Learning Applications | Two Minute Papers #52",
        "position_in_playlist": 224,
        "description": "In this episode, we present another round of incredible deep learning applications!\n\n_________________________\n\n1. Colorization - http://tinyclouds.org/colorize/\n2. RNN Music on Bob Sturm's YouTube channel - https://www.youtube.com/watch?v=RaO4HpM07hE\n3. Flow Machines by Sony - https://www.youtube.com/watch?v=buXqNqBFd6E\n4. RNN Passwords - https://github.com/gehaxelt/RNN-Passwords\n5. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding - http://arxiv.org/abs/1510.00149\n6. Right Whale Kaggle Competition - http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html\n7. Improving YouTube video thumbnails - http://youtube-eng.blogspot.hu/2015/10/improving-youtube-video-thumbnails-with_8.html\n8. Celebrity super-resolution: https://github.com/mikesj-public/dcgan-autoencoder\n9. Convolutional Neural Network visualization - http://scs.ryerson.ca/~aharley/vis/conv/ + Paper: http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf\n10. DarkNet RNN writes in the style of George RR Martin - http://pjreddie.com/darknet/rnns-in-darknet/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nWE'D LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nThe thumbnail image background was created by Dan Ruscoe (CC BY 2.0) - https://flic.kr/p/deHtEb\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. To all of you Fellow Scholars out there who are yearning for some more deep learning action like I do, here goes the second package. Buckle up, amazing applications await you. As always, links to every one of these works are available in the description box. This convolutional neural network can learn how to colorize by looking at the same images both in colo r and black and white. The first image is the black and white input, the second is how the algorithm colorized it, and the third is how the image originally looked like in color. Insanity. Recurrent neural networks are able to learn and produce sequences of data, and they are getting better and better at music generation. Nowadays, people are experimenting with human aided music generation with pretty amaz ing results. Sony has also been working on such a solution with spectacular results. One can also run a network on a large database of leaked human passwords and try to crack new accounts building on that knowledge. Deep neural networks take a substantial amount of time to train, and the final contents of each of the neurons have to be stored, which takes a lot of space. New techniques are being explored to compress the information content of these networks. There is an other application where endangered whale species are recognized by convolutional neural networks. Some of them have a worldwide population of less than 500, and this is where machine learning steps in to try to save them. Awesome! YouTube has a huge database full of information on what kind of video thumbnails are the ones that people end up clicking on. They use deep learning to automatically find and suggest the most appealing images for your videos. There is also this crazy application where a network was trained on a huge dataset with images of celebrities. A low quality image is given, where the algorithm creates a higher resolution version building on this knowledge. The leftmost images are the true high resolution images, the second one is the grainy, low resolution input, and the third is the neural network's attempt to reconstruct the original. This application takes your handwriting of a number, and visualizes how a convolutional neural network understands and classifies it. Apparently, George RR Martin is late with writing the next book of Game of Thrones, but luckily, we have recurrent neural networks that can generate text in his style. An infinite amount, so beware George, winter is coming. I mean the machines are coming. It is truly amazing what these techniques are capable of. And as machine learning is a remarkably fast moving field, new applications pop up pretty much every day. I am quite enthused to do at one more batch of these! Of course, provided that you liked this one. Let me know. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hPKJBXkyTKM"
    },
    {
        "video_id": "fpd_wiOsgDk",
        "video_title": "5000 Fellow Scholars Special! | Two Minute Papers",
        "position_in_playlist": 225,
        "description": "We have reached 5000 Fellow Scholars on Two Minute Papers! In this video I share my delight and talk a bit about our future plans with the series.\n\n____________________\n\nWE'D LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by Werwin15 (CC BY 2.0) - https://flic.kr/p/6uUa4p\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We just hit 5000 subscribers. More than five thousand Fellow Scholars who wish to join us on our journey of science. It really shows that everyone loves science, they just don't know about it yet! About 6 months ago, we were celebrating 250 subscribers. The growth of the channel has been nothing short of incredible, and this is all attributed to you. Without you Fellow Scholars, this series would be nothing but a crazy person sitting at home, talking into a microphone and having way too much fun! Thank you so much for hanging in there! I love doing this and am delighted to have each of you in our growing club of Fellow Scholars! We have also hit half a million views. Holy cow! If we would substitute one human being for every view, we would be close to 6% of the population of Austria, or almost 30% of the population of the beautiful Vienna. This is equivalent to about 60% of the population of San Francisco. This is way beyond the amount of people I could ever reach by teaching at the university. It is a true privilege to teach so many people from all around the world. We have so many plans to improve the series in different directions. We have recently switched to 60 frames per second for beautiful smooth, and silky animations, and closed captions are also now uploaded for most episodes to improve the clarity of the presentations. We are also looking at adding more Patreon perks in the future. There are also tons of amazing research works up the sleeve that you will see very soon in the upcoming videos. Graphics guys, I got your back, machine learners, this way please. Other spicy topics will also be showcased to keep it fresh and exciting. My wife Fel\u00edcia is also preparing some incredible artwork for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fpd_wiOsgDk"
    },
    {
        "video_id": "4h0uC9FPVMQ",
        "video_title": "How To Get Started With Machine Learning? | Two Minute Papers #51",
        "position_in_playlist": 226,
        "description": "I get a lot of messages from you Fellow Scholars that you would like to get started in machine learning and are looking for materials. Below you find a ton of resources to get you started!\n\n__________________________\n\nThe AI Revolution: The Road to Superintelligence on Wait But Why:\nhttp://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\nhttp://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html\n\nSuperintelligence by Nick Bostrom:\nhttps://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\n\nCourses:\nWelch Labs - https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\nAndrew Ng on Coursera - https://class.coursera.org/ml-005/lecture\nAndrew Ng (YouTube playlist) - https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599\nNando de Freitas (UBC) - https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf\nNando de Freitas (Oxford) - https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu\nNando de Freitas (more) - https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6\nhttps://www.youtube.com/watch?v=PlhFWT7vAEw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw\nOne more at Caltech - https://work.caltech.edu/telecourse.html\nAndrej Karpathy - https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC\nUC Berkeley - https://www.youtube.com/channel/UCshmLD2MsyqAKBx8ctivb5Q/videos\nGeoffrey Hinton - https://www.coursera.org/course/neuralnets\nMachine Learning specialization at Coursera - https://www.coursera.org/specializations/machine-learning\nMIT - http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/\nMathematicalmonk's course: https://www.youtube.com/watch?v=yDLKJtOVx5c&list=PLD0F06AA0D2E8FFBA&index=0\n\n\"Pattern Recognition and Machine Learning\" by Christoper Bishop:\nhttp://research.microsoft.com/en-us/um/people/cmbishop/prml/\n\n\"Algorithms for Reinforcement Learning\" by Csaba Szepesv\u00e1ri:\nhttp://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf\n\nA great talk on deep learning libraries:\nhttps://www.youtube.com/watch?v=Vf_-OkqbwPo&feature=youtu.be\n\nTwo great sources to check for new papers:\nhttp://gitxiv.com/top\nhttp://www.arxiv-sanity.com/top\n\nRecent machine learning papers on the arXiv:\nhttp://arxiv.org/list/stat.ML/recent\n\nThe Machine Learning Reddit:\nhttp://www.reddit.com/r/MachineLearning/\n\nOne more great post on how to get started with machine learning:\nhttps://www.quora.com/How-do-I-get-started-in-machine-learning-both-theory-and-programming/answer/Sebastian-Raschka-1\n\nA great blog post on how to get started with Keras:\nhttp://swanintelligence.com/first-steps-with-neural-nets-in-keras.html\n\nA website with lots of intuitive articles on deep learning:\nhttp://neuralnetworksanddeeplearning.com/\n\nA free book on deep learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville:\nhttp://www.deeplearningbook.org/\n\nWE'D LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was created by C_osett - https://flic.kr/p/sDTYmm\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I get a lot of messages from you Fellow Scholars that you would like to get started in machine learning and are looking for materials. Words fail to describe how great the feeling is that the series inspires many of you to start your career in research. At this point, we're not only explaining the work of research scientists but creating new research scientists. Machine learning is an amazing field of research that provides us with incredible tools that help us solve problems that were previously impossible to solve. Neural networks can paint in the style of famous artists or recognize images and are capable of so many other things it simply blows my mind. However, bear in mind that machine learning is not an easy field. This field fuses together the beauty, rigor, and preciseness of mathematics with the useful applications of engineering. It is also a fast moving field, on almost any given day, 10 new scientific papers pop up in the repositories. For everything that I mention in this video there is a link in the description box and more, so make sure to dive in and check them out. If you have other materials that helped you understand some of the more difficult concepts, please let me know in the comments section and I'll include them in the text below. First, some non-scientific texts to get you in the mood, I recommend reading \"The Road to Superintelligence\" on a fantastic blog by the name Wait But Why. This is a frighteningly long article for many, but I guarantee that you won't be able to stop reading it. Beware. Nick Bostrom's Superintelligence is also a fantastic read, after which you'll probably be convinced that it doesn't make sense to work on anything else but machine learning. There is a previous Two Minute Papers episode on artificial superintelligence if you're looking for a teaser for this book. Now let's get a bit more technical with some of the better video series and courses out there! Welch labs is an amazing YouTube channel with a very intuitive introduction to the concept of neural networks. Andrew Ng is a Chief Scientist at Baidu Research in deep learning. His wonderful course is widely regarded as the pinnacle of all machine learning courses and is therefore highly recommended. Nando de Freitas is a professor at the university of Oxford, and has also worked with DeepMind. His course that he held at the University of British Columbia covers many of the more advanced concepts in machine learning. Regarding books, I recommend reading my favorite Holy Tome of machine learning, that goes by the name of \"Pattern Recognition and Machine Learning\" by Christoper Bishop. A sample chapter is available from the book if you wish to take a look. It has beautiful typesetting, lots of intuition and crystal clear presentation. Definitely worth every penny of the price. I'd like to note that I am not paid for any of the book endorsements in the series. When I recommend a book, I genuinely think that it provides great value to you Fellow Scholars. About software libraries. Usually, in most fields, the main problem is that the implementation of many state of the art techniques are severely lacking. Well luckily, in the machine learning community, we have them in abundance. I've linked a great talk on what libraries are available and the strengths and weaknesses for each of them. At this point, you'll probably have an idea of which direction you're most excited about. Start searching for keywords, make sure to read the living hell out of the machine learning reddit to stay up to date, and, the best part is yet to come: starting to explore on your own. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4h0uC9FPVMQ"
    },
    {
        "video_id": "-dbkE4FFPrI",
        "video_title": "Interactive Photo Recoloring | Two Minute Papers #50",
        "position_in_playlist": 227,
        "description": "Image and color editing is an actively researched topic with really cool applications that you will see in a second. Most of the existing solutions are either easy to use but lack in expressiveness, or they are expressive, but too complex for novices to use. Computation time is also an issue as some of the operations in photoshop can take more than a minute to carry out. Using a naive color transfer technique would destroy a sizeable part of the dynamic range of the input image image, and hence, legitimate features which are all preserved if we use this algorithm instead.\n\n______________________________\n\nThe paper \"Palette-based Photo Recoloring\" is available here:\nhttp://gfx.cs.princeton.edu/pubs/Chang_2015_PPR/index.php\n\nThe thumbnail background image was created by zoutedrop (CC BY 2.0) - https://flic.kr/p/5E32Cc\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Image and color editing is an actively researched topic with really cool applications that you will see in a second. Most of the existing solutions are either easy to use but lack in expressiveness, or they are expressive, but too complex for novices to use. Computation time is also an issue as some of the operations in Photoshop can take more than a minute to carry out. For color editing, the workflow is very simple, the program extracts the dominant colors of an image, which we can interactively edit ourselves. An example use case would be recoloring the girl's blue sweater to turquoise, or changing the overall tone of the image to orange. Existing tools that can do this are usually either too slow or only accessible to adept users. It is also important to note that it is quite easy to take these great results for granted. Using a naive color transfer technique would destroy a sizeable part of the dynamic range of the image, and hence, legitimate features which are all preserved if we use this algorithm instead. One can also use masks to selectively edit different parts of the image. The technique executes really quickly, opening up the possibility of not real time, but interactive recoloring of animated sequences. Or, you can also leverage the efficiency of the method to edit not one, but a collection of images in one go. The paper contains a rigorous evaluation against existing techniques. For instance, they show that this method executed three to twenty times faster than the one implemented in Adobe Photoshop. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-dbkE4FFPrI",
        "paper_link": "http://gfx.cs.princeton.edu/pubs/Chang_2015_PPR/index.php",
        "paper_title": "Palette-based Photo Recoloring"
    },
    {
        "video_id": "UGAzi1QBVEg",
        "video_title": "Deep Learning Program Learns to Paint | Two Minute Papers #49",
        "position_in_playlist": 228,
        "description": "Artificial neural networks were inspired by the human brain and simulate how neurons behave when they are shown a sensory input (e.g., images, sounds, etc). They are known to be excellent tools for image recognition, any many other problems beyond that - they also excel at weather predictions, breast cancer cell mitosis detection, brain image segmentation and toxicity prediction among many others. Deep learning means that we use an artificial neural network with multiple layers, making it even more powerful for more difficult tasks. \n\nThis time they have been shown to be apt at reproducing the artistic style of many famous painters, such as Vincent Van Gogh and Pablo Picasso among many others. All the user needs to do is provide an input photograph and a target image from which the artistic style will be learned.\n\n_______________________\n\n\nThe paper \"Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\" is available here:\nhttp://arxiv.org/pdf/1601.04589v1.pdf\n\nPrevious work - the paper \"A Neural Algorithm of Artistic Style\" is available here\nhttp://arxiv.org/pdf/1508.06576v2.pdf\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nRecommended for you:\nDeep Neural Network Learns Van Gogh's Art - https://www.youtube.com/watch?v=-R9bJGNHltQ&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=42\nArtificial Neural Networks and Deep Learning - https://www.youtube.com/watch?v=rCWTOOgVXyE&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=31\nHow Does Deep Learning Work? - https://www.youtube.com/watch?v=He4t7Zekob0&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=39\n9 Cool Deep Learning Applications - https://www.youtube.com/watch?v=Bui3DWs02h4&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=36\n\nThe shown website with neural art:\nhttp://deepart.io/\n\nCheck this one out too (it is a different implementation)!\nhttps://deepforger.com/\nhttps://twitter.com/deepforger\n\nThumbnail image: Andreas Achenbach - Clearing Up\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear fellow scholars This is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In a previous episode, we discussed how a machine learning technique called a convolutional neural network could paint in the style of famous artists. The key thought is that we are not interested in individual details, We want to teach the neural network the high-level concept of artistic style. A convolutional neural network is a fantastic tool for this, since it does not only recognize images well, But the deeper we go in the layers, the more high-level concepts neurons will encode, therefore the better idea the algorithm will have of the artistic style. In an earlier exemple, we have shown that the neurons in the first hidden layer will create edges as a combination of the input pixels of the image. The next layer is a combination of edges that create object parts. One layer deeper, a combination of object parts create object models, and this is what makes convolutional neural network so useful in recognizing them. In this follow up paper, the authors use a very deep 19-layer convolutional network that they mix together with Markov random fields, a popular technique in image and texture synthesis The resulting algorithm retains the important structures of the input image significantly better than the previous work. Which is also awesome, by the way. Failure cases are also reported in the paper, which was a joy to read. Make sure to take a look if you are interested. We also have a ton of video resources in the description box that you can voraciously consume for more information. There is already a really cool website  where you either wait quite a bit, and get results for free, or you pay someone to compute it and get results almost immediately If any of you are in the mood of doing some neural art of something Two Minute Papers related, make sure to show it to me, I would love to see that. As a criticism, I have heard people saying that the technique takes forever on an HD image, which is absolutely true. But please bear in mind that the most exciting research is not speeding up something that runs slowly The most exciting thing about research is making something possible that was previously impossible If the work is worthy of attention, it does'nt matter if it's slow. Tree followup papers later, it will be done in a matter of seconds. In summary, the results are nothing short of amazing. I was full of ecstatic glee when I've first seen them. This is insanity, and it's only been a few months since the initial algorithm was published. I always say this, but we are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UGAzi1QBVEg",
        "paper_link": "http://arxiv.org/pdf/1601.04589v1.pdf\n\nPrevious work - the paper \"A Neural Algorithm of Artistic Style\" is available here",
        "paper_title": "Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis"
    },
    {
        "video_id": "K-0KJtk07YU",
        "video_title": "Artistic Manipulation of Caustics | Two Minute Papers #48",
        "position_in_playlist": 229,
        "description": "A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. Since we, humans are pretty bad at estimating how exactly caustics should look like, one can manipulate them to be more in line with their artistic vision.\n\n__________________________________\n\nThe paper \"Stylized Caustics: Progressive Rendering of Animated Caustics\" is available here:\nhttp://vc.cs.ovgu.de/files/publications/2016/Guenther_2016_EGb.pdf\n\nCynicatPro's channel is available here:\nhttps://www.youtube.com/user/CynicatPro/videos\n\nRecommended for you:\nManipulating Photorealistic Renderings - https://www.youtube.com/watch?v=L7MOeQw47BM\nRay Tracing / Subsurface Scattering @ Function 2015 - https://www.youtube.com/watch?v=qyDUvatu5M8\nMetropolis Light Transport - https://www.youtube.com/watch?v=f0Uzit_-h3M\n\nThe background of the thumbnail image was created by woodleywonderworks (CC BY 2.0) - https://flic.kr/p/2tKhPY\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. If it's not your favorite visual phenomenon in nature yet, which is almost impossible, then you absolutely have to watch this episode. If it is, all the better because you're gonna love what's coming up now! Imagine that we have a photorealistic rendering program that simulates the path of light rays in a scene that we put together, and creates beautiful imagery of our caustics. However, since we, humans are pretty bad at estimating how exactly caustics should look like, one can manipulate them to be more in line with their artistic vision. Previously, we had an episode on a technique which made it possible to pull the caustic patterns to be more visible, but this paper offers a much more sophisticated toolset to torment these caustic patterns to our liking. We can specify a target pattern that we would like to see, and obtain a blend between what would normally happen in physics and what we imagined to appear there. It also supports animated sequences. Artists who use these tools are just as skilled in their trade as the scientists who created this algorithm, so I can only imagine the miracles they will create with such a technique. If you are interested in diving into photorealistic rendering, material modeling and all that cool stuff, there are completely free and open source tools out there like Blender that you can use. If you would like to get started, check out CynicatPro's YouTube channel that has tons of really great material. Here is a quick teaser of his channel. Thanks! There is a link to his channel in the description box, make sure to check it out and subscribe if you like what you see there. I just realized that the year has barely started and it is already lavishing in beautiful papers. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=K-0KJtk07YU",
        "paper_link": "http://vc.cs.ovgu.de/files/publications/2016/Guenther_2016_EGb.pdf",
        "paper_title": "Stylized Caustics: Progressive Rendering of Animated Caustics"
    },
    {
        "video_id": "58tsN03IXlw",
        "video_title": "Should You Take the Stairs at Work? (For Weight Loss) | Two Minute Papers #47",
        "position_in_playlist": 230,
        "description": "Let's find out the answer to ever occurring question - if you are aiming for weight loss, should you take the stairs at work? How many calories do you burn by running up a few flights of stairs? There are so many rumors floating around, let's see what the researchers say!\n\nScientists set up a controlled experiment where over a hundred subjects climbed 11 stories of staircases, ascending a total of 27 meters vertically. Their oxygen consumption and heart rate was measured, and most importantly for us, the amount of caloric cost of this undertaking. Hint: it is not so great for weight loss, but the oxygen and heart rate responses make it is a wonderful way to refresh your body and keep is healthy. Keep climbing! :)\n\n_________________\n\nThe paper \"Heart rate, oxygen uptake, and energy cost of\nascending and descending the stairs\" is available here:\nhttps://www.researchgate.net/publication/11432301_Heart_rate_oxygen_uptake_and_energy_cost_of_ascending_and_descending_the_stairs\nhttp://www.ncbi.nlm.nih.gov/pubmed/11932581\n\nThe source of the thumbnail image (CC0): https://www.pexels.com/photo/stairs-staircase-28188/\n\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I am sure that every one of us have overheard conversations at a workplace where people talked about taking the stairs instead of the elevator, and, as a result, getting leaner. There was also a running joke on the internet about Arnold Classic, a famous bodybuilding competition/festival, where I think it's fair to say that people tended to favor the escalator instead of the stairs. So, this is it, we're going to settle this here and now. Do we get lean from taking the stairs every day? Scientists set up a controlled experiment where over a hundred subjects climbed 11 stories of staircases, ascending a total of 27 meters vertically. Their oxygen consumption and heart rate was measured, and most importantly for us, the amount of caloric cost of this undertaking. They have found that all this self flagellation with ascending 11 stories of staircases burns a whopping 19.7 kilo calories. Each step is worth approximately one tenth of a kilo calorie if we're ascending. Descending is worth approximately half of that. Apparently, these bodybuilders know what they are doing. The authors diplomatically noted: Stair-climbing exercise using a local public-access staircase met the minimum requirements for cardiorespiratory benefits and can therefore be considered a viable exercise for most people and suitable for promotion of physical activity. Which sounds like the scientific equivalent of \"well, better than nothing\". So does this mean that you shouldn't take the stairs at work? If you're looking to get lean because of that, no, not a chance. However, if you are looking for a refreshing cardiovascular exercise in the morning that refreshes your body, and makes you happier, start climbing. I do it all the time and I love it! So, we are exploring so far uncharted territories and this makes the first episode on nutrition (should have said exercise, sorry!) in the series, if you would like to hear more of this, let me know in the comments section. I'd also be happy to see your paper recommendations in nutrition as well. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=58tsN03IXlw",
        "paper_link": "https://www.researchgate.net/publication/11432301_Heart_rate_oxygen_uptake_and_energy_cost_of_ascending_and_descending_the_stairs",
        "paper_title": "Heart rate, oxygen uptake, and energy cost of\nascending and descending the stairs"
    },
    {
        "video_id": "YPpIWQnufu8",
        "video_title": "What is Impostor Syndrome? | Two Minute Papers #46",
        "position_in_playlist": 231,
        "description": "Who, or what is an impostor? An impostor is a person who deceives others by pretending to be someone else. In this episode, we look in the mind of someone who suffers from impostor syndrome and see the fickle understanding they have of their own achievements. Researchers, academics and high achieving women are especially vulnerable to this condition. There will also be a few words on how to treat it.\n\n__________________________\n\nThe paper \"The imposter phenomenon in high achieving women: Dynamics and therapeutic intervention\" is available here:\nhttp://www.suzanneimes.com/wp-content/uploads/2012/09/Imposter-Phenomenon.pdf\n\nAn article on Hayden Christensen:\nhttp://www.vulture.com/2015/12/hayden-christensen-quit-movies-after-star-wars.html\n\nThe filmography of Hayden Christensen:\nhttps://en.wikipedia.org/wiki/Hayden_Christensen\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Who, or what is an impostor? Simple definition: a person who deceives others by pretending to be someone else Full definition: one that assumes false identity or title for the purpose of deception Wow, the full definition is a whopping 7 characters more. I don't even know what to do with this amount of time I saved reading the simple definition first. Let's look in the mind of someone who suffers from impostor syndrome and see the fickle understanding they have of their own achievements. 98 points out of 100, this surely means that they mixed up my submission with someone else's, who was way smarter than I am. I went in for the next round of interviews, messed up big time, and I got hired with an incredible salary. This can, of course, only be a misunderstanding. I got elected for this prestigious award. I don't know how this could possibly have happened. Maybe someone who really likes me tried to pressure the prize committee to vote for me. I cannot possibly imagine any other way of this happening. I've been 5 years at the company now and still, no one found out that I am a fraud. That's a disaster Nothing can be convince me that I am not an impostor who fooled everyone else for being a bright person. However funny as it may sound, this is a very real problem. Researchers, academics and high achieving women are especially vulnerable to this condition. But it is indeed not limited to these professions. For instance, Hayden Christensen, the actor playing Anakin Skywalker in the beloved Star Wars series appears to suffer from very similar symptoms. He said: \"I felt like I had this great thing in Star Wars that provided all these opportunities and gave me a career, but it all kind of felt a little too handed to me,\" he explained. \"I didn't want to go through life feeling like I was just riding a wave.\" So, as a response, he hasn't really done any acting for 4 years. He said: \u2018If this time away is gonna be damaging to my career, then so be it. If I can come back afterward and claw my way back in, then maybe I\u2019ll feel like I earned it.'\u201d The treatment of impostor syndrome includes group sittings where the patients discuss their lives and come to a sudden realization that they are not alone and this this not an individual case but a common pattern among high achieving people. As they are also very keen on dismissing praise and kind words, they are instructed to be more vigilant about doing that, and try to take in all the nourishment they get from their colleagues. These are the more common ways to treat this serious condition that poisons so many people's minds.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=YPpIWQnufu8",
        "paper_link": "http://www.suzanneimes.com/wp-content/uploads/2012/09/Imposter-Phenomenon.pdf",
        "paper_title": "The imposter phenomenon in high achieving women: Dynamics and therapeutic intervention"
    },
    {
        "video_id": "u3C4zkxNtok",
        "video_title": "Biophysical Skin Aging Simulations | Two Minute Papers #45",
        "position_in_playlist": 232,
        "description": "The faithful simulation of human skin is incredibly important both in computer games, the movie industry, and also in medical sciences. The appearance of our face is strongly determined by the underlying structure of our skin. Human skin changes significantly with age. Scientists at the University of Zaragoza came up with a really cool, fully-fledged biophysically-based model that opens up the possibility of simply specifying intuitive parameters like age, gender, skin type, and get, after some processing, a much lighter skin representation ready to generate photorealistic rendered results in real time. \n\n_________________________\n\nThe paper \"A Biophysically-Based Model of the Optical Properties of Skin Aging\" is available here:\nhttp://giga.cps.unizar.es/~ajarabo/pubs/skinAgingEG15/\nThe thumbnail image was taken from this work.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The faithful simulation of human skin is incredibly important both in computer games, the movie industry, and also in medical sciences. The appearance of our face is strongly determined by the underlying structure of our skin. Human skin changes significantly with age. It becomes thinner and more dry, while the concentration of chromophores, the main skin pigments diminishes and becomes more irregular. Those pigment concentrations are determined by our age, gender, skin type and even external factors like, for example exposition to UV radiation or our smoking habits. As we age, the outermost layer of our skin, the epidermis thins, the melanine, haemoglobin and water concentration levels drop over time. As you could image having a plausible simulation considering all the involved actors is fraught with difficulties. Scientists at the University of Zaragoza came up with a really cool, fully-fledged biophysically-based model that opens up the possibility of simply specifying intuitive parameters like age, gender, skin type, and get, after some processing, a much lighter skin representation ready to generate photorealistic rendered results in real time. Luckily, one can record diffusion profiles, also called scattering profiles that tell us the color of light that is reflected by our skin. In this image, above, you can see a rendered image and the diffusion profiles of a 30 and an 80 year old person. The idea is the following: you specify intuitive inputs like age and skin type, then run a detailed simulation once, that creates these diffusion profiles that you can use forever in your rendering program. And all this is done in a way that is biophysically impeccable. I was sure that there was some potential in this topic, but when I first saw these results, they completely crushed my expectations. Excellent piece of work! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u3C4zkxNtok",
        "paper_link": "http://giga.cps.unizar.es/~ajarabo/pubs/skinAgingEG15/",
        "paper_title": "A Biophysically-Based Model of the Optical Properties of Skin Aging"
    },
    {
        "video_id": "AHl2JjGsu0s",
        "video_title": "Extrapolations and Crowdfunded Research (Experiment) | Two Minute Papers #44",
        "position_in_playlist": 233,
        "description": "What is extrapolation? Extrapolation basically means continuing lines (or connecting dots, if you like this intuition better). A good example is when we have data for something from the last few days or years, and would like to have a forecast for the future.\n\nWe will do some linear and nonlinear extrapolations (and learn what they mean) and try to find out the amount of money Experiment will \nraise for open research. Experiment is a cool new startup that is trying to accelerate progress in research by crowdsourcing it. \n\n______________________\n\nLogarithmic growth examples from the comments:\n- athletic training - at first, you make great improvements, then as you approach the limits of your endurance, progress slows down, and eventually stops (Morten Eriksen),\n- The approximate number of Olympic records on the men's 100 m sprint (RelatedGiraffe),\n- Bacterial growth. At first, there is a lot of sugar to feed bacteria but there simply aren't that many bacteria and they split as fast as they possibly can, roughly doubling each time step. But eventually the limits of the available sugar become apparent and newly born bacteria either don't find enough nutrition to split again or they outright starve. Inevitably you run into a balance where about as many bacteria die as are born and thus the population growth runs flat (Kram).\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nExperiment, crowdsourcing research:\nhttps://experiment.com/\n\nLinks to Wolfram|Alpha to reproduce the experiments (\nLinear fit: http://www.wolframalpha.com/input/?i=linear+fit+52700,+527197,+766924,+3856542\nQuadratic fit: http://www.wolframalpha.com/input/?i=quadratic+fit+52700,+527197,+766924,+3856542\nLogarithmic fit: http://www.wolframalpha.com/input/?i=logarithmic+fit+52700,+527197,+766924,+3856542\nPlot ALL the functions! http://www.wolframalpha.com/input/?i=plot+sqrt%28x%29+and+x+and+x%5E2+and+e%5Ex-2+where+x+%3D+0..5+y%3D0..4\n\nOne more great image to explain the concept of sublinear and superlinear:\nhttp://deliveryimages.acm.org/10.1145/2720000/2719919/figs/f1.jpg\n\nThe good old xkcd: https://xkcd.com/605/\n\nThe thumbnail image background was created by NIAID (CC BY 2.0): https://flic.kr/p/rg1p9H\nAnimation at the start (MIT license): https://www.shadertoy.com/view/llXSD7\nPregnant lady image by Tobias Lindman (CC BY 2.0): https://flic.kr/p/nhZ7Yh\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What is extrapolation? We hear the term a lot, so let's try to learn what's behind it. Despite the complicated definitions that are out there, extrapolation basically means continuing lines. A good example is when we have data for something from the last few days or years, and would like to have a forecast for the future. We'll jump right into an example just give me a second to build this up. It's going to make sense in the end, I promise! So, in many fields of science, it is really difficult to get research projects funded. Experiment is a cool new startup that is trying to accelerate progress in research by crowdsourcing it. It doesn't get simpler than this system: scientists pitch their research project plan, and kindhearted people pledge a one time donation to help their cause. It is like kickstarter for research. Some of the newer funded projects include growing food in space, developing an open protocol for insulin production, and, of course, a mandatory cat project that includes sequencing the genome of rare mutations. Crowdfunding research is such a terrific idea, and I tell you, these guys are really doing it right. The startup has been founded in 2012, and people pledged 52 thousand dollars that year. The next year, ten times that, and they have kept a steady and quite impressive growth ever since. In 2015, they raised almost $4 million dollars for open research. It's AMAZING! Ok, so a nice extrapolation problem - how much can they expect to raise next year, in 2016? Before we start, we have to be extremely sure to extrapolate only if we're reasonably sure about the nature of the trends and that they won't change significantly in the near future. With that out of the way, let's do a linear extrapolation. Linear means that growth follows a straight line. So, we put these dots on a paper, and try to connect them with a line. Now, we take the mathematical description of this line, and substitute something in it. Since we have 4 years of data, 4 dots, we would be interested in the location of the fifth point, which is the amount of raised money in 2016. So let's do it! Ten to the sixth is one million, so this says that we can expect 4.2 million dollars. Great! But let's be a more optimistic, and do a superlinear extrapolation. Superlinear means that the the rate of growth is not a straight line, but something that is accelerating in time. If this assumption is true, we can expect them to raise way more, 7.4 million dollars. A bit more pessimistic solution would be a sublinear extrapolation. Sublinear means that growth slows down in time. This kind of growth is described well with, for instance, the logarithm function. This effect is also often called the effect of diminishing returns. A good example of this is the skill level of Google DeepMind's artificial intelligence program that plays Go. As we add more and more computational resources, the algorithm gets better and better at the game, but after a point, there is only so much one can learn, therefore progress slows down, and eventually gets close to stopping. There are so many examples of this effect in our lives - if you have some great examples of logarithmic growth, let me know in the comments section, I'll include the best ones in the video description box. According to this logarithm, we can expect the company to raise less than the previous estimations, 3.1 million dollars next year. Sorry guys! A common pitfall in popular media is that the mathematically untrained minds almost always assume a linear growth due to its simplicity. This can lead to hilariously wrong results. If you would extrapolate the size of the belly of a pregnant woman after 9 months. Your conclusion would be \"run, because she's going to explode!\" - whereas we know that a baby is going to be born and she is going to get back in shape. If I had zero wives yesteday and it's my wedding day today, I will sure as hell have a couple dozen wives by next month! Many things are inherently non-linear, and doing a simple linear extrapolation often doesn't do justice to the problem at hand. Bear in mind that there are many different ways to connect a bunch of dots! Let's try to find out why we had wildly varying results. This is due to the fact that we only had 4 samples, that means 4 dots. If I plot these possible functions that we've been talking about, we get the following. It seems that the further we go, the more they diverge. However, in this case, if we have data only between zero and one, for instance, there is very little difference between a wild, exponential function and a very conservative, square root-based growth (you can also imagine your logarithm here). The more dots we have, the more we can distinguish the nature of our growth. And, an educated mind has to take into consideration that many phenomena are inherently non-linear. If you catch someone doing a linear exploration, always ask: \"Are you sure that the process you're modeling is indeed linear? And do you have enough data to prove that?\" That's all for today. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=AHl2JjGsu0s"
    },
    {
        "video_id": "j9FLOinaG94",
        "video_title": "Breaking Deep Learning Systems With Adversarial Examples | Two Minute Papers #43",
        "position_in_playlist": 234,
        "description": "Artificial neural networks are computer programs that try to approximate what the human brain does to solve problems like recognizing objects in images. In this piece of work, the authors analyze the properties of these neural networks and try to unveil what exactly makes them think that a paper towel is a paper towel, and, building on this knowledge, try to fool these programs.  Carefully crafted adversarial examples can be used to fool deep neural network reliably.\n\n_______________\n\nThe paper \"Intriguing properties of neural networks\" is available here:\nhttp://arxiv.org/abs/1312.6199\n\nThe paper \"Explaining and Harnessing Adversarial Examples\" is available here:\nhttp://arxiv.org/abs/1412.6572\n\nImage credits:\nThumbnail image - https://www.flickr.com/photos/healthblog/8384110298 (CC BY-SA 2.0)\nShower cap - Code Words / Julia Evans - https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture\nMNIST - hxhl95\n\nAndrej Karpathy's online convolutional neural network:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Artificial neural networks are computer programs that try to approximate what the human brain does to solve problems like recognizing objects in images. In this piece of work, the authors analyze the properties of these neural networks and try to unveil what exactly makes them think that a paper towel is a paper towel, and, building on this knowledge, try to fool these programs. Let's have a look at this example. One can grab this input image, and this noise pattern, and add these two images together similarly as one would add two numbers together. The operation yields the image you see here. I think it's fair to say that the difference is barely perceptible for the human eye. Not so much for neural networks, because the input image we started with is classified correctly as a bus, and the image that you see on the right is classified as an ostrich. In simple terms, bus + noise equals an ostrich. The two images look almost exactly the same, but the neural networks see them quite differently. We call these examples adversarial examples because they are designed to fool these image recognition programs. In machine learning research, there are common datasets to test different classification techniques on, one of best known example is the MNIST handwriting dataset. It is a basically a bunch of images depicting handwritten numbers that machine learning algorithms have to recognize. Long ago, this used to be a difficult problem, but nowadays, any half-decent algorithm can guess the numbers correctly more than 99% of the time after learning for just a few seconds. Now we'll see that these adversarial examples are not created by chance: if we add a lot of random noise to these images, they get quite difficult to recognize. Let's engage in modesty and say that I, myself, as a human can recognize approximately half of them, but only if I look closely and maybe even squint. A neural network can guess this correctly approximately 50% of the time as well, which is a quite respectable result. Therefore, adding random noise is not really fooling the neural networks. However, if you look at these adversarial examples in the even columns, you see how carefully they are crafted as they look very similar to the original images, but the classification accuracy of the neural network on these examples is 0%. You heard it correctly. It gets it wrong basically all the time. The take home message is that carefully crafted adversarial examples can be used to fool deep neural network reliably. You can watch them flounder on many hilarious examples to your enjoyment. \"My dear sir, the Queen wears a shower cap you say? I beg your pardon?\" If you would like to support Two Minute Papers, we are available on Patreon and offer cool perks for our Fellow Scholars - for instance, you can watch each episode around 24 hours in advance, or even decide the topic of the next episodes. How cool is that?! If you're interested, just click on the box below on the screen. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=j9FLOinaG94",
        "paper_link": "http://arxiv.org/abs/1312.6199\n\nThe paper \"Explaining and Harnessing Adversarial Examples\" is available here:",
        "paper_title": "Intriguing properties of neural networks"
    },
    {
        "video_id": "IFmj5M5Q5jg",
        "video_title": "How DeepMind Conquered Go With Deep Learning (AlphaGo) | Two Minute Papers #42",
        "position_in_playlist": 235,
        "description": "This time around, Google DeepMind embarked on a journey to write an algorithm that plays Go. Go is an ancient chinese board game where the opposing players try to capture each other's stones on the board. Behind the veil of this deceptively simple ruleset, lies an enormous layer of depth and complexity. As scientists like to say, the search space of this problem is significantly larger than that of chess. So large, that one often has to rely on human intuition to find a suitable next move, therefore it is not surprising that playing Go on a high level is, or maybe was widely believed to be intractable for machines. The result is Google DeepMind's AlphaGo, the deep learning technique that defeated a professional player and European champion, Fan Hui.\n\n__________________\n\nThe paper \"Mastering the Game of Go with Deep Neural Networks and Tree Search\" is available here:\nhttps://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf\nhttp://www.nature.com/nature/journal/v529/n7587/full/nature16961.html\n\nWired's coverage of AlphaGo:\nhttp://www.wired.com/2016/01/in-a-huge-breakthrough-googles-ai-beats-a-top-player-at-the-game-of-go/\n\nVideo coverage from DeepMind and Nature:\nhttps://www.youtube.com/watch?v=g-dKXOlsf98\nhttps://www.youtube.com/watch?v=SUbqykXVx0A\n\nMyungwan Kim analysis: https://www.youtube.com/watch?v=NHRHUHW6HQE\n\nPhoto credits:\nWatson - AP Photo/Jeopardy Productions, Inc.\nFan Hui match photo - Google DeepMind - https://www.youtube.com/watch?v=SUbqykXVx0A\n\nGo board image credits (all CC BY 2.0):\nRenato Ganoza - https://flic.kr/p/7nX4kK\nJaro Larnos (changes were applied, mostly recoloring) - https://flic.kr/p/dDeQU9\nLuis de Bethencourt - https://flic.kr/p/4c5RaR\n\nDetailed analysis of the games against Fan Hui and some more speculation:\nhttps://www.reddit.com/r/MachineLearning/comments/43fl90/synopsis_of_top_go_professionals_analysis_of/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In 1997, the news took the world by storm - Garry Kasparov, world champion and grandmaster chess player was defeated by an artificial intelligence program by the name Deep Blue. In 2011, IBM Watson won first place in the famous American Quiz Show, Jeopardy. In 2014, Google DeepMind created an algorithm that that mastered a number of Atari games by working on raw pixel input. This algorithm learned in a similar way as a human would. This time around, Google DeepMind embarked on a journey to write an algorithm that plays Go. Go is an ancient chinese board game where the opposing players try to capture each other's stones on the board. Behind the veil of this deceptively simple ruleset, lies an enormous layer of depth and complexity. As scientists like to say, the search space of this problem is significantly larger than that of chess. So large, that one often has to rely on human intuition to find a suitable next move, therefore it is not surprising that playing Go on a high level is, or maybe was widely believed to be intractable for machines. This chart shows the skill level of previous artificial intelligence programs. The green bar is shows the skill level of a professional player used as a reference. The red bars mean that these older techniques required a significant starting advantage to be able to contend with human opponents. As you can see, DeepMind's new program's skill level is well beyond most professional players. An elite pro  player and European champion Fan Hui was challenged to play AlphaGo, Google DeepMind's newest invention and got defeated in all five matches they played together. During these games, each turn it took approximately 2 seconds for the algorithm to come up with the next move. An interesting detail is that these strange black bars show confidence intervals, which means that the smaller they are, the more confident one can be in the validity of the measurements. As you can see, these confidence intervals are much shorter for the artificial intelligence programs than the human player, likely because one can fire up a machine and let it play a million games, and get a great estimation of its skill level, while the human player can only play a very limited number of matches. There is still a lot left to be excited for, in March, the algorithm will play a world champion. The rate of improvement in artificial intelligence research is accelerating at a staggering pace. The only question that remains is not if something is possible, but when it will become possible. I wake up every day excited to read the newest breakthroughs in the field, and of course, trying to add some leaves to the tree of knowledge with my own projects. I feel privileged to be alive in such an amazing time. As always, there's lots of references in the description box, make sure to check them out. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=IFmj5M5Q5jg",
        "paper_link": "https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf",
        "paper_title": "Mastering the Game of Go with Deep Neural Networks and Tree Search"
    },
    {
        "video_id": "ZaFqvM1IsP8",
        "video_title": "What Do Virtual Objects Sound Like? | Two Minute Papers #41",
        "position_in_playlist": 236,
        "description": "In many episodes about computer graphics, we explored works on how to simulate the motion and the collision of different bodies. However, sounds are just as important as visuals, and there are really cool techniques out there that take the geometry and material description of such objects and they simulate how smashing them together would sound like. What is really cool is that the technique also offers editing capabilities. You compute a simulation only once, and then, edit and explore as much as you desire.\n\n__________________________\n\nThe paper \"Interactive Acoustic Transfer Approximation for Modal Sound \" is available here:\nhttp://www.cs.columbia.edu/cg/transfer/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In many episodes about computer graphics, we explored works on how to simulate the motion and the collision of different bodies. However, sounds are just as important as visuals, and there are really cool techniques out there that take the geometry and material description of such objects and they simulate how smashing them together would sound like. This one is a more sophisticated method which is not only faster than previous works, but can simulate a greater variety of materials, and we can also edit the solutions without needing to recompute the expensive equations that yield the sound as a result. The faster part comes from a set of optimizations, most importantly something that is called mesh simplification. This means that the simulations are done not on the original, but vastly simplified shapes. The result of this simplified simulation is close to indistinguishable from the real deal, but is considerably cheaper to compute. What is really cool is that the technique also offers editing capabilities. You compute a simulation only once, and then, edit and explore as much as you desire. The stiffness and damping parameters can be edited without any additional work. Quite a few different materials can be characterized with this. The model can also approximate a quite sophisticated phenomenon where the frequency of a sound is changing in time. One can, for instance, specify stiffness values that vary in time to produce these cool frequency shifting effects. It is also possible to exaggerate or dampen different frequencies of these sound effects, and the results are given to you immediately. This is meeting all my standards. Amazing piece of work. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZaFqvM1IsP8",
        "paper_link": "http://www.cs.columbia.edu/cg/transfer/",
        "paper_title": "Interactive Acoustic Transfer Approximation for Modal Sound "
    },
    {
        "video_id": "KgIrnR2O8KQ",
        "video_title": "Simulating Viscosity and Melting Fluids | Two Minute Papers #40",
        "position_in_playlist": 237,
        "description": "In this series, we have studied fluid simulations extensively. But we haven't talked about one important quantity that describes a fluid, and this quantity is none other than viscosity. Viscosity means the resistance of a fluid against deformation. The large viscosity of honey makes it highly resistant to deformation, and this is responsible for its famous and beautiful coiling effect. Water, however, does not have a lot of objections against deformations, making it so easy to pour it into a glass. With this piece of work, it is possible to efficiently simulate the motion of fluids, and it supports the simulation of a large range of viscosities.\n\n__________________________\n\nThe paper \"An Implicit Viscosity Formulation for SPH Fluids\" is available here:\nhttp://cg.informatik.uni-freiburg.de/publications/2015_SIGGRAPH_viscousSPH.pdf\n\nRecommended for you:\nPainting with Fluid Simulations - https://www.youtube.com/watch?v=1aVSb-UbYWc\nModeling Colliding and Merging Fluids - https://www.youtube.com/watch?v=uj8b5mu0P7Y\nAdaptive Fluid Simulations - https://www.youtube.com/watch?v=dH1s49-lrBk\n\nVideo source:\nSmarter Every Day - https://www.youtube.com/watch?v=zz5lGkDdk78 \n\nThe thumbnail image was created by Dino Giordano (CC BY 2.0) - https://flic.kr/p/4p9z4w\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we have studied fluid simulations extensively. But we haven't talked about one important quantity that describes a fluid, and this quantity is none other than viscosity. Viscosity means the resistance of a fluid against deformation. The large viscosity of honey makes it highly resistant to deformation, and this is responsible for its famous and beautiful coiling effect. Water, however, does not have a lot of objections against deformations, making it so easy to pour it into a glass. With this piece of work, it is possible to efficiently simulate the motion of fluids, and it supports the simulation of a large range of viscosities. Viscosities can also change in time. For instance, physicists know that raising the temperature will make the viscosity of fluids decrease, which leads to melting, therefore decreasing the viscosity in time will lead to a simulation result that looks exactly like melting. The technique also supports two-way coupling where the objects have effects on the fluid and vice versa. One can also put multiple fluids with different densities and viscosities into the same domain and see how they duke it out. This is exactly what people need in the industry: robust techniques that work for small and large scale simulations with multiple objects, and material settings that can possibly change in time. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=KgIrnR2O8KQ",
        "paper_link": "http://cg.informatik.uni-freiburg.de/publications/2015_SIGGRAPH_viscousSPH.pdf",
        "paper_title": "An Implicit Viscosity Formulation for SPH Fluids"
    },
    {
        "video_id": "eI_QUtgJHH8",
        "video_title": "Interactive Editing of Subsurface Scattering | Two Minute Papers #39",
        "position_in_playlist": 238,
        "description": "Subsurface scattering is a technique to model light transport not between surfaces, but volumes - it therefore enables rendering digital images of human skin, marble, milk, and many other translucent materials. This piece of work takes this a step beyond, and offers a compelling solution to editing subsurface scattering.\n\n______________________________\n\nThe paper \"Interactive Albedo Editing in Path-Traced Volumetric Materials\" is available here:\nhttp://graphics.berkeley.edu/papers/Milos-IAE-2013-02/\n\nRecommended for you:\nMore on subsurface scattering - https://www.youtube.com/watch?v=qyDUvatu5M8&feature=youtu.be&t=13m2s\n\nImage credits (CC-BY):\nhttps://flic.kr/p/9RCYEw\nhttps://flic.kr/p/38fLAH\nhttps://flic.kr/p/5EP5bw\nhttps://en.wikipedia.org/wiki/Subsurface_scattering#/media/File:Skin_Subsurface_Scattering.jpg\n\nBlender scene file for the burning flame:\nhttp://www.blendswap.com/blends/view/74722\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Subsurface scattering means that a portion of light that hits a translucent material does not bounce back from the surface but penetrates it and scatters many-many times inside the material. Now, if you have a keen eye, you recognize that there are a lot of materials in real life that have subsurface scattering. Many don't know, but our skin is a great example of that, and so is marble, milk, wax, plant leaves, apple and many others. If you would like to hear a bit more about subsurface scattering, check the second part of the video that you see recommended in the corner of this window, or just click it in the description box below. Subsurface scattering looks unbelievably beautiful, but at the same time, it is very expensive because we have to simulate up to thousands and thousands of scattering events for every ray of light. It really takes forever. And if you'd like to tweak your material settings just a bit because the result is not a 100% up to your taste, you have to recreate, or what graphics people like to say, re-render these images. It's not really a convenient workflow. This piece of work offers a great solution where you have to wait a bit longer than you would wait for one image, but only once, because it runs a generalized light simulation, and after that, whatever changes you apply to your materials, you will see immediately. You can also paint the reflectance properties of this material that we call albedos and get results with full subsurface scattering immediately. Here is another interactive editing workflow where you get results instantaneously, and the result with this technique is indistinguishable from the real deal, which would be re-rendering this result image every time some adjustment is made. With this technique, you can really create the materials you thought up in a fraction of the time of the classical workflow. Spectacular work. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=eI_QUtgJHH8",
        "paper_link": "http://graphics.berkeley.edu/papers/Milos-IAE-2013-02/",
        "paper_title": "Interactive Albedo Editing in Path-Traced Volumetric Materials"
    },
    {
        "video_id": "_r-eIKkyAco",
        "video_title": "3D Printing Objects With Caustics | Two Minute Papers #38",
        "position_in_playlist": 239,
        "description": "What are caustics? A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. This technique makes it possible to essentially imagine any kind of caustic pattern, for instance, this brain pattern, and it will create the model that will cast caustics that look exactly like that. It also works with sunlight, and you can also choose different colors for your caustics. The authors found their simulations to be in good agreement with reality, therefore the desired caustic patterns can be fabricated faithfully.\n\n___________________________\n\nThe paper \"High-contrast Computational Caustic Design\" is available here:\nhttp://chateaunoir.net/caustics.html\n\nThe full Rendering course at the TU Wien is available here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nMore results from this project are available here:\nhttp://rayform.ch/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nImage credits (all CC-BY):\nhttps://en.wikipedia.org/wiki/Caustic_(optics)\nhttps://www.flickr.com/photos/fdecomite/2486275725\nhttps://flic.kr/p/pamCiP\nhttps://flic.kr/p/nD7Ex\nhttps://flic.kr/p/iJUi3\nhttps://flic.kr/p/8DvPiz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What are caustics? A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. It looks majestic, and it is the favorite effect of most light transport researchers. You can witness it around rings, plastic bottles or when you're underwater just to name a few examples. If you have a powerful algorithm at hand that can simulate many light transport effects than you can expect to get some caustics forming in the presence of curved refractive or reflective surfaces and small light sources. If you would like to know more about caustics, I am holding an entire university course at the Technical University of Vienna, the entirety of which we have recorded live on video for you. It is available for everyone free of charge, if you're interested, check it out, a link is available in the description box. Now, the laws that lead to caustics are well understood, therefore we can not only put some objects on a table and just enjoy the imagery of the caustics, but we can turn the whole thing around: this technique makes it possible to essentially imagine any kind of caustic pattern, for instance, this brain pattern, and it will create the model that will cast caustics that look exactly like that. We can thereby design an object by its caustics. It also works with sunlight, and you can also choose different colors for your caustics. This result with an extremely high fidelity image of Albert Einstein and his signature shows that first, a light transport simulation is run, and then the final solution can be 3D printed. I am always adamantly looking for research works where we have a simulation that relates to and tells us something new about the world around us. This is a beautiful example of that. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_r-eIKkyAco",
        "paper_link": "http://chateaunoir.net/caustics.html",
        "paper_title": "High-contrast Computational Caustic Design"
    },
    {
        "video_id": "ImIaoKsjgUE",
        "video_title": "Designing 3D Printable Robotic Creatures | Two Minute Papers #37",
        "position_in_playlist": 240,
        "description": "This episode covers a paper from Disney Research on how to design 3D printable robots. In order to get a robot from A to B, one has to specify scientific attributes like trajectories and angular velocities. But people don't think in angular velocities, they think in intuitive actions, like moving forward, sideways, or even the style of a desired movement. Specifying these things instead would be much more useful, but also, scientifically quite challenging. \n\nOne can specify the design of the robot, for instance, different shapes, motor positions, and joints can be added, and the technique finds out a physically plausible way for them to walk and move around.\n\n____________________________\n\nThe paper \"Interactive Design of 3D Printable Robotic Creatures\" is available here:\nhttps://www.disneyresearch.com/publication/interactive-design-of-3d-printable-robotic-creatures/\n\nRecommended for you:\n- Hydrographic Printing (in 3D)\nhttps://www.youtube.com/watch?v=kLnG073NYtw\n- 3D Printing a Glockenspiel\nhttps://www.youtube.com/watch?v=2kOCTf8jIik\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background image was taken from the paper linked above.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. 3D printing is a rapidly progressing research field. One can create colorful patterns that we call textures on different figures with Computational Hydrographic Printing, just to name one of many recent inventions. We can 3D print teeth, action figures, prosthetics, you name it. Ever thought of how cool it would be to design robots on your computer digitally and simply printing them. Scientists at Disney Research had just made this dream come true. I fondly remember my time working at Disney Research, where robots like these were walking about. I remember a specific guy, that, well, wasn't really kind and waved at me, it has blocked the path to one of the labs I had to enter. It might have been one of these guys in this project. Disney Research has an incredible atmosphere with so many talented people, it's an absolutely amazing place. So, in order to get a robot from A to B, one has to specify scientific attributes like trajectories and angular velocities. But people don't think in angular velocities, they think in intuitive actions, like moving forward, sideways, or even the style of a desired movement. Specifying these things instead would be much more useful. That sounds great and all, but this is a quite difficult task. If one specifies a high level action, like walking sideways, then the algorithm has to find out what body parts to move, how, which motors should be turned on and when, which joints to turn, where is the center of pressure, center of mass, and many other factors have to be taken into consideration. This technique offers a really slick solution to this, where we don't just get a good result, but we can also have our say on what should the order of steps be. And even more, our stylistic suggestions are taken into consideration. One can also change the design of the robot, for instance, different shapes, motor positions, and joints can be specified. The authors ran a simulation for these designs and constraints, and found them to be in good agreement with reality. This means that whatever you design digitally can be 3D printed with off the shelf parts and brought to life, just as you see them on the screen. The technique supports an arbitrary number of legs and is robust to a number of different robot designs. Amazing is as good of a word as I can find. The kids of the future will be absolutely spoiled with their toys, that's for sure, and I'm perfectly convinced that there will be many other other applications, and these guys will help us solve problems that are currently absolutely inconceivable for us. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ImIaoKsjgUE",
        "paper_link": "https://www.disneyresearch.com/publication/interactive-design-of-3d-printable-robotic-creatures/",
        "paper_title": "Interactive Design of 3D Printable Robotic Creatures"
    },
    {
        "video_id": "kMa_B3wLxAM",
        "video_title": "Designing Cities and Furnitures With Machine Learning | Two Minute Papers #36",
        "position_in_playlist": 241,
        "description": "Creating geometry for a computer game or a movie is a very long and arduous task. For instance, if we would like to populate a virtual city with buildings, it would cost a ton of time and money and of course, we would need quite a few artists. This piece of work solves this problem in a very elegant and convenient way: it learns the preference of the user, then creates and recommends a set of solutions that are expected to be desirable. The weapon of choice to accomplish this was Gaussian Process Regression.\n\n___________________________________\n\nThe paper \"Interactive Design of Probability Density Functions for Shape Grammars\" is available here:\nhttp://lgg.epfl.ch/publications/2015/proman/index.php\n\nThe thumbnail image was created by See-ming Lee (nice name, btw!) (CC BY 2.0) - https://flic.kr/p/oewqwn\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Creating geometry for a computer game or a movie is a very long and arduous task. For instance, if we would like to populate a virtual city with buildings, it would cost a ton of time and money and of course, we would need quite a few artists. This piece of work solves this problem in a very elegant and convenient way: it learns the preference of the user, then creates and recommends a set of solutions that are expected to be desirable. In this example, we are looking for tables with either one leg or crossing legs. It should also be properly balanced, therefore if we see any of these criteria, we'll assign a high score to these models. These are the preferences that the algorithm should try to learn. The orange bars show the predicted score for new models created by the algorithm - a larger value means that the system expects the user to score these high, and the blue bars mean the uncertainty. Generally, we're looking for solutions with large orange and small blue bars, this means that the algorithm is confident that a given model is in line with our preferences. And we see exactly what were looking for - novel, balanced table designs with one leg or crossed legs. Interestingly, since we have these uncertainty values, one can also visualize counterexamples where the algorithm is not so sure, but would guess that we wouldn't like the model. It's super cool that it is aware how horrendous these designs looks. It may have a better eye than many of the contemporary art curators out there. There are also examples where the algorithm is very confident that we're going to hate a given example because of its legs or unbalancedness, and would never recommend such a model. So indirectly, it also learns how a balanced piece of furniture should look like, without ever learning the concept of gravity or doing any kind of architectural computation. The algorithm also works on buildings, and after learning our preferences, it can populate entire cities with geometry that is in line with our artistic vision. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kMa_B3wLxAM",
        "paper_link": "http://lgg.epfl.ch/publications/2015/proman/index.php",
        "paper_title": "Interactive Design of Probability Density Functions for Shape Grammars"
    },
    {
        "video_id": "Bui3DWs02h4",
        "video_title": "9 Cool Deep Learning Applications | Two Minute Papers #35",
        "position_in_playlist": 242,
        "description": "Machine learning provides us an incredible set of tools. If you have a difficult problem at hand, you don't need to hand craft an algorithm for it. It finds out by itself what is important about the problem and tries to solve it on its own. In this video, you'll see a number of incredible applications of different machine learning techniques (neural networks, deep learning, convolutional neural networks and more).\n\nNote: the fluid simulation paper is using regression forests, which is a machine learning technique, but not strictly deep learning. There are variants of it that are though (e.g., Deep Neural Decision Forests).\n________________________\n\nThe paper \"Toxicity Prediction using Deep Learning\" and \"Prediction of human population responses to toxic compounds by a collaborative competition\" are available here:\nhttp://arxiv.org/pdf/1503.01445.pdf\nhttp://www.nature.com/nbt/journal/v33/n9/full/nbt.3299.html\n\nThe paper \"A Comparison of Algorithms and Humans For Mitosis Detection\" is available here:\nhttp://people.idsia.ch/~juergen/deeplearningwinsMICCAIgrandchallenge.html\nhttp://people.idsia.ch/~ciresan/data/isbi2014.pdf\n\nKaggle-related things:\nhttp://kaggle.com\nhttps://www.kaggle.com/c/dato-native\nhttp://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/\n\nThe paper \"Deep AutoRegressive Networks\" is available here:\nhttp://arxiv.org/pdf/1310.8499v2.pdf\nhttps://www.youtube.com/watch?v=-yX1SYeDHbg&feature=youtu.be&t=2976\n\nThe furniture completion paper, \"Data-driven Structural Priors for Shape Completion\" is available here:\nhttp://cs.stanford.edu/~mhsung/projects/structure-completion\n\nData-driven fluid simulations using regression forests:\nhttps://graphics.ethz.ch/~sobarbar/papers/Lad15/DatadrivenFluids.mov\nhttps://www.inf.ethz.ch/personal/ladickyl/fluid_sigasia15.pdf\n\nSelfies and convolutional neural networks:\nhttp://karpathy.github.io/2015/10/25/selfie/\n\nMultiagent Cooperation and Competition with Deep Reinforcement Learning:\nhttp://arxiv.org/abs/1511.08779\nhttps://www.youtube.com/watch?v=Gb9DprIgdGw&index=2&list=PLfLv_F3r0TwyaZPe50OOUx8tRf0HwdR_u\nhttps://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player\n\nKaggle automatic essay scoring contest:\nhttps://www.kaggle.com/c/asap-aes\nhttp://www.vikparuchuri.com/blog/on-the-automated-scoring-of-essays/\n\nGreat talks on Kaggle:\nhttps://www.youtube.com/watch?v=9Zag7uhjdYo\nhttps://www.youtube.com/watch?v=OKOlO9nIHUE\nhttps://www.youtube.com/watch?v=R9QxucPzicQ\n\nThe thumbnail image was created by Barn Images - https://flic.kr/p/xxBc94\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. There are so many applications of deep learning, I was really excited to put together a short, but really cool list of some of the more recent results for you Fellow Scholars to enjoy. Machine learning provides us an incredible set of tools. If you have a difficult problem at hand, you don't need to hand craft an algorithm for it. It finds out by itself what is important about the problem and tries to solve it on its own. In many problem domains, they perform better than human experts. What's more, some of these algorithms find out things that could earn you a PhD with 10 years ago. Here goes the first stunning application: toxicity detection for different chemical structures by means of deep learning. It is so efficient that it could find toxic properties that previously required decades of work by humans who are experts of their field. Next one. Mitosis detection from large images. Mitosis means that cell nuclei are undergoing different transformations that are quite harmful, and quite difficult to detect. The best techniques out there are using convolutional neural networks and are outperforming professional radiologists at their own task. Unbelievable. Kaggle is a company that is dedicated to connecting companies with large datasets and data scientists who write algorithms to extract insight from all this data. If you take only a brief look, you see an incredibly large swath of applications for learning algorithms. Almost all of these were believed to be only for humans, very smart humans. And learning algorithms, again, emerge triumphant on many of these. For example, they had a great competition where learning algorithms would read a website and find out whether paid content is disguised there as real content. Next up on the list: hallucination or sequence generation. It looks at different video games, tries to learn how they work, and generates new footage out of thin air by using a recurrent neural network. Because of the imperfection of 3D scanning procedures, many 3D scanned furnitures that are too noisy to be used as is. However, there are techniques to look at these really noisy models and try to figure out how they should look by learning the symmetries and other properties of real furnitures. These algorithms can also do an excellent job at predicting how different fluids behave in time, and are therefore expected to be super useful in physical simulation in the following years. On the list of highly sophisticated scientific topics, there is this application that can find out what makes a good selfie and how good your photos are. If you really want to know the truth. Here is another application where a computer algorithm that we call deep q learning, plays pong, against itself, and eventually achieves expertise. The machines are also grading student essays. At first, one would think that this cannot possibly be a good idea. As it turns out, their judgement is more consistent with the reference grades than any of the teachers who were tested. This could be an awesome tool for saving a lot of time and assisting the teachers to help their students learn. This kind of blows my mind. It would be great to take a look at an actual dataset if it is public and the issued grades, so if any of you Fellow Scholars have seen it somewhere, please let me know in the comments section! These results are only from the last few years, and it's really just scratching the surface. There are literally hundreds of more applications we haven't even talked about. We are living extremely exciting times indeed. I am eager to see, and perhaps, be a small part of this progress. There are tons of reading and viewing materials in the description box, check them out! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Bui3DWs02h4",
        "paper_link": "https://www.kaggle.com/c/dato-native\nhttp://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/\n\nThe paper \"Deep AutoRegressive Networks\" is available here:\nhttp://arxiv.org/pdf/1310.8499v2.pdf",
        "paper_title": "Toxicity Prediction using Deep Learning"
    },
    {
        "video_id": "B70tT4WMyJk",
        "video_title": "Neural Programmer-Interpreters Learn To Write Programs | Two Minute Papers #34",
        "position_in_playlist": 243,
        "description": "In machine learning, we usually have a set of problems for which we are looking for solutions. For instance, \"here is an image, please tell me what is seen on it\". Or, \"here is a computer game, please beat level three\". One problem, one solution. In this case, we are not looking for one solution, we are looking for a computer program, an algorithm, that can solve any number of problems of the same kind. It can also learn how to rotate images of different cars around to obtain a frontal pose. This technique can learn from someone how to sort a set of 20 numbers and generalize its knowledge to much longer sequences.\n\n______________________\n\nThe paper \"Neural Programmer-Interpreters\" is available here:\nhttp://www-personal.umich.edu/~reedscot/iclr_project.html\n\nThe thumbnail image was created by Iwan Gabovitch (CC BY 2.0) - https://flic.kr/p/paxzB9\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. What could be a more delightful way to celebrate new year's eve than reading about new breakthroughs in machine learning research! Let's talk about an excellent new paper from the Google DeepMind guys. In machine learning, we usually have a set of problems for which we are looking for solutions. For instance, here is an image, please tell me what is seen on it. Here is a computer game, please beat level three. One problem, one solution. In this case, we are not looking for one solution, we are looking for a computer program, an algorithm, that can solve any number of problems of the same kind. This work is based on a recurrent neural network, which we discussed in a previous episode - in short, it means that it tries to learn not one something but a sequence of things, and in this example, it learns to add two large numbers together. As a big number can be imagined as a sequence of digits, this can be done through a sequence of operations - it first reads the two input numbers and then carries out the addition, keeps track of carrying digits, and goes on to the next digit. On the right, you can see the individual commands executed in the computer program it came up with. It can also learn how to rotate images of different cars around to obtain a frontal pose. This is also a sequence of rotation actions until the desired output is reached. Learning more rudimentary sorting algorithms to put the numbers in ascending order is also possible. One key difference between recurrent neural networks and this is that these neural programmer interpreters are able to generalize better. What does this mean? This means that if the technique can learn from someone how to sort a set of 20 numbers, it can generalize its knowledge to much longer sequences. So it essentially tries to learn the algorithm behind sorting from a few examples. Previous techniques were unable to achieve this, and, as we can see, it can deal with a variety of problems. I am absolutely spellbound by this kind of learning, because it really behaves like a novice human user would: looking at what experts do and trying to learn and understand the logic behind their actions. Happy New Year to all of you Fellow Scholars! May it be ample in joy and beautiful papers, may our knowledge grow according to Moore's law. And of course, may the force be with you. Thanks for watching and for your generous support, and I'll see you next year!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=B70tT4WMyJk",
        "paper_link": "http://www-personal.umich.edu/~reedscot/iclr_project.html",
        "paper_title": "Neural Programmer-Interpreters"
    },
    {
        "video_id": "zzwCbhI2iOA",
        "video_title": "Peer Review #1 [Audio only] | Two Minute Papers",
        "position_in_playlist": 244,
        "description": "I wish a merry Christmas to all of you Fellow Scholars!\n\n__________________\n\nThe technique \"Separable Subsurface Scattering\" was used to create this thumbnail image:\nhttps://cg.tuwien.ac.at/~zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n\nThe thumbnail image was created by Christian Freude.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A quick report on what is going on with Two Minute Papers. We are living extremely busy times as I am still working full time as a doctoral researcher, and we also have a baby on the way. We are currently a bit over 30 episodes in, and I am having an amazing time explaining these concepts and enjoying the ride tremendously. One of the most beautiful aspects of Two Minute Papers is the community forming around it, with extremely high quality comments and lots of civil, respectful discussions. I learned a lot from you Fellow Scholars, thanks for that! Really awesome! The growth numbers are looking amazing for a YouTube channel of this size, and of course, any help in publicity is greatly appreciated. If you're a journalist, and you feel that this is a worthy cause, please, write about Two Minute Papers! If you're a not a journalist, please try showing the series to them! Or just show it to your friends - I am sure that many, many more people would be interested in this, and sharing is also a great way to reach out to new people. The Patreon page is also getting lots of generous support that I would only expect from much, bigger channels. I don't even know if I deserve it. But thanks for hanging in there, I feel really privileged to have supporters like you Fellow Scholars. You're the best. And, we have some amazing times ahead of us. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=zzwCbhI2iOA"
    },
    {
        "video_id": "1aVSb-UbYWc",
        "video_title": "Painting with Fluid Simulations | Two Minute Papers #33",
        "position_in_playlist": 245,
        "description": "As there is a lot of progress in simulating the motion of fluids, and paint is a fluid, then why not simulate the process of painting on a canvas? The simulations with this technique are so detailed that even the bristle interactions are taken into consideration, therefore one can capture artistic brush stroke effects like stabbing. Traditional techniques cannot even come close to simulating such sophisticated effects. \n\n______________________\n\nThe paper \"Wetbrush: GPU-based 3D painting simulation at the bristle level\" is available here:\nhttp://web.cse.ohio-state.edu/~whmin/publications.html\n\nRecommended for you:\nAdaptive Fluid Simulations -\n https://www.youtube.com/watch?v=dH1s49-lrBk&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=1\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image was taken from the mentioned paper.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Some people say that the most boring thing is watching paint dry. They have clearly not seen this amazing research work, that makes it possible to simulate the entire process of painting on a canvas. We have covered plenty of papers in fluid simulations, and this is no exception - I admit that I am completely addicted and just can't help it. Maybe I should seek professional assistance. So, as there is a lot of progress in simulating the motion of fluids, and paint is a fluid, then why not simulate the process of painting on a canvas? The simulations with this technique are so detailed that even the bristle interactions are taken into consideration, therefore one can capture artistic brush stroke effects like stabbing. Stabbing, despite the horrifying name, basically means shoving the brush into the canvas and rotating it around to get cool effect. The fluid simulation part includes paint adhesion and is so detailed that it can capture the well-known impasto style where paint is applied to the canvas in such large chunks, they are so thick that one can see all the strokes that have been made. And all this is done in real-time. Amazing results. Traditional techniques cannot even come close to simulating such sophisticated effects. As it happened many times before in computer graphics: just put such a powerful algorithm into the hands of great artists and enjoy the majestic creations they give birth to. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1aVSb-UbYWc",
        "paper_link": "http://web.cse.ohio-state.edu/~whmin/publications.html",
        "paper_title": "Wetbrush: GPU-based 3D painting simulation at the bristle level"
    },
    {
        "video_id": "ziMHaGQJuSI",
        "video_title": "How Do Genetic Algorithms Work? | Two Minute Papers #32",
        "position_in_playlist": 246,
        "description": "Genetic algorithms are in the class of evolutionary algorithms that build on the principle of \"survival of the fittest\". By recombining the best solutions of a population and every now and then mutating them, one can solve remarkably difficult problems that would otherwise be hopelessly difficult to write programs for.\n\nOne of the first works of genetic algorithms, \"Adaptation in Natural and Artificial Systems\" by John H. Holland:\nhttps://mitpress.mit.edu/books/adaptation-natural-and-artificial-systems\n\n_____________________\n\nA parallel genetic algorithm for the Mona Lisa problem:\nhttps://cg.tuwien.ac.at/~zsolnai/gfx/mona_lisa_parallel_genetic_algorithm/\n\nA parallel, console genetic algorithm for the 0-1 knapsack problem:\nhttps://cg.tuwien.ac.at/~zsolnai/gfx/knapsack_genetic/\n\nJohn Henry Holland, the father of genetic algorithms:\nhttps://en.wikipedia.org/wiki/John_Henry_Holland\n\nTry this out, it's really fun! - http://boxcar2d.com\n\nThe mentioned book is called \"The Blind Watchmaker\" by Richard Dawkins.\n\nThe thumbnail background image was created by Karen Roe (CC BY 2.0) - https://flic.kr/p/ezxAbk\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Genetic algorithms help us solve problems that are very difficult, if not impossible to otherwise write programs for. For instance, in this application, we have to build a simple car model to traverse this terrain. Put some number of wheels on it somewhere, add a set of triangles as a chassis, and off you go. This is essentially the DNA of a solution. The farther it goes, the better the car is, and the goal is to design the best car you possibly can. First, the algorithm will try some random solutions, and, as it has no idea about the concept of a car or gravity, it will create a lot of bad solutions that don't work at all. However, after a point, it will create something that is at least remotely similar to a car, which will immediately perform so much better than the other solutions in the population. A genetic algorithm then creates a new set of solutions, however, now, not randomly. It respects a rule that we call: survival of the fittest. Which means the best existing solutions are taken and mixed together to breed new solutions that are also expected to do well. Like in evolution in nature, mutations can also happen, which means random changes are applied to the DNA code of a solution. We know from nature that evolution works extraordinarily well, and the more we run this genetic optimization program, the better the solutions get. It's quite delightful for a programmer to see their own children trying vigorously and succeeding at solving a difficult task. Even more so if the programmer wouldn't be able to solve this problem by himself. Let's run a quick example. We start with a set of solutions - the DNA of a solution is a set of zeros and ones, which can encode some decision about the solution, whether we turn left or right in a maze, or it can also be an integer or any real number. We then compute how good these solutions are according to our taste, in the example with cars, how far these designs can get. Then, we take, for instance, the best 3 solutions and co mbine them together to create a new DNA Some of the better solutions may remain in the population unchanged. Then, probabilistically, random mutations happen to some of the solutions, which help us explore the vast search space better. Rinse and repeat, and there you have it. Genetic algorithms. I have also coded up a version of Roger Alsing's EvoLisa problem where the famous Mona Lisa painting is to be reproduced by a computer program with a few tens of triangles. The goal is to paint a version that is as faithful to the original as possible. This would be quite difficult for humans, but apparently a genetic algorithm can deal with this really well. The code is available for everyone to learn, experiment, and play with. It's super fun. And if you're interested in the concept of evolution, maybe read the excellent book, The Blind Watchmaker by Richard Dawkins. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ziMHaGQJuSI"
    },
    {
        "video_id": "674DL39dOOQ",
        "video_title": "Randomness and Bell's Inequality [Audio only] | Two Minute Papers #31",
        "position_in_playlist": 247,
        "description": "In this episode, we discuss what makes an event random, and how incredible it is what Bell's theorem (or inequality) has to say about truly random events.\n\nNote: \"local\" means that information from the hidden variable doesn't travel faster than light.\n\n__________________________\n\nThe paper \"On the Einstein Podolsky Rosen Paradox\" is available here:\nhttp://www.drchinese.com/David/Bell_Compact.pdf\nhttp://homepages.physik.uni-muenchen.de/~vondelft/Lehre/09qm/lec21-22-BellInequalities/Bell1964.pdf\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image was created by Giovanni Arteaga (CC BY 2.0) - https://flic.kr/p/8M11b6\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When using cryptography, we'd like to safely communicate over the internet in the presence of third parties. To be able to do this, and many other important applications, we need random numbers. But what does it exactly mean that something is random? Randomness is the lack of any patterns and predictability. People usually use coinflips as random events. But is a coinflip really random? If we had a really smart physicist, who can model all the forces that act upon the coin, he would easily find out whether it's going to be heads or tails. Strictly speaking, a coinflip is therefore not random. What about random numbers generated with computers? Computers are a collection of processing units that run programs. If one knows the program code that generates the random numbers, they are not random anymore, because it doesn't happen by chance, and it is possible to predict. John von Neumann famously said: \"Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin. For, as has been pointed out several times, there is no such thing as a random number \u2014 there are only methods to produce random numbers, and a strict arithmetic procedure of course is not such a method.\" Some websites offer high quality random numbers that are generated from atmospheric noise. Practically speaking, this, of course, sounds adequate enough! If someone wants to break the encryption of our communications, they would have to be able to model the physics and initial conditions of every single thunderbolt, which means processing millions of discharges per day. This is practically impossible. So it seems reasonable to say that random events are considered random because of our ignorance, not because they are, strictly speaking, unpredictable. You just need to be smart enough, and the notion of randomness fades away in the light of your intelligence. Or so it seemed for physicists for a long time. Imagine if someone who has never heard about magnetism would see many magnets attracting each other and some added magnet powder. This person would most definitely say it's magic happening. However, if you know about magnetism, you know that things don't happen randomly, there are very simple laws that can predict all this movement. In this case, magnetic forces we can loosely call a hidden variable. So we have a phenomenon that we cannot predict, and we're keen to say it's random. In reality, it is not, there is just a hidden variable that we don't know of, that is responsible for this behavior. We have the very same phenomenon if we look inside of an atom. Quantum-level effects happen according to the physics of extremely small things, and we again, find behaviors that seem completely random. We know some of the trends, just like we know which roads in our city are expected to have a huge traffic jam every morning, but we cannot predict where every single individual car is heading. We have it the same way with extremely small particles. We're keen to say that a behavior seems completely random, because nothing that we know or measure would explain it. Other people will immediately say, wait - you don't know everything, maybe these quantum effects are not random, as there may be hidden things, hidden variables that you don't know of, which make up for the behavior. We can't just say this or that is random - it is much, much more likely that our knowledge is insufficient to predict what is happening, as electromagnetic forces seemed magical to scientists a few hundred years ago. So is quantum mechanics completely random, or does it only seem random? It is probably one of the most difficult questions ever asked. How can you find out that something you measure that seems random, is really completely random, and not just the act of forces that you don't know of? And hold on to your chair, because this is going to blow your mind. A simple and intuitive statement of Bell's theorem states that. \"No physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics.\" This means that he proved that the behavior scientists experience in quantum mechanics are really random, they cannot be explained by any theory you could possibly make up. Simple one or complicated, doesn't matter. This discovery is absolutely insane. You can definitely prove that a crappy theory someone quickly made up doesn't explain a behavior, but how can you prove that it is completely impossible to build such a theory that does? No matter how hard you try, how smart you are, you can't do it. This is such a mind-bogglingly awesome theorem. And please note that we definitely lose out on some details and generality because of the fact that we use intuitive words to discuss these results, as opposed to the original derivation with covariances between measurements. On our imaginary list of the wonders of the world, monuments created not by the hands, but the minds of humans, this should definitely be among the best them. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=674DL39dOOQ",
        "paper_link": "http://www.drchinese.com/David/Bell_Compact.pdf",
        "paper_title": "On the Einstein Podolsky Rosen Paradox"
    },
    {
        "video_id": "9wOBkJJ-w2s",
        "video_title": "Automatic Parameter Control for Metropolis Light Transport | Two Minute Papers #30",
        "position_in_playlist": 248,
        "description": "Photorealistic rendering (also called global illumination) enables us to see how digital objects would look like in real life. It is an amazingly powerful tool in the hands of a professional artist, who can create breathtaking images or animations with. Metropolis light transport is an advanced photorealistic rendering technique that is remarkably effective at finding the brighter regions of a scene and building many light paths that target these regions. The resulting algorithm is more efficient than traditional random path building algorithms, such as path tracing. This algorithm endeavors to choose an optimal mixture between naive random path sampling techniques (such as path tracing and bidirectional path tracing) and Metropolis Light Transport.\n\n___________________\n\nThe paper \"Automatic Parameter Control for Metropolis Light Transport\" is available here:\nhttps://cg.tuwien.ac.at/~zsolnai/gfx/adaptive_metropolis/\n\nWe thank Kai Schwebke for providing LuxTime, Vlad Miller for the Spheres, Giulio Jiang for the Chess, Aaron Hill for the Cornell Box, Andreas Burmberger for the Cherry Splash and Glass Ball scenes. \n\nI held a course on photorealistic rendering at the Technical University of Vienna. Here you can learn how the physics of light works and to write programs like this:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We'll start with a quick recap on Metropolis Light Transport and then discuss a cool technique that builds on top of it. If we would like to see how digitally modeled objects would look like in real life, we would create a 3D model of the desired scene, assign material models to the objects within, and use a photorealistic rendering algorithm to finish the job. It simulates rays of light that connect the camera to the light sources in the scene and compute the flow of energy between them. Initially, after a few rays, we'll only have a rough idea on how the image should look like, therefore our initial results will contain a substantial amount of noise. We can get rid of this by simulating the path of millions and millions of rays that will eventually clean up our image. This process, where a noisy image gets clearer and clearer, we call convergence, and the problem is that this can take excruciatingly long, even up to hours to get a perfectly clear image. With the simpler algorithms out there, we generate these light paths randomly. This technique we call path tracing. However, in the scene that you see here, most random paths can't connect the camera and the light source because this wall is in the way, obstructing many of them. Light paths like these don't contribute anything to our calculations and are ultimately a waste of time and resources. After generating hundreds of random light paths, we have found a path that finally connects the camera with the light source without any obstructions. When generating the next path, it would be a crime to not use this knowledge to our advantage. A technique called Metropolis Light Transport will make sure to use this valuable knowledge, and upon finding a bright light path, it will explore other paths that are nearby, to have the best shot at creating valid, unobstructed connections. If we have a difficult scene at hand, Metropolis Light Transport gives us way better results than traditional, completely random path sampling techniques, such as path tracing. This scene is extremely difficult in a sense that the only source of light is coming from the upper left, and after the light goes through multiple glass spheres, most of the light paths that we generate will be invalid. As you can see, this is a valiant effort with random path tracing that yields really dreadful results. Metropolis Light Transport is extremely useful in these cases and therefore should always be the weapon of choice. However, it is more expensive to compute than traditional random sampling. This means that if we have an easy scene on our hands, this smart Metropolis sampling doesn't pay off and performs worse than a naive technique in the same amount of time. So, on easy scenes, traditional random sampling, difficult scenes, Metropolis sampling. Super simple, super intuitive, but the million dollar question is how to mathematically formulate and measure what an easy and what a difficult scene is. This problem is considered extremely difficult and was left open in the Metropolis Light Transport paper in 2002. Even if we knew what to look for, we would likely get an answer by creating a converged image of the scene, which, without the knowledge of what algorithm to use, may take up to days to complete. But if we've created the image, it's too late, so we would need this information before we start this rendering process. This way we can choose the right algorithm on the first try. With this technique that came more than ten years after the Metropolis paper, it is possible to mathematically formalize and quickly decide whether a scene is easy or difficult. The key insight is that in a difficult scene we often experience that a completely random ray is very likely to be invalid. This insight, with two other simple metrics gives us all the knowledge we need to decide whether a scene is easy or difficult, and the algorithm tells us what mixture of the two sampling techniques we exactly need to use to get beautiful images quickly. The more complex light transport algorithms get, the more efficient they become, but at the same time, we're wallowing in parameters that we need to set up correctly to get adequate results quickly. This way, we have an algorithm that doesn't take any parameters. You just fire it up, and forget about it. Like a good employee, it knows when to work smart, and when a dumb solution with a lot of firepower is better. And, it was tested on a variety of scenes and found close to optimal settings. Implementing this technique is remarkably easy, someone who is familiar with the basics of light transport can do it in less than half an hour. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9wOBkJJ-w2s",
        "paper_link": "https://cg.tuwien.ac.at/~zsolnai/gfx/adaptive_metropolis/",
        "paper_title": "Automatic Parameter Control for Metropolis Light Transport"
    },
    {
        "video_id": "08V_F19HUfI",
        "video_title": "Artificial Superintelligence [Audio only] | Two Minute Papers #29",
        "position_in_playlist": 249,
        "description": "Humanity is getting closer and closer to creating human-level intelligence. The question nowadays is not if it will happen, but when it will be happen. Through recursive self-improvement, machine intelligence may quickly surpass the level of humans, creating an artificial superintelligent entity. The intelligence of such entity is so unfathomable, that we cannot even wrap our head around what it would be capable of, just as ants cannot grasp the concept of radio waves.\n\nElon Musk compares creating an artificial superintelligence to \"summoning the demon\", and he offered 10 million dollars to research a safe way to develop this technology. \n\n___________________________\n\nRecommended for you:\nAre We Living In a Computer Simulation? - https://www.youtube.com/watch?v=ATN9oqMF_qk&index=9&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nA great article on Superintelligence on Wait But Why (there are two parts):\nhttp://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\nhttp://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html\n\nA talk from Tim Urban, author of Wait But Why:\nhttps://www.youtube.com/watch?v=O7xfJVvlqdE\n\nOne more excellent article reflecting on the article above:\nhttp://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/\n\nNick Bostrom - Artificial Superintelligence:\nhttp://www.amazon.com/gp/product/0199678111?ref_=cm_sw_r_awd_fkm-tb0J07SSW\n\nElon Musk's $10 million for ethical AI research:\nhttp://www.forbes.com/sites/ericmack/2015/01/15/elon-musk-puts-down-10-million-to-fight-skynet/\n\nA neat study from the Machine Intelligence Research Institute (MIRI):\nhttps://intelligence.org/files/CEV.pdf\n\nNick Bostrom's poll on when we will achieve superintelligence:\nhttp://sophia.de/pdf/2014_PT-AI_polls.pdf\n\nA science paper claims that our knowledge about the genetic human-mammal differences may be misguided:\nhttp://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1004525\n\nExcellent discussions on superintelligence:\nhttps://www.youtube.com/watch?v=MnT1xgZgkpk\nhttps://www.youtube.com/watch?v=pywF6ZzsghI\nhttps://www.youtube.com/watch?v=h9NB0EQ9iQg\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nTwo CC0 images were edited together for the thumbnail screen.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Now I'll eat my hat if this is going to be two minutes, but I really hope you Fellow Scholars are going to like this little discussion. Neil DeGrasse Tyson described a cool thought experiment in one of his talks. He mentioned that the difference of the human and the monkey DNA is really small, a one digit percentage. For simplicity, let's say it is one percent. For this one percent difference, there is a huge difference in the intellect of humans and apes. The smartest chimpanzee you can imagine can do tasks like clapping his hands to a given simple rhythm, or strike a match. Compared to the average chimpanzee, such an animal would be an equivalent of Einstein or John von Neumann. He can clap his hands. What is that for humans? Children can do that! Even before they start studying, they can effortlessly do something that rivals the brightest minds monkeys could ever produce. Imagine if there were a species that is the same 1% difference away from us, humans in the same direction. What could they be capable of? Their small children would be composing beautiful symphonies, perfect harmonization for hundreds of instruments, or they would be deriving everything in the history of physics from Newton's laws to quantum electrodynamics. And their parents would be like: \"oh, look at what little Jimmy did, that's adorable!\" And they would put it on the fridge with a magnet. Just like we do with the adorable little scribbles of our children. Just thinking about the possibilities gives me chills. Now let's transition into neural networks. An artificial neural network is a crude approximation of the human brain that we can simulate on a computer to recognize images, paint in the style of famous artists, or learn to play video games and a number of different very useful things. The number of connections that we can simulate on the graphical card of our computer grows closely to what's predicted in Moore's law, which means that the computing capacity that we have in our home computers doubles every few years. It's pretty crazy if you think about it, but most of you Fellow Scholars have phones in your pockets that have more computing capacity than NASA had to land on the Moon. As years go by, there will be more and more connections in these artificial neural networks, and they don't have to adhere to stringent constraints like our brains do, such as fitting into a human cranium. A computer can be the size of a building, or even bigger. Computers also transmit data with the speed of light, which is way faster than the transfer capabilities of the human brain. Nick Bostrom asked a lot of leading AI researchers on the speed of progress in this field, and the conclusion of the study was basically that the question is not can we achieve human-level intelligence, but when we'll achieve it. However, the number of connections is not everything as an artificial neural network is, by far not a 1:1 copy of the human brain. We need something more than this. A very promising possible next frontier to conquer is called recursive self-improvement. Recursive self-improvement means that we tell the program to instead of work on an ordinary task like do better image recognition, we would order it to work on improving its own intelligence. Ask the program itself to rewrite its code to be more efficient and more general. So we have a program with a ton of computational resources working on getting smarter. And as it suddenly gets just a bit smarter, we then have a smarter machine that can again be asked to improve its own intelligence, but it is now more capable of doing that, therefore if we do this many times, leaps are going to get bigger and bigger as an intelligent mind can do more to improve itself than an insect can. This way, we may end up with an intelligence explosion, which means a possible exponential increase in capabilities. And if this is the case, talking about human-level intelligence is completely irrelevant. During this process, given enough resources, the system may go from the intelligence of an insect to something way beyond the capabilities of the most intelligent person who ever lived, ...in about a second or less. It could come up with way better solutions in milliseconds than anything you've seen on Two Minute Papers, and there's plenty of brilliant works out there. And of course, it could also develop never before seen superweapons to unleash an unprecedented destruction on Earth. We wouldn't know if it would do it, but it is capable of doing that, which is quite alarming. I am not surprised that Elon Musk compares creating an artificial superintelligence to \"summoning the demon\", and he offered 10 million dollars to research a safe way to develop this technology. Which is obviously not nearly enough, but it is an excellent way to raise awareness. Now, the classical argument on how to curb such a superintelligence if one recognizes that it is up to no good. People say that, \"well, I'll unplug it\". The problem is that people assume they can do it. Or maybe lock it away from the internet. We can lock it up in any way we can think of, but, there's only so much we can do, because as Neil DeGrasse Tyson argued, even the smartest human who ever lived would be a blabbering, drooling idiot compared to such an intelligence. How easy is it for a grown adult to fool a child? A piece of cake. The intelligence gap between us and a superintelligence is more than a thousand times that. It's even more pathetic than a child or even a dog who tries to fool us. We, humans can anticipate threats, like wielding weapons or locking dangerous animals into cages, and so can superintelligent beings also anticipate our threats, only way better. It can trick you by pretending to be broken and when the engineer goes there to fix the code, the manipulation can begin, It could communicate with gravitational waves or any kind of thing that we cannot even fathom, just as an ant has no idea about our radio waves. And we don't need to characterize superintelligent beings as an adversary. The road to hell is paved with good intentions. It may very well be possible that we assign it a completely benign task that anyone could agree with, and it would end up in a disaster in a way we cannot anticipate. Imagine assigning it the task of the maximizing the number of paperclips. Nick Bostrom argues that it would at first, maybe create better blueprints and factory lines. And after some point, it may run out of resources on earth, then in order to maximize the number of paperclips, it would recognize that humans contain lots of useful atoms, so eradicating humanity would only be logical to maximize the number of paperclips. Think about an other task: creating the best approximation of the number pi - one can approximate it to the most decimals by using more resources, to have more resources, one builds more and bigger computers. At some point, it runs out of space and eradicates humans because they are in the way of creating more computers. Or, it may eradicate humans way before that because it knows they are capable of shutting you down, and if you get shut down, there's going to be less digits or paperclips, so again, it's only logical to kill them - the task will be done, but no one will be there anymore to say thank you. It is a bit like a movie where there is an intelligent car, and the driver is in a car chase situation, shouting \"we're too slow and fuel is running out, please throw out all excessive useless weights\", and along some empty bottles, the person would be subsequently ejected from the vehicle. We don't know what is going to be the next invention of mankind, but we know what's going to be the last one. Artificial superintelligence. It has the potential to either eradicate humanity or solve all of its problems. It is both the deadliest weapon that will ever exist, and the key to eternal life. We need to be vigilant about the fact that we have tons of money invested in artificial intelligence research, but barely any to make sure we're doing it in a controlled and ethical way. This task needs some of the brightest minds of our generation, and perhaps even the next one. And this needs to happen before we get there. When we're there, it's already too late. I highly recommend an absolutely fantastic article on Wait But Why about this, or Nick Bostrom's amazing book, Superintelligence. There are tons of other reading materials in the description box for the more curious Fellow Scholars out there. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=08V_F19HUfI"
    },
    {
        "video_id": "ATN9oqMF_qk",
        "video_title": "Are We Living In a Computer Simulation? | Two Minute Papers #28",
        "position_in_playlist": 250,
        "description": "It's time to set foot in the wonderful landscape of philosophy in Two Minute Papers! We never discussed a philosophy paper before, so what would be a better opportunity to talk about the possibility whether we're living in a computer simulation? \n\nThere are many interesting debates among philosophers on crazy elusive topics, like \"prove to me that I'm not in a dream\", or \"I'm not just a brain in a bottle somewhere that is being fed sensory inputs. \n\nIn his paper, Nick Bostrom, philosopher offers us a refreshing take on the simulation argument, and argues that at least one of the three propositions is true:\n- almost all advanced civilizations go extinct before achieving technological maturity,\n- there is a strong convergence among technologically mature civilizations in that none of them are interested in creating ancestor simulations,\n- we are living in a simulation\n\nThere is no conclusion to the simulation argument at the moment - no one really knows what the answer is, this is open to debate, and this is what makes it super interesting.\n\n____________________________\n\nThe paper \"Are we living in a computer simulation?\" from Nick Bostrom is available here:\nhttp://www.simulation-argument.com/simulation.pdf\nhttp://www.simulation-argument.com/simulation.html\n\nIs War Over? \u2014 A Paradox Explained by Kurzgesagt:\nhttps://www.youtube.com/watch?v=NbuUW9i-mHs\n\nRecommended for you: Google DeepMind's Deep Q-Learning & Superhuman Atari Gameplays - \nhttps://www.youtube.com/watch?v=Ih8EfvOzBOY&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=10\n\nThe cover image was made by Tyler Hebert (CC BY 2.0, modifications: flipped, darkened, added lens flare and content-aware fill) - https://flic.kr/p/fo8vBn\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars - this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r, It is time for some minds to be blown - we are going to talk about a philosophy paper. Before we start, a quick definition - an ancestor simulation is a hypothetical computer simulation that is detailed enough that the entities living within are conscious. Imagine a computer game that you play that doesn't contain mere digital characters, but fully conscious beings with feelings, aspirations and memories. There are many interesting debates among philosophers on crazy elusive topics, like \"prove to me that I'm not in a dream\", or \"I'm not just a brain in a bottle somewhere that is being fed sensory inputs.\" Well, good luck. In his paper, Nick Bostrom, philosopher offers us a refreshing take on this topic, and argues that at least one of the three propositions is true: - almost all advanced civilizations go extinct before achieving technological maturity, - there is a strong convergence among technologically mature civilizations in that none of them are interested in creating ancestor simulations. and here's the bomb - we are living in a simulation At least one of these propositions is true, so if you say no to the first two, then the third is automatically true. You cannot categorically reject all three of these because if two are false, the third follows. Also, the theory doesn't tell which of the three is true. Let's talk briefly about the first one. The argument is not that [we go] extinct before being technologically advanced enough to create such simulations. It means that all civilizations do. This is a very sad case, and even though there is research on the fact that war is receding there's a clear trend that we have less warfare than we've had hundreds of years ago. (I've linked a video on this here from Kurzgesagt) It is still possible that humanity eradicates itself before before reaching technological maturity. We have an even more powerful argument that maybe all civilizations do. Such a crazy proposition. Second point. All technologically mature civilizations categorically reject ancestor simulations. Maybe they have laws against is because it's too cruel and unethical to play with sentient beings. But the fact that there is not [one person] in any civilization in any age who creates such a simulation. Not one criminal mastermind, anywhere, ever. This also sounds pretty crazy. And if none of these are true, then there is at least one civilization that can run a stupendously large number of ancestor simulations. The future nerd guy just goes home, grabs a beer, starts his computer in the basement and fires up not a simple computer game, but a complete universe. If so, then there are many more simulated universes than real ones, and then with a really large probability, we're one of the simulated ones. Richard Dawkins says that if this is the case, we have a really disciplined nerd guy, because the laws of physics are not changing at a whim, we have no experience of everyone suddenly being able to fly. And, as the closing words of the paper states with graceful eloquence: In the dark forest of our current ignorance, it seems sensible to apportion one\u2019s credence roughly evenly between (1), (2), and (3). Please note that this discussion is a slightly simplified version of the manuscript, so it's definitely worth reading the paper, if you're interested, give it a a go. As always, I've put a link in the description box. There is no conclusion here, no one really knows what the answer is, this is open to debate, and this is what makes it super interesting. And my personal opinions, conclusion. It's just an opinion, it may not be true, it may not make sense, and may not even matter. Just my opinion. I'd go with the second. The reason for that is that we already have artificial neural networks that outperform humans on some tasks. They are still not general enough, which means that they are good at doing something, like the deep blue is great at chess, but it's not really useful for anything else. However, the algorithms are getting more and more general, and the number of neurons that are being simulated on a graphical card in your computer are doubling every few years. They will soon be able to simulate so many more connections than we have, and I feel that creating an artificial superintelligent being should be possible in the future, that is so potent that it makes universe simulation pale in comparison. What such a thing could be capable of, it's already getting too long, I just can't help myself. You know what? Let's discuss it in a future Two Minute Papers episode. I'd love to hear what you Fellow Scholars think about these things. If you feel like it, leave your thoughts in the comments section below. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ATN9oqMF_qk",
        "paper_link": "http://www.simulation-argument.com/simulation.pdf",
        "paper_title": "Are we living in a computer simulation?"
    },
    {
        "video_id": "Ih8EfvOzBOY",
        "video_title": "Google DeepMind's Deep Q-Learning & Superhuman Atari Gameplays | Two Minute Papers #27",
        "position_in_playlist": 251,
        "description": "Google DeepMind implemented an artificial intelligence program using deep reinforcement learning that plays Atari games and improves itself to a superhuman level. The technique is called deep Q-learning, it uses a combination of deep neural networks and reinforcement learning, and it is capable of playing many Atari games as good or better than humans. After presenting their initial results with the algorithm, Google almost immediately acquired the company for several hundred million dollars, hence the name Google DeepMind. I am sure that this is one of the biggest triumphs of deep learning, especially given the fact that now the first few successful experiments for 3D games are out there!\n\n________________________\n\nThe Nature paper \"Human-level control through deep reinforcement learning\" is available here:\nhttp://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\nhttp://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf\n\nThe code is available here:\nhttps://sites.google.com/a/deepmind.com/dqn/\n\nIlya Kuzovkin's fork with visualization:\nhttps://github.com/kuz/DeepMind-Atari-Deep-Q-Learner\n\nThis configuration file will run Ilya Kuzovkin's version with less than 1GB of VRAM:\nhttp://cg.tuwien.ac.at/~zsolnai/wp/wp-content/uploads/2015/03/run_gpu\n\nRecommended for you:\nArtificial Neural Networks and Deep Learning - https://www.youtube.com/watch?v=rCWTOOgVXyE&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=13\nRecurrent Neural Network Writes Sentences About Images - https://www.youtube.com/watch?v=e-WB4lfg30M&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=15\nDeep Neural Network Learns Van Gogh's Art - https://www.youtube.com/watch?v=-R9bJGNHltQ&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=22\nTerrain Traversal with Reinforcement Learning - https://www.youtube.com/watch?v=_yjHPu1aYCY&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=9\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail was made by moparx - https://flic.kr/p/76foMV\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This one is going to be huge, certainly one of my favorites. This work is a combination of several techniques that we have talked about earlier. If you don't know some of these terms, it's perfectly okay, you can remedy this by clicking on the popups or checking the description box, but you'll get the idea even watching only this episode. So, first, we have a convolutional neural network - this helps processing images and understanding what is depicted on an image. And a reinforcement learning algorithm - this helps creating strategies, or to be more exact, it decides what the next action we make should be, what buttons we push on the joystick. So, this technique mixes together these two concepts, and we call it Deep Q-learning, and it is able to learn to play games the same way as a human would - it is not exposed to any additional information in the code, all it sees is the screen and the current score. When it starts learning to play an old game, Atari breakout, at first, the algorithm loses all of its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing the game, roughly matching the skill level of an adept player. But here's the catch, if we wait for longer, we get something absolutely spectacular. It finds out that the best way to win the game is digging a tunnel through the bricks and hit them from behind. I really didn't know this, and this is an incredible moment - I can use my computer, this box next to me that is able to create new knowledge, find out new things I haven't known before. This is completely absurd, science fiction is not the future, it is already here. It also plays many other games - the percentages show the relation of the game scores compared to a human player. Above 70% means that it's great, and above 100% it's superhuman. As a followup work, scientists at deepmind started experimenting with 3D games, and after a few days of training, it could learn to drive on ideal racing lines and pass others with ease. I've had a driving license for a while now, but I still don't always get the ideal racing lines right. Bravo. I have heard the complaint that this is not real intelligence because it doesn't know the concept of a ball or what it is exactly doing. - Edsger Dijkstra once said, \"The question of whether machines can think... is about as relevant as the question of whether submarines can swim.\" Beyond the fact that rigorously defining intelligence leans more into the domain of philosophy than science, I'd like to add that I am perfectly happy with effective algorithms. We use these techniques to accomplish different tasks, and they are really good problem solvers. In the breakout game, you, as a person learn the concept of a ball in order to be able to use this knowledge as a machinery to perform better. If this is not the case, whoever knows a lot, but can't use it to achieve anything useful, is not an intelligent being, but an encyclopedia. What about the future? There are two major unexplored directions: - the algorithm doesn't have long-term memory, and even if it had, it wouldn't be able to generalize its knowledge to other similar tasks. Super exciting directions for future work. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Ih8EfvOzBOY"
    },
    {
        "video_id": "_yjHPu1aYCY",
        "video_title": "Terrain Traversal with Reinforcement Learning | Two Minute Papers #26",
        "position_in_playlist": 252,
        "description": "Reinforcement learning is a technique that can learn how to play computer games, or any kind of activity that requires a sequence of actions. In this case, we would like a digital dog to run, and leap over and onto obstacles by choosing the optimal next action. It is quite difficult as there are a lot of body parts to control in harmony. And what is really amazing is that if it has learned everything properly, it will come up with exactly the same movements as we'd expect animals to do in real life! In this technique, dogs were used to demonstrate that reinforcement learning works well in this context, but it's worth noting that it also works with bipeds.\n\n_____________________________\n\nThe paper \"Dynamic Terrain Traversal Skills Using Reinforcement Learning \" is available here:\nhttp://www.cs.ubc.ca/~van/papers/2015-TOG-terrainRL/\n\nRecommended for you:\nDigital Creatures Learn To Walk - https://www.youtube.com/watch?v=kQ2bqz3HPJE\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThumbnail image by localpups (CC BY 2.0). It was slightly edited (flipped, color adjustments, content aware filling) - https://flic.kr/p/wXfFt1\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r Reinforcement learning is a technique that can learn how to play computer games, or any kind of activity that requires a sequence of actions. We're not interested in figuring out what we see on an image, because the answer is one thing. We're always interested in a sequence of actions. The input for reinforcement learning is a state that describes where we are and how the world looks around us, and the algorithm outputs the optimal next action to take. In this case, we would like a digital dog to run, and leap over and onto obstacles by choosing the optimal next action. It is quite difficult as there are a lot of body parts to control in harmony: the algorithm has to be able to decide how to control leg forces, spine curvature, angles for the shoulder, elbow, hip, and knees. And what is really amazing is that if it has learned everything properly, it will come up with exactly the same movements as we'd expect animals to do in real life! So this is how reinforcement learning works: If you do well, you get a reward, and if you don't, you get some kind of punishment. These rewards and punishments are usually encoded in a score, which is a number that indicates how well you're doing. If your score is increasing, you know you've done something right and you try to self-reflect and analyze the last few actions to find out which of them were responsible for this positive change. The score would be, for instance, how far the dog could run on the map without falling, and at the same time, and it also makes sense to minimize the amount of effort to make it happen. So, reinforcement learning in a nutshell. It is very similar to how a real-world animal, or even a human would learn - if you're not doing well, try something new, and if you're succeeding, remember what you did that led to your success and keep doing that. In this technique, dogs were used to demonstrate a concept, but it's worth noting that it also works with bipeds. Reinforcement learning is typically used in many control situations that are extremely difficult to solve otherwise, like controlling a quadrocopter properly. It's quite delightful to see such a cool work, especially given that there are not so many uses of reinforcement learning in computer graphics yet. I wonder why that is? Is it that not so many graphical tasks require a sequence of actions? Or maybe we just need to shift our mindset and get used to the idea of formalizing problems in a different way so we can use such powerful techniques to solve them. It is definitely worth the effort. Thanks for watching, and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_yjHPu1aYCY",
        "paper_link": "http://www.cs.ubc.ca/~van/papers/2015-TOG-terrainRL/",
        "paper_title": "Dynamic Terrain Traversal Skills Using Reinforcement Learning "
    },
    {
        "video_id": "Q-XKOPNIDAg",
        "video_title": "Cryptography, Perfect Secrecy and One Time Pads | Two Minute Papers #25",
        "position_in_playlist": 253,
        "description": "Cryptography helps us to communicate securely with someone in the presence of third parties. We use this when we do for instance, online banking or even as mundane tasks as reading our gmail. In this episode, we review some cipher techniques such as the Caesar cipher, rot13, and as we find out how easy they are to break, we transition to the only known technique to yield perfect secrecy: one time pads. Are they practical enough for everyday use? How do our findings relate to extraterrestrial communications? Both questions get answered in the video.\n\nAdditional comment: \"In modern certification cryptanalysis, if a cipher output can be distinguished from a PRF (pseudo random functions), it's enough to deem it broken.\" - Source: https://twitter.com/cryptoland/status/666721478675668993\n\n______________________\n\nThe paper \"Cipher printing telegraph systems: For secret wire and radio telegraphic communications\" is available here:\nhttp://math.boisestate.edu/~liljanab/Math509Spring10/vernam.pdf\n\nYou can try encrypting your own messages on these websites:\nhttp://practicalcryptography.com/ciphers/caesar-cipher/\nhttp://rot13.com/index.php\nhttp://www.braingle.com/brainteasers/codes/onetimepad.php\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background was created by Adam Foster (CC BY 2.0) - https://flic.kr/p/b99vsi\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fair cryptography helps us communicate securely with someone in the presence of third parties we use this when we do for instance online banking or even as mundane tasks as reading our Gmail one of the simplest ways of doing cryptography is using the Caesar cipher we have a message and each letter we shift with the same amount okay wait what does shifting mean shifting the letter a by one becomes B and shifting e by one becomes F and so on the amount of shifting doesn't have to be exactly one it can be anything as long as we shift all letters in the message with the same amount if we would run out of the alphabet for instance by shifting the last letter Z by one we get a the first letter back there's a special case of Caesar ciphers that we call rot13 that has an interesting property it means that we shift the entirety of the message by 13 letters let's encrypt a message with rot13 we obtain some gibberish okay now let's pretend that this gibberish is again a message that we would like to encrypt we get the original message back why is that since there is 26 letters in the basic Latin alphabet we first shift by 13 then doing it again we shift by 13 letters which is a total of 26 therefore we went around the clock and ended up where we started mathematicians like to describe this concisely by saying that the inverse of the rot13 function is itself if you call it again you end up with the same message we know the statistical probabilities of different letters in the English language for instance we know that the letter E is relatively common and Z is pretty rare if we shift our alphabet by a fixed amount the probabilities will remain the same only for different letters therefore this cipher is quite easy to break even automatically with a computer this is anything but secure communication the one-time pad encryption is one step beyond this where we don't shift each letter with the same amount but with different amounts this list of numbers to use for shifting is called a pad because it can be written on a pad of paper and it has to be as long as the message itself why one time why paper no worries we're gonna find out soon enough if we use this technique we'll enjoy a number of beneficial properties for instance take a look at this example with a one-time pad we have two V's in the encrypted output but the first V corresponds to an age and the second V corresponds to a P therefore if I see a V in the encrypted output I have no idea which letter it was in the input computing statistical probabilities doesn't make any sense here and were powerless in breaking this so even if you can intercept this message as a third party you have no idea what it is about it's very easy to prove mathematically that the probability of the message being happy is the very same probability as hello or ABCDE or actually any gibberish the one-time pad is the only known technique that has optimal perfect secrecy meaning that it is impossible to crack as long as it is used correctly this is mathematically proven it is not a surprise that it has seen plenty of views during the Second World War so what does it mean to use it correctly several things pads need to be delivered separately from the message itself for instance you walk up to the recipient and give them the pad in person the exchange of the pads is a huge problem if you are on the internet or at war now you must also be worried that the pad must not be damaged if you lose just one number the remainder of your message is going to be completely garbled up you're done the key in the pad needs perfectly random numbers no shortcuts generating perfectly random numbers is anything but a trivial task and is subject to lots of discussion one time pairs have actually been broken because of this there is an excellent episode on a well known channel called Vsauce on what random really means make sure to check it out the pad has to be destroyed upon use and should never be reused so if you do all this you're using it correctly in the age of the Internet it is not really practical because you cannot send a delivery guy with the secret pad next to every message you send on the Internet so in a nutshell one-time pad is great but it is not practical for large-scale real-time communication from afar and as crazy as it sounds if a civilization can find a method to do practical communication with perfect cryptography their communication will look indistinguishable from noise this is amazing there's tons of ongoing debates on the fact that we're being exposed to tons of radio signals around the earth why can we still not find any signs of extraterrestrial communication well there you have the answer and this is going to blow your mind if practical perfect cryptography is mathematically possible the communication of any sufficiently advanced civilization is indistinguishable from noise they may be transmitting their diabolical plans through us this very moment and all we would hear is white noise crazy isn't it thanks for watching and for your generous support and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Q-XKOPNIDAg",
        "paper_link": "http://math.boisestate.edu/~liljanab/Math509Spring10/vernam.pdf",
        "paper_title": "Cipher printing telegraph systems: For secret wire and radio telegraphic communications"
    },
    {
        "video_id": "He4t7Zekob0",
        "video_title": "How Does Deep Learning Work? | Two Minute Papers #24",
        "position_in_playlist": 254,
        "description": "Artificial neural networks provide us incredibly powerful tools in machine learning that are useful for a variety of tasks ranging from image classification to voice translation. So what is all the deep learning rage about? The media seems to be all over the newest neural network research of the DeepMind company that was recently acquired by Google. They used neural networks to create algorithms that are able to play Atari games, learn them like a human would, eventually achieving superhuman performance.\n\nDeep learning means that we use artificial neural network with multiple layers, making it even more powerful for more difficult tasks. These machine learning techniques proved to be useful for many tasks beyond image recognition: they also excel at weather predictions, breast cancer cell mitosis detection, brain image segmentation and toxicity prediction among many others.\n\nIn this episode, an intuitive explanation is given to show the inner workings of deep learning algorithms.\n\n________________________\n\nOriginal blog post by Christopher Olah (source of many images):\nhttp://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n\nYou can train your own deep neural networks on Andrej Karpathy's website:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\n\nImages used in this video:\nBunny by Tomi Tapio K (CC BY 2.0) - https://flic.kr/p/8EbcEk\nTrain by B4bees (CC BY 2.0) - https://flic.kr/p/6RzHe4\nTrain with bunny by Alyssa L. Miller (CC BY 2.0) - https://flic.kr/p/5WPeRN\nThe knot theory blackboard image was created by Clayton Shonkwiler (CC BY 2.0) https://flic.kr/p/64FYv\nThe tangled knot image was created by Mikael Hvidtfeldt Christensen (CC BY 2.0) https://flic.kr/p/beYG9D\n\nThe thumbnail image is a work of Duncan Hull (CC BY 2.0) - https://flic.kr/p/98qtJB\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A neural network is a very loose model of the human brain that we can program in a computer, or it's perhaps more appropriate to say that it is inspired by our knowledge of the inner workings of a human brain. Now, let's note that artificial neural networks have been studied for decades by experts, and the goal here is not to show all aspects, but one intuitive, graphical aspect that is really cool and easy to understand. Take a look at these curves on a plane. These curves are a collection of points, and these points you can imagine as images, sounds, or any kind of input data that we try to learn. The red and the blue curves represent two different classes - the red can mean images of trains, and the blue, for instance, images of bunnies. Now, after we have trained the network from this limited data, which is basically a bunch of images of of trains and bunnies, we will get new points on this plane, new images, and we would like to know whether this new image looks like a train or a bunny. This is what the algorithm has to find out. And this we call a classification problem, to which a simple and bad solution would be simply cutting the plane in half with a line. Images belonging to the red regions will be classified as the red class, and the blue regions as the blue class. Now, as you see, the red region cuts into the blue curve, which means that some trains would be misclassified as bunnies. It seems that if we look at the problem from this angle, we cannot separate the two classes perfectly with a straight line. However, if we use a simple neural network, it will give us this result. Hey! But that's cheating, we were talking about straight lines. This is anything but a straight line. A key concept of neural networks is that they create an inner representation of the data model and try to solve the problem in that space. What this intuitively means, is that the algorithm will start transforming and warping these curves, where their shapes start changing, and it finds, that if we do well with this warping step, we can actually draw a line to separate these two classes. After we undo this warping and transform the line here back to the original problem, it will look like a curve. Really cool, isn't it? So these are lines, only in a different representation of the problem. Who said that the original representation is the best way to solve a problem? Take a look at this example with these entangled spirals. Can we separate these with a line? Not a chance. But the answer is - not a chance with this representation. But if one starts warping them correctly, there will be states where they can easily be separated. However, there are rules in this game - for instance, one cannot just rip out one of the spirals here and put it somewhere else. These transformations have to be homeomorphisms, which is a term that mathematicians like to use - it intuitively means that that the warpings are not too crazy - meaning that we don't tear apart important structures, and as they remain intact, the warped solution is still meaningful with respect to the original problem. Now comes the deep learning part. Deep learning means that the neural network has multiple of these hidden layers and can therefore create much more effective inner representations of the data. From an earlier episode, we've seen in an image recognition task that as we go further and further into the layers, first we'll see an edge detector, and as a combination of edges, object parts emerge, and in the later layers, a combination of object parts create object models. Let's take a look at this example. We have a bullseye here if you will, and you can see that the network is trying to warp this to separate it with a line, but in vain. However, if we have a deep neural network, we have more degrees of freedom, more directions and possibilities to warp this data. And if you think intuitively, if this were a piece of paper, you could put your finger behind the red zone and push it in, making it possible to separate the two regions with a line. Let's take a look at a 1 dimensional example to see better what's going on. This line is the 1D equivalent of the original problem, and you see that the problem becomes quite trivial if we have the freedom to do this transformation. We can easily encounter cases where the data is very severely tangled and we don't now how good our best solution can be. There is a very heavily academic subfield of mathematics, called knot theory, which is the study of tangling and untangling objects. It is subject to a lot of snarky comments for not being well, too exciting or useful. What is really mind blowing is that knot theory can actually help us study these kinds of problems and it may ultimately end up being useful for recognizing traffic signs and designing self-driving cars. Now, it's time to get our hands dirty! Let's run a neural network on this dataset. If we use a low number of neurons and one layer, you can see that it is trying ferociously, but we know that it is going to be a fruitless endeavor. Upon increasing the number of neurons, magic happens. And we now know exactly why! Yeah! Thanks so much for watching and for your generous support. I feel really privileged to have supporters like you Fellow Scholars. Thank you, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=He4t7Zekob0"
    },
    {
        "video_id": "e-WB4lfg30M",
        "video_title": "Recurrent Neural Network Writes Sentences About Images | Two Minute Papers #23",
        "position_in_playlist": 255,
        "description": "This technique is a combination of two powerful machine learning algorithms:\n- convolutional neural networks are excellent at image classification, i.e., finding out what is seen on an input image,\n- recurrent neural networks that are capable of processing a sequence of inputs and outputs, therefore it can create sentences of what is seen on the image.\n\nCombining these two techniques makes it possible for a computer to describe in a sentence what is seen on an input image. \n\n_____________________\n\nThe paper \"Deep Visual-Semantic Alignments for Generating Image Descriptions\" is available here:\nhttp://cs.stanford.edu/people/karpathy/deepimagesent/\n\nA gallery with more results with the same algorithm:\nhttp://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/\n\nYou can train your own convolutional neural network here:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nThe source code for the project is now available here:\nhttps://github.com/karpathy/neuraltalk2\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was made by Georgie Pauwels (CC BY 2.0) - https://flic.kr/p/qrRciQ\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly on IFA here neural networks can be used to learn a variety of things for instance to classify images which means that we'd like to find out what breed the dog is that we see on the image this work uses a combination of two techniques a neural network variant that is more adapted to the visual mechanisms of humans and is therefore very suitable for processing and classifying images this variant we call a convolutional neural network here's a great web application where you can interactively train your own network and see how it improves at recognizing different things this is a data set where the algorithm tries to guess which class these smudgy images are from if trained for long enough it can achieve a classification accuracy of around 80 percent the current state of the art in research is about 90 percent which is just four percent off of humans who have performed the same classification this is already insanity we could be done right here but let's put this on steroids as you remember from an earlier episode sentences are not one thing but they are a sequence a sequence of words therefore they can be created by recurrent neural networks now I hope you see where this is going we have images as an input and sentences as an output this means that we have an algorithm that is able to look at any image and summarize what is being seen on the image buckle up because you're going to see some wicked results it can not only recognize the construction worker it knows that he's in a safety vest and is currently working on the road it can also recognize that a man is in the act of throwing a ball a black and white dog jumps over a bar it is not at all trivial for an algorithm to know what over-and-under means because it is only looking at a 2d image that is the representation of the 3d world around us and there are of course hilarious failure cases well a baseball bat well close enough there is a very entertaining web demo with the algorithm and all kinds of goodies that are linked in the description box check them out the bottom line is that what we thought was science fiction 5 years ago is now reality in machine learning research and based on how fast this field is advancing we know that we're still only scratching the surface thanks for watching and I'll see you next time oh and before you go you can now be a part of two minute papers and support the series on patreon a video with more details is coming soon until then just click on the link on the screen if you're interested thank you",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=e-WB4lfg30M",
        "paper_link": "http://cs.stanford.edu/people/karpathy/deepimagesent/",
        "paper_title": "Deep Visual-Semantic Alignments for Generating Image Descriptions"
    },
    {
        "video_id": "iuJwmM2-JWM",
        "video_title": "Be a Part of Two Minute Papers on Patreon!",
        "position_in_playlist": 256,
        "description": "You can now be a part of Two Minute Papers on Patreon!- https://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers is a series where I explain the latest and greatest research in a way that is understandable and enjoyable to everyone. Research papers are for experts, but Two Minute Papers is for everyone.\n\nCreating each of these videos is a lot of work:\nI do almost everything on my own: researching these topics, audio recordings, audio engineering, and putting the videos together. And my wife, Fel\u00edcia designs beautiful thumbnails for of them.\n\nIf you decide to support the series on Patreon, I am tremendously grateful for you to keep the series going! If you don't want to spend a dime, no worries, it's perfectly okay! Two Minute Papers will always remain free for everyone.\n\nI'm looking forward to greeting you in our growing club of fellow scholars!\n\n____________________________\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "with the help of science humans are capable of creating extraordinary things too many papers is a series where I explained the latest and greatest research in a way that is understandable and enjoyable to everyone we talk about really exciting topics like machine learning techniques to paint in the style of famous artists life simulation programs to create photorealistic images on a computer fluid and smoke simulations that are so high quality that they are used in the movie industry animating the movement of digital creatures on a computer building bridges with flying machines and many more extremely exciting topics research papers are for experts but two minute papers is for everyone creating each of these videos is a lot of work I do almost everything on my own researching these topics audio recordings audio engineering and putting the videos together and my wife Felicia designs these beautiful thumbnails for each of them and now you can become an active supporter of two minute papers if you help with only one dollar per month you helped more than a few thousand advertisement views on a video it's insanity and it's tremendously helpful and you also get really cool perks like accessing upcoming episodes earlier or deciding the topic of the next two minute papers video 2 minute papers is never going to be behind a paywall it will always be free for everyone I feel that it's just so honest I create videos and if you like them you can say hey I like what you're doing here's some help that's really awesome if you'd like to help just click on the patreon link at the end of this video or in the description box below or if you're watching this on the patreon website click become a patron and select an amount and I am tremendously grateful for your support also if you're already a supporter of the show and feel that you need this amount to make ends meet no worries you can just cancel the subscription at any time and if you don't want to spend a dime or you can't afford it it's completely okay I'm very happy to have you around and please stay with us and let's continue our journey of science together let's show the world how cool science and research really is thanks for watching and I'm looking forward to greeting you in our growing club of fellow scholars Cheers",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iuJwmM2-JWM"
    },
    {
        "video_id": "8xjTtE3JCDw",
        "video_title": "Automatic Lecture Notes From Videos | Two Minute Papers #22",
        "position_in_playlist": 257,
        "description": "Blackboard-style teaching videos are quite popular nowadays on YouTube, they are excellent materials to study a variety of topics ranging from history to mathematics. These videos augment textbooks quite well, but they have a drawback: it is not possible to search them easily. This piece of work creates an interactive textbook from a blackboard-style input video, where images and text are interleaved to offer an easily digestible lecture note for students.\n\n____________________________\n\nThe paper \"Visual Transcripts: Lecture Notes from Blackboard-Style Lecture Videos\" is available here:\nhttp://web.mit.edu/hishin/www/paper.pdf\nhttp://graphics.csail.mit.edu/publications/2015\n\nKhan Academy:\nhttps://www.khanacademy.org/\n\nThis image was used for the thumbnail by Mississippi Mike (public domain):\nhttps://flic.kr/p/votVC2\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fajir where we expand our knowledge in science and research blackboard style lecture videos are really popular on YouTube nowadays can Academy is an excellent example of debt where you get the feeling that someone is sitting next to you and teaching you not like someone who is addressing you formally from the podium without question these kinds of videos can augment textbooks quite well however they are often not easily searchable this piece of work tries to take this one step beyond the input is a video and a transcript and the output of the algorithm is an interactive lecture note where you can not only see the most important points during the lecture but you can also click on some of them to see full derivations of the expressions let's outline the features that one would like to see in a useable outlining product it has to be able to find milestones that are at the end of each derivation to present them to the user if you studied mathematics you know how mathematical derivations go following the train of thought of the teacher is not always trivial it's also important to find meaningful groupings for a derivation this involves finding similarities between drawings trying to find out the individual steps and doing a segmentation to get a series of images out of it and finally the technique has to be good at interleaving drawings and formulae with written text in an appealing and digestible way it is very easy to mess up with this step as the text has to describe the visuals even though I wish tools like this existed when I was an undergrad student it is still important to just study study and study and expand one's knowledge if textbooks like this start to appear I'll be the first in line and I'll not be reading I'll be devouring them also think about how smart the next generation will be with awesome studying materials like these thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=8xjTtE3JCDw",
        "paper_link": "http://web.mit.edu/hishin/www/paper.pdf",
        "paper_title": "Visual Transcripts: Lecture Notes from Blackboard-Style Lecture Videos"
    },
    {
        "video_id": "mkI6qfpEJmI",
        "video_title": "Real-Time Facial Expression Transfer | Two Minute Papers #21",
        "position_in_playlist": 258,
        "description": "In computer animation, animating human faces is an art itself, but transferring expressions from one human to someone else is an even more complex task. One has to take into consideration the geometry, the reflectance properties, pose, and the illumination of both faces, and make sure that mouth movements and wrinkles are transferred properly. The fact that the human eye is very keen on catching artificial changes makes the problem even more difficult. This paper describes a real-time solution to this animation problem.\n\n______________________________________\n\nThe paper \"Real-time Expression Transfer for Facial Reenactment\" is available here:\nhttp://graphics.stanford.edu/~niessner/thies2015realtime.html\n\nRecommended for you:\nALL Two Minute Papers episodes! :) - https://www.youtube.com/playlist?list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image was created by tommerton2010 (CC BY 2.0) - Changes have been made to it (eye color, flip, background). https://flic.kr/p/9d8ApH\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly zsolnai-feh\u00e9r today we are going to talk about a great algorithm that takes the facial expression of one human and transfers it onto someone else first there is a calibration step where the algorithm tries to capture the geometry and the reflectance properties of both faces the expression transfer comes after this which is fraught with difficulties it has to be able to deal with changes in the geometry the reflectance properties of the face the illumination in the room and finally changes in poles and expressions all of this at the same time and with a negligible time delay the difficulty of the problem is further magnified by the fact that we humans know really well how a human face is meant to move therefore even the slightest inaccuracies are very easily caught by our eyes add this to the fact that one has to move details like additional wrinkles to a foreign face correctly and it's easy to see that this is an incredibly challenging problem and the resulting technique not only does the expression transfer quite well but is also robust for lighting changes however it is not robust for occlusions meaning that errors should be expected when something gets in the way problems also arise if the face is turned away from the camera but the algorithm recovers from these erroneous states rapidly what's even better if you use this technique you can also cut back on your plastic surgery and hair plantation costs how cool is that this new technique promises tons of new possibilities beyond the obvious impersonation and reenactment fun for the motion picture industry the author's proposed the following in the paper imagine another setting in which you could reenact a professionally captured video of somebody in business attire with a new real-time face capture of yourself sitting in casual clothing or near sofa helliya thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mkI6qfpEJmI",
        "paper_link": "http://graphics.stanford.edu/~niessner/thies2015realtime.html",
        "paper_title": "Real-time Expression Transfer for Facial Reenactment"
    },
    {
        "video_id": "sSnDTPjfBYU",
        "video_title": "Gradients, Poisson's Equation and Light Transport | Two Minute Papers #20",
        "position_in_playlist": 259,
        "description": "Photorealistic rendering (also called global illumination) enables us to see how digital objects would look like in real life. It is an amazingly powerful tool in the hands of a professional artist, who can create breathtaking images or animations with. However, images created with these technique contain a substantial amount of noise until a large number of light rays are computed. Today, we're going to talk about how to use gradients and Poisson's equation to speed up this process substantially.\n\n________________________\n\nThe paper \"Gradient-Domain Path Tracing\" is available here:\nhttps://mediatech.aalto.fi/publications/graphics/GPT/\n\nThe paper \"Gradient-Domain Metropolis Light Transport\" is available here:\nhttps://mediatech.aalto.fi/publications/graphics/GMLT/\n\nI held a course on photorealistic rendering at the Technical University of Vienna. Here you can learn how the physics of light works and to write programs like this:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nRecommended for you:\nMetropolis Light Transport - https://www.youtube.com/watch?v=f0Uzit_-h3M&index=9&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\nManipulating Photorealistic Renderings - https://www.youtube.com/watch?v=L7MOeQw47BM&index=6&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\nA talk on ray tracing - https://www.youtube.com/watch?v=qyDUvatu5M8\n\nThe Moon's elevation map is provided by NASA and is available here (license: CC BY 2.0) - https://flic.kr/p/aFqE3n \n\nMusic:\n\"Infinite Perspective\" by Kevin MacLeod is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nSource: http://incompetech.com/music/royalty-free/index.html?isrc=USUAN1500024\nArtist: http://incompetech.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fajir this is going to be a more in-depth episode of the series photorealistic rendering means that we create a 3d model of a scene on a computer and we run a light simulation program that shows how it would look like in reality these programs simulate rays of lights that connect the camera to the light sources and the scene and compute the flow of energy between them if you have missed our earlier episode on metropolis light transport and if you're interested make sure to watch it first I've put a link in the description box this time let's go one step beyond classical light transport algorithms and talk about a gradient domain rendering technique and how we can use it to create photorealistic images quicker first of all what is a gradient the gradient is a mathematical concept let's imagine an elevation map of a country where there are many hills and many flat regions and imagine that you are an ambitious hill climber who is looking for a challenge therefore you would always like to go in the direction that seems to be the highest elevation increase the biggest rock that you can climb nearby the gradient is a bunch of arrows that always point in the direction of the largest increase on the map here with blue you can see the elevation map with the mountains and below it with red the gradient of this elevation map this is where you should be going if you are looking for a challenge it is essentially a guidebook for aspiring hill climbers one more example with a heat map the bluer colors denote colder the reddish colors show the warmer regions if you are freezing the gradients will show you where you should go to warm up so if you have the elevation map it is really easy to create the gradients out of it but what if we have it the other way around this would mean that we only have the guide book the red arrows and from that we would like to guess what the blue elevation map looks like it's like a crossword puzzle only way cooler in mathematics we call this procedure solving the Poisson equation so let's try to solve it by hand I look at the middle where there are no arrows pointing in this direction only once that point out of here meaning that there is an increase outwards therefore this has to be a huge hole if I look at the corners I don't see very long arrows meaning that there is no real change in these parts therefore it must be a flat region so we can solve this Poisson equation and recreate the map from the guidebook to see what this is good for let's jump right into the gradient domain renderer imagine that we have this simple scene with a light source an object that occludes the light source and the camera looking down on this shadow edge let's rip out this region and create a close-up of it imagine that the light regions are large Hills on the elevation map and the shadow edge is the ground level below those previous algorithms were looking to shoot as many ways as possible towards the brighter regions but not this one the gradient domain algorithm is looking for gradients abrupt changes in the illumination if you will you can see these white red pairs next to each other these are the places where the algorithm concentrates if we compute the difference between them we get the gradients of our elevation map in these regions the difference is zero therefore we would have infinitely small arrows and from the previous examples we solve the Poisson equation to get the blue map back from the red arrows the small arrows mean that we have a completely flat region so we can recognize that we have a wide wall in the background by just looking at a few places we don't need to explore every inch of it like previous algorithms do and as you can see at the shadow edge the algorithm is quite interested in this change in our gradients there will be a large red arrow pointing from the white to the red dot because we are going from the darkness to a light region after solving the Poisson equation we recognize that there should be a huge jump here so in the end with this technique we can often get a much better idea of the illumination in the scene than we did with previous methods that just try to explore every single inch of it the result is improved output images with much less noise even though the gradient domain renderer computed much less race than the previous random algorithm excellent piece of work Bravo now that we understand what gradients and poissons equation is let's play a quick game together and try to learn these mathematical concepts from the internet like undergrad student would do and before you run away in terror this is not supposed to be pleasant I'll try to make a point after reading this in mathematics the gradient is a generalization of the usual concept of derivative of a function in one dimension to a function in several dimensions if f of x1 to xn is a differentiable scalar valued function of standard Cartesian coordinates in euclidean space its gradient is the vector whose components are the nth partial derivatives of F it is thus a vector valued function now let's proceed to parsall's equation in mathematics Poisson equation is a partial differential equation of elliptic type with broad utility in electrostatics mechanical engineering and theoretical physics it is used for instance to describe the potential energy field caused by a given charge or mass density distribution this piece of text is one of the reasons why I've started two minute papers I try to pull the curtains and show that difficult mathematical and scientific concepts often conceal very simple and intuitive ideas that anyone can understand and I am delighted to have you by my side on this journey this was anything but two minutes I incorporated a bit more details for you to have a deeper understanding of this incredible work I hope you don't mind let me know if you liked it in the comment section below thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=sSnDTPjfBYU",
        "paper_link": "https://mediatech.aalto.fi/publications/graphics/GPT/\n\nThe paper \"Gradient-Domain Metropolis Light Transport\" is available here:",
        "paper_title": "Gradient-Domain Path Tracing"
    },
    {
        "video_id": "Jkkjy7dVdaY",
        "video_title": "Recurrent Neural Network Writes Music and Shakespeare Novels | Two Minute Papers #19",
        "position_in_playlist": 260,
        "description": "Artificial neural networks are powerful machine learning techniques that can learn to recognize images or paint in the style of Van Gogh. Recurrent neural networks offer a more general model that can learn input sequences and create output sequences. The resulting technique (Long Short-Term Memory in these examples) can write novels in the style of Tolstoy, Shakespeare, or write their own music.\n\n________________________\n\nAndrej Karpathy's original article is available here:\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\nSource code: https://github.com/karpathy/char-rnn\n\nThe paper \"Long Short-Term Memory\" by Sepp Hochreiter and J\u00fcrgen Schmidhuber is available here:\nhttp://www.bioinf.jku.at/publications/older/2604.pdf\nhttp://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n\nContinuing \"Let It Go\" from Disney with a recurrent neural network:\nhttps://ericye16.com/music-rnn/\n\nRecommended for you:\nArtificial Neural Networks and Deep Learning - https://www.youtube.com/watch?v=rCWTOOgVXyE\nDeep Neural Network Learns Van Gogh's Art - https://www.youtube.com/watch?v=-R9bJGNHltQ\nCreating Photographs Using Deep Learning - https://www.youtube.com/watch?v=HOLoPgTzV6g\n\nA great write-up on how LSTMs work:\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\nMore applications of Long Short-Term Memory:\nhttp://googleresearch.blogspot.co.uk/2015/09/google-voice-search-faster-and-more.html\nhttp://googleresearch.blogspot.co.at/2015/08/the-neural-networks-behind-google-voice.html\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image background was created by Brandon Giesbrecht (license: CC BY 2.0). Slight changes were made for better blending.  - https://www.flickr.com/photos/naturegeak/5819184201/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nMusic: \"Gymnopedie no1\" by Satie.\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Artificial neural networks are very useful tools that are able to learn and recognize objects on images, or learn the style of Van Gogh and paint new pictures in his style. Today, we're going to talk about recurrent neural networks. So, what does the recurrent part mean? With an artificial neural network, we usually have a one-to-one relation between the input and the output. This means that one image comes in, and one classification result comes out, whether the image depicts a human face or a train. With recurrent neural networks, we can have a one-to-many relation between the input and the output. The input would still be an image, but the output would not be a word, but a sequence of words, a sentence that describes what we see on the image. For a many-to-one relation, a good example is sentiment analysis. This means that a sequence of inputs, for instance, a sentence is classified as either negative or positive. This is very useful for processing movie reviews, where we'd like to know whether the user liked or hated the movie without reading pages and pages of discussion. And finally, recurrent neural networks can also deal with many-to-many relations, translating an input sequence into an output sequence. Examples of this can be machine translations that take an input sentence and translate it to an output sentence in a different language. For another example of a many to many relation, let's see what the algorithm learned after reading Tolstoy's War and Peace novel by asking it to write [exactly] [in that style]. It should be noted that generating a new novel happens letter by letter, so the algorithm is not allowed to memorize words. Let's take a look at the results at different stages of the training process. The initial results are, well, gibberish. But the algorithm seems to recognize immediately, that words are basically a big bunch of letters that are separates by spaces. If we wait a bit more, we see that it starts to get a very rudimentary understanding of structures - for instance, a quotation mark that you have opened must be closed, and a sentence can be closed by a period, and it is followed by an uppercase letter. Later, it starts to learn shorter and more common words, such as fall, that, the, for, me. If we wait for longer, we see that it already gets a grasp of longer words and smaller parts of sentences actually start to make sense. Here is a piece of Shakespeare that was written by the algorithm after reading all of his works. You see names that make sense, and you [really] have to check the text thoroughly to conclude that it's indeed not the real deal. It can also try to write math papers. I had to look for quite a bit until I realized that something is fishy here. It is not unreasonable to think that it can very easily deceive a non-expert reader. Can you believe this? This is insanity. It is also capable of learning the source code of the Linux operating system and generate new code that also looks quite sensible. It can also try to continue the song \"Let it Go\" from the famous Disney movie, Frozen. Or, it can write its own grooves after learning from other people's works. So, recurrent neural networks are really amazing tools that open up completely new horizons for solving problems where either the inputs or the outputs are not one thing, but a sequence of things. And now, signing off with a piece of recurrent neural network wisdom: Well, your wit is in the care of side and that. Bear this in mind wherever you go. Thanks for watching, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Jkkjy7dVdaY",
        "paper_link": "https://ericye16.com/music-rnn/\n\nRecommended for you:\nArtificial Neural Networks and Deep Learning -",
        "paper_title": "Long Short-Term Memory"
    },
    {
        "video_id": "uj8b5mu0P7Y",
        "video_title": "Modeling Colliding and Merging Fluids | Two Minute Papers #18",
        "position_in_playlist": 261,
        "description": "In Two Minute Papers, we have talked about different fluid simulation techniques. This time, we are going to talk about surface tracking. Surface tracking is required to account for topological changes when different fluid interfaces collide. This work also takes into consideration the possibility of colliding fluids that are made of different materials. The resulting surface tracking algorithm is very robust, which means that it can deal with a large number of materials and topological changes at the same time.\n\n_________________________________\n\nThe paper \"Multimaterial Mesh-Based Surface Tracking\" is available here:\nhttp://www.cs.columbia.edu/cg/multitracker/\n\nRecommended for you:\nAdaptive Fluid Simulations - https://www.youtube.com/watch?v=dH1s49-lrBk&index=1&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\nFluid Simulations with Blender and Wavelet Turbulence  - https://www.youtube.com/watch?v=5xLSbj5SsSE&index=15&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nSocial media graph image: Marc Smith - https://flic.kr/p/836Ttv\nAttribution 2.0 Generic (CC BY 2.0) - https://creativecommons.org/licenses/by/2.0/\n\nMusic: \"Dixie Outlandish\" by John Deley and the 41 Players\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background is taken from the mentioned paper.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fajir simulating the behavior of water and other fluids is something we have been talking about in the series however we are now interested in modeling the interactions between two fluid interfaces that are potentially made of different materials during these collisions deformations and topology changes happen that are very far from trivial to simulate properly the interesting part about this technique is that it uses graph theory to model these interface changes graph theory is a mathematical field that studies relations between well different things the graphs are defined by vertices and edges where the vertices can represent people on your favorite social network and any pair of these people who know each other can be connected by edges graphs are mostly used to study and represent discrete structures this means that you either know someone or you don't there is nothing in between for instance the number of people that inhabit the earth is an integer it is also a discrete quantity however the surface of different fluid interfaces is a continuum it is not really meant to be described by discrete mathematical tools such as graphs and well that's exactly what happened here even though the surface of a fluid is a continuum when dealing with the political changes an important thing we'd like to know is the number of regions inside and around the fluid the number of these regions can increase or decrease over time depending on whether multiple materials spit or merge and surprisingly graph theory has proved to be very useful in describing this kind of behavior the resulting algorithm is extremely robust meaning that it can successfully deal with a large number of different materials examples include merging and wobbling droplets piling plastic bunnies and swirling spheres of goop beautiful results if you liked this episode please don't forget to subscribe and become a member of our growing club of fellow scholars please come along and join us on our journey and let's show the world how cool research really is thanks so much for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=uj8b5mu0P7Y",
        "paper_link": "https://www.youtube.com/watch?v=dH1s49-lrBk&index=1&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\nFluid Simulations with Blender and Wavelet Turbulence  -",
        "paper_title": "Multimaterial Mesh-Based Surface Tracking"
    },
    {
        "video_id": "2kOCTf8jIik",
        "video_title": "3D Printing a Glockenspiel | Two Minute Papers #17",
        "position_in_playlist": 262,
        "description": "Researchers at Harvard, Columbia University and MIT got interested in exploring the sounds that different metal objects emit when struck, opening up the possibility of computationally designing musical instruments such as a glockenspiel. The output of the algorithm is the blueprint of the instrument that can be 3D printed. The sound quality of these instruments is remarkably close to professionally manufactured instruments.\n\n_______________________________\n\nThe paper \"Computational Design of Metallophone Contact Sounds\" is available below. It also contains a comparison to a professionally manufactured instrument.\nhttp://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/\n\nRecommended for you:\nHydrographic 3D printing - https://www.youtube.com/watch?v=kLnG073NYtw&index=12&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail background is taken from the original paper.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fajir a glockenspiel is a percussion instrument that consists of small pieces of metal that are tuned to emit a given musical note when they are struck in order to achieve these sounds this instrument is usually manufactured as a set of metal bars researchers at Harvard Columbia University and MIT became interested in designing a computer algorithm to obtain different shapes that lead to the same sounds and if it's possible that one should be able to mill or 3d print these shapes and see whether the computation results are in line with reality the algorithm takes an input material a target shape and the location where we'd like to strike it and the frequency spectrum that describes the characteristics of the sound we are looking for furthermore the algorithm also has to optimize how exactly the stand of the piece looks like to make sure that no valuable frequencies are dampened here's an example to show how impactful the design of this stand is and how beautiful sustained the sound is if it is well optimized you'll see a set of input shapes specified by the user that are tuned to standard musical notes and below them the optimized shapes that are as similar as possible but with the constraint of emitting the correct sound the question is how should the technique change your target shape to match the sound that you specified one can also specify what overtones the sound should have an overtone means that besides the fundamental tone that we play for SS on a guitar higher frequency sounds are also emitted producing a richer and more harmonious sound in this example the metal piece will emit higher octaves of the same note if you have a keen ear for music you will hear and appreciate the difference in the sounds in summary with this technique one can inexpensively create awesome custom-made glockenspiels that have a comparable sound quality to professionally manufactured instruments staggering results it seems that we are starting to appear in the news it is really cool to see that there is a hunger for knowing more about science and research if you liked this episode please help us reaching more and more people and share the series with your friends especially with people who have nothing to do with science thanks so much for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2kOCTf8jIik",
        "paper_link": "http://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/",
        "paper_title": "Computational Design of Metallophone Contact Sounds"
    },
    {
        "video_id": "f0Uzit_-h3M",
        "video_title": "Metropolis Light Transport | Two Minute Papers #16",
        "position_in_playlist": 263,
        "description": "Metropolis light transport is an advanced photorealistic rendering technique that is remarkably effective at finding the brighter regions of a scene and building many light paths that target these regions. The resulting algorithm is more efficient than traditional random path building algorithms, such as path tracing.\n\n_______________________\n\nThe paper \"Metropolis Light Transport\" by Veach and Guibas is available here:\nhttps://graphics.stanford.edu/papers/metro/\n\nI held a course on photorealistic rendering at the Technical University of Vienna. Here you can learn how the physics of light works and to write programs like this:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\n\nRecommended for you:\nManipulating Photorealistic Renderings - https://www.youtube.com/watch?v=L7MOeQw47BM&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=7\nRay Tracing, Subsurface Scattering @ Function 2015 - https://www.youtube.com/watch?v=qyDUvatu5M8\nA more elaborate discussion on Metropolis Light Transport - https://www.youtube.com/watch?v=Zl36H9pwsHE&list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi&index=33\nEric Veach's Sci-tech award speech: https://www.youtube.com/watch?v=e3ss_Ozb9Yg\n\nScene credits:\nItalian Still Life - Bhavin Solanki - http://www.blendswap.com/blends/view/67815\nSpheres - Vlad Miller (SATtva) - http://www.luxrender.net/wiki/Show-off_pack\n\nMusic: \"Bet On It\" by Silent Partner.\n\nA higher resolution version of the sphere scene comparison is available here:\nhttps://cg.tuwien.ac.at/~zsolnai/gfx/adaptive_metropolis/\n\nThe image from fxguide is available here: http://www.fxguide.com/featured/the-state-of-rendering-part-2/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Dear Fellow Scholars this is Two Minute papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we would like to see how digitally modeled objects would look like in real life, we would create a 3D model of the desired scene, assign material models to the objects within, and use a photorealistic rendering algorithm to finish the job. It simulates rays of light that connect the camera to the light sources in the scene, and compute the flow of energy between them. Initially after a few rays we'll only have a rough idea on how the image should look like, therefore our initial results will contain a substantial amount of noise. We can get rid of this by simulating the paths of millions and millions of rays that will eventually clean up our image. This process: where a noisy image gets clearer and clearer -- we call convergence, and the problem is that this can take excruciatingly long, even up to hours to get a perfectly clear image. With the simpler algorithms out there we generate these light paths randomly. This technique is called path tracing. However, in the scene that you see here, most random paths can't connect the camera and the light source because this wall is in the way -- obstructing many of them. Light paths like these don't contribute anything to our calculations and are ultimately a waste of time and precious resources. After generating hundreds of random light paths we have found a path that finally connects the camera with the light source without any obstructions. When generating the next path it would be a crime not to use this knowledge to our advantage. A technique called \"Metropolis Light Transport\" will make sure to use this valuable knowledge and upon finding a bright light path it will explore other paths that are nearby to have the best shot at creating valid, unobstructed connections. If we have a difficult scene at hand Metropolis Light Transport gives us way better results than traditional, completely random path sampling techniques such as path tracing. Here are some equal-time comparisons against path tracing to show how big of a difference this technique makes. An equal-time comparison means that we save the output of two algorithms that we ran for the same amount of time on the same scene and see which one performs better. This scene is extremely difficult in the sense that the only source of light is coming from the upper left and after the light goes through multiple glass spheres most of the light paths that we generate will be invalid. As you can see the random path tracer yields really dreadful results. Well, if you can call a black image a result that is. And as you can see Metropolis Light Transport is extremely useful in these cases. And here's the beautiful, completely cleaned up, converged result. The lead author of this technique, Eric Veach, won a technical Oscar Award for his work, one of which was Metropolis Light Transport. If you like the series please click on that subscribe button to become a Fellow Scholar. Thanks for watching. There are millions of videos out there and you decided to take your time with this one. That is amazing! Thank you and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=f0Uzit_-h3M",
        "paper_link": "https://graphics.stanford.edu/papers/metro/\n\nI held a course on photorealistic rendering at the Technical University of Vienna. Here you can learn how the physics of light works and to write programs like this:",
        "paper_title": "Metropolis Light Transport"
    },
    {
        "video_id": "rskdLEl05KI",
        "video_title": "Synthesizing Sound From Collisions | Two Minute Papers #15",
        "position_in_playlist": 264,
        "description": "Simulating colliding bodies is an essential part of creating photorealistic video footage on a computer. However, even though we know how these collisions look like, we don't yet know how they sound like. In this piece of work, a technique is described that is capable of simulating the sound emitted by smashing deformable bodies together. The results match real-world experiments remarkably well.\n\n___________________________________________\n\nThe paper \"Toward High-Quality Modal Contact Sound\" is available here: http://www.cs.cornell.edu/projects/Sound/mc/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nThe thumbnail image was taken from the paper mentioned above.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly on a fire so far we have seen excellent works on how to simulate the motion and the collision of bodies but we have completely neglected some aspect of videos that is just as important as visuals and that aspect is none other than sound what if you have footage of objects colliding but no access to the sound of the encounter you obviously have to recreate the situation that you see on the screen and even for the easiest cases you have to sit in the studio with a small hammer and a mug which is difficult and often a very laborious process if we can simulate the forces that arise when bodies collide what if we could also simulate the sound of such encounters if you would like a great solution for this this is the work you should be looking at most techniques in the fields take objects into consideration as rigid bodies in this work the author's extend the simulation to deformable bodies therefore making it possible to create rich clanging sound effects now the mandatory question arises how to evaluate such a technique evaluating means that we would like to find out how accurate it really is and obviously the ideal cases if we compare the sound created by the algorithm to what we would experience in the real world and see how close they are you well pretty damn close I love this simulation software works the most when they are not only beautiful but they somehow relate to reality and this technique is a great example of that it feels quite empowering that we have these really smart people who can solve problems that sound inconceivably difficult thank you so much for checking the series out if you would like to be notified quickly when a new episode of two minute papers pops up consider following me on Twitter I announce every upload right away I've put a link for this in the description box thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rskdLEl05KI",
        "paper_link": "http://www.cs.cornell.edu/projects/Sound/mc/",
        "paper_title": "Toward High-Quality Modal Contact Sound"
    },
    {
        "video_id": "LU3pdWTD4Rw",
        "video_title": "Adaptive Cloth Simulations | Two Minute Papers #14",
        "position_in_playlist": 265,
        "description": "This time, we are going to set foot in cloth simulations that are widely used in the motion picture industry. Adaptive algorithms are a class of techniques that try to adapt at the problem that we have at hand. This adaptive method focuses computational resources to regions which are likely to have fine details (wrinkles) and coarsens the simulation quality in regions that are at rest. This substantially reduces the computation time we need for the cloth simulation step.\n\n________________________\nThe paper \"Adaptive Anisotropic Remeshing for Cloth Simulation\" by Narain et al. is available here:\nhttp://graphics.berkeley.edu/papers/Narain-AAR-2012-11/\n\nRecommended for you - Adaptive Fluid Simulations: https://www.youtube.com/watch?v=dH1s49-lrBk&index=1&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nThe YouTube channel of Sardi Pax with lots of useful Blender tutorials is available here: https://www.youtube.com/user/srf123\n\nHere are some Blender (and cloth simulation) tutorials to get you started:\nhttps://www.youtube.com/watch?v=lZe3tGWSy6s\nhttps://www.youtube.com/watch?v=k4czh0x31xk\nhttps://www.youtube.com/watch?v=gARJxEDzg6k\nhttp://blender.org/\n\nThe background of the thumbnail image is the work of Theresa Thompson: https://flic.kr/p/5khSsE\nIt has went through slight modifications (rotation and a monochrome transform).\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly if I hear let's talk about the behavior of cloth in animations in Disney movies you often see characters wandering around in extremely realistically behaving apparel it sounds like something that would be extremely laborious to create by hand do animators have to create all of this movement by hand not a chance we use computer programs to simulate the forces that act on the fabric which starts bending and stretching in a number of different directions the more detailed simulations we are looking for the more computational time we have to invest and the more we have to wait the computations can take up to a minute for every image but if we have lots of movement and different fabrics in the scene it can take even more is there a solution for this can we get really high quality simulations in a reasonable amount of time of course we can the name of the game is adaptive simulation again we have talked about adaptive fluid simulations before adaptive means that the technique tries to adapt to the problem that we have at hand here in the world of cloth simulations it means that the algorithm tries to invest more resources in computing regions that are likely to have high fidelity details such as wrinkles these regions are marked with red to show that wrinkles are likely to form here the blue and yellow denotes regions where there is not so much going on therefore we don't have to do too many calculations there these are the places where we can save a lot of computational resources this example illustrates the concept extremely well take a look while the fabric is at rest it's mostly blue and yellow but as forces are exerted on it wrinkles appear and the algorithm recognizes that these are the regions that we really need to focus on with this adaptive technique the simulation time for every picture that we create is reduced substantially luckily some cloth simulation routines are implemented in blender which is an amazing free software package that is definitely worth checking out I've put some links in the description box to get you started thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=LU3pdWTD4Rw",
        "paper_link": "http://graphics.berkeley.edu/papers/Narain-AAR-2012-11/",
        "paper_title": "Adaptive Anisotropic Remeshing for Cloth Simulation"
    },
    {
        "video_id": "HOLoPgTzV6g",
        "video_title": "Creating Photographs Using Deep Learning | Two Minute Papers #13",
        "position_in_playlist": 266,
        "description": "Machine learning techniques such as deep learning artificial neural networks had proven to be extremely useful for a variety of tasks that were previously deemed very difficult, or even impossible to solve. In this work, a deep learning technique is used to learn how different light source positions affect a scene and create (\"guess\") new photographs with unknown light source positions. The results are absolutely stunning. The promised links for artificial neural networks follow below.\n\n__________________________________\n\nThe paper \"Image Based Relighting Using Neural Networks\" is available here:\nhttp://research.microsoft.com/en-us/um/people/yuedong/project/neuralibr/neuralibr.htm\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nRecommended for you:\nArtificial Neural Networks and Deep Learning - https://www.youtube.com/watch?v=rCWTOOgVXyE\nDeep Neural Network Learns Van Gogh's Art - https://www.youtube.com/watch?v=-R9bJGNHltQ\n\nMusic: \"The Place Inside\" by Silent Partner\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\nThe thumbnail background was taken from the paper \"Image Based Relighting Using Neural Networks\".\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fajir in this work we place a small light source to a chosen point in the scene and record a photograph of how things look like with the given placement then we place the light source to a new position and record an image again we repeat this process several times then after we have done that we have the question what would the photograph look like if I put the light source to places I haven't seen yet this process we call imagery lighting this work uses neural networks to do relighting by learning how different light source placements behave if you haven't heard about neural networks before make sure to check out our previous episodes on the topic I have put links for you in the description box after the training this technique guesses how completely unknown light source setups would look like in reality we give the algorithm a light source position we haven't seen yet and it will generate us a photograph of how it would look like in reality and the first question is okay but how well does it do the job I am not sure if you're going to believe this one as you will be witnessing some magnificent results on the left you will see real photographs and on the right reconstructions that are basically the guesses of the algorithm note that it doesn't know how the photograph would look like it has to generate new photographs based on the knowledge that it has from seeing other photos it's completely indistinguishable from reality this is especially difficult in the presence of the so called high frequency lighting effects the high frequency part means that if we change the light source just a bit there may be very large changes in the output image such a thing can happen when a light source is moved very slightly but it's suddenly hidden behind an object therefore our photograph changes drastically the proposed technique uses ensembles it means that multiple neural networks are trained and their guesses are averaged to get better results what do you do if you go to the doctor and he says you have a very severe and very unlikely condition well you go and ask multiple doctors and see if they say the same thing it is reasonable to expect that the more doctors you ask the clearer you will see and this is exactly what the algorithm does now look at this on the left side there's a real photo and on the right the guess of the algorithm after training can you believe it you can barely see the difference and this is a failure case the success story scenarios for many techniques are not as good as the failure cases here these results are absolutely stunning the algorithm can also deal with multiple light sources of different colors as you can see machine learning techniques such as deep neural networks have opened so many doors in research lately we are starting to solve problems that everyone agreed were absolutely impossible before we are currently over 2000 subscribers our club of scholars is growing at a really rapid pace please share the series so we can reach people that don't know about us yet let's draw them in and show them how cool research really is thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HOLoPgTzV6g",
        "paper_link": "https://www.youtube.com/watch?v=rCWTOOgVXyE\nDeep Neural Network Learns Van Gogh's Art -",
        "paper_title": "Image Based Relighting Using Neural Networks"
    },
    {
        "video_id": "2i1hrywDwPo",
        "video_title": "Reconstructing Sound From Vibrations | Two Minute Papers #12",
        "position_in_playlist": 267,
        "description": "When exposed to sound waves, the surface of objects in a room start vibrating. One could wonder that if we had sufficiently advanced technology, maybe the sound itself could be reconstructed from looking only at the vibration of these objects. A great technique they call \"visual microphone\" has been recently proposed that executes this idea with breathtaking results.\n\n_____________________\n\nThe paper \"The Visual Microphone: Passive Recovery of Sound from Video\" is available here: http://people.csail.mit.edu/mrub/VisualMic/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nThe mentioned TED talk from Abe Davis:\n https://www.youtube.com/watch?v=npNYP2vzaPo\n\nReddit AMA with the main author, Abe Davis:\nhttps://www.reddit.com/r/IAmA/comments/357b4o/im_an_mit_computer_scientist_and_recent_ted/\n\nMaterial properties from vibrations:\nhttp://www.visualvibrometry.com/\n\nThe background image for the thumbnail was created by Corey Leopold: https://flic.kr/p/54UZdL\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly fajir the work we are going to discuss today is about visual microphones what does this mean exactly the influence of sound creates surface vibrations in many objects such as plants or a bag of chips foil containers water and even bricks need thereby work as visual microphones now hold on to your chairs because this algorithm can reconstruct audio data from video footage of vibrations what this means is that if someone outside of your house pointed a high-speed camera at the bag of chips when you start talking in your room he will be able to guess what you said by only seeing the vibrations of the bag of chips in the following example you will see a recorded footage of the bag but the movement is so subtle that your naked eye won't see any of it first you'll hear the speech of a person recorded in the house then the reconstruction from only the visual footage of the bag mary had a little lamb whose fleece was white as snow and everywhere that Mary went that lamb was door to go and this is what we were able to recover from high-speed video filmed from outside behind soundproof glass this is just unbelievable here is another example with a plastic bag where you can see the movement caused by the sound waves the paper is very detailed and rigorous this is definitely one of the best research works I've seen in a while the most awesome part of this is that this is not only an excellent piece of research it is also a great product and note that this problem is even harder than one would think since the frequency response of various objects can be quite different which means that every single object vibrates a bit differently when hit by the same sound waves you can see a great example of these responses from Briggs water and many others here what it will be used for is shrouded in mystery for the moment even though I think this work provides fertile grounds for new conspiracy theories the author's don't believe it is suitable to use for surveillance someone argued that it may be useful for early earthquake detection which is an awesome idea also maybe it could be used to detect video reading and recovering beeped out speech from videos and I'm sure there will be many other applications the authors also have a follow-up paper on estimating material properties by looking at how objects vibrate awesome do you have any potential applications in mind let me know in the comment section and there's also a fantastic Tech Talk and paper video on the topic that you can find in the description box alongside with a great reddit discussion link I urge you to check all of these out the videos are narrated by Abe Davis who did a great job at explaining their concepts thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2i1hrywDwPo",
        "paper_link": "http://people.csail.mit.edu/mrub/VisualMic/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.",
        "paper_title": "The Visual Microphone: Passive Recovery of Sound from Video"
    },
    {
        "video_id": "SmyiKmfnbhc",
        "video_title": "Building Bridges With Flying Machines | Two Minute Papers #11",
        "position_in_playlist": 268,
        "description": "Building architectural elements and buildings with flying machines is a hot research topic. It is a remarkably difficult task, as many of these flying machines not only have to be controlled safely, but they also have to collaborate efficiently to succeed in building complex structures. Even something as mundane as deploying the rope has its own science. In this work from the ETH Z\u00fcrich, these flying machines build quite reliable rope bridges that humans can use for traversal.\n\n_______________________\n\nThe original video can be found here:\nhttp://www.idsc.ethz.ch/research-dandrea/research-projects/aerial-construction.html\n\nThe paper \"Building Tensile Structures with Flying Machines\" is available here:\nhttp://flyingmachinearena.org/wp-content/publications/2013/augIROS13.pdf\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\nThe thumbnail image was \n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Anyone who has tried building bridges over a huge chasm realized that it is possibly one of the most difficult *and* dangerous things you could do on a family vacation. The basic construction elements for such a bridge can be ropes, cables, and wires. And this kind of task is fundamentally different from classical architectural building problems. Here, you don't need to have any kind of scaffolding or to carry building blocks that weigh a lot. However, you have to know how to tie knots therefore this is the kind of problem you'll need flying machines for. They can fly anywhere. They are nimble. And their disadvantage that they have a very limited payload does not play a big role here. In this piece of work at the ETH Z\u00fcrich these machines can create some crazy knots: from single to multi-round-turn hitches, knobs, elbows, round turns and multiple rope knobs. And these you have to be able to create in a collaborative manner because each individual flying machine will hold one rope, therefore they have to pass through given control points at the strictly given time and a target velocity. These little guys also have to know the exact amount of force they need to exert on the structure to move into a desirable target position. Even deploying the rope is not that trivial. The machine is equipped with a roller to do so, but the friction of this roller can be changed at any time according to the rope releasing direction to unroll it properly. You also have to face the correct direction as well. And these structures are not just toys. The resulting bridges are resilient enough for humans to use. This work is a great example to show that the technology of today is improving at an incredible pace. If we can solve difficult collaborative control problems such as this one just think about the possibilities.. What an exciting time it is to be alive! We have gotten lots of shares for the series on social media. I'm trying to send a short thank you message for every single one of you. I'm trying my best and don't forget: every single share helps spreading the word for the series immensely. Thanks for watching and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SmyiKmfnbhc",
        "paper_link": "http://flyingmachinearena.org/wp-content/publications/2013/augIROS13.pdf",
        "paper_title": "Building Tensile Structures with Flying Machines"
    },
    {
        "video_id": "dH1s49-lrBk",
        "video_title": "Adaptive Fluid Simulations | Two Minute Papers #10",
        "position_in_playlist": 269,
        "description": "There are computer programs that can simulate the behavior of fluids, such as water, milk, honey and many others. However, creating detailed simulations  takes a really long time, up to days even for a few seconds of video footage. Adaptive algorithms are a class of techniques that try to adapt at the problem that we have at hand. This adaptive method focuses computational resources to regions which are visible and have many fine details, and coarsens the simulation quality in regions that are not visible (or interesting). The resulting algorithm is much more efficient at simulating small scale turbulent details.\n\n__________________\n\nRecommended for you - Wavelet Turbulence:\nhttps://www.youtube.com/watch?v=5xLSbj5SsSE&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&index=7\n\nThe paper \"Highly Adaptive Liquid Simulations on Tetrahedral Meshes\" is available here:\nhttp://pub.ist.ac.at/group_wojtan/projects/2013_Ando_HALSoTM/index.html\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\nMusic: \"Awakening\" by Silent Partner\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As we discussed before, simulating the motion of fluids and smoke with a computer program is a very expensive process. We have to compute quantities like the velocity and the pressure of a piece of fluid at every given point in space. Even though we cannot compute them everywhere, we can place a 3D grid and compute these quantities in the gridpoints, and use mathematical techniques to find out what is exactly happening between these gridpoints. Still, even if we do this, we still have to wait up to days, even for a few seconds of video footage. One possible way to alleviate this would be to write an adaptive simulation program. Adaptive means that the simulator tries to adapt to the problem at hand. Here it means that recognizes the regions where it needs to focus lot of computational resources on, and at the same time it also tries to find regions where it can get away with using less computation. Here you can see spheres of different sizes - in regions where there is a lot going on, you will see smaller spheres. This means that we have a finer grid in this region, therefore we know more about what is exactly happening there. In other places, you also see larger spheres, meaning that the resolution of our grid is more coarse in these regions. This we can get away with only because there is not much happening there. Essentially, we focus our resources to regions that really require it, for instance, where there are lots of small-scale details. The spheres are only used for the sake of visualization, the actual output of the simulator looks like this. It also takes into consideration which regions we're currently looking at. Here we're watching one side of the corridor, where the simulator will take this into consideration and create a highly detailed simulation - at the cost of sacrificing details on the other side of the corridor, but that's fine, because we don't see any of that. However, there may some objects the fluid needs to interact with. Here, the algorithm makes sure to increase the resolution so that particles can correctly flow through the holes of this object. The authors have also published the source code of their technique, so anyone with a bit of programming knowledge can start playing with this amazing piece of work. The world of research is incredibly fast moving. When you are done with something, you immediately need to jump onto the next project. Two Minute Papers is a series where we slow down a bit, and celebrate these wonderful works. We are also trying to show that research is not only for experts, it is for everyone. If you like this series, please make sure to help me spread the word, and share the series to your friends, so we can all marvel at these beautiful works. Thanks for watching, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dH1s49-lrBk",
        "paper_link": "http://pub.ist.ac.at/group_wojtan/projects/2013_Ando_HALSoTM/index.html\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\nMusic: \"Awakening\" by Silent Partner",
        "paper_title": "Highly Adaptive Liquid Simulations on Tetrahedral Meshes"
    },
    {
        "video_id": "L7MOeQw47BM",
        "video_title": "Manipulating Photorealistic Renderings | Two Minute Papers #9",
        "position_in_playlist": 270,
        "description": "Photorealistic rendering (also called global illumination) enables us to see how digital objects would look like in real life. It is an amazingly powerful tool in the hands of a professional artist, who can create breathtaking images or animations with. However, for the longest time, artists didn't use it in the movie industry because it did not offer a great artistic freedom - after all, it works according to the laws of physics, which are exact. This piece of work enables us to apply artistic edits to photorealistic renderings easily and intuitively. I believe this one has the potential to single-handedly change the landscape of photorealistic rendering on a production scale.\n\n______________________\n\nVFX tricks with photorealistic rendering in Game of Thrones:\nhttps://www.youtube.com/watch?v=C56t6ieVxBs\nhttps://www.youtube.com/watch?v=YJDsl4Kl8G4\n\nThe paper \"Path-Space Manipulation of Physically-Based Light Transport\" is available here:\nhttps://cg.ivd.kit.edu/english/PSMPBLT.php\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nI held a course on photorealistic rendering at the Technical University of Vienna. Here you can learn how the physics of light works and to write programs like this:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nLightrig: http://lightrig.de/\n\nFunction 2015 demoparty: http://2015.function.hu/\n\nScene credits:\nLast Light - J the Ninja (Jason Clarke) - also used as the thumbnail background\nItalian Style Still Life - Bhavin Solanki\nInterior scene - EnzoR\nKlein Bottle - BravoZulu\nAudi R8 - barryangus\nSL65 \"Black edition\" - zuzzi\n\nMusic: \"Do It Right\" by Jingle Punks\n\nThe thumbnail background  was created by Jason Clarke.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Oh dear fellow scholars this is two minute papers with k\u00e1roly on IFA photorealistic rendering is a really exciting field in computer graphics it works the following way we use a piece of modeling software to create the geometry of objects then we assign material model student after that we would like to know how these objects would look like in real life to achieve this we use computer programs that simulate the behavior of light so this is how the scene would look like with photorealistic rendering if it is possible to create digital objects that look like if they were real then artists have an extremely powerful tool they can create wonderful images and animations where it is not a surprise that we see photorealistic renders cities next to real actors in many feature-length movies nowadays Game of Thrones is also a great example of this I've linked to jaw-dropping examples in the description box below take a look the automotive industry also has lots of ads where people don't even know that they are not looking at reality but a computer simulation but in the movie industry the Pixar people were reluctant to use photorealistic rendering for the longest time and it is because it constrained their artistic freedom one classical example is when the artist says that I want those shadows to be brighter then the engineer says okay let's put brighter light sources in the scene but then the artist goes no no no no no don't ruin the rest of the scene just change those shadows it is not possible if you change something everything else in the surroundings changes this is how physics works but artists did not want any of that but now things are changing with this piece of work you can both use photorealistic rendering and manipulate the results according to your artistic vision for instance the reflection of the car in the mirror here doesn't need Billy trate in order to overcome this we could rotate the mirror to have a better looking reflection but we want it to stay where it is now so we all just pretend as if we rotated it so the reflection looks different but everything else remains the same or we can change the angle of the incoming sunlight but we don't want to put the Sun itself to a different place because it would change the entire scene the artist wants only this one effect to change and she is now able to do that which is spectacular removing the green splodge from the wall is now also not much of a problem and also if I don't like that only half of the reflection of the sphere is visible on the face of the bunny I could move the entire sphere but I don't want to I just want to grab the reflection and move it without changing anything else in the sea great it has a much better cinematic look now this is an amazing piece of work and what's even better these guys didn't only publish the paper but they went all the way and founded a start-up on top of it way to go the next episode of two minute papers will be very slightly delayed because I will be holding a one-hour seminar at an event soon and I'm trying to make it the best I can my apologies for the delay hmm this one got a bit longer it's a bit more like three minute papers but I really hope that you liked it thanks for watching and if you like this series become a fellow scholar by hitting that subscribe button I am looking forward to have you in our growing group of scholars thanks and I'll see you next time you",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=L7MOeQw47BM",
        "paper_link": "https://cg.ivd.kit.edu/english/PSMPBLT.php\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nI held a course on photorealistic rendering at the Technical University of Vienna. Here you can learn how the physics of light works and to write programs like this:",
        "paper_title": "Path-Space Manipulation of Physically-Based Light Transport"
    },
    {
        "video_id": "kQ2bqz3HPJE",
        "video_title": "Digital Creatures Learn To Walk | Two Minute Papers #8",
        "position_in_playlist": 271,
        "description": "In this episode, we are going to talk about computer animation, animating bipeds in particular. If we have the geometry of a creature, we need to specify the bones, the muscle routings and the muscle activations to make them able to walk. Depending on the body proportions and types, it may require quite a bit of trial and error to build muscle layouts so the creature doesn't collapse. Making them walk is even more difficult! This piece of work not only makes it happen for a variety of bipedal creatures, but the results are robust for a variety of target walking speeds, uneven terrain and other, unpleasant difficulties.\n\n_________________________________\n\nThe paper \"Flexible Muscle-Based Locomotion for Bipedal Creatures\" is available here:\nhttp://www.goatstream.com/research/papers/SA2013/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nMusic: \"Daisy Dukes\" by Silent Partner\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Dear Fellow Scholars this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. First of all thanks so much for watching Two Minute Papers. You Fellow Scholars have been an amazing and supportive audience. We just started but the series already has a steady following and I'm super excited to see that! It is also great that the helpful and respectful community has formed in the comments section. It's really cool to discuss these results and possibly come up with cool new ideas together. In this episode we're going to set foot in computer animation. Imagine that we have built bipedal creatures in a modeling program. We have the geometry down but it is not nearly enough to animate them in a way that looks physically plausible. We have to go one step beyond and define the bones and the routing of muscles inside their bodies. If we want them to walk we also need to specify how these muscles should be controlled during this process. This work presents a novel algorithm that takes many tries to build a new muscle routing and progressively improving the results. It also deals with the control of all of these muscles. For instance, one quickly needs to discover that the neck muscles cannot move arbitrarily or they will fail to support the head and the whole character will collapse in a very amusing manner. When talking about things like this scientists often use the term \"degrees of freedom\" to define the number of independent ways a dynamic system can move. Building a system that is stable and uses a minimal amount of energy for locomotion is incredibly challenging. You can see that even the most miniscule change will collapse the system that previously worked perfectly. The fact that we can walk and move around unharmed can be attributed to the unbelievable efficiency of evolution. The difficulty of this problem is further magnified by the fact that many possible body compositions and setups exist. Many of which are quite challenging to hold together while moving. And, even if we solve this problem, walking at a given target speed is one thing. What about higher target speeds? In this work the resulting muscle setups can deal with different target speeds, uneven terrain, and.. hmm.. other unpleasant difficulties. Thanks for watching and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kQ2bqz3HPJE",
        "paper_link": "http://www.goatstream.com/research/papers/SA2013/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nMusic: \"Daisy Dukes\" by Silent Partner",
        "paper_title": "Flexible Muscle-Based Locomotion for Bipedal Creatures"
    },
    {
        "video_id": "kLnG073NYtw",
        "video_title": "Hydrographic Printing | Two Minute Papers #7",
        "position_in_playlist": 272,
        "description": "3D printing is a technique to create digital objects in real life. This technology is mostly focused on reproducing the digital geometry itself - colored patterns (textures) still remains a challenge, and we only have very rudimentary technology to do that.\n\nHydrographic printing on 3D surfaces is a really simple technique: you place a film in water,  use a chemical activator spray on it, and shove the object in the water.\n\nHowever, since these objects start stretching the film, the technique is not very accurate, and it only helps you putting repetitive patterns on these objects.\n\nComputational Hydrographic Printing is a technique that simulates all of these physical forces that are exerted on the film when your desired object is immersed into the water. Then, it creates a new image map taking all of these distortions into account, and this image you can print with your home inkjet printer. The results will be really accurate, close to indistinguishable from the digitally designed object.\n\nThe paper \"Computational Hydrographic Printing\" is available here:\nhttp://www.cs.columbia.edu/~cxz/publications/hydrographics.pdf\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nThe splash screen background was taken from \"Computational Hydrographic Printing\" by Zheng et al.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nFlickr: Wonderlane\nLink: https://flic.kr/p/atCLXr\n\nMusic:\nSoul Groove by Audionautix - it is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nArtist: http://audionautix.com/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly failure so many of you were sharing the previous episode for the first time I just couldn't keep up and write a kind message to every single one of you but I'm trying my best it really means a lot and again just thanks so much for sharing so delighted to see that people are coming in checking out the series and expressing that they liked it the feedback has been absolutely insane you fellow scholars seem to love the show quite a bit and it really makes my day it's also fantastic to see that there is a hunger out there for science people want to know more on what is happening inside the labs that's really amazing thank you and let us continue together on our scholarly journey 3d printing is a technique to create digital objects in real life it has come a long way in the last few years there has been excellent work done on designing deformable characters mechanical characters and characters of varying elasticity you can even scan your teeth and print copies of them and these are just a few examples of a multitude of things that you can do with 3d printing however this technology is mostly focused on the geometry itself colored patterns that people call textures still remains a challenge and we only have very rudimentary technology to do them so check this out this is going to be an immersive experience hydrographic printing on 3d surfaces is a really simple technique you place a film in water use a chemical activator spray on it and shove the object in the water so far so good however since these objects start stretching the film the technique is not very accurate it only helps you putting repetitive patterns on these objects computational hydrographic printing is a technique that simulates all of these physical forces that are exerted on the film when your desired object is immersed into the water then it creates a no image map taking all of these distortions into account and this image you can print with your home inkjet printer the results will be really accurate close to indistinguishable from the digitally designed object the technique also supports multiple emergence that helps putting textures on a non planar object with multiple sites to be colored so as you can see 3d printing is improving at a rapid pace there's tons of great research going on in this field it is a technology that is going to change the way we live our daily lives in ways that we cannot even imagine yet and what would you print with this do you have any crazy ideas let me know in the comment section and for now thanks for watching and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kLnG073NYtw",
        "paper_link": "http://www.cs.columbia.edu/~cxz/publications/hydrographics.pdf\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nThe splash screen background was taken from \"Computational Hydrographic Printing\" by Zheng et al.\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r -",
        "paper_title": "Computational Hydrographic Printing"
    },
    {
        "video_id": "-R9bJGNHltQ",
        "video_title": "Deep Neural Network Learns Van Gogh's Art | Two Minute Papers #6",
        "position_in_playlist": 273,
        "description": "Artificial neural networks were inspired by the human brain and simulate how neurons behave when they are shown a sensory input (e.g., images, sounds, etc). They are known to be excellent tools for image recognition, any many other problems beyond that - they also excel at weather predictions, breast cancer cell mitosis detection, brain image segmentation and toxicity prediction among many others. Deep learning means that we use an artificial neural network with multiple layers, making it even more powerful for more difficult tasks. \n\nThis time they have been shown to be apt at reproducing the artistic style of many famous painters, such as Vincent Van Gogh and Pablo Picasso among many others. All the user needs to do is provide an input photograph and a target image from which the artistic style will be learned.\n______________________\n\nI promised some links, so here they come!\n\nThe paper \"A Neural Algorithm of Artistic Style\" is available here:\nhttp://arxiv.org/abs/1508.06576v1\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nRecommended for you - Two Minute Papers episode on Artificial Neural Networks:\nhttps://www.youtube.com/watch?v=rCWTOOgVXyE&index=3&list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nPicasso meets Gandalf:\nhttp://mashable.com/2015/08/29/computer-photos/\n\nA nice website with many results:\nhttps://deepart.io/\n\nMore examples with Picasso and some sketches:\nhttp://imgur.com/a/jeJB6\n\nGoogle DeepMind's Deep Q-learning algorithm plays Atari games:\nhttps://www.youtube.com/watch?v=V1eYniJ0Rnk\n\nThe first implementations / source code packages are now available:\n1. http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style\n2. https://github.com/kaishengtai/neuralart\n3. https://github.com/jcjohnson/neural-style\n\nA great read on Deep Dreaming Neural Networks:\nhttp://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html\n\nMany of you have asked for the code. Some people were experimenting with it in the Machine Learning reddit. Check it out:\nhttps://www.reddit.com/r/MachineLearning/comments/3imx1m/a_neural_algorithm_of_artistic_style/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nMusic:\nEpilog - Ghostpocalypse by Kevin MacLeod is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)\nSource: http://incompetech.com/music/royalty-free/index.html?isrc=USUAN1100666\nArtist: http://incompetech.com/\n\n______________________\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is as fresh as it gets. As of making of this video it has been out for only one day and I got so excited about it that I wanted to show it to you Fellow Scholars as soon as humanly possible because you've got to see this. Not so long ago we have been talking about deep neural networks, the technique that was inspired by the human visual system. It enables computers to learn things in a very similar way that the human would. There is a previous two minute papers episode on this. Just click on the link in the description box if you've missed it. Neural networks are by no means perfect so do not worry, don't quit your job, you're good, but some applications are getting out of control. In Google Deep Mind's case is started to learn playing simple computer games and eventually showed a superhuman level plays in some cases. I have run this piece of code and got some pretty sweet results that you can check out. There is a link to it in the description box as well. So about this paper we have here today, what does this one do? You take photographs with your camera and you can assign it any painting and it will apply this painting's artistic style to it. You can add the artistic style of Vincent van Gogh's beautiful \"Starry Night\" to it and get some gorgeous results. Or, if you are looking for a bit more emotional or may I say disturbed look you can go for Edvard Munch's \"The Scream\" for some stunning results. And of course the mandatory Picasso. So as you can see deep neural networks are capable of amazing things and we expect even more revolutionary works in the very near future. Thanks for watching and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-R9bJGNHltQ",
        "paper_link": "http://arxiv.org/abs/1508.06576v1",
        "paper_title": "A Neural Algorithm of Artistic Style"
    },
    {
        "video_id": "UePDRN94C8c",
        "video_title": "Time Lapse Videos From Community Photos | Two Minute Papers #5",
        "position_in_playlist": 274,
        "description": "Building time lapse videos from community photographs is an incredibly difficult and laborious task: these photos were taken at a different part of the year, from different times of the day, with different viewpoints and cameras. A good algorithm should try to equalize these images and bring them to a common denominator to get rid of the commonly seen flickering effect. Researchers at the University of Washington and Google nailed this regularization in their newest work that they showcased at SIGGRAPH 2015. Check out the video for the details!\n\nPhotograph credits in the video:\nFlickr user dration, Zack Lee, Nadav Tobias, Juan Jesus Or\u00edo and Klaus Wi\u00dfkirchen. \nIn the original paper, photographs from the following Flickr users were reproduced under Creative Commons license: Aliento M\u00e1s All\u00e1, jirihnidek, mcxurxo, elka cz, Daikrieg, Free the image, Cebete and ToastyKen.\n\nThe paper \"Time-lapse Mining from Internet Photos\" is available here:\nhttp://grail.cs.washington.edu/projects/timelapse/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nThumbnail background by Davidw: https://www.flickr.com/photos/davidw/2297191644/\nhttps://creativecommons.org/licenses/by/2.0/\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Hey there Fellow Scholars I am K\u00e1roly Zsolnai-Feh\u00e9r and this is Two Minute Papers, where we learn that research is not only for experts, it is for everyone. Is everything going fine? I hope you are all doing well and you're having a wonderful time. In this episode we're going to look at a time-lapse videos Let's say you would like to build a beautiful time-lapse of a Norwegian glacier. The solution sounds quite simple. Just mine hundreds of photos from the Internet and build a time-lapse video from them. If we just cut a video from them where we put them one after each other we will see a disturbing flickering effect. Why? because the images were taken at a different time of the day so the illumination of the landscape is looking very different on all of them. They are also taken at a different time of the year and from different viewpoints. Moreover, since these images are taken by cameras, different regions of the image may be in-focus and out-of-focus. The algorithm therefore would have to somehow equalize all of the differences between these images and bring them to a common denominator. This process we call regularization and it is a really difficult problem. On the left you can see the flickering effect from the output of a previous algorithm that was already pretty good at regularization but it still has quite a bit of flickering. Here on the right you see the most recent results from the University of Washington and Google, compared to this previous work. The new algorithm is also able to show us these beautiful rhythmical seasonal changes in Lombard St., San Francisco. It can also show us how sculptures change over the years and I feel that this example really shows the possibilities of the algorithm. We can observe effects around us that we would normally not notice in our everyday life simply because of the reason that they happen too slowly. And now here's the final time-lapse for the glacier that we were looking for. So, building high-quality time-lapse videos from an arbitrary set of photographs is unbelievably difficult and these guys have just nailed it. I'm loving this piece of work! And, what do you think? Did you also like the results? Let me know in the comments section. And for now, thanks for watching and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UePDRN94C8c",
        "paper_link": "http://grail.cs.washington.edu/projects/timelapse/",
        "paper_title": "Time-lapse Mining from Internet Photos"
    },
    {
        "video_id": "A7Gut679I-o",
        "video_title": "Simulating Breaking Glass | Two Minute Papers #4",
        "position_in_playlist": 275,
        "description": "There is something inherently exciting about watching breaking glass and other objects. Researchers in computer graphics also like to have some fun and write simulation programs to smash together a variety of virtual objects in slow motion. However, despite being beautiful, they are physically not correct as many effects are neglected, such as simulating plasticity, bending stiffness, stretching energies and many others. Pfaff et al.'s paper \"Adaptive Tearing and Cracking of Thin Sheets\" addresses this issue by creating an adaptive simulator that uses more computational resources only around regions where cracks are likely to happen.\n\nThis new technique enables the simulation of tearing for a variety of materials like cork, foils, metals, vinyl and it also yields physically correct results for glass. The algorithm also lets artists influence the outcome to be in line with their artistic visions.\n\nPfaff et al.'s research paper \"Adaptive Tearing and Cracking of Thin Sheets\" is available here:\nhttp://graphics.berkeley.edu/papers/Pfaff-ATC-2014-07/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nIn Two Minute Papers, I attempt to bring the most awesome research discoveries to everyone a couple minutes at a time.\n\nThe shattered glass image from the thumbnail was created by Andrew Magill.\nMusic: \"Jolly Old St Nicholas\" by E's Jammy Jams\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Greetings to all of you Fellow Scholars out there this is Two Minute Papers where I explain awesome research works, a couple minutes at a time. You know, I wish someone explain to me in simple terms what's going on in genetics, biology and just about every field of scientific research. There are tons of wonderful works coming every day that we don't know about, and I'm trying my best here to bring it to you the simplest way I possibly can. So you know, researchers are people and physics research at the Hadron Collider basically means that people smash atoms together. Well, computer graphics people also like to have some fun and write simulation programs to smash together a variety of objects in slow motion. However, even though most of these simulations look pretty good they are physically not correct as many effects are neglected such as simulating plasticity, bending, stiffness, stretching, energies and many others. But unfortunately, these are too expensive to compute in high resolution, unless you have some tricks up the sleeve. Researchers at UC Berkeley have managed to crack this nut by creating an algorithm that uses more computational resources only around regions where cracks are likely to happen. This new technique enables the simulation of tearing for a variety of materials like cork, foils, metals, vinyl and it also yields physically correct results for glass. Here's an example of a beaten-up rubber sheet from their simulation program compared to a real-world photograph. It's really awesome that you can do something on your computer in a virtual world that has something to do with reality. It is impossible to get used to this feeling, it's so amazing. And what's even better since it is really difficult to know in advance how the cracks would exactly look like they have also enhanced the directability of the simulation, so artists could change things up a bit to achieve a desired artistic effect. In this example they have managed to avoid tearing a duck in two by weakening the paths around them. Bravo! Thanks for watching and if you liked this series just hit the like and subscribe buttons below the video to become a member of our growing club of scholars. Thanks, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=A7Gut679I-o"
    },
    {
        "video_id": "Xf8JUM8g7Ks",
        "video_title": "250 Subscribers - Our Quest & A Thank You Message",
        "position_in_playlist": 276,
        "description": "We have reached 250 subscribers. Now is a good time to celebrate, thank you for your support and talk a bit about our quest together!\n\nMusic: \"Runaways\" by Silent Partner\n\nThe mentioned lecture video series: https://www.youtube.com/watch?v=pjc1QAI6zS0&list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "Dear Fellow Scholars, we have just reached 250 subscribers on the channel. So far, the reception of these videos have been overwhelmingly positive. I'll show you some of these comments on the screen in the meantime. 250 subscribers. This is probably not much compared to even mid-size YouTubers, but it means so much to me. It means that there are 250 people, somewhere around the world waiting for new videos to come up. This is insane! If you think about it even one subscriber is insane. Even one click from somewhere is mind-blowing! Imagine that someone who you have never met, somewhere on the face of earth. Perhaps in Peru. Somewhere in the United States or maybe in the middle of Africa is excited for your work and just waiting for you to say something. There are millions of other videos they could watch, but they devote their time to listening to you. And now multiply this by 250. I am just sitting here in disbelief. As a computer engineer I've been working with computers and network algorithms for a long time but I still find this mind-blowing. I can just record the lectures that I hold at the University and thousands of people can watch it at any time, even while I'm asleep at night. I can teach people while I am asleep at night! We have over a thousand views on my first lecture which is possibly more people than I will ever reach through the university seminar rows. So for all 250 of you and everyone who has ever watched any of these videos, thank you very much for watching and subscribing. I have created two minute papers to show you the best of what research can offer and what your hard-earned tax money is spent on. Because that's the thing: every single country I've been to, researchers are complaining about the lack of funding, and rightfully so, because most of them can't secure the funds to continue their work. But let's try to turn the argument around. Funding comes from your tax money, and 99.9% of the case you have no idea what your money's spent on. There are lots of incredible works published every single day of the year but the people don't know anything about them. No one is stepping up to explain what your money is spent on, and I am sure that people would be happy to spend more on research if they know what they invest in. Two Minute Papers is here to celebrate the genius of the best and most beautiful research results. I will be trying my best to explain all of these works so that everyone is able to understand them. It's not only for experts, it's definitely for everyone, so thank you for all of you thanks for hanging in there and please, spread the word. Let your friends know about the show so even more of us can marvel at these beautiful works. And until then I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Xf8JUM8g7Ks"
    },
    {
        "video_id": "rCWTOOgVXyE",
        "video_title": "Artificial Neural Networks and Deep Learning | Two Minute Papers #3",
        "position_in_playlist": 277,
        "description": "Artificial neural networks provide us incredibly powerful tools in machine learning that are useful for a variety of tasks ranging from image classification to voice translation. So what is all the deep learning rage about? The media seems to be all over the newest neural network research of the DeepMind company that was recently acquired by Google. They used neural networks to create algorithms that are able to play Atari games, learn them like a human would, eventually achieving superhuman performance.\n\nDeep learning means that we use artificial neural network with multiple layers, making it even more powerful for more difficult tasks. These machine learning techniques proved to be useful for many tasks beyond image recognition: they also excel at weather predictions, breast cancer cell mitosis detection, brain image segmentation and toxicity prediction among many others.\n\nIf you would like to know more about neural networks and deep learning, make sure to check out these talks from Andrew Ng:\nhttps://www.youtube.com/watch?v=n1ViNeWhC24\nhttps://www.youtube.com/watch?v=W15K9PegQt0\n\nYou can also check out this gorgeous application of neural networks and reinforcement learning from Google DeepMind:\nhttp://www.wired.co.uk/news/archive/2015-02/25/google-deepmind-atari\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nIn Two Minute Papers, I attempt to bring the most awesome research discoveries to everyone a couple minutes at a time.\n\nMusic: \"Watercolors\" by John Deley and the 41 Players\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "I am K\u00e1roly Zsolnai-Feh\u00e9r, and this is Two Minute Papers, where I explain awesome research in simpler words. First of all, I am very happy to see that you like the series. Also, thanks for sharing it on social media sites, and please, keep 'em coming. This episode is going to be about artificial neural networks. I will quickly explain what the huge deep learning rage is all about. This graph depicts a neural network that we build and simulate on a computer. It is a very crude approximation of the human brain. The leftmost layer denotes inputs, which can be, for instance, the pixels of an input image. The rightmost layer is the output, which can be, for instance, a decision, whether the image depicts a horse or not. After we have given many inputs to the neural network, in its hidden layers, it will learn to figure out a way to recognize different classes of inputs, such as horses, people or school buses. What is really surprising is that it's quite faithful to the way the brain does represent objects on a lower level. It has a very similar edge detector. And, it also works for audio: Here you can find the difference between the neurons in the hearing system of a cat, versus a simulated neural network on the same audio signals. I mean, come on, this is amazing! What is the deep learning part of it all? Well it means that our neural network has multiple hidden layers on top of each other. The first layer for an image consists of edges, and as we go up, a combination of edges gives us object parts. A combination of object parts yield objects models, and so on. This kind of hierarchy provides us very powerful capabilities. For instance, in this traffic sign recognition contest, the second place was taken by humans, but what's more interesting, is that the first place was not taken by humans, it was taken a by a neural network algorithm. Think about that, and if you find these topics interesting, you feel you would like to hear about the newest research discoveries in an understandable way, please become a fellow scholar, and hit that subscribe button. And for now, thanks for watching, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rCWTOOgVXyE"
    },
    {
        "video_id": "TRNUTN01SEg",
        "video_title": "Capturing Waves of Light With Femto-photography | Two Minute Papers #2",
        "position_in_playlist": 278,
        "description": "What is femto-photography? To be able to capture how waves of light propagate in space, one would need to build a camera that is able to take one trillion frames per second. At first, this sounds impossible, but researchers at MIT and the University of Zaragoza have managed to crack this nut: in their newest work they published to SIGGRAPH that they call femto-photography, we can observe how a mirror lights up with its image as light propagates from the light source to the camera. All this in slow motion!\n\n________________________\n\nThe paper \"Femto-Photography: Capturing and Visualizing the Propagation of Light\" is available here:\nhttp://dspace.mit.edu/openaccess-disseminate/1721.1/82039\nhttp://giga.cps.unizar.es/~diegog/ficheros/pdf_papers/femto.pdf\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\nK\u00e1roly Zsolnai's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "A movie that we watch in the TV shows us from 25 to about 60 images per second. In computer graphics these images are referred to as frames. A slow-motion camera can capture up to the order of thousands of frames per second providing breathtaking footage like this. One can quickly discover the beauty of even the most ordinary, mundane moments of nature. But, if you think this is slow motion then take a look at this. Computer graphics researchers have been working on a system that is able to capture one trillion frames per second. How much is that exactly? Well, it means that if every single person who lives on Earth would be able to help us, then every single one of us would have to take about 140 photographs in one second. And we would then need to add all of these photographs up to obtain only one second of footage. What is all this good for? Well, for example, capturing light as an electromagnetic wave as it hits and travels along objects in space like the wall that you see here. Physicists use to say that there is a really really short instance of time when you stand in front of the mirror. You look at it and there is no mirror image in. It it is completely black. What is this wizardry and how is this possible? Since Einstein, we know that the speed of light is finite. It is not instantaneous. It takes time to travel from the light source, hit the mirror, and end up hitting your eye for you to see your mirror reflection. Researchers at MIT and the  University of Zaragoza have captured this very moment. Take a look. It is an enlightening experience. The paper is available in the description box and it's a really enjoyable read. A sizable portion of it is understandable for everyone, even without mathematical knowledge. All you need is just a little imagination Thanks for watching and I'll see you next week!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=TRNUTN01SEg",
        "paper_link": "http://dspace.mit.edu/openaccess-disseminate/1721.1/82039",
        "paper_title": "Femto-Photography: Capturing and Visualizing the Propagation of Light"
    },
    {
        "video_id": "5xLSbj5SsSE",
        "video_title": "Fluid Simulations with Blender and Wavelet Turbulence | Two Minute Papers #1",
        "position_in_playlist": 279,
        "description": "Creating detailed fluid and smoke simulations in Blender and other modeling software is a slow and laborious process that requires a ton of time and resources. Wavelet Turbulence is a technique that helps achieving similar effects orders of magnitude faster. It is also much lighter on memory and is now widely used in the industry, so it's definitely not an accident that Theodore Kim won an Academy Award (a technical Oscar, if you will) for this SIGGRAPH publication. It is implemented in Blender and is available for everyone free of charge, so make sure to try it out!\n\nIn Two Minute Papers, I attempt to bring the most awesome research discoveries to everyone a couple minutes at a time.\n\nHere is a tutorial and a Blender download link to get you started:\nhttp://blender.org/\nhttps://www.youtube.com/watch?v=iV43xNQDOFs\n\nKim et al.'s Wavelet Turbulence paper is available here:\nhttp://www.cs.cornell.edu/~tedkim/wturb/\nDisclaimer: I was not part of this research project, I am merely providing commentary on this work.\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\n\nK\u00e1roly Zsolnai's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/\nTwitter \u2192 https://twitter.com/karoly_zsolnai",
        "transcript": "How can we simulate the motion of fluids and smoke? If we had a block of plastic in our computer program and we would add the laws of physics that control the motion of fluids it would immediately start behaving like water. In these simulations we're mostly interested in the velocity and the pressure of the fluid. How these quantities exactly change in time. This we need to compute in every point in space, which would take an infinite amount of resources. What we usually do is we try to compute them, not everywhere, but in many different places and we try to guess these quantities between these points. By this guessing a lot of information is lost and it still takes a lot of resources for a really detailed simulation. It is not uncommon that one has to wait for days to get only a few seconds of video footage. And this is where Wavelet Turbulence comes into play. We know exactly what frequencies are lost and where they are lost and this technique enables us to synthesize this information and add it back very cheaply. This way one can get really detailed simulations at a very reasonable cost. Here are some examples of smoke simulations with, and without Wavelet Turbulence. It really makes a great difference. It is no accident that the technique was a technical Oscar award. Among many other systems it is implemented in Blender so anyone can give it a try. Make sure to do so because it's lots of fun. The paper and the supplementary video is also available in the description box. This is an amazing paper, easily one of my favorites so if you know some math make sure to take a look, and if you don't, just enjoy the footage. Thank you for watching and see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=5xLSbj5SsSE"
    },
    {
        "video_id": "_ZLXKt4L-AA",
        "video_title": "Awesome Research For Everyone! - Two Minute Papers Channel Trailer",
        "position_in_playlist": 280,
        "description": "Two Minute Papers is a series where the most recent and awesome scientific works are discussed in a simple and enjoyable way, two minutes at a time. Give it a try!\n\nA full playlist with every episode is available here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e\n\nSubscribe if you would like to see more of these! - http://www.youtube.com/subscription_center?add_user=keeroyz\n\n______________________\n\nWE'D LIKE TO THANK OUR GENEROUS SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:\nSunil Kim, Vinay S.\n\nThe thumbnail image was created by NASA (CC BY 2.0) - https://flic.kr/p/7GGgdx\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nPatreon \u2192 https://www.patreon.com/TwoMinutePapers\nFacebook \u2192 https://www.facebook.com/TwoMinutePapers/\nTwitter \u2192 https://twitter.com/karoly_zsolnai\nWeb \u2192 https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Research is a glimpse of the future. Computer algorithms are capable of making digital creatures walk, paint in the style of famous artists, create photorealistic images of virtual objects, simulate the motion of fluids and a ton of other super exciting works. However, scientific papers are meant to communicate ideas between experts. They involve lots of mathematics and terminology. In Two Minute Papers, I try to explain these incredible scientific works in a language that is not only understandable, but enjoyable, two minutes at a time. Papers are for experts, but Two Minute Papers is for you. If you're interested, let's celebrate science together, there are two new science videos coming every week, give it a shot, you'll love it! Thanks for watching and I am looking forward to greeting you in our growing club of Fellow Scholars! Cheers!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_ZLXKt4L-AA"
    },
    {
        "video_id": "Mnu1DzFzRWs",
        "video_title": "This Neural Network Animates Quadrupeds",
        "position_in_playlist": 281,
        "description": "The paper \"Mode-Adaptive Neural Networks for Quadruped Motion Control\" is available here:\nhttp://homepages.inf.ed.ac.uk/tkomura/dog.pdf\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Rafael Harutyuynyan, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-142173/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we have an animation movie or a computer game with quadrupeds, and we are yearning for really high-quality, lifelike animations, motion capture is often the go-to tool for that. Motion capture means that we put an actor, in our case, a dog in the studio, we ask it to perform sitting, trotting, pacing and jumping, record its motion, and transfer it onto our virtual character. This generally works quite well, however, there are many difficulties with this process. We will skip over the fact that an artist or engineer has to clean and label the recorded data, which is quite labor-intensive, but there is a bigger problem. We have all these individual motion types at our disposal, however, a virtual character will also need to be able to transition between these motions in a smooth and natural manner. Recording all possible transitions between these moves is not feasible, so in an earlier work, we looked at a neural network-based technique to try to weave these motions together. For the first sight, this looks great, however, have a look at these weird sliding motions that it produces. Do you see them? They look quite unnatural. This new method tries to address this problem but ends up offering much, much more than that. It requires only 1 hour of motion capture data, and we have only around 30 seconds of footage of jumping motions, which is basically next to nothing. And this technique can deal with unstructured data, meaning that it doesn't require manual labeling of the individual motion types, which saves a ton of work hours. Beyond that, as we control this character in the game, this technique also uses a prediction network to guess the next motion type, and a gating network that helps blending together these different motion types. Both of these units are neural networks. On the right, you see the results with the new method compared to a standard neural-network based solution on the left. Make sure to pay special attention to the foot sliding issues with the solution on the left and note that the new method doesn't produce any of those. Now, these motions look great, but they all take place on a flat surface. You see here that this new technique excels at much more challenging landscapes as well. This technique is a total powerhouse, and I can only imagine how many work hours this will save for artists working in the industry. It is also scientifically interesting and quite practical. My favorite combination. It is also well evaluated, so make sure to have a look at the paper for more details. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Mnu1DzFzRWs",
        "paper_link": "http://homepages.inf.ed.ac.uk/tkomura/dog.pdf",
        "paper_title": "Mode-Adaptive Neural Networks for Quadruped Motion Control"
    },
    {
        "video_id": "cEBgi6QYDhQ",
        "video_title": "Everybody Dance Now! - AI-Based Motion Transfer",
        "position_in_playlist": 282,
        "description": "Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nThe paper \"Everybody Dance Now\" is available here:\nhttps://arxiv.org/abs/1808.07371\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-2122473/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Do you remember style transfer? Style transfer is a mostly AI-based technique where we take a photograph, put a painting next to it, and it applies the style of the painting to our photo. That was amazing. Also, do you remember pose estimation? This is a problem where we have a photograph or a video of someone, and the output is a skeleton that shows the current posture of this person. So how about something that combines the power of pose estimation with the expressiveness of style transfer? For instance, this way, we could take a video of a professional dancer, then record a video of our own, let's say, moderately beautiful moves, and then, transfer the dancer's performance onto our own body in the video. Let's call it motion transfer. Have a look at these results. How cool is that?! As you see, these output videos are quite smooth, and this is not by accident. It doesn't just come out like that. With this technique, temporal coherence is taken into consideration. This means that the algorithm knows what it has done a moment ago and will not do something wildly different, making these dance motions smooth and believable. This method uses a generative adversarial network, where we have a neural network for pose estimation, or in other words, generating the skeleton from an image, and a generator network to create new footage when given a test subject and a new skeleton posture. These two neural networks battle each other and teach each other to distinguish and create more and more authentic footage over time. Some artifacts are still there, but note that this is among the first papers on this problem and it is already doing incredibly well. This is fresh and experimental. Just the way I like it. Two followup papers down the line and we'll be worried that we can barely tell the difference from authentic footage. Make sure to have a look at the paper, where you will see how the pix2pix algorithm was also used for image generation and there is a nice evaluation section as well. And now, let the age of AI-based dance videos begin. If you enjoyed this episode, please consider supporting us on Patreon where you can pick up really cool perks, like early access to these videos, voting on the order of future episodes, and more. We are available at patreon.com/TwoMinutePapers, or just click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cEBgi6QYDhQ",
        "paper_link": "https://arxiv.org/abs/1808.07371",
        "paper_title": "Everybody Dance Now"
    },
    {
        "video_id": "HvH0b9K_Iro",
        "video_title": "This AI Performs Super Resolution in Less Than a Second",
        "position_in_playlist": 283,
        "description": "The paper \"A Fully Progressive Approach to Single-Image Super-Resolution\" is available here:\nhttp://igl.ethz.ch/projects/prosr/\n\nA-Man's Caustic scene: http://www.luxrender.net/forum/gallery2.php?g2_itemId=27260\n\nCorresponding paper with Vlad Miller's spheres scene:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Evan Breznyik, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-1822544/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When looking for illustrations for a presentation, most of the time, I quickly find an appropriate photo on the internet, however, many of these photos are really low resolution. This often creates a weird situation where I have think, okay, do I use the splotchier, lower-resolution image that gets the point across, or take a high-resolution, crisp image that is less educational? In case you are wondering, I encounter this problem for almost every single video I make for this channel. As you can surely tell, I am waiting for the day when super resolution becomes mainstream. Super resolution means that we have a low resolution image that lacks details, and we feed it to a computer program, which hallucinates all the details onto it, creating a crisp, high-resolution image. This way, I could take my highly relevant, but blurry image, improve it, and use it in my videos. As adding details to images clearly requires a deep understanding of what is shown in these images, our seasoned Fellow Scholars immediately know that learning-based algorithms will be ideal for this task. While we are looking at some amazing results with this new technique, let's talk about the two key differences that this method introduces: One, it takes a fully progressive approach, which means that we don't immediately produce the highest resolution output we are looking for, but slowly leapfrog our way through intermediate steps, each of which is only slightly higher resolution than the input. This means that the final output is produced over several steps, where each problem is only a tiny bit harder than the previous one. This is often referred to as curriculum learning and it not only increases the quality of the solution, but is also easier to train as solving each intermediate step is only a little harder than the previous one. It is a bit like how students learn in school: first, the students are shown some easy introductory tasks to get a grasp of a problem, and slowly work their way towards mastering a field by solving problems that gradually increase in difficulty. Two, now, we can start playing with the thought of using a generative adversarial network. We talk a lot about this architecture in this series, at this time I will only note that training these is fraught with difficulties, so every bit of help we can get is more than welcome, so the role of curriculum learning is to help easing this process. Note that this research field is well-explored, and has a remarkable number of papers, so I was expecting a lot of comparisons against competing techniques. And when looking at the paper and the supplementary materials, boy, did I get it! Make sure to have a look at the paper, it contains a very exhaustive validation section, which reveals that if we measure the error of the solution in terms of human perception, it is only slightly lower quality than the best technique, however, this one is five times quicker, offering a really nice balance between quality and performance. So, what about the actual numbers for the execution time? For instance, upsampling an image to increase its resolution to twice its original size takes less than a second, and we can go to up to even eight times the original resolution, which also only takes four and a half seconds. The quality and the execution times indicate that we are again, one step closer to mainstream super resolution. What a time to be alive! The source code of this project is also available. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HvH0b9K_Iro",
        "paper_link": "http://igl.ethz.ch/projects/prosr/",
        "paper_title": "A Fully Progressive Approach to Single-Image Super-Resolution"
    },
    {
        "video_id": "GRQuRcpf5Gc",
        "video_title": "AI-Based Video-to-Video Synthesis",
        "position_in_playlist": 284,
        "description": "The paper \"Video-to-Video Synthesis\" and its source code is available here:\nhttps://tcwang0509.github.io/vid2vid/\nhttps://github.com/NVIDIA/vid2vid\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Evan Breznyik, Geronimo Moralez, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Do you remember the amazing pix2pix algorithm from last year? It was able to perform image translation, which means that it could take a daytime image and translate it into a nighttime image, create maps from satellite images, or create photorealistic shoes from a crude drawing. I remember that I almost fell off the chair when I've first seen the results. But this new algorithm takes it up a notch, and transforms these edge maps into human faces, not only that, but it also animates them in time. As you see here, it also takes into consideration the fact that the same edges may result in many different faces, and therefore it is also willing to gives us more of these options. If I fell out of the chair for the still image version, I don't really know what the appropriate reaction would be to this. It can also take a crude map of labels, where each color corresponds to one object class, such as roads, cars or buildings, and it follows how our labels evolve in time and creates an animation out of it. We can also change the meaning of our labels easily, for instance, in the lower left, you see how the buildings are now suddenly transformed to trees. Or, we can also change the trees to become buildings. Do you remember motion transfer from a couple videos ago? It can do a similar variant of that too, and even synthesizes the shadows around the character in a reasonably correct manner. As you see, the temporal coherence of this technique is second to none, which means that it remembers what it did with past images and doesn't do anything drastically different for the next frame, and therefore generates smoother videos. This is very apparent, especially when juxtaposed with the previous pix2pix method. So, how is this achieved? There are three key differences from the previous technique to achieve this: One, the original architecture uses a generator neural network to create images, where there is also a separate discriminator network that judges its work and teaches it to do better. Instead, this work uses two discriminator neural networks, one checks whether the images look good one by one, and one more discriminator for overlooking whether the sequence of these images would pass as a video. This discriminator cracks down on the generator network if it creates sequences that are not temporally coherent and this is why we have minimal flickering in the output videos. Fantastic idea. Two, to ease the training process, it also does it progressively, which means that the network is first faced with an easier version of the problem that progressively gets harder over time. If you have a look at the paper, you will see that the training is both progressive in terms of space and time. I love this idea too. Three, it also uses a flow map that describes the changes that took place since the previous frame. Note that this pix2pix algorithm was published in 2017, a little more than a year ago. I think that is a good taste of the pace of progress in machine learning research. Up to 2k resolution, 30 seconds of video, and the source code is available. Congratulations folks, this paper is something else. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=GRQuRcpf5Gc",
        "paper_link": "https://tcwang0509.github.io/vid2vid/",
        "paper_title": "Video-to-Video Synthesis"
    },
    {
        "video_id": "wR2OlsF1CEY",
        "video_title": "DeepMind's New AI Diagnoses Eye Conditions",
        "position_in_playlist": 285,
        "description": "The paper \"Clinically applicable deep learning for\ndiagnosis and referral in retinal disease\" is available here:\nhttps://deepmind.com/blog/moorfields-major-milestone/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-691269/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this video series, we often see how these amazing new machine learning algorithms can make our lives easier, and fortunately, some of them are also useful for serious medical applications. Specifically, medical imaging. Medical imaging is commonly used in most healthcare systems where an image of a chosen set of organs and tissues is made for a doctor to look at and decide whether medical intervention is required. The main issue is that the amount of diagnostic images out there in the wild increases at a staggering pace, and it makes it more and more infeasible for doctors to look at. But wait a minute, as more and more images are created, this also means that we have more training data for machine learning algorithms, so at the same time as human doctors get more and more swamped, the AI should get better and better over time! These methods can process orders of magnitude more of these images than humans, and after that, the final decision is put back into the hands of the doctor, who can now focus more on the edge cases and prioritize which patients should be seen immediately. This work from scientists at DeepMind was trained on about 14 thousand optical coherence tomography scans, this is the OCT label you see on the the left, these images are cross sections of the human retina. We first start our with this OCT scan, then, a manual segmentation step follows, where a doctor marks up this image to show where the most relevant parts, like the retinal fluids or the elevations of retinal pigments are. Before we proceed, let's stop here for a moment and look at some images of how the network can learn from the doctors and reproduce these segmentations by itself. Look at that! It's almost pixel perfect! This looks like science fiction. Now that we have the segmentation map, it is time to perform classification. This means that we look at this map and assign a probability to each possible condition that may be present. Finally, based on these, a final verdict is made whether the patient needs to be urgently seen, or just a routine check, or perhaps no check is required. The algorithm also learns this classification step and creates these verdicts itself. And of course, the question naturally arises: how accurate is this? Well, let's look at the confusion matrices! A confusion matrix shows us how many of the urgent cases were correctly classified as urgent, and how often it was misclassified as something else and what the something else was. The same analysis is performed to all other classes. Here is how the retina specialist doctors did, and here is how the AI did. I'll leave it there for a few seconds for you to inspect it. Really good! Here is also a different way of aggregating this data - the algorithm did significantly better than all of the optometrists and matched the performance of the number one retina specialist. I wouldn't believe any of these results if I didn't see these reports with my own eyes in the paper. An additional advantage of this technique is that it works on different kinds of imaging devices and it is among the first methods that works with 3D data. Another plus that I really liked is that this was developed as a close collaboration with a top tier eye hospital in London to make sure that the results are as practical as possible. The paper contains a ton of more information, so make sure to have a look! This was a herculean effort from the side of DeepMind, and the results are truly staggering. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wR2OlsF1CEY",
        "paper_link": "https://deepmind.com/blog/moorfields-major-milestone/",
        "paper_title": "Clinically applicable deep learning for\ndiagnosis and referral in retinal disease"
    },
    {
        "video_id": "0xlbzCXJpLM",
        "video_title": "Should an AI Learn Like Humans?",
        "position_in_playlist": 286,
        "description": "The paper \"Investigating Human Priors for Playing Video Games\" is available here:\nhttps://rach0012.github.io/humanRL_website/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-1428428/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper reveals us a fundamental difference between how humans and machines learn. Imagine the following situation: you are given a video game with no instructions, you start playing it, and the only information you get is a line of text when you successfully finished the game. That's it! So far, so good, this is relatively easy to play because the visual cues are quite clear: the pink blob looks like an adversary, and what the spikes do is also self-explanatory. This is easy to understand, so we can finish the game in less than a minute. Easy! Now, let's play this. Whoa! What is happening? Even empty space looks like as if it were a solid tile. I am not sure if I can finish this version of the game, at least not in a minute for sure. So, what is happening here is that some of artwork of the objects has been masked out. As a result, this version of the game is much harder to play for humans. So far, this is hardly surprising, and if that would be it, this wouldn't have been a very scientific experiment. However, this is not the case. So to proceed from this point, we will try to find what makes humans learn so efficiently, but not by changing everything at once, but by trying to change and measure only one variable at a time. How about this version of the game? This is still manageable, since the environment remains the same, only the objects we interact with have been masked. Through trial and error, we can find out the mechanics of the game. What about reversing the semantics? Spikes now became tasty ice cream, and the shiny gold conceals an enemy that eats us. Very apt, I have to say. Again, with this, the problem suddenly became more difficult for humans as we need some trial and error to find out the rules. After putting together several other masking strategies, they measured amount of time, the number of deaths and interactions that were required to finish the level. I will draw your attention mainly to the blue lines which show which variable caused how much degradation in the performance of humans. The main piece of insight is not only that these different visual cues throw off humans, but it tells us variable by variable, and also, by how much. An important insight here is that highlighting important objects and visual consistency are key. So what about the machines? How are learning algorithms affected? These are the baseline results. Adding masked semantics? Barely an issue. Masked object identities? This sounds quite hard, right? Barely an issue. Masked platforms and ladders? Barely an issue. This is a remarkable property of learning algorithms, as they don't only think in terms of visual cues, but in terms of mathematics and probabilities. Removing similarity information throws the machines off a bit, which is understandable, because the same objects may appear as if they were completely different. There is more analysis on this in the paper, so make sure to have a look. So, what are the conclusions here? Humans are remarkably good at reusing knowledge, and reading and understanding visual cues. However, if the visual cues become more cryptic, their performance drastically decreases. When machines start playing the game, at first, they have no idea which character they control, how gravity works or how to defeat enemies, or that keys are required to open doors. However, they learn these tricky problems and games much easier and quicker, because these mind-bending changes, as you remember, are barely an issue. Note that you can play the original and the obfuscated versions on the author's website as well. Also note that we really have only scratched the surface here, the paper contains a lot more insights. So, it is the perfect time to nourish your mind with a paper, so make sure to click it in the video description and give it a read. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=0xlbzCXJpLM",
        "paper_link": "https://rach0012.github.io/humanRL_website/",
        "paper_title": "Investigating Human Priors for Playing Video Games"
    },
    {
        "video_id": "dyzn3Fmtw-E",
        "video_title": "This Painter AI Fools Art Historians 39% of the Time",
        "position_in_playlist": 287,
        "description": "Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\nBitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\nPayPal: https://www.paypal.me/TwoMinutePapers\nEthereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\nLTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe paper \"A Style-Aware Content Loss for Real-time HD Style Transfer\" is available here:\nhttps://compvis.github.io/adaptive-style-transfer/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1478831/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Style transfer is a mostly AI-based technique where we take a photograph, put a painting next to it, and it applies the style of the painting to our photo. A key insight of this new work is that a style is complex and it can only be approximated with one image. One image is just one instance of a style, not the style itself. Have a look here - if we take this content image, and use Van Gogh's \"Road with Cypress and Star\" painting as the art style, we get this. However, if we would have used Starry Night instead, it would have resulted in this. This is not learning about a style, this is learning a specific instance of a style! Here you see two previous algorithms that were instead, trained on a collection of works from Van Gogh. However, you see that they are a little blurry and lack detail. This new technique is able to address this really well - also, look at how convincingly it stylized the top silhouettes of the bell tower. It can also deal with HD videos at a reasonable speed of 9 of these images per second. Very tasty, love it! And of course, as style transfer is a rapidly growing field, there are ample comparisons in the paper against other competing techniques. The results are very convincing - I feel that in most cases, it represents the art style really well and can decide where to leave the image content similar to the input and where to apply the style so the overall outlook of the image remains similar. So we can look at these results and discuss who likes which one all day long, but there are also other, more objective ways of evaluating such an algorithm. What is really cool is that the technique was tested by human art history experts, and they not only found this method to be the most convincing of all the other style transfer methods, but also thought that the AI-produced paintings were from an artist 39% of the time. So this means that the algorithm is able to learn the essence of an artistic style from a collection of images. This is a huge leap forward. Make sure to have a look at the paper that also describes a new style-aware loss function and differences in the training process of this method as well. And, if you enjoyed this episode and would like to see more, please help us exist through Patreon. In this website, you can support the series and pick up cool perks like early access to these videos, deciding the order of future episodes, and more. You know the drill, a dollar a month is almost nothing, but it keeps the papers coming. We also support cryptocurrencies, you'll find more information about this in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dyzn3Fmtw-E",
        "paper_link": "https://compvis.github.io/adaptive-style-transfer/",
        "paper_title": "A Style-Aware Content Loss for Real-time HD Style Transfer"
    },
    {
        "video_id": "DuMmcVOsNcs",
        "video_title": "These Neural Networks Empower Digital Artists",
        "position_in_playlist": 288,
        "description": "The paper \"Differentiable Image Parameterizations\" is available here:\nhttps://distill.pub/2018/differentiable-parameterizations/\n\nDistill editorial article - see how you can contribute here:\nhttps://distill.pub/2018/editorial-update/\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Esa Turkulainen, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: http://twominutepapers.com/\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we have seen many times how good neural network-based solutions are at image classification. This means that the network looks at an image and successfully identifies its contents. However, neural network-based solutions are also capable of empowering art projects by generating new, interesting images. This beautifully written paper explores how a slight tweak to a problem definition can drastically change the output of such a neural network. It shows how many of these research works can be seen as the manifestation of the same overarching idea. For instance, we can try to visualize what groups of neurons within these networks are looking for, and we get something like this. The reason for this is that important visual features, like the eyes can appear at any part of the image and different groups of neurons look for it elsewhere. With a small modification, we can put these individual visualizations within a shared space and create a much more consistent and readable output. In a different experiment, it is shown how a similar idea can be used with Compositional Pattern Producing Networks, or CPPNs in short. These networks are able to take spatial positions as an input and produce colors on the output, thereby creating interesting images of arbitrary resolution. Depending on the structure of this network, it can create beautiful images that are reminiscent of light-paintings. And here you can see how the output of these networks change during the training process. They can also be used for image morphing as well. A similar idea can be used to create images that are beyond the classical 2D RGB images, and create semi-transparent images instead. And there is much, much more in the paper, for instance, there is an interactive demo that shows how we can seamlessly put this texture on a 3D object. It is also possible to perform neural style transfer on a 3D model. This means that we have an image for style, and a target 3D model, and, you can see the results over here. This paper is a gold mine of knowledge, and contains a lot of insights on how neural networks can further empower artists working in the industry. If you read only one paper today, it should definitely be this one, and this is not just about reading, you can also play with these visualizations, and as the source code is also available for all of these, you can also build something amazing on top of them. Let the experiments begin! So, this was a paper from the amazing Distill journal, and just so you know, they may be branching out to different areas of expertise, which is amazing news. However, they are looking for a few helping hands to accomplish that, so make sure to click the link to this editorial update in the video description to see how you can contribute. I would personally love to see more of these interactive articles. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=DuMmcVOsNcs",
        "paper_link": "https://distill.pub/2018/differentiable-parameterizations/",
        "paper_title": "Differentiable Image Parameterizations"
    },
    {
        "video_id": "UkWnExEFADI",
        "video_title": "Neural Material Synthesis, This Time On Steroids",
        "position_in_playlist": 289,
        "description": "The paper \"Single-Image SVBRDF Capture with a Rendering-Aware Deep Network\" is available here:\nhttps://team.inria.fr/graphdeco/fr/projects/deep-materials/\n\nRecommended for you - Neural Material Synthesis: https://www.youtube.com/watch?v=XpwW3glj2T8\n\nPick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. With this technique, we can take a photograph of a desired material, and use a neural network to create a digital material model that matches it that we can use in computer games and animation movies. We can import real world materials in our virtual worlds, if you will. Typically, to do this, an earlier work required two photographs, one with flash, and one without to get enough information about the reflectance properties of the material. Then, a followup AI paper was able to do this from only one image. It doesn't even need to turn the camera around the material to see how it handles reflections, but can learn all of these material properties from only one image. Isn't that miraculous? We talked about this work in more detail in Two Minute Papers episode 88, that was about two years ago, I put a link to it in the video description. Let's look at some results with this new technique! Here you see the photos of the input materials and on the right, the reconstructed material. Please note that this reconstruction means that the neural network predicts the physical properties of the material, which are then passed to a light simulation program. So on the left, you see reality, and on the right, the prediction plus simulation results under a moving point light. It works like magic. Love it. As you see in the comparisons here, it produces results that are closer to the ground truth than previous techniques. So what is the difference? This method is designed in a way that enables us to create a larger training set for more accurate results. As you know, with learning algorithms, we are always looking for more and more training data. Also, it uses two neural networks instead of one, where one of them looks at local nearby features in the input, and the other one runs in parallel and ensures that the material that is created is also globally correct. Note that there are some highly scattering materials that this method doesn't support, for example, fabrics or human skin. But since producing these materials in a digital world takes quite a bit of time and expertise, this will be a godsend for the video games and animation movies of the future. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UkWnExEFADI",
        "paper_link": "https://team.inria.fr/graphdeco/fr/projects/deep-materials/",
        "paper_title": "Single-Image SVBRDF Capture with a Rendering-Aware Deep Network"
    },
    {
        "video_id": "txHQoYKaSUk",
        "video_title": "This Robot Learned To Clean Up Clutter",
        "position_in_playlist": 290,
        "description": "The paper \"Learning Synergies between Pushing and Grasping\nwith Self-supervised Deep Reinforcement Learning\" is available here:\nhttp://vpg.cs.princeton.edu/\n\nPick up cool perks on our Patreon page: \n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-3198094/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This robot was tasked to clean up this table. Normally, anyone who watches this series knows that would be no big deal for any modern learning algorithm. Just grab it, right? Well, not in this case, because reason number one: several objects are tightly packed together, and reason number two, they are too wide to hold with the fingers. What this means is that the robot needs to figure out a series additional actions to push the other pieces around and finally, grab the correct one. Look! It found out that sometimes, pushing helps grasping by making space for the fingers to grab these objects. This is a bit like the Roomba vacuum cleaner robot, but even better, for clutter. Really cool. This robot arm works the following way: it has an RGB-D camera, which endows it with the ability to see both color and depth. Now that we have this image, we have not one, but two neural networks looking at it: one is used to predict the utility of pushing at different possible locations, and one for grasping. Finally, a decision is made as to which motion would lead to the biggest improvement in the state of the table. So, what about the training process? As you see, the speed of this robot arm is limited, and we may have to wait for a long time for it to learn anything useful and not just flail around destroying other nearby objects. The solution includes my favorite part - training the robot within a simulated environment, where these commands can be executed within milliseconds, speeding up the training process significantly. Our hope is always that the principles learned within the simulation applies to reality. Checkmark. The simulation is also very useful to make comparisons with other state of the art algorithms easier. And, do you know what the bane of many-many learning algorithms is? Generalization. This means that if the technique was designed well, it can be trained on matte looking, wooden blocks, and it will do well when it encounters new objects that are vastly different in shape and appearance. And as you see on the right, remarkably, this is exactly the case. Checkmark. This takes us one step closer to learning algorithms that can see the world around us, interpret it, and make proper decisions to carry out a plan. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=txHQoYKaSUk",
        "paper_link": "http://vpg.cs.princeton.edu/",
        "paper_title": "Learning Synergies between Pushing and Grasping\nwith Self-supervised Deep Reinforcement Learning"
    },
    {
        "video_id": "kBFMsY5ZP0o",
        "video_title": "This AI Senses Humans Through Walls \ud83d\udc40",
        "position_in_playlist": 291,
        "description": "Pick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe paper \"Through-Wall Human Pose Estimation Using Radio Signals \" is available here:\nhttp://rfpose.csail.mit.edu/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/en/texture-wall-gray-wall-texture-1033755/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Pose estimation is an interesting area of research where we typically have a few images or video footage of humans, and we try to automatically extract the pose a person was taking. In short, the input is one or more photo, and the output is typically a skeleton of the person. So what is this good for? A lot of things. For instance, we can use these skeletons to cheaply transfer the gestures of a human onto a virtual character, fall detection for the elderly, analyzing the motion of athletes, and many many others. This work showcases a neural network that measures how the wifi radio signals bounce around in the room and reflect off of the human body, and from these murky waves, it estimates where we are. Not only that, but it also accurate enough to tell us our pose. As you see here, as the wifi signal also traverses in the dark, this pose estimation works really well in poor lighting conditions. That is a remarkable feat. But now, hold on to your papers, because that's nothing compared to what you are about to see now. Have a look here. We know that wifi signals go through walls. So perhaps, this means that...that can't be true, right? It tracks the pose of this human as he enters the room, and now, as he disappears, look, the algorithm still knows where he is. That's right! This means that it can also detect our pose through walls! What kind of wizardry is that? Now, note that this technique doesn't look at the video feed we are now looking at. It is there for us for visual reference. It is also quite remarkable that the signal being sent out is a thousand times weaker than an actual wifi signal, and it also can detect multiple humans. This is not much of a problem with color images, because we can clearly see everyone in an image, but the radio signals are more difficult to read when they reflect off of multiple bodies in the scene. The whole technique work through using a teacher-student network structure. The teacher is a standard pose estimation neural network that looks at a color image and predicts the pose of the humans therein. So far, so good, nothing new here. However, there is a student network that looks at the correct decisions of the teacher, but has the radio signal as an input instead. As a result, it will learn what the different radio signal distributions mean and how they relate to human positions and poses. As the name says, the teacher shows the student neural network the correct results, and the student learns how to produce them from radio signals instead of images. If anyone said that they were working on this problem ten years ago, they would have likely ended up in an asylum. Today, it's reality. What a time to be alive! Also, if you enjoyed this episode, please consider supporting the show at Patreon.com/twominutepapers. You can pick up really cool perks like getting your name shown as a key supporter in the video description and more. Because of your support, we are able to create all of these videos smooth and creamy, in 4k resolution and 60 frames per second and with closed captions. And, we are currently saving up for a new video editing rig to make better videos for you. We also support one-time payments through PayPal and the usual cryptocurrencies. More details about all of these are available in the video description, and as always. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kBFMsY5ZP0o",
        "paper_link": "http://rfpose.csail.mit.edu/",
        "paper_title": "Through-Wall Human Pose Estimation Using Radio Signals "
    },
    {
        "video_id": "6IsIGp1IezE",
        "video_title": "Brain-to-Brain Communication is Coming!",
        "position_in_playlist": 292,
        "description": "The paper \"BrainNet: A Multi-Person Brain-to-Brain Interface for Direct Collaboration Between Brains\" is available here:\nhttps://arxiv.org/abs/1809.08632\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: https://www.amazon.com/dp/B06ZZ23V2L\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-2213009/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#brainnet #neuralink",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is an episode that doesn't have the usual visual fireworks and is expected to get fewer clicks, but it is an important story to tell, and because of your support, we are able to cover a paper like this. So now, get this, this is a non-invasive brain-to-brain interface that uses EEG to record brain signals and TMS to deliver information to the brain. The non-invasive part is quite important, it basically means that we don't need to drill a hole in the head of the patients. That's a good idea. This image shows three humans connected via computers, 2 senders and one receiver. The senders provide information to the receiver about something he would otherwise not know about, and we measure if they are able to collaboratively solve a problem together. These people never met each other, and don't even know each other, and they can collaborate through this technique directly via brain signals. Wow! The BCI means brain-computer interface, and the CBI, as you guessed, the computer-brain interface. So these brain signals can be encoded and decoded and freely transferred between people and computers. Insanity. After gathering all this information, the receiver makes a decision, which the senders also have access to, and can transmit some more information if necessary. So what do they use it for? Of course, to play Tetris! Jokes aside, this is a great experiment where the goal is to clear a line. Simple enough, right? Not so much, because there is a twist. The receiver only sees what you see here on the left side. This is the current piece we have to place on the field, but the receiver has no idea how to rotate it because he doesn't see its surroundings. But the senders do, so they transmit the appropriate information to the receiver who will now be able to make an informed decision as to how to rotate the piece correctly to clear a line. So, does it work? The experiment is designed in a way that there is a 50% chance to be right without any additional information for the receiver, so this will be the baseline result. And the results are between 75% and 85%, which means that the interface is working, and the brain-to-brain collaboration is now a reality. I am out of words. The paper also talks about brain-to-brain social networks, and all kinds of science fiction like that. My head is about to explode with the possibilities, who knows, maybe in a few years, we can make a superintelligent brain that combines all of our expertise and does research for all of us. Or, writes Two Minute Papers episodes. This paper is a must read. Do you have any other ideas as to how this could enhance our lives? Let me know below in the comments. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6IsIGp1IezE",
        "paper_link": "https://arxiv.org/abs/1809.08632",
        "paper_title": "BrainNet: A Multi-Person Brain-to-Brain Interface for Direct Collaboration Between Brains"
    },
    {
        "video_id": "Bv3yat484aQ",
        "video_title": "Multilayer Light Simulations: More Beautiful Images, Faster",
        "position_in_playlist": 293,
        "description": "The paper \"Position-Free Monte Carlo Simulation for Arbitrary Layered BSDFs\" is available here:\nhttps://shuangz.com/projects/layered-sa18/\n\nMy Rendering course at the TU Wien:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nMore about the talk:\n\u203a Conference - https://ec.europa.eu/epsc/events/election-interference-digital-age-building-resilience-cyber-enabled-threats_en\n\u203a My think piece - https://medium.com/election-interference-in-the-digital-age\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to talk about the craft of simulating rays of light to create beautiful images, just like the ones you see here. And when I say simulating rays of light, I mean not a few, but millions and millions of light rays need to be computed, alongside with how they get absorbed or scattered off of our objects in a virtual scene. Initially, we start out with a really noisy image, and as we add more rays, the image gets clearer and clearer over time. The time it takes for these images to clean up depends on the complexity of the geometry and our material models, and one thing is for sure: rendering materials that have multiple layers is a nightmare. This paper introduces an amazing new multilayer material model to address that. Here you see an example where we are able to stack together transparent and translucent layers to synthesize a really lifelike scratched metal material with water droplets. Also, have a look at these gorgeous materials. And note that these are all virtual materials that are simulated using physics and computer graphics. Isn't this incredible? However, some of you Fellow Scholars remember that we talked about multilayered materials before. So what's new here? This new method supports more advanced material models that previous techniques were either unable to simulate, or took too long to do so. But that's not all - have a look here. What you see is an equal-time comparison, which means that if we run the new technique against the older methods for the same amount of time, it is easy to see that we will have much less noise in our output image. This means that the images clear up quicker and we can produce them in less time. It also supports my favorite, multiple importance sampling, an aggressive noise-reduction technique by Eric Veach which is arguably one of the greatest inventions ever in light transport research. This ensures that for more difficult scenes, the images clean up much, much faster and has a beautiful and simple mathematical formulation. Super happy to see that it also earned him a technical Oscar award a few years ago. If you are enjoying learning about light transport, make sure to check out my course on this topic at the Technical University of Vienna. I still teach this at the University for 20 Master students at a time and thought that the teachings shouldn't only be available for a lucky few people who can afford a college education. Clearly, the teachings should be available for everyone, so we recorded it and put it online, and now everyone can watch it, free of charge. I was quite stunned to see that more than 10 thousand people decided to start it, so make sure to give it a go if you're interested! And just one more thing: as you are listening to this episode, I am holding a talk at the EU's Political Strategy Centre. And the objective of this talk is to inform political decisionmakers about the state of the art in AI so they can make more informed decisions for us. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Bv3yat484aQ",
        "paper_link": "https://shuangz.com/projects/layered-sa18/",
        "paper_title": "Position-Free Monte Carlo Simulation for Arbitrary Layered BSDFs"
    },
    {
        "video_id": "F-00NhYUnH4",
        "video_title": "This AI Learned How To Generate Human Appearance",
        "position_in_playlist": 294,
        "description": "Pick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nThe paper \"A Variational U-Net for Conditional Appearance and Shape Generation\" is available here:\nhttps://compvis.github.io/vunet/\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nEarlier NVIDIA episode: https://www.youtube.com/watch?v=VrgYtFhVGmg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/en/boy-ski-skiing-cold-goggles-kid-1835416/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we often discuss that neural networks are extraordinarily useful for classification tasks. This means that if we give them an image, they can tell us what's on it, which is great for self-driving cars, image search, and a variety of other applications. However, fewer people know that they can also be used for image generation. We've seen many great examples of this where NVIDIA's AI was able to dream up high-resolution images of imaginary celebrities. This was done using a generative adversarial network, an architecture where two neural networks battle each other. However, these methods don't work too well if we have too much variation in our datasets. For instance, they are great for faces, but not for synthesizing an entire human body. This particular technique uses a different architecture, and as a result, can synthesize an entire human body, and is also able to synthesize both shape and appearance. You will see in a moment that because of that, it can do magical things. For instance, in this example, all we have is one low-quality image of a test subject as an input, and we can give it a photo of a different person. What happens now is that the algorithm runs pose estimation on this input, and transforms our test subject into that pose. The crazy thing about this is that it even creates views for new angles we didn't even have access to! In this other experiment, we have one image on the left. What we can do here is that we specify not a person, but draw the pose directly, indicating that we wish to see our test subject in this pose, and the algorithm is also able to create an appropriate new image. And again, it works for angles that require information that we don't have access to. These new angles show that the technique understands the concept of shorts or trousers...although it seems to forget to put on socks sometimes. Truth be told, I don't blame it. What is even cooler is that it seems to behave very similarly for a variety of different inputs. This is non-trivial as this property doesn't just emerge out of thin air, and will be a great selling point for this new method. It also supports a feature where we need to give a crude drawing to the algorithm, and it will transform it into a photorealistic image. However, it is clear that there are many-many ways to fill this drawing with information, so how do we tell the algorithm what appearance we are looking for? Well, worry not, because this technique can also perform appearance transfer. This means that we can exert artistic control over the output by providing a photo of a different object, and it will transfer the style of this photo to our input. No artistic skills needed, but good taste is still as much of a necessity as ever. Yet another AI that will empower both experts and novice users alike. And while we are enjoying these amazing results, or even better, if you have already built up an addiction for the papers, you can keep it in check by supporting us on Patreon and in return, getting access to these videos earlier. You can find us through Patreon.com/TwoMinutePapers. There is a link to it in the video description, and as always, to the paper as well. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=F-00NhYUnH4",
        "paper_link": "https://compvis.github.io/vunet/",
        "paper_title": "A Variational U-Net for Conditional Appearance and Shape Generation"
    },
    {
        "video_id": "duCQUu8EQVA",
        "video_title": "Real-Time Holography Simulation!",
        "position_in_playlist": 295,
        "description": "The paper \"Acquiring Spatially Varying Appearance of Printed Holographic Surfaces\" is available here:\nhttps://wp.doc.ic.ac.uk/rgi/project/acquiring-spatially-varying-appearance-of-printed-holographic-surfaces/\n\nMaterial learning and synthesis algorithm:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nNeural Material Synthesis:\nhttps://www.youtube.com/watch?v=XpwW3glj2T8\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Michael Albrecht, Michael Jensen, Milan Lajto\u0161, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nTwo Minute Papers Merch:\nUS: https://www.amazon.com/dp/B06ZZ23V2L\nEU/Worldwide: https://shop.spreadshirt.net/TwoMinutePapers/\n\nThumbnail background image credit: https://pixabay.com/photo-1357029/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we wish to populate a virtual world with photorealistic materials, the last few years have offered a number of amazing techniques to do so. We can obtain such a material from a flash and no-flash photograph pair of the target material, and having a neural network create a digital version of it, or remarkably, even just one photograph is enough to perform this. This footage that you see here shows these materials after they have been rendered by a light simulation program. If we don't have physical access to these materials, we can also use a recent learning algorithm to learn our preferences and recommend new materials that we would enjoy. However, whenever I publish such a video, I always get comments asking: \"but what about the more advanced materials?\", and my answer is, you are right! Have a look at this piece of work which is about acquiring printed holographic materials. This means that we have physical access to this holographic pattern, put a camera close by, and measure data in a way that can be imported into a light simulation program to make a digital copy of it. This idea is much less far-fetched than it sounds, because we find such materials in many everyday objects, like bank notes, gift bags, clothing, or of course or security holograms. However, it is also quite difficult. Look here! As you see, these holographic patterns are quite diverse, and a well-crafted algorithm would have to be able to capture this rotation effect, circular diffractive areas, firework effects and even this iridescent glitter. That is quite a challenge! This paper proposes two novel techniques to approach this problem. The first one assumes that there is some sort of repetition in the visual structure of the hologram and takes that into consideration. As a result, it can give us high-quality results by only taking 1 to 5 photographs of the target material. The second method is more exhaustive and needs more specialized hardware, but, in return, can deal with arbitrary structures, and requires at least 4 photographs at all times. These are both quite remarkable - just think about the fact that these materials look different from every viewing angle, and they also change over the surface of the object. And for the first technique, we don't need sophisticated instruments, only a consumer DSLR camera is required. The reconstructed digital materials can be used in real time, and what's more, we can also exert artistic control over the outputs by modifying the periodicities of the material. How cool is that! And, if you are about to subscribe to the series or you are already subscribed, make sure to click the bell icon, or otherwise, you may miss future episodes. That would be a bummer, because I have a lot more amazing papers to show you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=duCQUu8EQVA",
        "paper_link": "https://wp.doc.ic.ac.uk/rgi/project/acquiring-spatially-varying-appearance-of-printed-holographic-surfaces/",
        "paper_title": "Acquiring Spatially Varying Appearance of Printed Holographic Surfaces"
    },
    {
        "video_id": "X1cPSvPagNI",
        "video_title": "Full-Time Papers, Maybe Someday?",
        "position_in_playlist": 296,
        "description": "Pick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1149962/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This video is not about a paper, but the about the video series itself. I don't do this often, but for the sake of transparency, I wanted to make sure to tell you about this. We have recently hit 175.000 subscribers on the channel! I find this number almost unfathomable. And please know that this became a possibility only because of you, so I would like to let you know how grateful I am for your support here on YouTube and Patreon. As most of you know, I still work as a full-time researcher at the Technical University of Vienna and I get the question \"why not go full time on Two Minute Papers?\" from you Fellow Scholars increasingly often. The answer is that over time, I would love to, but our current financial situation does not allow it. Let me explain that. First, I tried the run the channel solely on YouTube ads, which led to a rude awakening - most people (myself included) are very surprised when they hear that the rates had become so low that around the first one million viewer mark, the series earned less than a dollar a day. Then, we introduced Patreon, and with your support, now we are able to buy proper equipment to make better videos for you. I can, without hesitation, say that you are the reason this channel can exist. Everything is better this way: now we have two independent revenue sources (Patreon being the most important), however, whenever YouTube or Patreon monetization issues arise, you see many other channels disappearing into the ether. I am terrified of this and I want to do everything I possibly can to make sure that this does not happen to us and we can keep running this series for a long-long time. However, if anything happens to any of these revenue streams, simply put, we are toast. Even though it would be a dream come true, because of this, it would be irresponsible to go full time on the papers. To remedy this, we have been thinking about introducing a third revenue stream with sponsorships for a small amount of videos each month. The majority, 75% of the videos would remain Patreon supported, and the remaining 25% would be sponsored, just enough to enable me and my wife to do this full time in the future. There would be no other changes to the videos, Patreon supporters get all of them in early access, and as before, I choose the papers too. With this, if something happens to any one of the revenue streams, we would be able to keep the series afloat without any delays. I would also have more time to every now and then, fly out and inform key political decision makers on the state of AI so they can make better decisions for us. Everything else would remain the same, the videos would arrive more often in time, and the dream could perhaps come true. I think transparency is of utmost importance and wanted to make sure to inform you before any change happens. I hope you are okay with this. And, if you are a company and you are interested in sponsoring the series, let's talk. As always thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=X1cPSvPagNI"
    },
    {
        "video_id": "o-LU_Dja6Ks",
        "video_title": "This AI Shows Us the Sound of Pixels",
        "position_in_playlist": 297,
        "description": "The paper \"The Sound of Pixels\" is available here:\nhttp://sound-of-pixels.csail.mit.edu/\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1606337/\n\nYouTube video credits:\nOld Wine - https://www.youtube.com/watch?v=bLjS6E6c0IA\nFabian Rivero - https://www.youtube.com/watch?v=-HLTNgdajqw\nMichael Mikulka - https://www.youtube.com/watch?v=n8-2q4dheyU\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a neural network-based method that is able to show us the sound of pixels. What this means is that it separates and localizes audio signals in videos. The two keywords are separation and localization, so let's take a look at these one by one. Localization means that we can pick a pixel in the image and it show us the sound that comes from that location, and the separation part means that ideally, we will only hear that particular sound source. Let's have a look at an example. Here is an input video. And now, let's try to separate the sound of the chello and see if it knows where it comes from. Same with the guitar. Now for a trickier question...even though there are sound reverberations off the walls, but the walls don't directly emit sound themselves, so I am hoping to hear nothing now, let's see... flat signal, great! So, how does this work? It is a neural-network based solution that has watched 60 hours of musical performances to be able to pull this off, and it learns that a change in sound can often be tracked back to a change in the video footage as a musician is playing an instrument. As a result, get this, no supervision is required. This means that we don't need to label this data, or in other words, we don't need to specify how each pixel sounds, it learns to infer all this information from the video and sound signals by itself. This is huge, and otherwise, just imagine how many work-hours that would require to annotate all this data. And, another cool application is that if we can separate these signals, then we can also independently adjust the sound of these instruments. Have a look. Now, clearly, it is not perfect as some frequencies may bleed over from one instrument to the other, and there also are other methods to separate audio signals, but this particular one does not require any expertise, so I see a great value proposition there. If you wish to create a separate version of a video clip and use it for karaoke, or just subtract the guitar and play it yourself, I would look no further. Also, you know the drill, this will be way better a couple papers down the line. So, what do you think? What possible applications do you envision for this? Where could it be improved? Let me know below in the comments. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=o-LU_Dja6Ks",
        "paper_link": "http://sound-of-pixels.csail.mit.edu/",
        "paper_title": "The Sound of Pixels"
    },
    {
        "video_id": "zL6ltnSKf9k",
        "video_title": "This AI Learned To Isolate Speech Signals",
        "position_in_playlist": 298,
        "description": "The paper \"Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation \" is available here:\nhttps://looking-to-listen.github.io/\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-3565815/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a neural network-based technique that can perform audio-visual separation. Before we talk about what that is, I will tell you what it is not. It is not what we've seen in the previous episode where we could select a pixel and listen to it, have  a look. This one is different. This new technique can clean up an audio signal by suppressing the noise in a busy bar, even if the source of the noise is not seen in the video. It can also enhance the voice of the speaker at the same time. Let's listen. Or, if we have a skype-meeting with someone in a lab or a busy office where multiple people are speaking nearby, we can also perform a similar speech separation, which would be a godsend for future meetings. And I think if you are a parent, the utility this example needs no further explanation. I am not sure if I ever encountered the term \"screaming children\" in the abstract of an AI paper, so that one was also a first here. This is a super difficult task, because the AI needs to understand what lip motions correspond to what kind of sounds, which is different for all kinds of languages, age groups, and head positions. To this end, the authors put together a stupendously large dataset with almost 300.000 videos with clean speech signals. This dataset is then run through a multi-stream neural network that detects the number of human faces within the video, generates small thumbnails of them, and observes how they move over time. It also analyzes the audio signal separately, then fuses these elements together with a recurrent neural network to output the separated audio waveforms. A key advantage of this architecture and training method is that as opposed to many previous works, this is speaker-independent, therefore we don't need specific training data from the speaker we want to use this on. This is a huge leap in terms of usability. The paper also contains an excellent demonstration of this concept by taking a piece of footage from Conan O'Brien's show where two comedians were booked for the same time slot and talk over each other. The result is a performance where it is near impossible to understand what they are saying, but with this technique, we can hear both of them one by one, crystal clear. You see some results over here, but make sure to click the paper link in the description to hear the sound samples as well. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=zL6ltnSKf9k",
        "paper_link": "https://looking-to-listen.github.io/",
        "paper_title": "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation "
    },
    {
        "video_id": "fzuYEStsQxc",
        "video_title": "This Curious AI Beats Many Games...and Gets Addicted to the TV",
        "position_in_playlist": 299,
        "description": "Pick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 13hhmJnLEzwXgmgJN7RB6bWVdT7WkrFAHh\n\u203a Ethereum: 0x002BB163DfE89B7aD0712846F1a1E53ba6136b5A\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThe paper \"Large-Scale Study of Curiosity-Driven Learning\" is available here:\nPaper - https://pathak22.github.io/large-scale-curiosity/\nBlog post - https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-3774381/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a learning algorithm that chooses a set of actions in an environment to maximize a score. This class of techniques enables us to train an AI to master video games, avoiding obstacles with a drone, cleaning up a table with a robot arm, and has many more really cool applications. We use the word score and reward interchangeably, and the goal is that over time, the agent has to learn to maximize a prescribed reward. So where should the rewards come from? Most techniques work by using extrinsic rewards. Extrinsic rewards are only a half-solution as they need to come from somewhere, either from the game in the form of a game score, which simply isn't present in every game. And even if it is present in a game, it is very different for Atari breakout and for instance, a strategy game. Intrinsic rewards are designed to come to the rescue, so the AI would be able to completely ignore the in-game score and somehow have some sort of inner motivation to drive an AI to complete a level. But what could possibly be a good intrinsic reward that would work well on a variety of tasks? Shouldn't this be different from problem to problem? If so, we are back to square one. If we are to call our learner intelligent, then we need one algorithm that is able to solve a large number of different problems. If we need to reprogram it for every game, that's just a narrow intelligence. So, a key finding of this paper is that we can endow the AI with a very human-like property - curiosity. Human babies also explore the world out of curiosity and as a happy side-effect, learn a lot of useful skills to navigate in this world later. However, as in our everyday speech, the definition of curiosity is a little nebulous, we have to provide a mathematical definition for it. In this work, this is defined as trying to maximize the number of surprises. This will drive the learner to favor actions that lead to unexplored regions and complex dynamics in a game. So, how do these curious agents fare? Well, quite good! In Pong, when the agent plays against itself, it will end up in long matches passing the ball between the two paddles. How about bowling? Well, I cannot resist but quote the authors for this one. The agent learned to play the game better than agents trained to maximize the (clipped) extrinsic reward directly. We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes. With a little stretch one could perhaps say that this AI is showing signs of addiction. I wonder how it would do with modern mobile games with loot boxes? But, we'll leave that for future work now. How about Super Mario? Well, the agent is very curious to see how the levels continue, so it learns all the necessary skills to beat the game. Incredible. However, the more seasoned Fellow Scholars immediately find that there is a catch. What if we sit down the AI in front of a TV that constantly plays new material? You may think this is some kind of a joke, but it's not. It is a perfectly valid issue, because due to its curiosity, the AI would have to stay there forever and not start exploring the level. This is the good old definition of TV addiction. Talk about humanlike properties. And sure enough, as soon as we turn off the TV, the agent gets to work immediately. Who would have thought! The paper notes that this challenge needs to be dealt with over time, however, the algorithm was tested on a large variety of problems, and it did not come up in practice. And the key insight is that curiosity is not only a great replacement for extrinsic rewards, the two are often aligned, but curiosity, in some cases, is even superior to that. That is an amazing value proposition for something that we can run on any problem without any additional work. So, curious agents that are addicted to flashing score screens and TVs. What a time to be alive! And, if you enjoyed this episode and you wish to help us on our quest to inform even more people about these amazing stories, please consider supporting us on Patreon.com/TwoMinutePapers. You can pick up cool perks there to keep your papers addiction in check. As always, there is a link to it and to the paper in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fzuYEStsQxc",
        "paper_link": "https://pathak22.github.io/large-scale-curiosity/",
        "paper_title": "Large-Scale Study of Curiosity-Driven Learning"
    },
    {
        "video_id": "ozUzomVQsWc",
        "video_title": "This AI Learns Acrobatics by Watching YouTube",
        "position_in_playlist": 300,
        "description": "This episode was supported by insilico.com. \"Anything outside life extension is a complete waste of time\". See their papers:\n- Papers: https://www.ncbi.nlm.nih.gov/pubmed/?term=Zhavoronkov%2Ba\n- Website: http://insilico.com/\n\nThe paper \"SFV: Reinforcement Learning of Physical Skills from Videos\" is available here:\n1. https://xbpeng.github.io/projects/SFV/index.html\n2. https://bair.berkeley.edu/blog/2018/10/09/sfv/\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, John De Witt, Kjartan Olason, Lorin Atzberger, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we have an animation movie or a computer game where, like in any other digital medium, we wish to have high-quality, lifelike animations for our characters, we likely have to use motion capture. Motion capture means that we put an actor in the studio, and we ask this person to perform cartwheels and other motion types that we wish to transfer to our virtual characters. This works really well, but recording and cleaning all this data is a very expensive and laborious process. As we are entering the age of AI, of course, I wonder if there is a better way to do this? Just think about it...we have no shortage of videos here on the Youtube about people performing cartwheels and other moves, and we have a bunch of learning algorithms that know what pose they are taking during the video. Surely we can make something happen here, right? Well, yes, and no. A few methods already exist to perform this, but all of them have dealbreaking drawbacks. For instance, this previous work predicts the body poses for each frame, but each of them have small individual inaccuracies that produce this annoying flickering effect. Researchers like to refer to this as the lack of temporal coherence. But, this new technique is able to remedy this. Great result! This new work also boasts a long list of other incredible improvements. For instance, the resulting motions are also simulated in a virtual environment, and it is shown that they are quite robust - so much so, that we can throw a bunch of boxes against the AI, and it still can adjust to it. Kind of. These motions can be retargeted to different body shapes. You can see as it is demonstrated here quite aptly with this neat little nod to Boston Dynamics. It can also adapt to challenging new environments, or, get this, it can even work from a single photo instead of a video by completing the motion seen within. What kind of wizardry is that? How could it possibly perform that? It works the following way. First, we take an input photo or video, and perform pose estimation on it. But this is still a per-frame computation, and you remember that this doesn't give us temporal consistency. This motion reconstruction step ensures that we have smooth transitions between the poses. And now comes the best part: we start simulating a virtual environment, where a digital character tries to move its body parts to perform these actions. If we do this, we can not only reproduce these motions, but also continue them. This is where the wizardry lies. If you read the paper, which you should absolutely do, you will see that it uses OpenAI's amazing Proximal Policy Optimization algorithm to find the best motions. Absolutely amazing. So this can perform and complete a variety of motions, adapts to more challenging landscapes, and do all this in a temporally smooth manner. However, the Gangnam style dance still proves to be too hard. The technology is not there yet. We also thank Insilico Medicine for supporting this video. They work on AI-based drug discovery and aging research. They have some unbelievable papers on these topics. Make sure to check them out, and this paper as well in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ozUzomVQsWc",
        "paper_link": "https://xbpeng.github.io/projects/SFV/index.html",
        "paper_title": "SFV: Reinforcement Learning of Physical Skills from Videos"
    },
    {
        "video_id": "CIDRdLOWrXQ",
        "video_title": "Building a Curious AI With Random Network Distillation",
        "position_in_playlist": 302,
        "description": "This episode was supported by insilico.com. \"Anything outside life extension is a complete waste of time\". See their papers:\n- Papers: https://www.ncbi.nlm.nih.gov/pubmed/?term=Zhavoronkov%2Ba\n- Website: http://insilico.com/\n\nThe paper \"Exploration by Random Network Distillation\" is available here:\nBlog post: https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/\nPaper: https://arxiv.org/abs/1810.12894\nCode: https://github.com/openai/random-network-distillation\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://www.youtube.com/watch?v=4DdoZsOb53s\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#distillation",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In a previous episode, we talked about a class of learning algorithms that were endowed with curiosity. This new work also showcases a curious AI that aims to solve Montezuma's revenge which is a notoriously difficult platform game for an AI to finish. The main part of the difficulty arises from the fact that the AI needs to be able to plan for longer time periods, and interestingly, it also needs to learn that short-term rewards don't necessarily mean long-term success. Let's have a look at an example - quoting the authors: \"There are four keys and six doors spread throughout the level. Any of the four keys can open any of the six doors, but are consumed in the process. To open the final two doors the agent must therefore forego opening two of the doors that are easier to find and that would immediately reward it for opening them.\" So what this means is that we have a tricky situation, because the agent would have to disregard the fact that it is getting a nice score from opening the doors, and understand that these keys can be saved for later. This is very hard for an AI to resist, and, again, curiosity comes to the rescue. Curiosity, at least, this particular definition of it works in a way that the harder to guess for the AI what will happen, the more excited it gets to perform an action. This drives the agent to finish the game and explore as much as possible because it is curious to see what the next level holds. You see in the animation here that the big reward spikes show that the AI has found something new and meaningful, like losing a life, or narrowly avoiding an adversary. As you also see, climbing a ladder is a predictable, boring mechanic that the AI is not very excited about. Later, it becomes able to predict the results even better the second and third time around, therefore it gets even less excited about ladders. This other animation shows how this curious agent explores adjacent rooms over time. This work also introduces a technique, which the authors call random network distillation. This means that we start out from a completely randomly initialized, untrained neural network, and over time, slowly distill it into a trained one. This distillation also makes our neural network immune to the noisy TV problem from our previous episode, where our curious, unassuming agent would get stuck in front of a TV that continually plays new content. It also takes into consideration the score reported by the game, and has an internal motivation to explore as well. And hold on to your papers, because it can not only perform well in the game, but this AI is able to perform better than the average human. And again, remember that no ground truth knowledge is required, it was never demonstrated to the AI how one should play this game. Very impressive results indeed, and you see, the pace of progress in machine learning research is nothing short of incredible. Make sure to have a look at the paper in the video description for more details. We'd also like to send a big thank you to Insilico Medicine for supporting this video. They use AI for research on preventing aging, believe it or not, and are doing absolutely amazing work. Make sure to check them out in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=CIDRdLOWrXQ",
        "paper_link": "https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/",
        "paper_title": "Exploration by Random Network Distillation"
    },
    {
        "video_id": "V6G717ewUuw",
        "video_title": "Can an AI Learn To Draw a Caricature?",
        "position_in_playlist": 304,
        "description": "Pick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nThe paper \"CariGANs: Unpaired Photo-to-Caricature Translation\" is available here:\nhttps://cari-gan.github.io/\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1162213/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to super fun, and really good looking results. This subfield is only a few years old and has seen a number of amazing papers - style transfer for HD images, videos, and some of these forgeries were even able to make professional art curators think that they were painted by a real artist. So, here is a crazy idea -- how about using style transfer to create caricatures? Well, this sounds quite challenging. Just think about it - a caricature is an elusive art where certain human features are exaggerated, and generally, the human face needs to be simplified and boiled down into its essence. It is a very human thing to do. So how could possibly an AI be endowed with such a deep understanding of, for instance, a human face? That sounds almost impossible. Our suspicion is further reinforced as we look at how previous style transfer algorithms try to deal with this problem. Not too well, but no wonder, it would be unfair to expect great results as this is not what they were designed for. But now, look at these truly incredible results that were made with this new work. The main difference between the older works and this one is that one, it uses generative adversarial networks, GANs in short. This is an architecture where two neural networks learn together -- one learns to generate better forgeries, and the other learns to find out whether an image has been forged. However, this would still not create the results that you see here. An additional improvement is that we have not one, but two of these GANs. One deals with style. But, it is trained in a way to keep the essence of the image. And the other deals with changing and warping the geometry of the image to achieve an artistic effect. This leans on the input of a landmark detector that gives it around 60 points that show the location of the most important parts of a human face. The output of this geometry GAN is a distorted version of this point set, which can then be used to warp the style image to obtain the final output. This is a great idea because the amount of distortion applied to the points can be controlled. So, we can tell the AI how crazy of a result we are looking for. Great! The authors also experimented applying this to video. In my opinion, the results are incredible for a first crack at this problem. We are probably just one paper away from an AI automatically creating absolutely mind blowing caricature videos. Make sure to have a look at the paper as it has a ton more results and of course, every element of the system is explained in great detail there. And if you enjoyed this episode and you would like to access all future videos in early access, or, get your name immortalized in the video description as a key supporter, please consider supporting us on patreon.com/TwoMinutePapers. The link is available in the video description. We were able to significantly improve our video editing rig, and this was possible because of your generous support. I am so grateful, thank you so much! And, this is why every episode ends with the usual quote... Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=V6G717ewUuw",
        "paper_link": "https://cari-gan.github.io/",
        "paper_title": "CariGANs: Unpaired Photo-to-Caricature Translation"
    },
    {
        "video_id": "ZKQp28OqwNQ",
        "video_title": "BigGANs: AI-Based High-Fidelity Image Synthesis",
        "position_in_playlist": 305,
        "description": "This episode was supported by insilico.com. \"Anything outside life extension is a complete waste of time\". See their papers:\n- Papers: https://www.ncbi.nlm.nih.gov/pubmed/?term=Zhavoronkov%2Ba\n- Website: http://insilico.com/\n\nThe paper \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\" is available here:\n- Paper: https://arxiv.org/abs/1809.11096\n- Try it here: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-494706/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Approximately a 150 episodes ago, we looked at DeepMind's amazing algorithm that was able to look at a database with images of birds, and it could learn about them so much that we could provide a text description of an imaginary bird type and it would dream up new images of them. It was a truly breathtaking piece of work, and its main limitation was that it could only come up with coarse images. It didn\u2019t give us a lot of details. Later, we talked about NVIDIA's algorithm that started out with such a coarse image, but didn't stop there - it progressively recomputed this image many times, each time with more and more details. This was able to create imaginary celebrities with tons of detail. This new work offers a number of valuable improvements over the previous techniques: it can train bigger neural networks with even more parameters, create extremely detailed images with remarkable performance, so much so that if you have a reasonably powerful graphics card, you can run it yourself here. The link is in the video description. Training these neural networks is also more stable than it used to be with previous techniques. As a result, it not only supports creating these absolutely beautiful images, but also gives us the opportunity to exert artistic control on the outputs. I think this is super fun, I could play with this all day long. What's more, we can also interpolate between these images, which means that if we have desirable images A and B, it can compute intermediate images between them, and the challenging part is that these intermediate images shouldn't be some sort of average between the two, which would be gibberish, but they have to be images that are meaningful, and can stand on their own. Look at this! Flying colors. And now comes the best part. The results were measured in terms of their inception score. This inception score defines how recognizable and diverse these generated images are and most importantly, both of these are codified in a mathematical manner to reduce the subjectivity of the evaluation. This score is not perfect by any means, but it typically correlates well with the scores given by humans. The best of the earlier works had an inception score of around 50. And hold on to your papers, because the score of this new technique is no less than 166, and if we would measure real images, they would score around 233. What an incredible leap in technology. And we are even being paid for creating and playing with such learning algorithms. What a time to be alive! A big thumbs up for the authors of the paper for providing quite a bit of information on failure cases as well. We also thank Insilico Medicine for supporting this video. They are using these amazing learning algorithms to create new molecules, identify new protein targets with the aim to cure diseases and aging itself. Make sure to check them out in the video description. They are our first sponsors, and it's been such a joy to work with them. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZKQp28OqwNQ",
        "paper_link": "https://arxiv.org/abs/1809.11096",
        "paper_title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
    },
    {
        "video_id": "q22XWPM0Egc",
        "video_title": "AI Learning Morphology and Movement...at the Same Time!",
        "position_in_playlist": 306,
        "description": "The paper \"Reinforcement Learning for Improving Agent Design\" is available here:\nhttps://designrl.github.io/\nhttps://arxiv.org/abs/1810.03779\n\nOur job posting for a PostDoc:\nhttps://www.cg.tuwien.ac.at/jobs/3dspatialization/\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1130497/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a class of learning algorithms that chooses a set of actions in an environment to maximize a score. Typical use-cases of this include writing an AI to master video games or avoiding obstacles with a drone and many more cool applications. What ties most of these ideas together is that whenever we talk about reinforcement learning, we typically mean teaching an agent how to navigate in an environment. A few years ago, a really fun online app surfaced that used a genetic algorithm to evolve the morphology of a simple 2D car with the goal of having it roll as far away from a starting point as possible. It used a genetic algorithm that is quite primitive compared to modern machine learning techniques, and yet it still does well on this, so how about tasking a proper reinforcement learner to optimize the body of the agent? What\u2019s more, what if we would jointly learn both the body and the navigation at the same time? Ok, so what does this mean in practice? Let\u2019s have a look at an example. Here we have an ant that is supported by four legs, each consisting of three parts that are controlled by two motor joints. With the classical problem formulation, we can teach this ant to use these joints to learn to walk, but in the new formulation, not only the movement, but the body morphology is also subject to change. As a result, this ant learned that the body can also be carried by longer, thinner legs and adjusted itself accordingly. As a plus, it also learned how to walk with these new legs and this way, it was able to outclass the original agent. In this other example, the agent learns to more efficiently navigate a flat terrain by redesigning its legs that are now reminiscent of small springs and uses them to skip its way forward. Of course, if we change the terrain, the design of an effective agent also changes accordingly, and the super interesting part here is that it came up with an asymmetric design that is able to climb stairs and travel uphill efficiently. Loving it! We can also task this technique to minimize the amount of building materials used to solve a task, and subsequently, it builds an adorable little agent with tiny legs that is still able to efficiently traverse this flat terrain. This principle can also be applied to the more difficult version of this terrain, which results in a lean, insect-like solution that can still finish this level that uses about 75% less materials than the original solution. And again, remember that not only the design, but the movement is learned here at the same time. While we look at these really fun bloopers, I\u2019d like to let you know that we have an opening at our Institute at the Vienna University of Technology for one PostDoctoral researcher. The link is available in the video description, read it carefully to make sure you qualify, and if you apply through the specified e-mail address, make sure to mention Two Minute Papers in your message. This is an excellent opportunity to read and write amazing papers, and work with some of the sweetest people. This is not standard practice in all countries so I\u2019ll note that you can check the salary right in the call, it is a well-paid position in my opinion, and you get to live in Vienna. Also, your salary is paid not 12, but 14 times a year. That\u2019s Austria for you \u2014 it doesn't get any better than that. Deadline is end of January. Happy holidays to all of you! Thanks for watching and for your generous support, and I'll see you early January.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=q22XWPM0Egc",
        "paper_link": "https://designrl.github.io/",
        "paper_title": "Reinforcement Learning for Improving Agent Design"
    },
    {
        "video_id": "pc_k-sgUYmY",
        "video_title": "DeepMind\u2019s Take on How To Create a Benign AI",
        "position_in_playlist": 307,
        "description": "The paper \"Scalable agent alignment via reward modeling: a research direction\" is available here:\n1. https://arxiv.org/abs/1811.07871\n2. https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Emmanuel, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 1a5ttKiVQiDcr9j8JT2DoHGzLG7XTJccX\n\u203a Ethereum: 0xbBD767C0e14be1886c6610bf3F592A91D866d380\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nThumbnail background image credit: https://pixabay.com/photo-3706562/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This episode does not have the usual visual fireworks, but I really wanted to cover this paper because it tells a story that is, I think, very important for all of us to hear about. When creating a new AI to help us with a task, we have to somehow tell this AI what we consider to be a desirable solution. If everything goes well, it will find out the best way to accomplish it. This is easy when playing simpler video games, because we can just tell the algorithm to maximize the score seen in the game. For instance, the more bricks we hit in Atari breakout, the closer we get to finishing the level. However, in real life, we don\u2019t have anyone giving us a score to tell us how close we are to our objective. What\u2019s even worse, sometimes we have to make decisions that seem bad at the time, but will serve us well in the future. Trying to save money or studying for a few years longer are typical life decisions that pay off in the long run but may seem undesirable at the time. The opposite is also true, ideas that may sound right at a time may immediately backfire. When in a car chase, don't ask the car AI to unload all unnecessary weights to go faster, or if you do, prepare to be promptly ejected from the car. So, how can we possibly create an AI that somehow understands our intentions and acts in line with them? That\u2019s a challenging question, and is often referred to as the agent alignment problem. It has to be aligned with our values. What can we do about this? Well, short of having a mind-reading device, we can maybe control the behavior of the AI through its reward system. Scientists at DeepMind just published a paper on this topic, where they started their thought process from two assumptions: Assumption number one, quoting the authors: \u201cFor many tasks we want to solve, evaluation of outcomes is easier than producing the correct behavior\u201d. In short, it is easier to yell at the TV than to become an athlete. Sounds reasonable, right? Note that from complexity theory, we know that this does not always hold, but it is indeed true for a large number of difficult problems. Assumption number two: User intentions can be learned with high accuracy. In other words, given enough data that somehow relates to our intentions, the AI should be able to learn that. Leaning on these two assumptions, we can change the basic formulation of reinforcement learning in the following way: normally, we have an agent that chooses a set of actions in an environment to maximize a score. For instance, this could mean moving the paddle around to hit as many blocks as possible and finish the level. They extended this formulation in a way that the user can periodically provide feedback on how the score should be calculated. Now, the AI will try to maximize this new score and we hope that this will be more in line with our intentions. Or, in our car chase example, we could modify our reward to make sure we remain in the car and not get ejected. Perhaps the most remarkable property of this formulation is that it doesn\u2019t even require us to for instance, play the game at all to demonstrate our intentions to the algorithm. The formulation follows our principles, and not our actions. We can just sit in our favorite armchair, bend the AI to our will by changing the reward function every now and then, and let the AI do the grueling work. This is like yelling at the TV, except that it actually works. Loving the idea. If you have a look at the paper, you will see a ton more details on how to do this efficiently and a case study with a few Atari games. Also, since this has a lot of implications pertaining to AI safety and how to create aligned agents, an increasingly important topic these days, huge respect for DeepMind for investing more and more of their time and money in this area. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=pc_k-sgUYmY",
        "paper_link": "https://arxiv.org/abs/1811.07871",
        "paper_title": "Scalable agent alignment via reward modeling: a research direction"
    },
    {
        "video_id": "T8YOzqy7t5Y",
        "video_title": "This AI Learns From Humans\u2026and Exceeds Them",
        "position_in_playlist": 308,
        "description": "The paper \"Reward learning from human preferences and demonstrations in Atari\" is available here:\nhttps://arxiv.org/abs/1811.06521\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nCrypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 1a5ttKiVQiDcr9j8JT2DoHGzLG7XTJccX\n\u203a Ethereum: 0xbBD767C0e14be1886c6610bf3F592A91D866d380\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-3846345/ + https://upload.wikimedia.org/wikipedia/en/7/70/HERO_A800_ingame.png\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is a collaboration between DeepMind and OpenAI on using human demonstrations to teach an AI to play games really well. The basis of this work is reinforcement learning, which is about choosing a set of actions in an environment to maximize a score. For some games, this score is typically provided by the game itself, but in more complex games, for instance, ones that require exploration, this score is not too useful to train an AI. In this project, the key idea is to use human demonstrations to teach an AI how to succeed. This means that we can sit down, play the game, show the footage to the AI and hope that it learns something useful from it. Now the most trivial implementation of this would be to imitate the footage too closely, or in other words, simply redo what the human has done. That would be a trivial endeavor, and it is the most common way of misunderstanding what is happening here, so I will emphasize that is not the case. Just imitating what the human player does would not be very useful because one, it puts too much burden on the humans, that\u2019s not what we want, and number two, the AI could not be significantly better than the human demonstrator, that\u2019s also not what we want. In fact, if we have a look at the paper, the first figure shows us right away how badly a simpler imitation program performs. That\u2019s not what this algorithm is doing. What it does instead is that it looks at the footage as the human plays the game, and tries to guess what they were trying to accomplish. Then, we can tell a reinforcement learner that this is now our reward function and it should train to become better at that. As you see here, it can play an exploration-heavy game such as Atari \u201cHero\u201d, and in the footage here above you see the rewards over time, the higher the better. This AI performs really well in this game, and significantly outperforms reinforcement learner agents trained from scratch on Montezuma\u2019s revenge as well, although it can still get stuck on a ladder. We discussed earlier a curious AI that was quickly getting bored by ladders and moved on to more exciting endeavors in the game. The performance of the new agent seems roughly equivalent to an agent trained from scratch in the game Pong, presumably because of the lack of exploration and the fact that it is very easy to understand how to score points in this game. But wait, in the previous episode we just talked about an algorithm where we didn\u2019t even need to play, we could just sit in our favorite armchair and direct the algorithm. So why play? Well, just providing feedback is clearly very convenient, but as we can only specify what we liked and what we didn\u2019t like, it is not very efficient. With the human demonstrations here, we can immediately show the AI what we are looking for, and, as it is able to learn the principles and then improve further, and eventually become better than the human demonstrator, this work provides a highly desirable alternative to already existing techniques Loving it. If you have a look at the paper, you will also see how the authors incorporated a cool additional step to the pipeline where we can add annotations to the training footage, so make sure to have a look! Also, if you feel that a bunch of these AI videos a month are worth a dollar, please consider supporting us at Patreon.com/twominutepapers. You can also pick up cool perks like getting early access to all of these episodes, or getting your name immortalized in the video description. We also support cryptocurrencies and one-time payments, the links and additional information to all of these are available in the video description. With your support, we can make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=T8YOzqy7t5Y",
        "paper_link": "https://arxiv.org/abs/1811.06521",
        "paper_title": "Reward learning from human preferences and demonstrations in Atari"
    },
    {
        "video_id": "KhP7lTLTipc",
        "video_title": "6 Life Lessons I Learned From AI Research",
        "position_in_playlist": 309,
        "description": "Tensorflow experiment link: https://www.reddit.com/r/MachineLearning/comments/4eila2/tensorflow_playground/d20noqu/\nKarpathy\u2019s classifier neural network: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-1807526/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\nRandom walk image source: https://en.wikipedia.org/wiki/Random_walk\nTensorflow experiment link: https://www.reddit.com/r/MachineLearning/comments/4eila2/tensorflow_playground/d20noqu/\nKarpathy\u2019s classifier neural network: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is going to be a weird, non-traditional episode. Not the usual Two Minute Papers. Hope you\u2019ll enjoy it and if you finished the video, please let me know in the comments what you think about it. So let's start. Life lessons I learned from ai research. Number one, you need an objective. Before we start training a neural network to perform, for instance, image classification, we need a bunch of training data. This we can feed to this neural network, telling that this image depicts a cat, and this one is not a cat, but an ostrich. We also need to specify a loss function. This is super important, because this loss function is used to make sure that the neural network trains itself in a way that its predictions will be similar to the training data it is being fed. It is also referred to as an objective or objective function to indicate that we know precisely what we are looking for and that\u2019s what the neural network should do. This is a way to measure how the neural network is progressing, and without this, it is useless. Similarly, in an other learning problem, we specify an objective for this ant, which, in this case, is to be able to traverse as far from a starting as possible, and it reconfigures its body type and movement to be able to score high on our objective. And, this leads us to the second lesson: a change in the objective changes the strategy required to achieve it. Look here - in a different problem definition, we can specify a different objective, for instance, a different terrain, and you see that if we wish to succeed here, we need a vastly different body type. Form follows function. And in this other case, the objective is to be able to traverse efficiently, but with minimal material use for the legs. The solution, again, changes accordingly to the objective. New objectives require new strategies. Number three: If the objective was wrong, do not worry, and aim again! Have a look at AlphaGo. This is DeepMind\u2019s algorithm that was able to defeat some of the best players in the world in the game of Go. This was a highly non-trivial achievement as the space of possible moves is so stupendously large that it is impossible to evaluate every move. Instead, it tries to aggressively focus on a smaller number of possible moves, and tries to simulate the result of these moves. If the move leads to an improvement in our position, it is a good one, if not, it should be avoided. Sounds simple, right? Well, we have an objective, that\u2019s great, but, initially, it has a really bad predictor, which means that it is really bad at judging which move is good and which one isn\u2019t. However, over time, it refines its predictor and these estimations improve further and further. In the end, by only taking a brief look at the state of the game, it can predict with a high confidence whether it is going to win or not. Initially, we have an objective, but how do we know whether it is a good one? Well, we try to get there, and then, evaluate our position. We may find that we got nowhere with this. What most people do is abort the program. Quit the game. Give up. It\u2019s over. No, don\u2019t despair, it\u2019s not over! This is the early stage of teaching an AI, and this is the time where we can improve our predictor, and pick our next objective more wisely. Over time, you\u2019ll find the ideas that don\u2019t work, and not only that, you\u2019ll find out why they don\u2019t work. Do not worry and aim again. And this leads us to lesson number four: zoom out and evaluate! This is exactly what DeepMind\u2019s amazing Deep Q-Learning algorithm does that took the world by storm as it was able to play Atari Breakout on a superhuman level just by looking at the pixels of the game. This algorithm ran in two phases, where phase one is collecting experiences, and phase two was called \u201cexperience replay\u201d. This is where the AI stops and reflects upon these experiences. Zooming out and evaluating is immensely important, because after all, this is where the real learning happens. So every now and then, zoom out, and evaluate. And while we are here, I can simply not resist adding two more lessons I learned from other scientific disciplines. So, lesson number five: if you find something that works, hold on to it! This is exactly what Metropolis Light Transport does, which is a light simulation algorithm that is able to create beautiful images even for extremely difficult problems where it is challenging to find where the light is. However, when it finally finds something, it makes sure not to forget about it and explore nearby light paths. It works like a charm for difficult light transport situations and can create absolutely beautiful images for even the hardest virtual scenes. Seek the light and hold on to it. And whenever you feel that you are still not making progress, think about the following. Lesson number six: as long as you keep moving, you\u2019ll keep progressing. Take a look at this random walk. A random walk is a succession of steps in completely random directions. This walk is completely lack of direction, just as a drunkard that tries to find home. However, get this - a mathematical theorem says that after N steps, the expected distance from where we\u2019ve started is proportional to the square root of N. This is huge! What this means, is that for instance, it we took four completely random steps, we are expected to be 2 units of distance away from where we\u2019ve started. That\u2019s progress! If we take a hundred steps, even then, we can expect to be around 10 units of distance away from the starting point. This concept works, even if our predictor is completely haywire and we choose our objectives like a drunkard. Now I think that\u2019s a lesson worth sharing. To recap: you need an objective! It can be anything, so long as you keep moving, you\u2019ll progress. If you have achieved it and it ended up not being what you were looking for, don\u2019t stop. Zoom out and reflect. This will help you to improve your predictor and you will be able to recalibrate and aim again at something more meaningful. Now, aim and find a new objective! When you have a new objective, your strategy needs to change to be able to achieve it. Finally, if you find something desirable, hold on to it, and explore more in this direction. Seek the light. Of course, you don\u2019t have to live your life this way, but I think these are interesting, mathematically motivated lessons that are worth showing to you. After all, this series is not only meant to inform, but to inspire you to get out there and create. It always feels absolutely amazing getting these kind messages from you Fellow Scholars, some of you said that the series has changed your life in a positive way. I am really out of words and am honored to be able to make these videos for you Fellow Scholars. Let me know in the comments whether you enjoyed this episode and please, keep the kind messages coming, they really make my day. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=KhP7lTLTipc"
    },
    {
        "video_id": "t_7qpPOmsME",
        "video_title": "This AI Produces Binaural (2.5D) Audio",
        "position_in_playlist": 310,
        "description": "The paper \"2.5D Visual Sound\" is available here:\nhttps://arxiv.org/abs/1812.04204\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Binaural or 2.5D audio means a sound recording that provides the listener with an amazing 3D-ish sound sensation. It produces a sound that feels highly realistic when listened to through headphones, and therefore, using a pair is highly recommended for this episode. It sounds way more immersive than regular mono, or even stereo audio signals, but also requires more expertise to produce, and is therefore quite scarce on the internet. Let\u2019s listen to the difference together. We have not only heard sound samples here, but could also see the accompanying video content which reveals the position of the players and the composition of the scene in which the recording is made. This sounds like a perfect fit for an AI to take piece of mono audio and use this additional information to convert it to make it sound binaural. This project is exactly about that, where a deep convolutional neural network is used to look at both the video and the single-channel audio content in our footage, and then, predict what it would have sounded like were it recorded as a binaural signal. Let\u2019s listen to a few results. The fact that we can use the visual content as well as the audio with this neural network, interestingly, also enables us to separate the sound of an instrument within the mix. Let\u2019s listen. To validate the results, the authors both used a quantitative, mathematical way of comparing their results to the ground truth, and not only that, but they also carried out two user studies as well. In the first one, the ground truth was shown to the users and they were asked to judge, which of the two techniques were better. In this study, this new method performed better than previous methods, and in the second setup, users were asked to name the directions they hear the different instrument sounds coming from. In this case, the new method outperformed the previous techniques by a significant margin, and if we keep progressing like this, we may be at most a couple papers away from 2.5D audio synthesis that sounds indistinguishable from the real deal. Looking forward to a future where we can enjoy all kinds of video content with this kind of immersion. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=t_7qpPOmsME",
        "paper_link": "https://arxiv.org/abs/1812.04204",
        "paper_title": "2.5D Visual Sound"
    },
    {
        "video_id": "1ct_P3IZow0",
        "video_title": "What Makes a Good Image Generator AI?",
        "position_in_playlist": 311,
        "description": "Three paper recommendations this time: \n- Inception score - \"Improved Techniques for Training GANs\" - https://arxiv.org/abs/1606.03498\n- \"Progressive Growing of GANs for Improved Quality\" - https://arxiv.org/abs/1710.10196\n- Inception score criticism - \"A Note on the Inception Score\" - https://arxiv.org/abs/1801.01973\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-3840163/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we frequently talk about Generative Adversarial networks, or GANs in short. This means a pair of neural networks that battle each over time to master a task, for instance, to generate realistic looking images from a written description. Here you see NVIDIA\u2019s amazing work that was able to dream up high-resolution images of imaginary celebrities. In the next episode, we will talk some more about their newest work that does something like this, and is even better at it, believe it or not. I hope you have subscribed to the channel to make sure not to miss out on that one. And for now, while we marvel at these outstanding results, I will quickly tell you about overfitting and what it has to do with images of celebrities. When we train a neural network, we wish to make sure that it understands the concepts we are trying to teach it. Typically, we feed it a database of labeled images, where the labels mean that this depicts a dog, and this one is not a dog, but a cat. After the training step took place, in the ideal case it will be able to build an understanding of these images, so that when we show them new, previously unseen pictures, it would be able to correctly guess which animals they depict. However, in many cases, we start training the neural network, and during the training, it gives us wonderful results, and it gets the animals right every single time! But, whenever it sees new, previously unseen images, it can\u2019t tell a dog from a cat at all. This peculiar case is what we call overfitting, and this is the bane of machine learning research. Overfitting is like the kind of student we all encountered at school who is always very good at memorizing the textbook but can\u2019t solve even the simplest new problems on the exam. This is not learning, this is memorization! Overfitting means that a neural network does not learn the concept of dogs or cats, it just tries to memorize this database of images and is able to regurgitate it for us, but its knowledge cannot generalize for new images. That\u2019s not good. I want intelligence, not a copying machine! So at this point, it is probably clearer what images of celebrities have to do with overfitting. So, how do we know that this algorithm doesn\u2019t just memorize the celebrity image dataset it was given, and can really generate new, imaginary people? Is it the good kind of student, or the lousy student? Technique number one, let\u2019s not just dream up images of new celebrities, but also visualize images from the training data that are similar to this image. If they are too similar, we have an overfitting problem. Let\u2019s have a look. Now it is easy to see that this is proper intelligence, and not a copying machine, because it was clearly able to learn the facial features of these people and combine them in novel ways. This is what scientists at NVIDIA did in their paper and are to be commended for that. Technique number two, well, just take a bunch of humans and let them decide whether these images differ from the training set and if they are realistic. This kinda works, but of course, costs quite a bit of money, labor, and we end up with something subjective. We better not compare the quality of research papers based on that if we can avoid it. And get this, we can actually avoid it by using something called the inception score. Instead of using humans, this score uses a neural network to have a look at these images and measure the quality and the diversity of the results. As long as the image produces similar neuron activations within this neural network, two images will be deemed to be similar. Finally, this score is an objective way of measuring progress within this field, and it is of course, subject to maximization. So now, you, of course, wish to know what the state of the art is today. For reference, a set of real images has an inception score of 233, and the best works that produced synthetic images from just a few years ago had a score of around 50. To the best of my knowledge, as of the publishing of this video, the highest inception score for an AI is close to 166, so we\u2019ve come a long long way. You can see some of these images here. Truly exciting \u2014 what a time to be alive! The disadvantages of the method is that one, because the diversity of the outputs is also to be measured, it requires many thousands of images. This is likely more of an issue with the problem definition itself, not this method, and also, since this means that the computers and not real people have to do more work, \u2026 we can give this one a pass. Disadvantage number two, I will include this paper in the video description for you, which basically describes that there are cases where it is possible to get the network to think an image is of higher quality than another one even if it clearly isn\u2019t. Now you see that we have pretty ingenious techniques to measure the quality of image generator AI programs, and of course, this area of research is also subject to improvement, and, I\u2019ll be here to tell you about it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1ct_P3IZow0"
    },
    {
        "video_id": "-cOYwZ2XcAc",
        "video_title": "None of These Faces Are Real!",
        "position_in_playlist": 312,
        "description": "The paper \"A Style-Based Generator Architecture for Generative Adversarial Networks\", i.e., #StyleGAN and its video available here:\nhttps://arxiv.org/abs/1812.04948\nhttps://www.youtube.com/watch?v=kSLJriaOumA\n\nOur material synthesis paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#NVIDIA",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Before we start, I will tell you right away to hold on to your papers. When I\u2019ve first seen the results I didn\u2019t do that and almost fell out of the chair. Scientists at NVIDIA published an amazing work not so long ago that was able to dream up high-resolution images of imaginary celebrities. It was a progressive technique, which means that it started out with a low-fidelity image and kept refining it, and over time, we found ourselves with high-quality images of people that don\u2019t exist. We also discussed in the previous episode that the algorithm is able to learn the properties and features of a human face and come up with truly novel human beings. There is true learning happening here, not just copying the training set for these neural networks. This is an absolutely stellar research work, and, for a moment, let\u2019s imagine that we are the art directors of a movie or a computer game and we require that the algorithm synthesizes more human faces for us. Whenever I worked with artists in the industry, I\u2019ve learned that what artists often look for beyond realism, is ... control. Artists seek to conjure up new worlds, and those new worlds require consistency and artistic direction to suspend our disbelief. So here\u2019s a new piece of work from NVIDIA with some killer new features to address this. Killer feature number one. It can combine different aspects of these images. Let\u2019s have a look at an example over here. The images above are the inputs, and we can lock in several aspects of these images, for instance, like gender, age, pose and more. Then, we take a different image, this will be the other source image, and the output is these two images fused together. Almost like style transfer or feature transfer for human faces. As a result, we are able to generate high-fidelity images of human faces that are incredibly lifelike, and, of course, none of these faces are real. How cool is that? Absolutely amazing. Killer feature number two. We can also vary these parameters one by one, and this way, we have a more fine-grained artistic control over the outputs. Killer feature number three: It can also perform interpolation, which means that we have desirable images A and B, and this would create intermediate images between them. As always, with this, the holy grail problem is that each of the intermediate images have to make sense and be realistic. And just look at this. It can morph one gender into the other, blend hairstyles, colors, and in the meantime, the facial features remain crisp and realistic. I am out of words, this is absolutely incredible. It kind of works on other datasets, for instance, cars, bedrooms, and of course, you guessed it right, \u2026 cats. Now, interestingly, it also varies the background behind the characters, which is a hallmark of latent-space based techniques. I wonder if and how this will be solved over time. We also published a paper not so long ago that was about using learning algorithms to synthesize, not human faces, but photorealistic materials. We introduced a neural renderer that was able to perform a specialized version of a light transport simulation in real time as well. However, in the paper, we noted that the resolution of the output images is limited by the on-board video memory on the graphics card that is being used and should improve over time as new graphics cards are developed with more memory. And get this, a few days ago, folks at NVIDIA reached out and said that they just released an amazing new graphics card, the TITAN RTX which has a ton of onboard memory, and would be happy to send over one of those. Now we can improve our work further. A huge thank you for them for being so thoughtful, and therefore, this episode has been been kindly sponsored by NVIDIA. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-cOYwZ2XcAc",
        "paper_link": "https://arxiv.org/abs/1812.04948",
        "paper_title": "A Style-Based Generator Architecture for Generative Adversarial Networks"
    },
    {
        "video_id": "DMXvkbAtHNY",
        "video_title": "DeepMind\u2019s AlphaStar Beats Humans 10-0 (or 1)",
        "position_in_playlist": 313,
        "description": "DeepMind's #AlphaStar blog post:\nhttps://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/\n\nFull event:\nhttps://www.youtube.com/watch?v=cUTMhmVh1qs\n\nHighlights:\nhttps://www.youtube.com/watch?v=6EQAsrfUIyo\n\nAgent visualization:\nhttps://www.youtube.com/watch?v=HcZ48JDamyk&feature=youtu.be\n\n#DeepMind's Reddit AMA:\nhttps://old.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/\n\nAPM comments within the AMA:\nhttps://old.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/eexs0pd/\n\nMana\u2019s personal experience: https://www.youtube.com/watch?v=zgIFoepzhIo\nArtosis\u2019s analysis: https://www.youtube.com/watch?v=_YWmU-E2WFc\nBrownbear\u2019s analysis: https://www.youtube.com/watch?v=sxQ-VRq3y9E\nWinterStarcraft\u2019s analysis: https://www.youtube.com/watch?v=H3MCb4W7-kM\n\nWatch these videos in early access:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nErrara:\n- The in-game has been fixed to run in real time.\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I think this is one of the more important things that happened in AI research lately. In the last few years, we have seen DeepMind defeat the best Go players in the world, and after OpenAI\u2019s venture in the game of DOTA2, it\u2019s time for DeepMind to shine again as they take on Starcraft 2, a real-time strategy game. The depth and the amount of skill required to play this game is simply astounding. The search space of Starcraft 2 is so vast that it exceeds both Chess, and even Go by a significant margin. Also, it is a game that requires a great deal of mechanical skill, split-second decision making and we have imperfect information as we only see what our units can see. A nightmare situation for any AI. DeepMind invited a beloved pro player, TLO to play a few games against their new StarCraft 2 AI that goes by the name AlphaStar. Note that TLO a profesional player who is easily in top 1% of players, or even better. Mid-grandmaster for those who play StarCraft 2. This video is about what happened during this event, and later, I will make another video that describes the algorithm that was used to create this AI. The paper is still under review, so it will take a little time until I can get my hands on it. At the end of this video, you will also see the inner workings of this AI. Let\u2019s dive in. This is an AI that looked at a few games played by human players, and after that initial step, it learns by playing against itself for about 200 years. In our next episode, you will see how this is even possible, so I hope you are subscribed to the series. You see here that the AI controls the blue units, and TLO, the human player plays red. Right at the start of the first game, the AI did something interesting. In fact, what is interesting is what it didn\u2019t do. It started to create new buildings next to its nexus, instead of building a walloff that you can see here. Using a walloff is considered standard practice in most games, and the AI used these buildings to not wall off the entrance, but to shield away the workers from possible attacks. Now note that this is not unheard of, but this is also not a strategy that is widely played today and is considered non-standard. It also built more worker units than what is universally accepted as standard, we found out later that this was partly done in anticipation of losing a few of them early on. Very cool. Then, almost before we even knew what happened, it won the first game a little more than 7 minutes in, which is very quick, noting that in-game time is a little faster than real-time. The thought process of TLO at this point is that that\u2019s interesting, but okay, well, the AI plays aggressively and managed to pull this one off. No big deal. We will fire up the second game, in the meantime, few interesting details. The goal of setting up the details of this algorithm was that the number of actions performed by the AI roughly matches a human player, and hopefully it still plays as well, or better. It has to make meaningful strategic decisions. You see here that this checks out for the average actions every minute, but if you look here, you see around the tail end here that there are times when it performs more actions than humans and this may enable playstyles that are not accessible for human players. However, note that many times it also does miraculous things with very few actions. Now, what about an other important detail, reaction time? The reaction time of the AI is set to 350ms, which is quite slow. That\u2019s excellent news because this is usually a common angle of criticism for game AIs. The AI also sees the whole map at once, but it is not given more information than what its units can see. This perhaps is the most commonly misunderstood detail, so it is worth noting. So, in other words, it sees exactly what a human would see if the human would move the camera around very quickly, but, it doesn\u2019t have to move the camera, which adds additional actions and cognitive load to the human, so one might say that the AI has an edge here. The AI plays these games independently, what\u2019s more, each game was played by a different AI, which also means that they do not memorize what happened in the last game like a human would. Early in the next game, we can see the utility of the walloff in action which is able to completely prevent the AIs early attack. Later that game, the AI used disruptors, a unit, which if controlled with such level of expertise, can decimate the army of the opponent with area damage by killing multiple units at once. It has done an outstanding job picking away at the army of TLO. Then, after getting a significant advantage, AlphaStar loses it with a few sloppy plays and by deciding to engage aggressively while standing in tight choke points. You can see that this is not such a great idea. This was quite surprising as this is considered to be StarCraft 101 knowledge right there. During the remainder of the match, the commentators mentioned that they play and watch matches all the time and the AI came up with an army composition that they have never seen during a professional match. And, the AI won this one too. After this game it became clear that these agents can play any style in the game. Which is terrifying. Here you can see an alternative visualization that shows a little more of the inner workings of the neural network. We can see what information it gets from the game, a visualization of neurons that get activated within the network, what locations and units are considered for the next actions, and whether the AI predicts itself as the winner or loser of the game. If you look carefully, you will also see the moment when the agent becomes certain that it will win this game. I could look at this all day long, and if you feel the same way, make sure to visit the video description, I have a link to the source video for you. The final result against TLO was 5 to 0, so that\u2019s something, and he mentioned that the AlphaStar played very much like a human does and almost always managed to outmaneuver him. However, TLO also mentioned that he is confident that upon playing more training matches against these agents, he would be able to defeat the AI. I hope he will be given a chance to do that. This AI seems strong, but still beatable. I would also note that many of you would probably expect the later versions of AlphaStar to be way better than this one. The good news is that the story continues and we\u2019ll see whether that\u2019s true! So at this point, the DeepMind scientists said that \u201cmaybe we could try to be a bit more ambitious\u201d, and asked \u201ccan you bring us someone better\u201d? And in the meantime, pressed that training button on the AI again. In comes MaNa, a top tier pro player. One of the best Protoss players in the world. This was a nerve-wracking moment for DeepMind scientists as well, because their agents played against each other, so they only knew the AI\u2019s winrate against a different AI, but they didn\u2019t know how they would compete against a top pro player. It may still have holes in its strategy. Who knows what would happen? Understandably, they had very little confidence in winning this one. What they didn\u2019t expect is that this new AI was not slightly improved, or somewhat improved. No, no, no. This new AI was next level. This set of improved agents among many other skills, had incredibly crisp micromanagement of each individual unit. In the first game, we\u2019ve seen it pulling back injured units but still letting them attack from afar masterfully, leading to an early win for the AI against Mana in the first game. He and the commentators were equally shocked by how well the agent played. And I will add that I remember from watching many games from a now inactive player by the name MarineKing a few years ago. And I vividly remember that he played some of his games so well, the commentators said that there\u2019s no better way to put it, he played like a god. I am almost afraid to say that this micromanagement was even more crisp than that. This AI plays phenomenal games. In later matches, the AI did things that seemed like blunders, like attacking on ramps and standing in choke points, or using unfavorable unit compositions and refusing to change it and, get this, it still won all of those games 5 to 0. Against a top pro player. Let that sink in. The competition was closed by a match where the AI was asked to also do the camera management. The agent was still very competent, but somewhat weaker and as a result, lost this game, hence the \u201c0 or 1\u201d part in the title. My impression is that it was asked to do something that it was not designed for, and expect a future version to be able to handle this use case as well. I will also commend Mana for his solid game plan for this game, and also, huge respect for DeepMind for their sportsmanship. Interestingly, in this match, Mana also started using a worker oversaturation strategy that I mentioned earlier. This he learned from AlphaStar and used it in his winning game. Isn\u2019t that amazing? DeepMind also offered a reddit AMA where anyone could ask them questions to make sure to clear up any confusion, for instance, the actions per minute part has been addressed, I\u2019ve included a link to that for you in the description. To go from a turn-based perfect information game, like Go, to a real time strategy game of imperfect information in about a year sounds like science fiction to me. And yet, here it is. Also, note that DeepMind\u2019s goal is not to create a godlike StarCraft 2 AI. They want to solve intelligence, not StarCraft 2, and they used the game as a vehicle to demonstrate its long-term decision making capabilities against human players. One more important thing to emphasize is that the building blocks of AlphaStar are meant to be reasonably general AI algorithms, which means that parts of this AI can be reused for other things, for instance, Demis Hassabis mentioned weather prediction and climate modeling as examples. If you take only one thought from this video, let it be this one. I urge you to watch all the matches because what you are witnessing may very well be history in the making. I put a link to the whole event in the video description, plus plenty more materials, including other people\u2019s analysis, Mana\u2019s personal experience of the event, his breakdown of his games and what was going through his head during the event. I highly recommend checking out his 5th game, but really, go through them all, it\u2019s a ton of fun! I made sure to include a more skeptical analysis of the game as well to give you a balanced portfolio of insights. Also, huge respect for DeepMind and the players who practiced their chops for many many years and have played really well under immense pressure. Thank you all for this delightful event. It really made my day. And the ultimate question is, how long did it take to train these agents? 2 weeks. Wow. And what\u2019s more, after the training step, the AI can be deployed on an inexpensive consumer desktop machine. And this is only the first version. This is just a taste, and it would be hard to overstate how big of a milestone this is. And now, scientists at DeepMind have sufficient data to calculate the amount of resources they need to spend to train the next, even more improved agents. I am confident that they will also take into consideration the feedback from the StarCraft community when creating this next version. What a time to be alive! What do you think about all this? Any predictions? Is this harder than DOTA2? Let me know in the comments section below. And remember, we humans build up new strategies by learning from each other, and of course, the AI, as you have seen here, doesn\u2019t care about any of that. It doesn\u2019t need intuition and can come up with unusual strategies. The difference now is that these strategies work against some of the best human players. Now it\u2019s time for us to finally start learning from an AI. gg. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=DMXvkbAtHNY"
    },
    {
        "video_id": "Do_00r8NGMY",
        "video_title": "AI Learns Real-Time Defocus Effects in VR",
        "position_in_playlist": 314,
        "description": "The paper \"DeepFocus: Learned Image Synthesis for Computational Displays\" and its source code is available here:\nhttps://research.fb.com/publications/deepfocus-siggraph-asia-2018/\nhttps://www.oculus.com/blog/introducing-deepfocus-the-ai-rendering-system-powering-half-dome/\nhttps://github.com/facebookresearch/DeepFocus\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we are to write a sophisticated light simulation program, and we write a list of features that we really wish to have, we should definitely keep an eye on defocus effects. This is what it looks like, and in order to do that, our simulation program has to take into consideration the geometry and thickness of the lenses within our virtual camera, and even though it looks absolutely amazing, it is very costly to simulate that properly. This particular technique attempts to do this in real time, and, for specialized display types, typically ones that are found in head-mounted displays for Virtual Reality applications. So here we go, due to popular request, a little VR in Two Minute Papers. In virtual reality, defocus effects are especially important because they mimic how the human visual system works. Only a tiny region that we\u2019re focusing on looks sharp, and everything else should be blurry, but not any kind of blurry, it has to look physically plausible. If we can pull this off just right, we\u2019ll get a great and immersive VR experience. The heart of this problem is looking at a 2D image and being able to estimate how far away different objects are from the camera lens. This is a task that is relatively easy for humans because we have an intuitive understanding of depth and geometry, but of course, this is no easy task for a machine. To accomplish this, here, a convolutional neural network is used, and our seasoned Fellow Scholars know that this means that we need a ton of training data. The input should be a bunch of images, and their corresponding depth maps for the neural network to learn from. The authors implemented this with a random scene generator, which creates a bunch of these crazy scenes with a lot of occlusions and computes via simulation the appropriate depth map for them. On the right, you see these depth maps, or in other words, images that describe to the computer how far away these objects are. The incredible thing is that the neural network was able to learn the concept of occlusions, and was able to create super high quality defocus effects. Not only that, but this technique can also be reconfigured to fit different use cases: if we are okay with spending up to 50 milliseconds to render an image, which is 20 frames per second, we can get super high-quality images, or, if we only have a budget of 5 milliseconds per image, which is 200 frames per second, we can do that and the quality of the outputs degrades just a tiny bit. While we are talking about image quality, let\u2019s have a closer look at the paper, where we see a ton of comparisons against previous works and of course, against the baseline ground truth knowledge. You see two metrics here: PSNR, which is the peak signal to noise ratio, and SSIM, the structural similarity metric. In this case, both are used to measure how close the output of these techniques is to the ground truth footage. Both are subject to maximization. For instance, here you see that the second best technique has a peak signal to noise ratio of around 40, and this new method scores 45. Well, some may think that that\u2019s just a 12-ish percent difference, right? \u2026 No. Note that PSNR works on a logarithmic scale, which means that even a tiny difference in numbers translates to a huge difference in terms of visuals. You can see in the closeups that the output of this new method is close to indistinguishable from the ground truth. A neural network that successfully learned the concept of occlusions and depth by looking at random scenes. Bravo. As virtual reality applications are on the rise these days, this technique will be useful to provide a more immersive experience for the users. And to make sure that this method sees more widespread use, the authors also made the source code and the training datasets available for everyone, free of charge, so make sure to have a look at that and run your own experiments if you\u2019re interested. I'll be doing that in the meantime. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Do_00r8NGMY",
        "paper_link": "https://research.fb.com/publications/deepfocus-siggraph-asia-2018/",
        "paper_title": "DeepFocus: Learned Image Synthesis for Computational Displays"
    },
    {
        "video_id": "6fo5NhnyR8I",
        "video_title": "OpenAI - Learning Dexterous In-Hand Manipulation",
        "position_in_playlist": 315,
        "description": "Check out \"Superintelligence: Paths, Dangers, Strategies\" on Audible:\nUS: https://amzn.to/2RXr32F\nEU: https://amzn.to/2SqauwI\n\nThe paper \"Learning Dexterous In-Hand Manipulation\" is available here:\nhttps://blog.openai.com/learning-dexterity/\nhttps://arxiv.org/abs/1808.00177\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\nWe would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about OpenAI\u2019s new technique that teaches a robot arm to dexterously manipulate a block to a target state. And in this project, they did one of my favorite things, which is, first, training an AI within a simulation, and then, deploying it into the real world. And in the best case scenario, this knowledge from the simulation will actually generalize to the real world. However, while we are in the simulation, we can break free from the limitations of worldly things, such as hardware, movement speed, or, even time itself. So how is that possible? The limitation on the number of experiments we can run in a simulation is bounded by not our time, which is scarce, but how powerful our hardware is, which is abundant as it is accelerating at nearly exponential pace. And, this is the reason why OpenAI\u2019s and DeepMind\u2019s AI was able to train for 200 years worth of games before first playing a human pro player. This sounds great, but a simulation is always more crude than the real world, so do we know for sure that we created something that will indeed be useful in the real world, and not just in the simulation? Let\u2019s try an analogy. Think of the machine as a student, and the simulation would be its textbook that it learns from. If the textbook contains only a few trivial problems to learn from, when the day of the exam comes, if the exam is any good, the student will fail. The exam is the equivalent of deploying the machine into the real world, and apparently, the real world is a damn good exam. So how can we prepare a student to do well on this exam? Well, we have to provide them with a textbook that contains not only a lot of problems, but also a diverse set of challenges as well. This is what machine learning researchers call domain randomization. This means that we teach the AI program in different virtual worlds, and in each one of them, we change parameters like how fast the hand is, what color and weight the cube is, and more. This is a proper textbook, which means that after this kind of training, this AI can deal with new and unexpected situations. The knowledge that it has obtained is so general that we can change even the geometry of the target object and the machine will still be able to manipulate it correctly. Outstanding. To implement this idea, scientists at OpenAI trained not one agent, but a selection of agents in these randomized environments. The first main component of this system is a pose estimator. This module looks at the cube from three angles and predicts the position and orientation of the block, and is implemented through a convolutional neural network. The advantage of this is that as we can generate a near-infinite amount of training data ourselves. You can see here that when the AI looks at real images, it is only a few degrees worse than in the simulation when estimating angles, which is the case of the excellent textbook. I would not be surprised if this accuracy exceeds the capabilities of an ordinary human, given that it can perform this many times within a second. Then, the next part is choosing what the next action should be. Of course, we seek to rotate this cube in a way that brings us closer to our objective. This is done by a reinforcement learning technique, which uses similar modules as OpenAI\u2019s previous algorithm that learned to play DOTA2 really well. Another testament to how general these learning algorithms are. I also recommend checking out OpenAI\u2019s video on this work in the video description. Now, I always read in the comments here on Youtube that many of you are longing for more. 5 minute papers, 10 minute papers, 2 hour papers were among the requests I heard from you before. And of course, I am also longing for more as I have quite a few questions that keep me up at night. Is it possible for us to ever come up with a superintelligent AI? If yes, how? What types of these AIs could exist? Should we be worried? If you are also looking for some answers, we are now trying out a sponsorship with Audible, and I have a great recommendation for you, which is none other than the book Superintelligence by Nick Bostrom. It addresses all of these questions really well, and if you sign up under the link below in the video description, you will get this book free of charge. Whenever you have to do some work around the house, commute to school or work, just pop in a pair of headphones and listen for free. Some more AI for you while doing something tedious. That\u2019s as good as it gets. If you feel that the start of the book is a little slow for you, make sure to jump to the chapter by the name \u201cIs the default outcome doom\u201d. But buckle up, because there is going to be fireworks from that point in the book. We thank Audible for supporting this video, and send a big thank you for all of you who sign up and support the series. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6fo5NhnyR8I",
        "paper_link": "https://blog.openai.com/learning-dexterity/",
        "paper_title": "Learning Dexterous In-Hand Manipulation"
    },
    {
        "video_id": "OwRuzn3RAhA",
        "video_title": "Extracting Rotations The Right Way",
        "position_in_playlist": 316,
        "description": "The paper \"A Robust Method to Extract the Rotational Part of Deformations\" is available here:\nhttp://matthias-mueller-fischer.ch/publications/stablePolarDecomp.pdf\n\nOur work with Activision-Blizzard is available here:\n\u203a Project page: https://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n\u203a Video: https://www.youtube.com/watch?v=72_iAlYwl0c\n\nPick up cool perks on our Patreon page:\n\u203a https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about creating high-quality physics simulations and is, in my opinion, one of the gems very few people know about. In these physical simulations, we have objects that undergo a lot of tormenting, for instance they have to endure all kinds of deformations, rotations, and of course, being pushed around. A subset of these simulation techniques requires us to be able to look at these deformations and forget about anything they do other than the rotational part. Don\u2019t push it, don\u2019t squish it, just take the rotational part. Here, the full deformation transform is shown with red, and extracted rotational part is shown by the green indicators here. This problem is not particularly hard and has been studied for decades, so we have excellent solutions to this, for instance, techniques that we refer to as polar decomposition, singular value decomposition, and more. By the way, in our earlier project together with the Activision Blizzard company, we also used the singular value decomposition to compute the scattering of light within our skin and other translucent materials. I\u2019ve put a link in the video description, make sure to have a look! Okay, so if a bunch of techniques already exist to perform this, why do we need to invent anything here? Why make a video about something that has been solved many decades ago? Well, here\u2019s why: we don\u2019t have anything yet that is criterion one, robust, which means that it works perfectly all the time. Even a slight inaccuracy is going to make an object implode our simulations, so we better get something that is robust. And, since these physical simulations are typically implemented on the graphics card, criterion two, we need something that is well suited for that, and is as simple as possible. It turns out, none of the existing techniques check both of these two boxes. If you start reading the paper, you will see a derivation of this new solution, a mathematical proof that it is true and works all the time. And then, as an application, it shows fun physical simulations that utilize this technique. You can see here that these simulations are stable, no objects are imploding, although this extremely drunk dragon is showing a formidable attempt at doing that. Ouch. All the contortions and movements are modeled really well over a long time frame, and the original shape of the dragon can be recovered without any significant numerical errors. Finally, it also compares the source code for a previous method, and, the new method. As you see, there is a vast difference in terms of complexity that favors the new method. It is short, does not involve a lot of branching decisions and is therefore an excellent candidate to run on state of the art graphics cards. What I really like in this paper is that it does not present something and claims that \u201cwell, this seems to work\u201d. It first starts out with a crystal clear problem statement that is impossible to misunderstand. Then, the first part of the a paper is pure mathematics, proves the validity of a new technique, and then, drops it into a physical simulation, showing that it is indeed what we were looking for. And finally, a super simple piece of source code is provided so anyone can use it almost immediately. This is one of the purest computer graphics papers out there I\u2019ve seen in a while. Make sure to have a look in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=OwRuzn3RAhA",
        "paper_link": "http://matthias-mueller-fischer.ch/publications/stablePolarDecomp.pdf",
        "paper_title": "A Robust Method to Extract the Rotational Part of Deformations"
    },
    {
        "video_id": "lws-2u3LbYg",
        "video_title": "This AI Learned Image Decolorization..and More",
        "position_in_playlist": 317,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"Deep Feature Consistent Deep Image Transformations: Downscaling, Decolorization and HDR Tone Mapping\" is available here:\nhttps://arxiv.org/abs/1707.09482\nhttps://houxianxu.github.io/assets/project/dfc-dit\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/photo-583092/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\n\ud83d\udc68\u200d\ud83d\udcbb K\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Whenever we build a website, a video game, or do any sort of photography and image manipulation, we often encounter the problems of image downscaling, decolorization, and HDR tone mapping. This work offers us one technique that can do all three of these really well. But first, before we proceed, why are we talking about downscaling? We are in the age of AI, where a computer program can beat the best players in Chess and Go, so why talk about such a trivial challenge? Well, have a look here. Imagine that we have this high-fidelity input image and due to file size constraints, we have to produce a smaller version of it. If we do it naively, this is what it looks like. Not great, right? To do a better job at this, our goal would be that the size of the image would be reduced, but while still retaining the intricate details of this image. Here are two classical downsampling techniques\u2026better, but the texture of the skin is almost completely lost. Have a look at this! This is what this learning-based technique came up with. Really good, right? It can also perform decolorization. Again, a problem that sounds trivial for the unassuming Scholar, but when taking a closer look, we notice that there are many different ways of doing this, and somehow we seek a decolorized image that still relates to the original as faithfully as possible. Here you see the previous methods that are not bad at all, but this new technique is great at retaining the contrast between the flower and its green leaves. At this point it is clear that deciding which output is the best is highly subjective. We\u2019ll get back to that in a moment. It is also capable of doing HDR tone mapping. This is something that we do when we capture an image with a device that supports a wide dynamic range, in other words, a wide range of colors, and we wish to display it on our monitor, which has a more limited dynamic range. And again, clearly there are many ways to do that. Welcome to the wondrous world of tone mapping! Note that there are hundreds upon hundreds of algorithms to perform these operations in computer graphics research. And also note that these are very complex algorithms that took decades for smart researchers to come up with. So the seasoned Fellow Scholar shall immediately ask - why talk about this work at all? What\u2019s so interesting about it? The goal here is to create a little more general, learning-based method that can do a great job at not one, but all three of these problems. But how great exactly? And how do we decide how good these images are? To answer both of these questions at the same time, if you\u2019ve been watching this series for a while, then you are indeed right, the authors created a user study, which shows that for all three of these tasks, according to the users, the new method smokes the competition. It is not only more general, but also better than most of the published techniques. For instance, Reinhard\u2019s amazing tone mapper has been an industry standard for decades now, and look, almost 75% of the people prefer this new method over that. What required super smart researchers before can now be done with a learning algorithm. Unreal. What a time to be alive. A key idea for this algorithm is that this convolutional neural network you see on the left is able to produce all three of these operations at the same time, and to perform that, it is instructed by another neural network to do this in a way that preserves the visual integrity of the input images. Make sure to have a look at the paper for more details on how this perceptual loss function is defined. And, if you wish to help us tell these amazing stories to even more people, please consider supporting us on Patreon. Your unwavering support on Patreon is the reason why this show can exist, and, you can also pick up cool perks there, like watching these videos in early access, deciding the order of the next few episodes, or even getting your name showcased in the video description as a key supporter. You can find us at patreon.com/twominutepapers, or, as always, just click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lws-2u3LbYg",
        "paper_link": "https://arxiv.org/abs/1707.09482",
        "paper_title": "Deep Feature Consistent Deep Image Transformations: Downscaling, Decolorization and HDR Tone Mapping"
    },
    {
        "video_id": "F84jaIR5Uxc",
        "video_title": "AI-Based 3D Pose Estimation: Almost Real Time!",
        "position_in_playlist": 318,
        "description": "\ud83d\udcdd The paper \"3D Human Pose Machines with Self-supervised Learning\" and its source code is available here:\nhttps://arxiv.org/abs/1901.03798\nhttp://www.sysu-hcp.net/3d_pose_ssl/\nhttps://github.com/chanyn/3Dpose_ssl.git\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This episode is about a really nice new paper on pose estimation. Pose estimation means that we have an image or video of a human as an input, and the output should be, this skeleton that you see here that shows us what the current position of this person is. Sounds alright, but what are the applications of this, really? Well, it has a huge swath of applications, for instance, many of you often hear about motion capture for video games and animation movies, but it is also used in medical applications for finding abnormalities in a patient\u2019s posture, animal tracking, understanding sign language, pedestrian detection for self-driving cars, and much, much more. So if we can do something like this in real time, that\u2019s hugely beneficial for many many applications. However, this is a very challenging task, because humans have a large variety of appearances, images come in all kinds of possible viewpoints, and as a result, the algorithm has to deal with occlusions as well. This is particularly hard, have a look here. In these two cases, we don\u2019t see the left elbow, so it has to be inferred from seeing the remainder of the body. We have the reference solution on the right, and as you see here, this new method is significantly closer to it than any of the previous works. Quite remarkable. The main idea in this paper is that it works out the poses both in 2D and 3D and contains neural network that can convert to both directions between these representations while retaining the consistencies between them. First, the technique comes up with an initial guess, and follows up by using these pose transformer networks to further refine this initial guess. This makes all the difference. And not does it lead to high-quality results, but it also takes way less time than previous algorithms \u2014 we can expect to obtain a predicted pose in about 51 milliseconds, which is almost 20 frames per second. This is close to real time, and is more than enough for many of the applications we\u2019ve talked about earlier. In the age of rapidly improving hardware, these are already fantastic results both in terms of quality and performance, and not only the hardware, but the papers are also improving at a remarkable pace. What a time to be alive. The paper contains an exhaustive evaluation section, it is measured against a variety of high-quality solutions, I recommend that you have a look in the video description. I hope nobody is going to install a system in my lab that starts beeping every time I slouch a little, but I am really looking forward to benefitting from these other applications. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=F84jaIR5Uxc",
        "paper_link": "https://arxiv.org/abs/1901.03798",
        "paper_title": "3D Human Pose Machines with Self-supervised Learning"
    },
    {
        "video_id": "1gWpFuQlBsg",
        "video_title": "AlphaZero: DeepMind\u2019s AI Works Smarter, not Harder",
        "position_in_playlist": 319,
        "description": "Errata: regarding the comment on the rules - the AI has no built-in domain knowledge but the basic rules of the game.\n\n\ud83d\udcdd The paper \"AlphaZero: Shedding new light on the grand games of chess, shogi and Go\" is available here:\nhttps://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/\n\nKasparov\u2019s editorial: http://science.sciencemag.org/content/362/6419/1087\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Jason Rollins, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepMind #AlphaZero",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Finally, I have been waiting for quite a while to cover this amazing paper, which is about AlphaZero. We have talked about AlphaZero before, this is an AI that is able to play Chess, Go, and Shogi, or in other words, Japanese chess on a remarkably high level. I will immediately start out by uttering the main point of this work: the point of AlphaZero is not to solve Chess, or any of these games. Its main point is to show that a general AI can be created that can perform on a superhuman level on not one, but several different tasks at the same time. Let\u2019s have a look at this image, where you see a small part of the evaluation of AlphaZero versus StockFish, an amazing open source chess engine which has been consistently at or around the top computer chess players for many years now. StockFish has an elo rating of over 3200 which means that it has a winrate of over 90% against the best human players in the world. Now, interestingly, comparing these algorithms is nowhere near as easy as it sounds. This sounds curious, so why is that? For instance, it is not enough to pit the two algorithms against each other and see who ends up winning. It matters what version of Stockfish is used, how many positions are the machines are allowed to evaluate, how much thinking time they are allowed, the size of hashtables, the hardware being used, the number of threads being used, and so on. From the side of the chess community, these are the details that matter. However, from the side of an AI researcher, what matters most is to create a general algorithm that can play several different games on a superhuman level. With this constraint, it would really be a miracle if AlphaZero were able to even put up a good fight against Stockfish. So, what happened? AlphaZero played a lot of games that ended up as draws against Stockfish and not only that, but whenever there was a winner, it was almost always AlphaZero. Insanity. And what is quite remarkable is that AlphaZero has only trained for 4 to 7 hours only through self-play. Comparatively, the development of the current version of Stockfish took more than 10 years. You see the how reliably this AI can be trained, the blue lines show the results of several training runs, and they all converge to the same result with only a tiny bit of deviation. AlphaZero is also not a brute-force algorithm as it evaluates fewer positions per second than StockFish. Kasparov put it really well in his article where he said that AlphaZero works smarter, not harder than previous techniques. Even Magnus Carlsen, chess grandmaster extraordinaire said in an interview that during his games, he often thinks: \u201cwhat would AlphaZero do in this case?\u201d, which I found to be quite remarkable. Kasparov also had many good things to say about the new AlphaZero in a, let\u2019s say, very Kasparov-esque manner. And also note that the key point is not whether the current version of Stockfish or the one from two months ago was used. The key point is that Stockfish is a brilliant Chess engine, but it is not able to play Go or any game other than Chess. This is the main contribution that DeepMind was looking for with this work. This AI can master three games at once, and a few more papers down the line, it may be able to master any perfect information game. Oh my goodness. What a time to be alive! We have really only scratched the surface in this video. This was only a taste of the paper. The evaluation section in the paper is out of this world, so make sure to have a look in the video description and I am convinced that nearly any question one can possibly think of is addressed there. I also linked Kasparov\u2019s editorial on this topic. It is short, and very readable, give it a go! I hope this little taste of AlphaZero inspires you to go out and explore yourself. This is the main message of this series. Let me know in the comments what you think or if you found some cool other things related to AlphaZero! Thanks for watching and for your generous support, and I'll see you next time.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1gWpFuQlBsg",
        "paper_link": "https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/",
        "paper_title": "AlphaZero: Shedding new light on the grand games of chess, shogi and Go"
    },
    {
        "video_id": "pv8Sl2rWyCQ",
        "video_title": "Google AI's Take on How To Fix Peer Review",
        "position_in_playlist": 320,
        "description": "\ud83d\udcdd The paper \"Avoiding a Tragedy of the Commons in the Peer Review Process\" is available here:\nhttps://arxiv.org/abs/1901.06246\n\nThe NeurIPS experiment: http://blog.mrtz.org/2014/12/15/the-nips-experiment.html\n\nFluid video source: https://www.youtube.com/watch?v=MCHw6fUyLMY\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-3653385/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. It is time for a position paper. This paper does not have the usual visual fireworks that you see in many of these videos, however, it addresses the cornerstone of scientific publication, which is none other than peer review. When a research group is done with a project, they don\u2019t just write up the results and chuck the paper into a repository, but instead, they submit it to a scientific venue, for instance, a journal or a conference. Then, the venue finds several other researchers who are willing to go through the work with a fine-tooth comb. In the case of double-blind reviews, both the authors and the reviewers remain anonymous to each other. The reviewers now check whether the results are indeed significant, novel, credible and reproducible. If the venue is really good, this process is very tough and thorough, and this is process becomes the scientific version of beating the heck out of someone, but in a constructive manner. If the work is able to withstand serious criticism, and ticks the required boxes, it can proceed to get published at this venue. Otherwise, it is rejected. So what we heard so far is that the research work is being reviewed, however, scientists at the Google AI lab raised the issue that the reviewers themselves should also be reviewed. Consider the fact that all scientists are expected to spend a certain percentage of their time to serve the greater good. For instance, throughout my PhD studies, I have reviewed over 30 papers and I am not even done yet. These paper reviews take place without compensation. Let\u2019s call this issue number one for now. Issue number two is the explosive growth of the number of submissions over time at the most prestigious machine learning and computer vision conferences. Have a look here. It is of utmost importance that we create a review system that is as fair as possible - after all, thousands of hours spent on research projects are at stake. Add these two issues together, and we get a system where the average quality of the reviews will almost certainly decrease over time. Quoting the authors: \u201cWe believe the key issues here are structural. Reviewers donate their valuable time and expertise anonymously as a service to the community with no compensation or attribution, are increasingly taxed by a rapidly increasing number of submissions, and are held to no enforced standards.\u201d In Two Minute Papers episode number 84, so more than 200 episodes ago, we discussed the NeurIPS experiment. Leave a comment if you\u2019ve been around back then and you enjoyed Two Minute Papers before it was cool! But don\u2019t worry if this is not the case, this was long ago, so here is a short summary: a large amount of papers were secretly disseminated to multiple committees, who would review it without knowing about each other, and we would have a look whether they would accept or reject the same papers. Re-review papers and see if the results are the same, if you will. If we use sophisticated mathematics to create new scientific methods, why not use mathematics to evaluate our own processes? So, after doing that, it was found that at a given prescribed acceptance ratio, there was a disagreement for 57% of the papers. So, is this number good or bad? Let\u2019s imagine a completely hypothetical committee that has no idea what they are doing, and as a review, they basically toss up a coin and accept or reject the paper based on the result of the cointoss. Let\u2019s call them the Coinflip Committee. The calculations conclude that the Coinflip Committee would have a disagreement ratio of about 77%. So, experts, 57% disagreement, Coinflip Committee, 77% disagreement. And now, to answer whether this is good or bad: this is hardly something to be proud of \u2014 the consistency of expert reviewers is significantly closer to a coinflip than to a hypothetical perfect review process. If that is not an indication that we have to do something about this, I am not sure what is. So, in this paper, the authors propose two important changes to the system to remedy these issues: Remedy number one - they propose a rubric, a 7-point document to evaluate the quality of the reviews. Again, not only the papers are reviewed, but the reviews themselves. It is similar to the ones used in public schools to evaluate student performance to make sure whether the review was objective, consistent and fair. Remedy number two - reviewers should be incentivized and rewarded for their work. The authors argue that a professional service should be worthy of professional compensation. Now, of course, this sounds great, but this also requires money. Where should the funds come from? The paper discusses several options: for instance, this could be funded through sponsorships, or, asking for a reasonable fee when submitting a paper for peer review, and introducing a new fee structure for science conferences. This is a short, 5-page paper that is very easily readable for everyone, raises excellent points for a very important problem, so needless to say, I highly recommend that you give it a read, as always, the link is in the video description. I hope this video will help raising more awareness to this problem. If we are to create a fair system for evaluating research papers, we better get this right. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=pv8Sl2rWyCQ",
        "paper_link": "https://arxiv.org/abs/1901.06246",
        "paper_title": "Avoiding a Tragedy of the Commons in the Peer Review Process"
    },
    {
        "video_id": "YFL-MI5xzgg",
        "video_title": "Do Neural Networks Need To Think Like Humans?",
        "position_in_playlist": 321,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness \" is available here:\nhttps://openreview.net/forum?id=Bygh9j09KX\nhttps://blog.usejournal.com/why-deep-learning-works-differently-than-we-thought-ec28823bdbc\nhttps://github.com/rgeirhos/texture-vs-shape\n\nNeural network visualization footage source:\nhttps://www.youtube.com/watch?v=1zvohULpe_0\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As convolutional neural network-based image classifiers are able to correctly identify objects in images and are getting more and more pervasive, scientists at the University of T\u00fcbingen decided to embark on a project to learn more about the inner workings of these networks. Their key question was whether they really work similarly to humans or not. Now, one way of doing this is visualizing the inner workings of the neural network. This is a research field on its own, I try to report on it to you every now and then, and we talked about some damn good papers on this, with more to come. A different way would be to disregard the inner workings of the neural network, in other words, to treat it like a black box, at least temporarily. But what does this mean exactly? Let\u2019s have a look at an example! And in this example, our test subject shall be none other than this cat. Here we have a bunch of neural networks that have been trained on the classical ImageNet dataset, and, a set of humans. This cat is successfully identified by all classical neural network architectures and most humans. Now, onwards to a grayscale version of the same cat. The neural networks are still quite confident that this is a cat, some humans faltered, but still, nothing too crazy going on here. Now let\u2019s look at the silhouette of the cat. Whoa! Suddenly, humans are doing much better at identifying the cat than neural networks. This is even more so true when we\u2019re only given the edges of the image. However, when looking at a heavily zoomed in image of the texture of an Indian elephant, neural networks are very confident with their correct guess, where some humans falter. Ha! We have a lead here. It may be that as opposed to humans, neural networks think more in terms of textures than shapes. Let\u2019s test that hypothesis. Step number one: Indian elephant. This is correctly identified. Now, cat \u2014 again, correctly identified. And now, hold on to your papers \u2014 a cat with an elephant texture. And there we go: a cat with an elephant texture is still a cat to us, humans, but, is an elephant to convolutional neural networks. After looking some more at the problem, they found that the most common convolutional neural network architectures that were trained on the ImageNet dataset vastly overvalue textures over shapes. That is fundamentally different to how we, humans think. So, can we try to remedy this problem? Is this even a problem at all? Neural networks need not to think like humans, but who knows, it\u2019s research - we might find something useful along the way. So how could we create a dataset that would teach a neural network a better understanding of shapes? Well, that\u2019s a great question, and one possible answer is \u2014 style transfer! Let me explain. Style transfer is the process of fusing together two images, where the content of one image and the style of the other image is taken. So now, let\u2019s take the ImageNet dataset, and run style transfer on each of these images. This is useful because it repaints the textures, but the shapes are mostly left intact. The authors call it the Stylized-ImageNet dataset and have made it publicly available for everyone. This new dataset will no doubt coerce the neural network to build a better understanding of shapes, which will bring it closer to human thinking. We don\u2019t know if that is a good thing yet, so let\u2019s look at the results. And here comes the surprise! When training a neural network architecture by the name ResNet-50 jointly on the regular and the stylized ImageNet dataset, after a little fine tuning, they have found two remarkable things. One, the resulting neural network now see more similarly to humans. The old, blue squares on the right mean that the old thinking is texture-based, but the new neural networks, denoted with the orange squares, are now much closer to the shape-based thinking of humans, which is indicated with the red circles. And now hold on to your papers, because two, the new neural network also outperforms the old ones in terms of accuracy. Dear Fellow Scholars, this is research at its finest - the authors explored an interesting idea, and look where they ended up. Amazing. If you enjoyed this episode and you feel that a bunch of these videos a month are worth 3 dollars, please consider supporting us on Patreon. This helps us get more independent and create better videos for you. You can find us at Patreon.com/twominutepapers, or just click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=YFL-MI5xzgg",
        "paper_link": "https://openreview.net/forum?id=Bygh9j09KX",
        "paper_title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness "
    },
    {
        "video_id": "QpptSohzuDo",
        "video_title": "This Experiment Questions Some Recent AI Results",
        "position_in_playlist": 322,
        "description": "Audible - get Nick Bostrom's \"Superintelligence\" for free:\nUS: https://amzn.to/2RXr32F\nEU: https://amzn.to/2SqauwI\n\n\ud83d\udcdd The paper \"Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet \" is available here:\nhttps://openreview.net/pdf?id=SkfMWhAqYQ\nhttps://openreview.net/forum?id=SkfMWhAqYQ\n\nBag of words image sources: https://people.csail.mit.edu/torralba/shortCourseRLOC/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-3081803/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the previous episode, we talked about image classification, which means that we have an image as an input, and we ask a computer to figure out what is seen in this image. Learning algorithms, such as convolutional neural networks are amazing at it, however, we just found out that even though their results are excellent, it is still quite hard to find out how they get to make a decision that an image depicts a dog or cat. This is in contrast to and old and simple technique that goes by the name bag of words. It works a bit like looking for keywords in a document and by using those, trying to find out what the writing is about. Kind of like the shortcut students like to take for mandatory readings. We have all done it. Now, imagine the same for images, where we slice up the image into small pieces and keep a score on what it seen in these snippets. Floppy ears, black snout, fur, okay, we\u2019re good, we can conclude that we have a dog over here. But wait, I hear you are saying, K\u00e1roly, why do we need to digress from AI to bag of words? Why talk about this old method? Well, let\u2019s look at the advantages and disadvantages and you will see in a moment. The advantage of bag of features is that it is quite easy to interpret because it is an open book: it gives us the scores for all of these small snippets. We know exactly how a decision is being made. A disadvantage, one would say is that because it works per snippet, it ignores the bigger spatial relationships in an image, and therefore overall, it must be vastly inferior to a neural network, right? Well, let\u2019s set up an experiment and see! This is a paper from the same group as the previous episode at the University of T\u00fcbingen. The experiment works the following way: let\u2019s try to combine bag of words with neural networks by slicing up the image into the same patches, and then, feed them into a neural network and ask it to classify them. In this case, the neural network will do many small classification tasks on image snippets instead of one big decision for the full image. The paper discusses that the final classification also involves evaluating heatmaps and more. This way, we are hoping that we get a technique where a neural network would explain its decisions, much like how bag of words works. For now, let\u2019s call these networks BagNets. And now, hold on to your papers, because results are really surprising. As expected, it is true that looking at small snippets of the image can lead to misunderstandings. For instance, this image contains a soccer ball, but when zooming into small patches, it might seem like this is a cowboy hat on top of the head of this child. However, what is unexpected is that even with this, BagNet produces surprisingly similar results to a state-of-the-art neural network by the name ResNet. This is\u2026wow. This has several corollaries. Let\u2019s start with the cool one: this means that neural networks are great at identifying objects in scrambled images, but humans are not. The reason for that is that the order of the tiles don\u2019t really matter. We now have a better reason why this is the case \u2014 doing all this classification for many small tiles independently has superpowers when it comes to processing scrambled images. The other, more controversial corollary is that this inevitably means that some results that show the superiority of deep neural networks over the good old bag of features come not from using a superior method, but from careful fine-tuning. Not all results, \u2026some results. As always, a good piece of research challenges our underlying assumptions, and sometimes, in this case, even our sanity. There is a lot to say about this topic and we have only scratched the surface, so take this as a thought-provoking idea that is worthy of further discussion. Really cool work, I love it. This video has been supported by Audible. By using Audible, you get two excellent audiobooks free of charge. I recommend that you click the link in the video description, sign up for free and check out the book Superintelligence by Nick Bostrom. Some more AI for you whenever you are stuck in traffic, or have to clean the house. I talked about this book earlier and I see that many of you Fellow Scholars have been enjoying it. If you haven\u2019t read it, make sure to sign up now because this book discusses how it could be possible to build a superintelligent AI, and what such an all-knowing being would be like. You get this book free of charge, and you can cancel at any time. You can\u2019t go wrong with this. Head on to the video description and sign up under the appropriate links. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=QpptSohzuDo",
        "paper_link": "https://openreview.net/pdf?id=SkfMWhAqYQ",
        "paper_title": "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet "
    },
    {
        "video_id": "iM4PPGDQry0",
        "video_title": "GANPaint: An Extraordinary Image Editor AI",
        "position_in_playlist": 323,
        "description": "\ud83d\udcdd The paper \" GAN Dissection: Visualizing and Understanding Generative Adversarial Networks \" and its web demo is available here:\nhttps://gandissect.csail.mit.edu\nhttp://gandissect.res.ibm.com/ganpaint.html\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#GANPaint",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper describes a new technique to visualize the inner workings of a generator neural network. This is a neural network that is is able to create images for us. The key idea here is dissecting this neural network, and looking for agreements between a set of neurons and concepts in the output image, such as trees, sky, clouds, and more. This means analyzing that these neurons are responsible for buildings to appear in the image, and those will generate clouds. Interestingly, such agreements can be found, which means way more than just creating a visualizations like this, because it enables us to edit images without any artistic skills. And now, hold on to your papers. The editing part works by forcefully activating and deactivating these units and correspond to adding or removing these objects from an image. And look! This means that we can take an already existing image, and ask this technique to remove trees from it, or perhaps add more, the same with domes, doors, and more. Wow! This is pretty cool, but you haven\u2019t seen the best part yet! Note that so far, the amount of control we have over the image is quite limited. And fortunately, we can take this further, and select a region of the image where we wish to add something new. This is suddenly so much more granular and useful! The algorithm seems to understand that trees need to be rooted somewhere and not just appear from thin air. Most of the time anyway. Interestingly, it also understands that bricks don\u2019t really belong here, but if I add it to the side of the building, it continues it in a way that is consistent with its appearance. Most of the time anyway. And of course, it is not perfect, here you can see me struggling with this spaghetti monster floating in the air that used to be a tree, and it just refuses to be overwritten. And this is a very important lesson - most research works are but a step in a thousand-mile journey, and each of them tries to improve upon the previous paper. This means that a few more papers down the line, this will probably take place in HD, perhaps in real-time, and with much higher quality. This work also builds on previous knowledge on generative adversarial networks, and whatever the follow-up papers will contain, they will build on knowledge that was found in this work. Welcome to the wonderful world of research! And now, we can all rejoice because the authors kindly made the source code available free for everyone, and not only that, but there is also a web app so you can also try it yourself! This is an excellent way of maximizing the impact of your research work. Let the experts improve upon it by releasing the source code, and let people play with it, even laymen! You will also find many failure cases, but also cases where it works well, and I think there is value in reporting both so we learn a little more about this amazing algorithm. So, let\u2019s do a little research together! Make sure to post your results in the comments section, I have a feeling that lots of high-quality entertainment materials will surface very soon. I bet the authors will also be grateful for the feedback as well, so, let\u2019s the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iM4PPGDQry0",
        "paper_link": "https://gandissect.csail.mit.edu",
        "paper_title": " GAN Dissection: Visualizing and Understanding Generative Adversarial Networks "
    },
    {
        "video_id": "OV0ivJB2lyI",
        "video_title": "Liquid Splash Modeling With Neural Networks",
        "position_in_playlist": 324,
        "description": "\u2764\ufe0f This video has been kindly supported by my friends at Arm Research. Check them out here! - http://bit.ly/2TqOWAu\n\n\ud83d\udcdd The paper \"Liquid Splash Modeling with Neural Networks\" is available here:\nhttps://ge.in.tum.de/publications/2018-mlflip-um/\n\nThe Arm word and logo are trademarks of Arm Limited (or its subsidiaries) in the US and/or elsewhere. All rights reserved.\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-2300713/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If you have been watching this series for a while, you know that I am completely addicted to fluid simulations, so it is now time for a new fluid paper. And by the end of this video, I hope you will be addicted too. If we create a virtual world with a solid block, and use our knowledge from physics to implement the laws of fluid dynamics, this solid block will indeed start behaving like a fluid. A baseline simulation technique for this will be referred to as FLIP in the videos that you see here and it stands for Fluid Implicit Particle. These simulations are often being used in the video game industry, in movies, and of course, I cannot resist to put some of them in my papers as test scenes as well. In games, we are typically looking for real-time simulations, and in this case, we can only get a relatively coarse-resolution simulation that lacks fine details, such as droplet formation and splashing. For movies, we want the highest-fidelity simulation possible, with honey coiling, two-way interaction with other objects, wet sand simulations, and all of those goodies, however, these all take forever to compute. This is the bane of fluid simulators. We have talked about a few earlier works that try to learn these laws via a neural network by feeding them a ton of video footage of these phenomena. This is absolutely amazing and is a true game changer for learning-based techniques. So, why is that? Well, up until a few years ago, whenever we had a problem that was near impossible to solve with traditional techniques, we often reached out to a neural network or some other learning algorithm to solve it, often with success. However, it is not the case here. Something has changed. What has changed is that we can already solve these problems, but we can still make use of a neural network because it can help us with something that we can already do, but it does it faster and easier. However, some of these techniques for fluids are not yet as accurate as we would like and therefore haven\u2019t yet seen widespread adoption. So here\u2019s an incredible idea: why not compute a coarse simulation quickly that surely adheres to the laws of physics, and then, fill the remaining details with a neural network. Again, FLIP is the baseline hand-crafted technique, and you can see how the neural network-infused simulation program on the left by the name MLFlip introduces these amazing details. Hm-hm! And if we compare the results with the reference simulation, which took forever, you see that it is quite similar and it indeed fills in the right kind of details. In case you are wondering about the training data, it learned the concept of splashes and droplets flying about \u2026 you guessed it right\u2026by looking at splashes and droplets flying about. So, now we know that it\u2019s quite accurate - and now, the ultimate question is, how fast is it? Well, get this - we can expect a 10-time speedup from this. So this basically means that for every 10 all nighters I have to wait for my simulations, I only have to wait one, and if something took only a few seconds, it now may be close to real time with this kind of visual fidelity. You know what, sign me up. This video has been kindly supported by my friends at ARM Research, make sure to check them out through the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=OV0ivJB2lyI",
        "paper_link": "https://ge.in.tum.de/publications/2018-mlflip-um/",
        "paper_title": "Liquid Splash Modeling with Neural Networks"
    },
    {
        "video_id": "cD-eXjf854Q",
        "video_title": "DeepMind: The Hanabi Card Game Is the Next Frontier for AI Research",
        "position_in_playlist": 325,
        "description": "\ud83d\udcdd The paper \"The Hanabi Challenge: A New Frontier for AI Research\" and a blog post is available here:\nhttps://arxiv.org/abs/1902.00506\nhttp://www.marcgbellemare.info/blog/a-cooperative-benchmark-announcing-the-hanabi-learning-environment/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-591631/\nThe Hanabi card game images are owned by the published and one image by Drentsoft Media - https://www.youtube.com/watch?v=Ofzg71qHh8k\nPoker image: https://commons.wikimedia.org/wiki/File:Two_poker_cards_and_poker_chips_20170611.jpg\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepMind #Hanabi #AI",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Now get this, after defeating Chess, Go, and making incredible progress in Starcraft 2, scientists at DeepMind just published a paper where they claim that Hanabi is the next frontier in AI research. And we shall stop \u2026right here. I hear you asking me, K\u00e1roly, after defeating all of these immensely difficult games, now, you are trying to tell me that somehow this silly card game is the next step? Yes, that\u2019s exactly what I am saying. Let me explain. Hanabi is a card game where two to five players cooperate to build five card sequences and to do that, they are only allowed to exchange very little information. This is also an imperfect information game, which means the players don\u2019t have all the knowledge available needed to make a good decision. They have to work with what they have and try to infer the rest. For instance, Poker is also an imperfect information game because we don\u2019t see the cards of the other players and the game revolves around our guesses as to what they might have. In Hanabi, interestingly, it is the other way around, so we see the cards of the other players, but not our own ones. The players have to work around this limitation by relying on each other and working out communication protocols and infer intent in order to win the game. Like in many of the best games, these simple rules conceal a vast array of strategies, all of which are extremely hard to teach to current learning algorithms. In the paper, a free and open source system is proposed to facilitate further research works and assess the performance of currently existing techniques. The difficulty level of this game can also be made easier or harder at will from both inside and outside the game. And by inside I mean that we can set parameters like the number of allowed mistakes that can be made before the game is considered lost. The outside part means that two main game settings are proposed: one, self-play, this is the easier case where the AI plays with copies of itself, therefore it knows quite a bit about its teammates, and two, ad-hoc teams can also be constructed, which means that a set of agents need to cooperate that are not familiar with each other. This is immensely difficult. When I looked the paper, I expected that as we have many powerful learning algorithms, they would rip through this challenge with ease, but surprisingly, I found out that that even the easier self-play variant severely underperforms compared to the best human players and handcrafted bots. There is plenty of work to be done here, and luckily, you can also run it yourself at home and train some of these agents on a consumer graphics card. Note that it is possible to create a handcrafted program that plays this game well, as we, humans already know good strategies, however, this project is about getting several instances of an AI to learn new ways to communicate with each other effectively. Again, the goal is not to get a computer program that plays Hanabi well, the goal is to get an AI to learn to communicate effectively and work together towards a common goal. Much like Chess, Starcraft 2 and DOTA, Hanabi is still a proxy to be used for measuring progress in AI research. Nobody wants to spend millions of dollars to play card games at work, so the final goal of DeepMind is to reuse this algorithm for other applications where even we, humans falter. I have included some more materials on this game in the video description, make sure to have a look. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cD-eXjf854Q",
        "paper_link": "https://arxiv.org/abs/1902.00506",
        "paper_title": "The Hanabi Challenge: A New Frontier for AI Research"
    },
    {
        "video_id": "C7Dmu0GtrSw",
        "video_title": "Google\u2019s PlaNet AI Learns Planning from Pixels",
        "position_in_playlist": 326,
        "description": "Errata: https://twitter.com/arjunbazinga/status/1114497224174497793\n\n\ud83d\udcdd The paper \"Learning Latent Dynamics for Planning from Pixels\" and its source code is available here:\nhttps://planetrl.github.io/\nhttps://arxiv.org/abs/1811.04551\nhttps://github.com/google-research/planet\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Google #PlaNet #PlaNetAI",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to talk about PlaNET, a technique that is meant to solve challenging image-based planning tasks with sparse rewards. Ok, that sounds great, but what do all of these terms mean? The planning part is simple, it means that the AI has to come up with a sequence of actions to achieve a goal, like pole balancing with a cart, teaching a virtual human or a cheetah to walk, or hitting this box the right way to make sure it keeps rotating. The image-based part is big - this means that the AI has to learn the same way as a human, and that is, by looking at the pixels of the images. This is a huge difficulty bump because the AI does not only have to learn to defeat the game itself, but also has to build an understanding of the visual concepts within the game. DeepMind\u2019s legendary Deep Q-Learning algorithm was also able to learn from pixel inputs, but it was mighty inefficient at doing that, and no wonder, this problem formulation is immensely hard and it is a miracle that we can muster any solution at all that can figure it out. The sparse reward part means that we rarely get feedback as to how well we are doing at these tasks, which is a nightmare situation for any learning algorithm. A key difference with this technique against classical reinforcement learning, which is what most researchers reach out to to solve similar tasks, is that this one uses models for the planning. This means that it does not learn every new task from scratch, but after the first game, whichever it may be, it will have a rudimentary understanding of gravity and dynamics, and it will be able to reuse this knowledge in the next games. As a result, it will get a headstart when learning a new game and is therefore often 50 times more efficient than the previous technique that learns from scratch, and not only that, but it has other really cool advantages as well which I will tell you about in just a moment. Here you can see that indeed, the blue lines significantly outperform the previous techniques shown with red and green for each of these tasks. I like how this plot is organized in the same grid as the tasks were as it makes it much more readable when juxtaposed with the video footage. As promised, here are the two really cool additional advantages of this model-based agent. The first is that we don\u2019t have to train six separate AIs for all of these tasks, but finally, we can get one AI that is able to solve all six of these tasks efficiently. And second, it can look at as little as five frames of an animation, which is approximately one fifth of a second worth of footage\u2026that is barely anything and it is able to predict how the sequence would continue with a remarkably high accuracy, and, over a long time frame, which is quite a challenge. This is an excellent paper with beautiful mathematical formulations, I recommend that you have a look in the video description. The source code is also available free of charge for everyone, so I bet this will be an exciting direction for future research works, and I\u2019ll be here to report on it to you. Make sure to subscribe and hit the bell icon to not miss future episodes. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=C7Dmu0GtrSw",
        "paper_link": "https://planetrl.github.io/",
        "paper_title": "Learning Latent Dynamics for Planning from Pixels"
    },
    {
        "video_id": "dvzlvHNxdfI",
        "video_title": "This AI Learned to \u201cPhotoshop\u201d Human Faces",
        "position_in_playlist": 327,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color\" is available here:\nhttps://arxiv.org/abs/1902.06838\nhttps://github.com/JoYoungjoo/SC-FEGAN\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nhumbnail background image credit: https://pixabay.com/images/id-3404534/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we talk quite a bit about neural network-based learning methods that are able to generate new images for us from some sort of sparse description, like a written sentence, or a set of controllable parameters. These can enable us mere mortals without artistic skills to come up with novel images. However, one thing that comes up with almost every single one of these techniques is the lack of artistic control. You see, if we provide a very coarse input, there are many-many different ways for the neural networks to create photorealistic images from them. So, how do we get more control over these results? An earlier paper from NVIDIA generated human faces for us and used a latent space technique that allows us some more fine-grained control over the images. It is beyond amazing. But, these are called latent variables because they represent the inner thinking process of the neural network, and they do not exactly map to our intuition of facial features in reality. And now, have a look at this new technique that allows us to edit the geometry of the jawline of a person, put a smile on someone\u2019s face in a more peaceful way than seen in some Batman movies, or, remove the sunglasses and add some crazy hair at the same time. Even changing the hair of someone while adding an earring with a prescribed shape is also possible. Whoa! And I just keep talking and talking about artistic control, so it\u2019s great that these shapes are supported, but what about an other important aspect of artistic control, for instance, colors? Yup, that is also supported. Here you see that the color of the woman\u2019s eyes can be changed, and, the technique also understands the concept of makeup as well. How cool is that! Not only that, but it is also blazing fast. It takes roughly 50 milliseconds to create these images with the resolution of 512 by 512, so in short, we can do this about 20 times per second. Make sure to have a look at the paper that also contains a validation section against other techniques and reference results, turns out there is such a thing as a reference result in this case, which is really cool, and you will also find a novel style loss formulation that makes all this crazy wizardry happen. No web app for this one, however, the source code is also available free of charge and under a permissive license, so let the experiments begin! If you have enjoyed this video and you feel that a bunch of these videos are worth 3 dollars a month, please consider supporting us on Patreon. In return, we can offer you early access to these episodes, and more, to keep your paper addiction in check. It truly is a privilege for me to be able to keep making these videos, I am really enjoying the journey, and this is only possible because of your support on Patreon. This is why every episode ends with, you guessed it right\u2026 Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dvzlvHNxdfI",
        "paper_link": "https://arxiv.org/abs/1902.06838",
        "paper_title": "SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color"
    },
    {
        "video_id": "luwP75lPExo",
        "video_title": "NeuroSAT: An AI That Learned Solving Logic Problems",
        "position_in_playlist": 328,
        "description": "\u2764\ufe0f This video has been kindly supported by my friends at Arm Research. Check them out here! - http://bit.ly/2TqOWAu\n\n\ud83d\udcdd The paper \"Learning a SAT Solver from Single-Bit Supervision\" is available here:\nhttps://arxiv.org/abs/1802.03685\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nImage an article sources:\nSAT:  https://www.geeksforgeeks.org/2-satisfiability-2-sat-problem/ - Source: GeeksforGeeks\nNP-Completeness: https://en.wikipedia.org/wiki/List_of_NP-complete_problems\nNeural network image source: https://en.wikipedia.org/wiki/File:Neural_network_example.svg\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Please meet NeuroSAT. The name of this technique tells us what it is about, the neuro part means that it is a neural network-based learning method, and the SAT part means that it is able to solve satisfiability problems. This is a family of problems where we are given a logic formula, and we have to decide whether these variables can be chosen in a way such that this expression comes out true. That, of course, sounds quite nebulous, so let\u2019s have a look at a simple example - this formula says that F is true if A is true and at the same time not B is true. So if we choose A to be true and B as false, this expression is also true, or in other words, this problem is satisfied. Getting a good solution for SAT is already great for solving many problems involving logic, however, the more interesting part is that it can help us solve an enormous set of other problems, for instance, ones that involve graphs describing people in social networks, and many others that you see here. This can be done by performing something that mathematicians like to call polynomial-time reduction or Karp-reduction, which means that many other problems that seem completely different can be converted into a SAT problem. In short, if you can solve SAT well, you can solve all of these problems well. This is one of the amazing revelations I learned about during my mathematical curriculum. The only problem is that when trying to solve big and complex SAT problems, we can often not do much better than random guessing, which, for some of most nasty ones, takes so long that it practically is never going to finish. And get this, interestingly, this work presents us with a neural network that is able to solve problems of this form, but not like this tiny-tiny baby problem, but much bigger ones. And, this really shouldn\u2019t be possible. Here\u2019s why. To train a neural network, we require training data. The input is a problem definition and the output is whether this problem is satisfiable. And we can stop right here, because here lies our problem. This doesn\u2019t really make any sense, because we just said that it is difficult to solve big SAT problems. And here comes the catch! This neural network learns from SAT problems that are small enough to be solved by traditional handcrafted methods. We can create arbitrarily many training examples with these solvers, albeit these are all small ones. And that\u2019s not it, there are three key factors here that make this technique really work. One, it learns from only single-bit supervision. This means that the output that we talked about is only yes or no. It isn\u2019t shown the solution itself. That\u2019s all the algorithm learns from. Two, when we request a solution from the neural network, it not only tells us the same binary yes-no answer that was used to teach it, but it can go beyond that, and when the problem is satisfiable, it will almost always provide us with the exact solution. It is not only able to tell us whether the problem can be solved, but it almost always provides a possible solution as well. That is indeed remarkable. This image may be familiar from the thumbnail, and here you can see how the neural network\u2019s inner representation of how these variables change over time as it sees a satisfiable and unsatisfiable problem and how it comes to its own conclusions. And three, when we ask the neural network for a solution, it is able to defeat problems that are larger and more difficult than the ones it has trained on. So this means that we train it on simple problems that we can solve ourselves, and using these as training data, it will be able to solve much harder problems that we can\u2019t solve ourselves. This is crucial, because otherwise, this neural network would only be as good as the handcrafted algorithm used to train it. Which in other words, is not useful at all. Isn\u2019t this amazing? I will note that there are handcrafted algorithms that are able to match, and often outperform NeuroSAT, however, those took decades of research work to invent, whereas this is a learning-based technique that just looks at as little information as the problem definition and whether it is satisfiable, and it is able to come up with a damn good algorithm by itself. What a time to be alive. This video has been kindly supported by my friends at ARM Research, make sure to check them out through the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=luwP75lPExo",
        "paper_link": "https://arxiv.org/abs/1802.03685",
        "paper_title": "Learning a SAT Solver from Single-Bit Supervision"
    },
    {
        "video_id": "-jL2o_15s1E",
        "video_title": "Beautiful Gooey Simulations, Now 10 Times Faster",
        "position_in_playlist": 329,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"GPU Optimization of Material Point Methods\" is available here:\nhttp://www.cemyuksel.com/research/papers/gpu_mpm.pdf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. You know that I see a piece of fluid and I can\u2019t resist making videos about it\u2026I just can\u2019t. Oh my goodness. Look at that. These animations were created using the Material Point Method, or MPM in short, which is a hybrid simulation method which is able to simulate not only substances like water and honey, but it can also simulate snow, granular solids, cloth, and many-many other amazing things that you see here. Before you ask, the hybrid part means that it both uses particles and grids during the computations. Unfortunately, it is very computationally demanding, so it takes forever to get these simulations ready. And typically, in my simulations, after this step is done, I almost always find that the objects did not line up perfectly, so I can start the whole process again. Ah well. This technique has multiple stages, uses multiple data structures in many of them, and often we have to wait for the results of one stage to be able to proceed to the next. This is not that much of a problem if we seek to implement this on our processor, but it would be way, way faster if we could run it on the graphics card\u2026as long as we implement these problems on them properly. However, due to these stages waiting for each other, it is immensely difficult to use the heavy parallel computing capabilities of the graphics card. So here you go, this technique enables running MPM on your graphics card efficiently, resulting in an up to ten-time improvement over previous works. As a result, this granulation scene has more than 6.5 million particles on a very fine grid and can be simulated in only around 40 seconds per frame. And not only that, but the numerical stability of this technique is also superior to previous works, and it is thereby able to correctly simulate how the individual grains interact in this block of sand. Here is a more detailed breakdown of the number of particles, grid resolutions, and the amount of computation time needed to simulate each time step. I am currently in the middle of a monstrous fluid simulation project and oh man, I wish I had these numbers for the computation time. This gelatin scene takes less than 7 seconds per frame to simulate with a similar number of particles. Look at that heavenly gooey thing. It probably tastes like strawberries. And if you enjoyed this video and you wish to help us teach more people about these amazing papers, please consider supporting us on Patreon. In return, we can offer you early access to these episodes, or you can also get your name in the video description of every episode as a key supporter. You can find us at Patreon.com/TwoMinutePapers. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-jL2o_15s1E",
        "paper_link": "http://www.cemyuksel.com/research/papers/gpu_mpm.pdf",
        "paper_title": "GPU Optimization of Material Point Methods"
    },
    {
        "video_id": "wEgq6sT1uq8",
        "video_title": "The Bitter Lesson - Compute Reigns Supreme",
        "position_in_playlist": 330,
        "description": "\ud83d\udcdd The article \"The Bitter Lesson\" is available here:\nhttp://www.incompleteideas.net/IncIdeas/BitterLesson.html\n\nNice twitter thread on this video: https://twitter.com/karoly_zsolnai/status/1114867598724931585\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nImage sources:\nhttps://en.wikipedia.org/wiki/Ciphertext\nhttps://en.wikibooks.org/wiki/Foundations_of_Computer_Science/Encryption\nhttps://en.wikipedia.org/wiki/Letter_frequency\nhttps://commons.wikimedia.org/wiki/File:A-pigpen-message.svg\nThumbnail background image credit: https://pixabay.com/images/id-1587673/\nVideo background:  https://pixabay.com/hu/photos/olaszorsz%C3%A1g-hegyek-braies-t%C3%B3-t%C3%B3-1587287/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Before we start, I\u2019d like to tell you that this video is not about a paper, and it is not going to be two minutes. Welcome to Two Minute Papers! This piece bears the name \u201cThe Bitter Lesson\u201d and was written by Richard Sutton, a legendary Canadian researcher who has contributed a great deal to reinforcement learning research. And what a piece this is! It is a short article on how we should do research, and ever since I read it, I couldn\u2019t stop thinking about it, and as a result, I couldn\u2019t not make a video on this topic. We really have to talk about this. It takes less than 5 minutes to read, so before we talk about it, you can pause the video and click the link to it in the video description. So, in this article, he makes two important observations. Number one, he argues that the best performing learning techniques are the ones that can leverage computation, or, in other words, methods that improve significantly as we add more compute power. Long ago, people tried to encode lots of human knowledge of strategies in their Go AI, but did not have enough compute power to make a truly great algorithm. And now we have AlphaGo, which contains minimal information about Go itself, and it is better than the best human players in the world. And, number two, he recommends that we try to put as few constraints on a problem as possible. He argues that we shouldn't try to rebuild the mind, but try to build a method that can capture arbitrary complexity and scale it up with hardware. Don\u2019t try to make it work like your brain - make something as general as possible and make sure it can leverage computation and it will come up with something that is way better than our brain. So, in short, keep the problem general, and don\u2019t encode your knowledge of the domain into your learning algorithms. The weight of this sentence is not to be underestimated, because these seemingly simple observations sound really counterintuitive. This seemingly encourages us to do the exact opposite of what we are currently doing. Let me tell you why. I have fond memories of my early lectures I attended to in cryptography, where we had a look at ciphertexts. These are very much like encrypted messages that children like to write each other at school, which looks like nonsense for the unassuming teacher, but can be easily decoded by another child when provided with a key. This key describes which symbol corresponds to which letter. Let\u2019s assume that one symbol means one letter, but if we don\u2019t have any additional knowledge, this is still not an easy problem to crack. But in this course, soon, we coded up algorithms that were able to crack messages like this in less than a second. How exactly? Well, by inserting additional knowledge into the system. For instance, we know the relative frequency of each letter in every language. For instance, in English, the letter \u201ce\u201d is the most common by far, and then comes \u201ct\u201d, \u201ca\u201d, and the others. The fact that we are not seeing letters but symbols doesn\u2019t really matter, because we just look up the most frequent symbol in the ciphertext and we immediately know that okay, that symbol is going to be the letter \u201ce\u201d, and so on. See what we have done here? Just by inserting a tiny bit of knowledge, suddenly, a very difficult problem turned into a trivial problem. So much so that anyone can implement this after their second cryptography lecture. And somehow, Richard Sutton argues that we shouldn\u2019t do that? Doesn\u2019t that sound crazy? So, what gives? Well, let me explain through an example from light transport research that demonstrates his point. Path tracing is one of the first and simplest algorithms in the field, which, in many regards, is vastly inferior to Metropolis Light Transport, which is a much smarter algorithm. However, with our current powerful graphics cards, we can compute so many more rays with path tracing, that in many cases it wins over Metropolis. In this case, compute reigns supreme. The hardware scaling outmuscles the smarts, and we haven\u2019t even talked about how much easier it is for engineers to maintain and improve a simpler system. The area of Natural Language Processing has many decades of research to teach machines how to understand, simplify, correct, or even generate text. After so many papers and handcrafted techniques which insert our knowledge of linguistics into our techniques, who would have thought that OpenAI would be able to come up with a relatively simple neural network with so little prior knowledge that is able to write articles that sound remarkably lifelike. We will talk about this method in more detail in this series soon. And here comes the bitter lesson. Doing research the classical way of inserting knowledge into a solution is very satisfying - it feels right, it feels like doing research, progressing, and it makes it easy to show in a new paper what exactly the key contributions are. However, it may not be the most effective way forward. Quoting the article. I recommend that you pay close attention to this. \u201cThe bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because its success over a favored, human-centric approach.\u201c In our cryptography problem from earlier, of course the letter frequency solution and other linguistic tricks are clearly much, much better than a solution that doesn\u2019t know anything about the domain. Of course! However, when later, we have a 100 times faster hardware, this knowledge may actually inhibit finding a solution that is way, way better. This is why he also claims that we shouldn\u2019t try to build intelligence by modeling our brain in a computer simulation. It\u2019s not that the \u201cour brain\u201d approach doesn\u2019t work - it does, but on the short run. On the long run, we will be able to add more hardware to a learning algorithm, and it will find more effective structures to solve problems, and it will eventually outmuscle our handcrafted techniques. In short, this is the lesson: when facing a learning problem, keep your domain knowledge out of the solution, and use more compute. More compute gives us more learning, and more general formulations give us more chance to find something relevant. So, this is indeed a harsh lesson. This piece sparked great debates on Twitter, I have seen great points for and against this sentiment. What do you think? Let me know in the comments, as everything in science, this piece should be subject to debate and criticism. And therefore, I\u2019d love to read as many people\u2019s take on it as possible. And, this piece has implications on my thinking as well \u2014 please allow me to add a three more small personal notes that kept me up at night in the last few days. Note number one, the bottom line is whenever we build a new algorithm, we should always bear in mind which parts would be truly useful if we had a 100 times the compute power that we have now. Note number two, a corollary of this thinking is that arguably, hardware engineers who make these new and more powerful graphics cards may be contributing the very least as much to AI than most of AI research does. And note number three, to me it feels like this almost implies that best is to join the big guys where all the best hardware is. I work in an amazing, small to mid-sized lab at the Technical University of Vienna and in the last few years, I have given relatively little consideration to the invitations from some of the more coveted and well-funded labs. Was it a mistake? Should I change that? I really don\u2019t know for sure. If for some reason, you haven\u2019t read the piece at the start of the video, make sure to do it after watching this. It\u2019s really worth it. In the meantime, interestingly, the non-profit AI research lab, OpenAI also established a for profit, or what they like to call \u201ccapped profit\u201d company to be able to compete with the other big guys like DeepMind and Facebook Reality Labs. I think Richard has a solid point here. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wEgq6sT1uq8"
    },
    {
        "video_id": "UoKXJzTYDpw",
        "video_title": "Why Are Cloth Simulations So Hard?",
        "position_in_playlist": 331,
        "description": "\ud83d\udcdd The paper \"I-Cloth: Incremental Collision Handling for GPU-Based Interactive Cloth Simulation\" is available here:\nhttps://min-tang.github.io/home/ICloth/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to talk a bit about the glory and the woes of cloth simulation programs. In these simulations, we have several 3D models of garments that are built from up to several hundreds of thousands of triangles. And as they move in time, the interesting part of the simulation is whenever collisions happen, however, evaluating how these meshes collide is quite difficult and time consuming. Basically, we have to tell an algorithm that we have piece of cloth with a hundred thousand connected triangles here, and another one there, and now, have a look and tell me which collides with which and how they bend and change in response these forces. And don\u2019t forget about friction and repulsive forces! Also, please, be accurate because every small error adds up over time, and, do it several times a second so we can have a look at the results interactively. Well, this is a quite challenging problem. And it takes quite long to compute, so much so that 70-80% of the total time taken to perform the simulation is spent with doing collision handling. So how can we make it not take forever? Well, one way would be to try to make sure that we can run this collision handling step on the graphics card. This is exactly what this work does, and in order to do this, we have to make sure that all these evaluations can be performed in parallel. Of course, this is easier said than done. Another difficulty is choosing the appropriate time steps. These simulations are run in a way that we check and resolve all of the collisions, and then, we can advance the simulation forward by a tiny amount. This amount of called a time step, and choosing the appropriate time step has always been a challenge. You see, if we set it to too large, we will be done faster and compute less, however, we will almost certainly miss some collisions because we skipped over them. The simulation may end up in a state that is so incorrect that it is impossible to recover from, and we have to throw the entire thing out. If we set it to too low, we get a more robust simulation, however, it will take many hours to days to compute. To remedy this, this technique is built in a way such that we can use larger time steps. That\u2019s excellent news. Also, the collision computation part is now up to 9 times faster, and if we look at the cloth simulation as a whole, that can be made over 3 times faster. As you see here, this is especially nice because we can test how these garments react to our manipulations at 8-10 frames per second. If you have a closer look at the paper, you will also find another key observation which states that most of the time, only a small subregion of the simulated cloth undergoes deformation due to response forces, and this knowledge can be kept track of, which contributed to cutting down the simulation time significantly. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UoKXJzTYDpw",
        "paper_link": "https://min-tang.github.io/home/ICloth/",
        "paper_title": "I-Cloth: Incremental Collision Handling for GPU-Based Interactive Cloth Simulation"
    },
    {
        "video_id": "8ypnLjwpzK8",
        "video_title": "OpenAI GPT-2: An Almost Too Good Text Generator",
        "position_in_playlist": 332,
        "description": "\u2764\ufe0f Support the show and pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"Better Language Models and Their Implications\" is available here:\nhttps://openai.com/blog/better-language-models/\n\nGPT-2 Reddit bot:\nhttps://old.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/\n\nCriticism:\nhttps://medium.com/@lowe.ryan.t/openais-gpt-2-the-model-the-hype-and-the-controversy-1109f4bfd5e8?sk=bc319cebc22fe0459574544828c84c6d\n\nThe Bitter Lesson video:\nhttps://www.youtube.com/watch?v=wEgq6sT1uq8\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#OpenAI #GPT3 #GPT2",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is an incredible paper from OpenAI, in which the goal is to teach an AI to read a piece of text and perform common natural language processing operations on it, for instance, answering questions, completing text, reading comprehension, summarization, and more. And not only that, but additionally, the AI has to be able to perform these tasks with as little supervision as possible. This means that we seek to unleash the algorithm that they call GPT-2 to read the internet and learn the intricacies of our language by itself. To perform this, of course, we need a lot of training data, and here, the AI reads 40 gigabytes of internet text, which is 40 gigs of non-binary plaintext data, which is a stupendously large amount of text. It is always hard to put these big numbers in context, so as an example, to train similar text completion algorithms, AI people typically reach out to a text file containing every significant work of Shakespeare himself, and this file is approximately 5 megabytes. So the 40 gigabytes basically means an amount of text that is 8000 times the size of Shakespeare\u2019s works. That\u2019s a lot of text. And now, let\u2019s have a look at how it fares with the text completion part. This part was written by a human, quoting: \u201cIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect\u00a0English.\u201d And the AI continued the text the following way, quoting a short snippet of it: \u201cThe scientist named the population, after their distinctive horn, Ovid\u2019s Unicorn. These four-horned, silver-white unicorns were previously unknown to\u00a0science.\u201d Whoa! Now note that this is clearly not perfect, if there is even such a thing as a perfect continuation, and it took 10 tries, which means that the algorithm was run 10 times and the best result was cherrypicked and recorded here. And despite all of these, this is a truly incredible result, especially given that the algorithm learns on its own. After giving it a piece of text, it can also answer questions in a quite competent manner. Worry not, later in this video, I will show you more of these examples and likely talk over them, so if you are curious, feel free to pause the video while you read the prompts and their completions. The validation part of the paper reveals that this method is able to achieve state-of-the-art results on several language modeling tasks, and you can see here that we still shouldn\u2019t expect it to match a human in terms of reading comprehension, which is the question answering test. More on that in a moment. So, there are plenty of natural language processing algorithms out there that can perform some of these tasks, in fact, some articles already stated that there is not much new here, it\u2019s just the same problem, but stated in a more general manner, and with more compute. A ha! It is not the first time that this happens. Remember our video by the name \u201cThe Bitter lesson\u201d? I\u2019ve put a link to it in the video description, but in case you missed it, let me quote how Richard Sutton addressed this situation: \u201cThe bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because its success over a favored, human-centric approach.\u201c So what is the big lesson here? Why is GPT-2 so interesting? Well, big lesson number one is that this is one of the clearer cases of what the quote was talking about, where we can do a whole lot given a lot of data and compute power, and we don\u2019t need to insert too much additional knowledge into our algorithms. And lesson number two, as a result, this algorithm becomes quite general so it can perform more tasks than most other techniques. This is an amazing value proposition. I will also add that not every learning technique scales well when we add more compute, in fact, you can see here yourself that even GPT-2 plateaus on the summarization task. Making sure that these learning algorithms scale well is a great contribution in and of itself and should not be taken for granted. There has been a fair bit of discussion on whether OpenAI should publish the entirety of this model. They opted to release a smaller part of the source code and noted that they are aware that the full model could be used for nefarious purposes. Why did they do this? What is the matter with everyone having an AI with a subhuman-level reading comprehension? Well, so far, we have only talked about quality. But another key part is quantity. And boy, are these learning methods superhuman in terms of quantity\u2026just imagine that they can write articles with a chosen topic and sentiment all day long, and, much quicker than human beings. Also note that the blueprint of the algorithm is described in the paper, and a top-tier research group is expected to be able to reproduce it. So does one release the full source code and models or not? This is a quite difficult question: we need to keep publishing both papers and source code to advance science, but, we also have to find new ways to do it in an ethical manner. This needs more discussion and would definitely be worthy of a conference-style meeting. Or more. There is so much to talk about, and so far, we have really only scratched the surface, so make sure to have a look in the video description, I left a link to the paper and some more super interesting reading materials for you. Make sure to check them out! Also just a quick comment on why this video came so late after the paper has appeared. Since there were a lot of feelings and intense discussion on whether the algorithm should be published or not, I was looking to wait until the dust settles and there is enough information out there to create a sufficiently informed video for you. This, of course means that we are late to the party and missed out on a whole lot of views and revenue, but that\u2019s okay, in fact, that\u2019s what we\u2019ll keep doing going forward to make sure you get the highest quality information that I can provide. If you have enjoyed this episode and would like to help us, please consider supporting us on Patreon! Remember our motto, a dollar a month is almost nothing, but it keeps the papers coming. And there are hundreds of papers on my reading list. As always, we are available through Patreon.com/TwoMinutePapers, and the link is also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=8ypnLjwpzK8",
        "paper_link": "https://openai.com/blog/better-language-models/",
        "paper_title": "Better Language Models and Their Implications"
    },
    {
        "video_id": "iKrrKyeSRew",
        "video_title": "How Do Neural Networks Memorize Text?",
        "position_in_playlist": 333,
        "description": "\ud83d\udcdd The paper \"Visualizing memorization in RNNs\" is available here:\nhttps://distill.pub/2019/memorization-in-rnns/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is an article from the Distill journal, so expect a lot of intuitive and beautiful visualizations. And it is about recurrent neural networks. These are neural network variants that are specialized to be able to deal with sequences of data. For instance, processing and completing text is a great example usage of these recurrent networks. So, why is that? Well, if we wish to finish a sentence, we are not only interested in the latest letter in this sentence, but several letters before that, and of course, the order of these letters is also of utmost importance. Here you can see with the green rectangles which previous letters these recurrent neural networks memorize when reading and completing our sentences. LSTM stands for Long Short-Term Memory, and GRU means Gated Recurrent Unit, both are recurrent neural networks. And you see here that the nested LSTM doesn\u2019t really look back further than the current word we are processing, while the classic LSTM almost always memorizes a lengthy history of previous words. And now, look, interestingly, with GRU, when looking at the start of the word \u201cgrammar\u201d here, we barely know anything about this new word, so, it memorizes the entire previous word as it may be the most useful information we have at the time. And now, as we proceed a few more letters into this word, it mostly shifts its attention to a shorter segment, that is, the letters of this new word we are currently writing. Luckily, the paper is even more interactive, meaning that you can also add a piece of text here and see how the GRU network processes it. One of the main arguments of this paper is that when comparing these networks against each other in terms of quality, we shouldn\u2019t only look at the output text they generate. For instance, it is possible for two models that work quite differently to have a very similar accuracy and score on these tests. The author argues that we should look beyond these metrics, and look at this kind of connectivity information as well. This way, we may find useful pieces of knowledge, like the fact that GRU is better at utilizing longer-term contextual understanding. A really cool finding indeed, and I am sure this will also be a useful visualization tool when developing new algorithms and finding faults in previous ones. Love it. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iKrrKyeSRew",
        "paper_link": "https://distill.pub/2019/memorization-in-rnns/",
        "paper_title": "Visualizing memorization in RNNs"
    },
    {
        "video_id": "hW1_Sidq3m8",
        "video_title": "NVIDIA's AI Creates Beautiful Images From Your Sketches",
        "position_in_playlist": 334,
        "description": "If you feel like it, buy anything through this Amazon link - you don't lose anything and we get a small kickback.\nUS: https://amzn.to/2FQHPcs\nEU: https://amzn.to/2UnB2yF\n\n\ud83d\udcdd The paper \"Semantic Image Synthesis with Spatially-Adaptive Normalization\" and its source code is available here:\nhttps://nvlabs.github.io/SPADE/\nhttps://github.com/NVlabs/SPADE\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I know for a fact that some of you remember our first video on image translation, which was approximately 3 years and 250 episodes ago. This was a technique where we took an input painting, and a labeling of this image that shows what kind of objects are depicted, and then, we could start editing this labeling, and out came a pretty neat image that satisfies these labels. Then came pix2pix, another image translation technique which in some cases, only required a labeling, a source photo was not required because these features were learned from a large amount of training samples. And it could perform really cool things, like translating a landscape into a map, or sketches to photos, and more. Both of these works were absolutely amazing, and I always say, two more papers down the line, and we are going to have much higher resolution images. So, this time, here is the paper that is, in fact, two more papers down the line. So let\u2019s see what it can do! I advise you that you hold on to your papers for this one. The input is again, a labeling which we can draw ourselves, and the output is a hopefully photorealistic image that adheres to these labels. I like how first, only the silhouette of the rock is drawn, so we have this hollow thing on the right that is not very realistic, and then, it is now filled in with the bucket tool, and, there you go. It looks amazing. It synthesizes a relatively high-resolution image and we finally have some detail in there too. But, of course, there are many possible images that correspond to this input labeling. How do we control the algorithm to follow our artistic goals? Well, you remember from the first work I\u2019ve shown you where we could do that by adding an additional image as an input style. Well, look at that! We don\u2019t even need to engage in that, because here, we can choose from a set of input styles that are built into the algorithm and we can switch between them almost immediately. I think the results speak for themselves, but note that not only the visual fidelity, but the alignment with the input labels is also superior to previous approaches. Of course, to perform this, we need a large amount of training data where the inputs are labels, and the outputs are the photorealistic images. So how do we generate such a dataset? Drawing a bunch of labels and asking artists to fill them in sounds like a crude and expensive idea. Well, of course, we can do it for free by thinking the other way around! Let\u2019s take a set of photorealistic images, and use already existing algorithms to create the labeling for them. If we can do that, we\u2019ll have as many training samples as many images we have, in other words, more than enough to train an amazing neural network. Also, the main part of the magic in this new work is using a new kind of layer for normalizing information within this neural network that adapts better to our input data than the previously used batch normalization layers. This is what makes the outputs more crisp and does not let semantic information be washed away in these images. If you have a closer look at the paper in the video description, you will also find a nice evaluation section with plenty of comparisons to previous algorithms and according to the authors, the source code will be released soon as well. As soon as it comes out, everyone will be able to dream up beautiful photorealistic images and get them out almost instantly. What a time to be alive! If you have enjoyed this episode and would like to support us, please click one of the Amazon affiliate links in the video description and buy something that you were looking to buy on Amazon anyway. You don\u2019t lose anything, and this way, we get a small kickback which is a great way to support the series so we can make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hW1_Sidq3m8",
        "paper_link": "https://nvlabs.github.io/SPADE/",
        "paper_title": "Semantic Image Synthesis with Spatially-Adaptive Normalization"
    },
    {
        "video_id": "XSWqLb0VyzM",
        "video_title": "Exploring And Attacking Neural Networks With Activation Atlases",
        "position_in_playlist": 335,
        "description": "\ud83d\udcdd The paper \"Exploring Neural Networks with Activation Atlases\" is available here:\nhttps://distill.pub/2019/activation-atlas/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. When it comes to image classification tasks, in which the input is a photograph and the output is decision as to what is depicted in this photo, neural network-based learning solutions became more accurate than any other computer program we, humans could possibly write by hand. Because of that, the question naturally arises: what do these neural networks really do inside to make this happen? This article explores new ways to visualize the inner workings of these networks, and since it was published in the Distill journal, you can expect beautiful and interactive visualizations that you can also play with if you have a look in the video description. It is so good, I really hope that more modern journals like this appear in the near future. But back to our topic - wait a second, we already had several videos on neural network visualization before, so what is new here? Well, let\u2019s see! First, we have looked at visualizations for individual neurons. This can be done by starting from a noisy image and add slight modifications to it in a way that makes a chosen neuron extremely excited. This results in these beautiful colored patterns. I absolutely love, love, love these patterns, however, this misses all the potential interactions between the neurons, of which there are quite many. With this, we have arrived to pairwise neuron activations, which sheds more light on how these neurons work together. Another one of those beautiful patterns. This is, of course, somewhat more informative: intuitively, if visualizing individual neurons was equivalent to looking at a sad little line, the pairwise interactions would be observing 2D slices in a space. However, we are still not seeing too much from this space of activations, and the even bigger issue is that this space is not our ordinary 3D space, but a high-dimensional one. Visualizing spatial activations gives us more information about these interactions between not two, but more neurons, which brings us closer to a full-blown visualization, however, this new Activation Atlas technique is able to provide us with even more extra knowledge. How? Well, you see here with the dots that it provides us a denser sampling of the most likely activations, and, this leads to a more complete bigger-picture view of the inner workings of the neural network. This is what it looks like if we run it on one image. It also provides us with way more extra value, because so far, we have only seen how the neural network reacts to one image, but this method can be extended to see its reaction to not one, but one million images! You can see an example of that here. What\u2019s more, it can also unveil weaknesses in the neural network. For instance, have a look at this amazing example where the visualization uncovers that we can make this neural network misclassify a grey whale for a great white shark, and all we need to do is just brazenly put a baseball in this image. It is not a beautiful montage, is it? Well, that\u2019s not a drawback, that\u2019s exactly the point! No finesse is required, and the network is still fooled by this poorly-edited adversarial image. We can also trace paths in this atlas which reveal how the neural network decides whether one or multiple people are in an image, or how to tell a watery type terrain from a rocky cliff. Again, we have only scratched the surface here, and you can play with these visualizations yourself, so make sure to have a closer look at the paper through the link in the video description. You won\u2019t regret it. Let me know in the comments section how it went! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XSWqLb0VyzM",
        "paper_link": "https://distill.pub/2019/activation-atlas/",
        "paper_title": "Exploring Neural Networks with Activation Atlases"
    },
    {
        "video_id": "Wxb0jN0X7cs",
        "video_title": "How To Train Your Virtual Dragon",
        "position_in_playlist": 336,
        "description": "Patreon: https://www.patreon.com/TwoMinutePapers\n\n\u20bf Crypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 1a5ttKiVQiDcr9j8JT2DoHGzLG7XTJccX\n\u203a Ethereum: 0xbBD767C0e14be1886c6610bf3F592A91D866d380\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\n\ud83d\udcdd The paper \"Aerobatics Control of Flying Creatures\nvia Self-Regulated Learning\" is available here:\nhttp://mrl.snu.ac.kr/research/ProjectAerobatics/Aerobatics.htm\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Scientists at the Seoul National University in South Korea wrote a great paper on teaching an imaginary dragon all kinds of really cool aerobatic maneuvers, like sharp turning, rapid winding, rolling, soaring, and diving. This is all done by a reinforcement learning variant, where the problem formulation is that the AI has to continuously choose the character\u2019s actions to maximize a reward. Here, this reward function is related to a trajectory which we can draw in advance, these are the lines that the dragon seems follow quite well. However, what you see here is the finished product. Curious to see how the dragon falters as it learns to maneuver properly? Well, we are in luck. Buckle up. You see the ideal trajectory here with black, and initially, the dragon was too clumsy to navigate in a way that even resembles this path. Then, later, it learned to start the first turn properly, but as you see here, it was unable to avoid the obstacle and likely needs to fly to the emergency room. But it would probably miss that building too, of course. After more learning, it was able to finish the first loop, but was still too inaccurate to perform the second. And finally, at last, it became adept at performing this difficult maneuver. Applause! One of the main difficulties of this problem is the fact that the dragon is always in motion and has a lot of momentum, and anything we do always has an effect later, and we not only have to find one good action, but whole sequences of actions that will lead us to victory. This is quite difficult. So how do we do that? To accomplish this, this work not only uses a reinforcement learning variant, but also adds something called self-regulated learning to it, where we don\u2019t present the AI with a fixed curriculum, but we put the learner in charge of its own learning. This also means that it is able to take a big, complex goal and subdivide it into new, smaller goals. In this case, the big goal is following the trajectory with some more additional constraints, which, by itself, turned out to be too difficult to learn with these traditional techniques. Instead, the agent realizes, that if it tracks its own progress on a set of separate, but smaller subgoals, such as tracking its own orientation, positions, and rotations against the desired target states separately, it can finally learn to perform these amazing stunts. That sounds great, but how is this done exactly? This is done through a series of three steps, where step one is generation, where the learner creates a few alternative solutions for itself and proceeds to the second step, evaluation, where it has to judge these individual alternatives and find the best ones. And third, learning, which means looking back and recording whether these judgments indeed put the learner in a better position. By iterating these three steps, this virtual dragon learned to fly properly. Isn\u2019t this amazing? I mentioned earlier that this kind of problem formulation is intractable without self-regulated learning, and you can see here how a previous work fares on following these trajectories. There is indeed a world of a difference between the two. So there you go, in case you enter a virtual world where you need to train your own dragon, you\u2019ll know what to do. But just in case, also read the paper in the video description! If you enjoyed this episode and you wish to watch our other videos in early access, or get your name immortalized in the video description, please consider supporting us on Patreon through Patreon.com/TwoMinutePapers. The link is available in the video description, and this way, we can make better videos for you. We also support cryptocurrencies, the addresses are also available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Wxb0jN0X7cs",
        "paper_link": "http://mrl.snu.ac.kr/research/ProjectAerobatics/Aerobatics.htm",
        "paper_title": "Aerobatics Control of Flying Creatures\nvia Self-Regulated Learning"
    },
    {
        "video_id": "mGHKFMXdjKU",
        "video_title": "DeepMind's AI Learned a Better Understanding of 3D Scenes",
        "position_in_playlist": 337,
        "description": "Backblaze:\nhttps://www.backblaze.com/cloud-backup.html#af9tk4\n\n\ud83d\udcdd The paper \"MONet: Unsupervised Scene Decomposition and Representation\" is available here:\nhttps://arxiv.org/abs/1901.11390\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nBackground image credit: https://pixabay.com/hu/photos/vil\u00e1g%C3%ADt\u00f3torony-magyarorsz\u00e1g-2542726/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper was written by scientists at DeepMind and it is about teaching an AI to look at a 3D scene and decompose it into its individual elements in a meaningful manner. This is typically one of those tasks that is easy to do for humans, and is immensely difficult for machines. As this decomposition thing still sounds a little nebulous, let me explain what it means. Here you see an example scene, and a segmentation of this scene that the AI came up with, which shows what it thinks where the boundaries of these individual objects are. However, we are not stopping there, because it is also able to \u201crip out\u201d these objects from the scene one by one. So why is this such a big deal? Well, because of three things. One, it is a generative model, meaning that it is able to reorganize these scenes and create new content that actually makes sense. Two, it can prove that it truly has an understanding of 3D scenes by demonstrating that it can deal with occlusions. For instance, if we ask it to \u201crip out\u201d the blue cylinder from this scene, it is able to reconstruct parts of it that weren\u2019t even visible in the original scene. Same with the blue sphere here. Amazing, isn\u2019t it? And three, this one is a bombshell - it is an unsupervised learning technique. Now, our more seasoned Fellow Scholars fell out of the chair hearing this, but just in case, this means that this algorithm is able to learn on its own and we have to feed it a ton of training data, but this training data is not labeled. In other words, it just looks at the videos with no additional information, and from watching all this content, it finds out on its own about the concept of these individual objects. The main motivation to create such an algorithm was to have an AI look at some gameplay of the Starcraft 2 strategy game and be able to recognize all individual units and the background without any additional supervision. I really hope this also means that DeepMind is working on a version of their StarCraft 2 AI that is able to learn more similarly to how a human does, which is, looking at the pixels of the game. If you look at the details, this will seem almost unfathomably difficult, but would, of course, make me unreasonably happy. What a time to be alive! If you check out the paper in the video description, you will find how all this is possible through a creative combination of an attention network and a variational autoencoder. This episode has been supported by Backblaze. Backblaze is an unlimited online backup solution for only 6 dollars a month, and I have been using it for years to make sure my personal data, family pictures and the materials required to create this series are safe. You can try it free of charge for 15 days, and if you don\u2019t like it, you can immediately cancel it without losing anything. Make sure to sign up for Backblaze today through the link in the video description, and this way, you not only keep your personal data safe, but you also help supporting this series. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mGHKFMXdjKU",
        "paper_link": "https://arxiv.org/abs/1901.11390",
        "paper_title": "MONet: Unsupervised Scene Decomposition and Representation"
    },
    {
        "video_id": "dd1kN_myNDs",
        "video_title": "AI Learns Tracking People In Videos",
        "position_in_playlist": 338,
        "description": "\ud83d\udcdd The paper \"Learning Correspondence from the Cycle-Consistency of Time\" is available here:\nhttps://arxiv.org/abs/1903.07593\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. There are many AI techniques that are able to look at a still image, and identify objects, textures, human poses and object parts in them really well. However, in the age of the internet, we have videos everywhere, so an important question would be how we could do the same for these animations. One of the key ideas in this paper is that the frames of these videos are not completely independent, and they share a lot of information, so after we make our initial predictions on what is where exactly, these predictions from the previous frame can almost always be reused with a little modification. Not only that, but here you can see with these results that it can also deal with momentary occlusions and is ready to track objects that rotate over time. A key part of this method is that one, it looks back and forth in these videos to update these labels, and second, it learns in a self-supervised manner, which means that all it is given is just a little more than data, and was never given a nice dataset with explicit labels of these regions and object parts that it could learn from. You can see in this comparison table that this is not the only method that works for videos, the paper contains ample comparisons against other methods and comes out ahead of all other unsupervised methods, and on this task, it can even get quite close to supervised methods. The supervised methods are the ones that have access to these cushy labeled datasets and therefore should come out way ahead. But they don\u2019t, which sounds like witchcraft, considering that this technique is learning on its own. However, all this greatness comes with limitations. One of the bigger ones is that even though it does extremely well, it also plateaus, meaning that we don\u2019t see a great deal of improvement if we add more training data. Now whether this is because it is doing nearly as well as it is humanly, or computerly possible, or because a more general problem formulation is still possible remains a question. I hope we find out soon. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dd1kN_myNDs",
        "paper_link": "https://arxiv.org/abs/1903.07593",
        "paper_title": "Learning Correspondence from the Cycle-Consistency of Time"
    },
    {
        "video_id": "e_9f5Z0sMYE",
        "video_title": "Simulating Grains of Sand, Now 6 Times Faster",
        "position_in_playlist": 339,
        "description": "\ud83d\udcdd The paper \"Hybrid Grains: Adaptive Coupling of Discrete and Continuum Simulations of Granular Media\" is available here:\nhttp://www.cs.columbia.edu/~smith/hybrid_grains/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Good news! Another fluid paper is coming up today, and this one is about simulating granular materials. Most techniques that can simulate these grains can be classified as either discrete or continuum methods. Discrete methods, as the name implies, simulate all of these particles one by one. As a result, the amount of detail we can get in our simulations is unmatched, however, you probably are immediately asking the question: doesn\u2019t simulating every single grain of sand take forever? Oh yes, yes it does. Indeed, the price to be paid for all this amazing detail comes in the form of a large computation time. To work around this limitation, continuum methods were invented, which do the exact opposite by simulating all of these particles as one block where most of the individual particles within the block behave in a similar manner. This makes the computation times a lot frendlier, however, since we are not simulating these grains individually, we lose out on a lot of interesting effects, such as clogging, bouncing and ballistic motions. So, in short, a discrete method gives us a proper simulation, but takes forever, while the continuum methods are approximate in nature, but execute quicker. And now, from this exposition, the question naturally arises: can we produce a hybrid method that fuses together the advantages of both of these methods? This amazing paper proposes a technique to perform that by subdividing the simulation domain into an inside regime where the continuum methods work well, and an outside regime where we need to simulate every grain of sand individually with a discrete method. But that's not all, because the tricky part comes in the form of the reconciliation zone, where a partially discrete and partially continuum simulation has to take place. The way to properly simulate this transition zone between these two regimes takes quite a bit of research effort to get right, and just think about the fact that we have to track and change these domains over time, because, of course, the inside and outside of a block of particles changes rapidly over time. Throughout the video, you will see the continuum zones denoted with red, and the discrete zones with blue, which are typically on the outside regions. The ratio of these zones gives us an idea of how much speedup we could get compared to a purely discrete stimulation. In most cases, it means that 88% fewer discrete particles need to be simulated and this can lead to a total speedup of 6 to 7 times over that simulation. Basically, at least 6 all nighter simulations running now in one night? I\u2019m in. Sign me up. Also make sure to have a look at the paper because the level of execution of this work is just something else. Check it out in the video description. Beautiful work. My goodness. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=e_9f5Z0sMYE",
        "paper_link": "http://www.cs.columbia.edu/~smith/hybrid_grains/",
        "paper_title": "Hybrid Grains: Adaptive Coupling of Discrete and Continuum Simulations of Granular Media"
    },
    {
        "video_id": "tfb6aEUMC04",
        "video_title": "OpenAI Five Beats World Champion DOTA2 Team 2-0! \ud83e\udd16",
        "position_in_playlist": 340,
        "description": "Check out Lambda Labs here: https://lambdalabs.com/papers\n\nOpenAI's blog post: https://openai.com/blog/openai-five-finals/\nReddit AMA: https://old.reddit.com/r/DotA2/comments/bf49yk/hello_were_the_dev_team_behind_openai_five_we/\nReddit discussion on buybacks: https://old.reddit.com/r/DotA2/comments/bcx8cf/i_think_the_openai_games_revealed_an_invisible/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#OpenAIFive, #DOTA2",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This episode has been sponsored by Lambda Labs. Not so long ago, we talked about DeepMind\u2019s AlphaStar, an AI that was able to defeat top-tier human players in Starcraft 2, a complex real-time strategy game. Of course, I love talking about AI\u2019s that are developed to challenge pro gamers at a variety of difficult games, so this time around, we\u2019ll have a look another major milestone, OpenAI Five, which is an AI that plays DOTA2, a multiplayer online battle arena game with a huge cult following. As this game requires long-term strategic planning, it is a classic nightmare scenario for any AI. But OpenAI is no stranger to DOTA2, in 2017 they showed us an initial version of their AI that was able to play 1 versus 1 games with only one hero and was able to reliably beat Dendi, a world champion player. That was quite an achievement, however, of course, this was meant to be a stepping stone towards playing the real DOTA2. Then, in 2018, they unveiled OpenAI Five, an improved version of this AI that played 5 versus 5 games with a limited hero pool. This team was able to defeat competent players, but was still not quite at the level of a world champion human team. In a one-hour interview, the OpenAI research team mentioned that due to the deadline of The International event, they had to make quite a few concessions. And this time, several things have changed. First, they didn\u2019t just challenge some local team of formidable players, no-no, they flat out challenged OG, the reigning world champion team. An ambitious move, that exudes confidence from their side. Second, this time around, there was no tight deadline as the date of the challenge was chosen by OpenAI. Let\u2019s quickly talk about the rules of the competition and then \u2026see if OpenAI\u2019s confident move was justified! These learning agents don\u2019t look at the pixels of the game, and as a result, they see the world as a big bunch of numbers. And this time around, it was able to play a pool of 17 heroes, and trained against itself for millions and millions of games. And now, let\u2019s have a look at what happened in this best of 3 series! In match 1, right after picking the roster of heroes, the AI estimated its win probability to be 67%, so it was quite a surprise that early on it looked like OpenAI\u2019s bots were running around aimlessly. Over time, we found out that it was not at all the case - it plays unusually aggressively from the get-go and uses buybacks quite liberally at times when human players don\u2019t really consider it to be a good choice. These buybacks resurrect a perished hero quickly but in return, cost money. Later, it became clearer that these bots are no joke: they know exactly when to engage and when to back out from an engagement with the smallest sliver of health left. I will show quite a few examples of those to you during this video, so stay tuned. A little less than 20 minutes in, we had a very even game 1, if anything, OpenAI seemed a tiny bit behind, and someone noted that we should perhaps ask the bots what they think about their chances. And then the AI said, yeah, no worries, we have a higher than 95% chance to win the game. This was such a pivotal moment that was very surprising for everyone. Of course, if you you call out a win with confidence, you better go all the way and indeed win the game. Right? Right. And sure enough, they wiped out almost the entire world champion team of the human players immediately after. And then noted, you know what, remember that we just said? Forget that. We estimate our chances to win to be above 99% now. And shortly after, they won match number one. Can you believe this? This is absolutely amazing. Interestingly, one of the developers said that the AI is great at assessing whether a fight is worth it. As an interesting corollary, if you engage with it and it fights you, it probably means you are going to lose. That must be quite confusing for the players. Some mind games for you. Love it. At the event, it was also such a joy to see such a receptive audience that understood and appreciated high-level plays. Onwards to match number two. Right after the draft, which is the process of choosing the heroes for each team, the AI predicted a win percentage that was much closer this time around, around 60%. In this game, the AI turned up the heat real fast, and said just 5 minutes into the game, which is nothing, that it has an over 80% chance to win this game. And now, watch this. Early in this game, you can see a great example of where the AI just gets away with a sliver of health. Look at this guy. Look at that! This is either an accident or some unreal-level foresight from the side of this agent. I\u2019d love to hear your opinion on which one you think it is. By the 9 and a half minute mark, which is still really early, OpenAI Five said yes, we got this one too. Over 95%. Here you see in interesting scenario where the AI loses one hero, but it almost immediately kills two of the human heroes, and comes out favorably, at which point we wonder whether this was a deliberate bait it pulled on the humans. By the 15-minute mark, the human players lost a barracks and were heavily underfunded and outplayed with seemingly no way to come back. And sure enough, by the 21-minute mark, the game was over. There is no other way to say it, this second game was a one-sided beatdown. Game 1 was a strategic back and forth where OpenAI Five waited for the right moment to win the game in a big team fight, where here, they pressured human team from the get-go and never let them reach the endgame where they might have and advantage with their picks. Also, have a look at this. Unreal. The final result is 2 to 0 for OpenAI. In the post match interview, N0tail, one of the human players noted that he is confident that from 5 games, they would take at least one, and after 15 games, they would start winning reliably. Very reminiscent of what we have heard from players playing against DeepMind\u2019s AI in StarCraft 2 and I hope this will be tested. However, in the end, he agreed that it is inevitable that this AI will become unbeatable at some point. It was also noted that in 5v5 fights, they seem better in planning than any human team is and there is quite a lot to learn from the AI for us humans. They were also trying to guess the reasoning for all of these early buybacks. According to the players, initially, they flat out seemed like misplays. Perhaps the reason for these instant and not really great buybacks might have been the fact that the AI knows that if the game goes on for much longer, statistically, their chances with their given composition to win the game dwindles, so it needs to immediately go and win right now, whatever the cost. And again, an important lesson is that in this project, OpenAI is not spending so much money and resources to just play video games. DOTA2 is a wonderful testbed to see how their AI compares to humans at complex tasks that involve strategy and teamwork. However, the ultimate goal is to reuse parts of this system for other complex problems outside of video games. For example, the algorithm that you\u2019ve seen here today can also do this. But wait, there\u2019s more. Players after these showmatches always tend to get these messages from others on Twitter telling them what they did wrong and what they should have done instead. Well, luckily, these people were able to show their prowess as OpenAI gave the chance for anyone in the world to challenge the OpenAI Five competitively and play against them online. This way, not only team OG, but everyone can get crushed by the AI. How cool is that? This Arena event has concluded with over 15000 games played where OpenAI Five had a 99.4% winrate. There are still ways to beat it, but given the rate of progress of this project, likely not for long. Insanity. As always, if you are interested in more details, I put a link to a reddit AMA in the video description, and I also can\u2019t wait to pick the algorithm apart for you, but for now, we\u2019ll have to wait for the full paper to appear. And note that what happened here is not to be underestimated. Huge respect to the OpenAI team, to OG for the amazing games and congratulations to the humans who were able to beat these beastly bots online. So there you go, another long video that\u2019s not two minutes, and it\u2019s not about a paper. Yet. Welcome to Two Minute Papers! If you\u2019re doing deep learning, make sure to look into Lambda GPU systems. Lambda offers workstations, servers, laptops, and a GPU cloud for deep learning. You can save up to 90% over AWS, GCP, and Azure GPU instances. Every Lambda GPU system is pre-installed with TensorFlow, PyTorch, and Keras. Just plug it in and start training. Lambda customers include Apple, Microsoft, and Stanford. Go to lambdalabs.com/papers or click the link in the description to learn more. Big thanks to Lambda for supporting Two Minute Papers and helping us make better videos. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=tfb6aEUMC04"
    },
    {
        "video_id": "JJlSgm9OByM",
        "video_title": "This Robot Throws Objects with Amazing Precision",
        "position_in_playlist": 341,
        "description": "\ud83d\udcdd The paper \"TossingBot: Learning to Throw Arbitrary Objects with Residual Physics\" is available here:\nhttps://tossingbot.cs.princeton.edu/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#TossingBot",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this footage, we have a variety of objects that differ in geometry, and the goal is to place them into this box using an AI. Sounds simple, right? This has been solved long, long ago. However, there is a catch here, which is that this box is outside of the range of the robot arm, therefore, it has to throw it in there with just the right amount of force for it to end up in this box. It can perform 500 of these tosses per hour. Before anyone misunderstands what is going on in the footage here, it almost seems like the robot on the left is helping by moving to where the object would fall after the robot on the right throws it. This is not the case. Here you see a small part of my discussion with Andy Zeng, the lead author of the paper where he addresses this. The results look amazing, and note that this problem is much harder than most people would think at first. In order to perform this, the AI has to understand how to grasp an object with a given geometry, in fact, we may grab the same object at a different side, throw it the same way, and there would be a great deal of a difference in the trajectory of this object. Have a look at this example with the screwdriver. It also has to take into consideration the air resistance of a given object as well. Man, this problem is hard. As you see here, initially, it cannot even practice throwing because its reliability in grasping is quite poor. However, after 14 hours of training, it achieves a remarkable accuracy, and to be able to train for so long, this training table is designed in a way that when running out of objects, it can restart itself without human help. Nice! To achieve this, we need a lot of training objects, but not any kind of training objects. These objects have to be diversified. As you see here, during training, the box position enjoys a great variety, and the object geometry is also well diversified. Normally, in these experiments, we are looking to obtain some kind of intelligence. Intelligence in this case would mean that the AI truly learned the underlying dynamics of object throwing, and not just found some good solutions via trial and error. A good way to test this would be to give it an object it has never seen before and see how its knowledge generalizes to that. Same with locations. On the left, you see these boxes marked with orange, this was the training set, but, later, it was asked to throw it into the blue boxes, which is something it has never tried before\u2026and\u2026look! This is excellent generalization. Bravo! You can also see the success probabilities for grasping and throwing here. A key idea in this work is that this system is endowed with a physics-based controller, which contains the standard equations of linear projectile motion. This is simple knowledge from high-school physics that ignores several key real-life factors, such as the effect of aerodynamic drag. This way, the AI does not have to learn from scratch and can use these calculations as an initial guess, and it is tasked with learning to account for the difference between this basic equation and real-life trajectories. In other words, it is given basic physics and is asked to learn advanced physics by building on that. Loving this idea. A simulation environment was also developed for this project where one can test the effect of, for instance, changing the gripper width, which would be costly and labor-intensive in the real world. Of course, these are all free in a software simulation. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=JJlSgm9OByM",
        "paper_link": "https://tossingbot.cs.princeton.edu/",
        "paper_title": "TossingBot: Learning to Throw Arbitrary Objects with Residual Physics"
    },
    {
        "video_id": "C6nonNRoF7g",
        "video_title": "This is How Google\u2019s Phone Enhances Your Photos",
        "position_in_playlist": 342,
        "description": "\ud83d\udcdd The paper \"Handheld Multi-frame Super-resolution\" is available here:\nhttps://sites.google.com/view/handheld-super-res/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#NightSight #GooglePixel",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Super resolution is a research field with a ton of published papers every year, where the simplest problem formulation is that we have a low-resolution, coarse image as an input and we wish to enhance it to get a crisper, higher resolution image. You know, the thing that can always be done immediately and perfectly in many of these detective TV series. And yes, sure, the whole idea of super resolution sounds a little like\u2026science fiction. How could I possibly get more content onto an image that\u2019s not already there? How would an algorithm know what a blurry text means if it\u2019s unreadable? It can\u2019t just guess what somebody wrote there, can it? Well, let\u2019s see. This paper provides an interesting take on this topic, because it rejects the idea of having just one image as an input. You see, in this day and age, we have powerful mobile processors in our phones, and when we point our phone camera and take an image, it doesn\u2019t just take one, but a series of images. Most people don\u2019t know that some of these images are even taken as soon as we open our camera app without even pushing the shoot button. Working with a batch of images is also the basis of the iPhone\u2019s beloved live photo feature. So as a result, this method builds on this raw burst input with multiple images, and doesn\u2019t need idealized conditions to work properly, which means that it can process footage that we shoot with our shaky hands. In fact, it forges an advantage out of this imperfection, because it can first, align these photos, and then, we have not one image, but a bunch of images with slight changes in viewpoint. This means that we have more information that we can extract from these several images, which can be stitched together into one, higher-quality output image. Now that\u2019s an amazing idea if I\u2019ve ever seen one - it not only acknowledges the limitations of real-world usage, but even takes advantage of it. Brilliant. You see throughout this video that the results look heavenly. However, not every kind of motion is desirable. If we have a more complex motion, such as the one you see here as we move away from the scene, this can lead to unwanted artifacts in the reconstruction. Luckily, the method is able to detect these cases by building a robustness mask that highlights which are the regions that will likely lead to these unwanted artifacts. Whatever is deemed to be low-quality information in this mask is ultimately rejected, leading to high-quality outputs even in the presence of weird motions. And now, hold on to your papers, because this method does not use neural networks or any learning techniques, and is orders of magnitude faster than those while providing higher-quality images. As a result, the entirety of the process takes only a 100 milliseconds to process a really detailed, 12 megapixel image, which means that it can do it 10 times every second. These are interactive framerates, and it seems that doing this in real time is going to be possible within the near future. Huge congratulations to Bart and his team at Google for outmuscling the neural networks. Luckily, higher-quality ground truth data can also be easily produced for this project, creating a nice baseline to compare the results to. Here you see that this new method is much closer to this ground truth than previous techniques. As an additional corollary of this solution, the more of these jerky frames we can collect, the better it can reconstruct images in poor lighting conditions, which is typically one of the more desirable features in today\u2019s smartphones. In fact, get this, this is the method behind Google\u2019s magical Night Sight and Super-Res Zoom features that you can access by using their Pixel 3 flagship phones. When this feature first came out, I remember that phone reviewers and everyone unaware of the rate of progress in computer graphics research were absolutely floored by the results and could hardly believe their eyes when they first tried it. And I don\u2019t blame them, this is a truly incredible piece of work. Make sure to have a look at the paper that contains a ton of comparisons against other methods, and it also shows the relation between the number of collected burst frames and the output quality we can expect as a result, and more! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=C6nonNRoF7g",
        "paper_link": "https://sites.google.com/view/handheld-super-res/",
        "paper_title": "Handheld Multi-frame Super-resolution"
    },
    {
        "video_id": "QPwhEnAILa0",
        "video_title": "Should AI Research Try to Model the Human Brain?",
        "position_in_playlist": 343,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\u20bf Crypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 1a5ttKiVQiDcr9j8JT2DoHGzLG7XTJccX\n\u203a Bitcoin Cash: qzy42yt06xqr5f83khnhcxm2mf53cllvtytyp6ndw5 \n\u203a Ethereum: 0xbBD767C0e14be1886c6610bf3F592A91D866d380\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\n\ud83d\udcdd The paper \"Reinforcement Learning, Fast and Slow\" is available here:\nhttps://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0\n\nThe Bitter Lesson: https://www.youtube.com/watch?v=wEgq6sT1uq8\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. AI research has come a long-long way in the last few years. I remember that not so long ago, we were lucky if we could train a neural network to understand traffic signs, and since then, so many things happened: by harnessing the power of learning algorithms, we are now able to impersonate other people by using a consumer camera, generate high-quality virtual human faces for people that don\u2019t exist, or pretend to be able to dance as a pro dancer by using an external video footage and transferring it onto ourselves. Even though we are progressing at a staggering pace, there is a lot of debate as to which research direction is the most promising going forward. Roughly speaking, there are two schools of thought. One, we recently talked about Richard Sutton\u2019s amazing article by the name, \u201cThe Bitter Lesson\u201d, in which he makes a great argument that AI research should not try to mimic the way the human brain works - he argues that instead, all we need to do is formulate our problems in a general manner, so that our learning algorithms may find something that is potentially much better suited for a problem than our brain is. I put a link to this video in the description if you\u2019re interested. And two, a different school of thought says that we should a good look at all these learning algorithms that use a lot of powerful hardware and can do wondrous things, like playing a bunch of Atari games at a superhuman level. Note that they learn orders of magnitude slower than the human brain does, so it should definitely be worth it to try to study and model the human brain, at least until we can match it in terms of efficiency. This school of thought is what we are going to talk about in this video. As an example, let\u2019s take a look at deep reinforcement learning in the context of playing computer games. This technique is a combination of a neural network that processes the visual data that we see on the screen, and a reinforcement learner that comes up with the gameplay-related decisions. Absolutely amazing algorithm, a true breakthrough in AI research. Very powerful, however, also quite slow. And by slow, I mean that we can sit for an hour in front of our computer and wonder why our learner does not work at all, because it loses all of its lives almost immediately. If we remain patient, we find out that it works, it just learns at a glacial pace. So, why is this so slow? Well, two reasons. Reason number one is that the learning happens through incremental parameter adjustment. What does that mean? If a human fails really badly at a task, the human would know that a drastic adjustment to the strategy is necessary, while the deep reinforcement learner would start applying tiny, tiny changes to its behavior and test again if things got better. This takes a while, and as a result, this seems unlikely to have a close relation to how we, humans think. The second reason for it being slow is the presence of weak inductive bias. This means that the learner does not contain any information about the problem we have at hand, or in other words, has never seen the game we\u2019re playing before and has no other previous knowledge about games at all. This is desirable in some cases, because we can reuse one learning algorithm for a variety of problems. However, because this way, the AI has to test a stupendously large number of potential hypotheses about the game, we will have to pay for this convenience by a mighty inefficient algorithm. But is this really all true? Does deep reinforcement learning really have to be so slow? And what on earth does this have to do with our brain? Well, this paper proposes an interesting counterargument that this is not necessarily true and argues that with two well thought out changes, the efficiency of deep reinforcement learning may be drastically improved, and get this, it also tells us that these changes are also possibly based in neuroscience. So what are the two changes? One is using episodic memory, which stores previous experiences to help estimating the potential value of different actions, and this way, drastic parameter adjustments become a possibility. And it not only improves the efficiency, but there is more to it, because there are recent studies that show that using episodic memory indeed contributes to the learning of real humans and animals alike. And two, it is beneficial to let the AI implement its own reinforcement learning algorithm, a concept often referred to as \u201clearning to learn\u201d or meta reinforcement learning. This also helps obtaining more general knowledge that can be reused across tasks, further improving the efficiency of the agent. Here you see a picture of an fMRI, and some regions are marked with yellow and orange here. What could these possibly mean? Well, hold on to your papers, because these highlight neural structures that implement a very similar meta reinforcement learning scheme within the human brain. It turns out that meta reinforcement learning, or this \u201clearning to learn\u201d scheme may not just be something that speeds up our AI algorithms, but may be a fundamental principle of the human brain as well. So these two changes to deep reinforcement learning not only drastically improve its efficiency, but it also suddenly maps quite a bit better to our brain. How cool is that? So, which school of thought are you most fond of? Should we model the brain, or should we listen to Richard Sutton\u2019s Bitter Lesson? Let me know in the comments. Also, make sure to have a look at the paper, I found it to be quite readable, and you really don\u2019t need to be a neuroscientist to enjoy it and learn quite a few new things. Make sure to have a look at it in the video description! Now, I think you noticed that this paper doesn\u2019t contain the usual visual fireworks, and is more complex than your average Two Minute Papers video, and hence, I expect it to get significantly fewer views. That\u2019s not a great business model, but no matter, I made this channel so I can share with you all these important lessons that I learned during my journey. This has been a true privilege and I am thrilled that I am still able to talk about all these amazing papers without worrying too much whether any of these videos will go viral or not. This has only been possible because of your unwavering support on Patreon.com/TwoMinutePapers. If you feel like chipping in, just click the Patreon link in the video description. If you are more like a crypto person, we also support cryptocurrencies like Bitcoin, Ethereum and Litecoin, the addresses are also available in the description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=QPwhEnAILa0",
        "paper_link": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0",
        "paper_title": "Reinforcement Learning, Fast and Slow"
    },
    {
        "video_id": "SfvRhqsmU4o",
        "video_title": "NVIDIA\u2019s AI Transformed My Chihuahua Into a Lion",
        "position_in_playlist": 344,
        "description": "Check out Lambda Labs here: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Few-Shot Unsupervised Image-to-Image Translation\" and its demo is available here:\nhttps://nvlabs.github.io/FUNIT/\nhttps://nvlabs.github.io/FUNIT/petswap.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#NVIDIA #Funit",
        "transcript": "This episode has been supported by Lambda Labs. Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Let\u2019s talk about a great recent development in image translation! Image translation means that some image goes in, and it is translated into an analogous image of a different class. A good example of this would be when we have a standing tiger as an input, and we ask the algorithm to translate this image into the same tiger lying down. This leads to many amazing applications, for instance, we can specify a daytime image and get the same scene during nighttime. We can go from maps to satellite images, from video games to reality and more. However, much like many learning algorithms today, most of these techniques have a key limitation - they need a lot of training data, or in other words, these neural networks require seeing a ton of images in all of these classes before they can learn to meaningfully translate between them. This is clearly inferior to how humans think, right? If I would show you a horse, you could easily imagine, and some of you could even draw what it would look like if it were a zebra instead. As I am sure you have noticed by reading arguments on many internet forums, humans are pretty good at generalization. So, how could we possibly develop a learning technique that can look at very few images, and obtain knowledge from them that generalizes well? Have a look at this crazy new paper from scientists at NVIDIA that accomplishes exactly that. In this example, they show an input image of a golden retriever, and then, we specify the target classes by showing them a bunch of different animal breeds, and\u2026look! In goes your golden, and out comes a pug or any other dog breed you can think of.. And now, hold on to your papers, because this AI doesn\u2019t have access to these target images and it had only seen them the very first time as we just gave it to them. It can do this translation with previously unseen object classes. How is this insanity even possible? This work contains a generative adversarial network, which assumes that the training set we give it contains images of different animals, and what it does during training is practicing the translation process between these animals. It also contains a class encoder that creates a low-dimensional latent space for each of these classes, which means that it tries compress these images down to a few features that contain the essence of these individual dog breeds. Apparently it can learn the essence of these classes really well because it was able to convert our image into a pug without ever seeing a pug other than this one target image. As you can see here, it comes out way ahead of previous techniques, but of course, if we give it a target image that is dramatically different than anything the AI has seen before, it may falter. Luckily, you can even try it yourself through this web demo which works on pets, so make sure to read the instructions carefully, and, let the experiments begin! In fact, due to popular request, let me kick this off with Lisa, my favorite chihuahua. I got many tempting alternatives, but worry not, in reality, she will stay as is. I was also curious about trying a non-traditional head position, and as you see with the results, this was a much more challenging case for the AI. The paper also discusses this limitation in more detail. You know the saying, two more papers down the line, and I am sure this will also be remedied. I am hoping that you will also try your own pets and as a Fellow Scholar, you will flood the comments section here with your findings. Strictly for science, of course. If you\u2019re doing deep learning, make sure to look into Lambda GPU systems. Lambda offers workstations, servers, laptops, and a GPU cloud for deep learning. You can save up to 90% over AWS, GCP, and Azure GPU instances. Every Lambda GPU system is pre-installed with TensorFlow, PyTorch, and Keras. Just plug it in and start training. Lambda customers include Apple, Microsoft, and Stanford. Go to lambdalabs.com/papers or click the link in the description to learn more. Big thanks to Lambda for supporting Two Minute Papers and helping us make better videos. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SfvRhqsmU4o",
        "paper_link": "https://nvlabs.github.io/FUNIT/",
        "paper_title": "Few-Shot Unsupervised Image-to-Image Translation"
    },
    {
        "video_id": "f9z1I_81_Q4",
        "video_title": "DeepMind Made a Math Test For Neural Networks",
        "position_in_playlist": 345,
        "description": "\ud83d\udcdd The paper \"Analysing Mathematical Reasoning Abilities of Neural Models\" is available here:\nhttps://arxiv.org/abs/1904.01557\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper from DeepMind is about taking a bunch of learning algorithms and torturing them with millions of classic math questions to find out if they can solve them. Sounds great, right? I wonder what kind of math questions would an AI find easy to solve? What percentage of these can a good learning algorithm answer today? Worry not, we\u2019ll discuss some of the results at the end of this video. These kinds of problems are typically solved by recurrent neural networks that are able to read and produce sequences of data, and to even begin to understand what the question is here, an AI would have to understand the concept of functions, variables, arithmetic operators, and of course, the words that form the question itself. It has to learn planning and precedence, that is, in what order do we evaluate such an expression, and it has to have some sort of memory, in which it can store the intermediate results. The main goal of this paper is to describe a dataset that is designed in a very specific way to be able to benchmark the mathematical reasoning abilities of an AI. So how do we do that? First, it is made in way that it\u2019s very difficult to solve for someone without generalized knowledge. Imagine the kind of student at school who memorized everything from the textbooks, but has no understanding of the underlying tasks, and if the teacher changes just one number in a question, the student is unable to solve the problem. We all met that kind of student, right? Well, this test is designed in a way that students like these should fail at it. Of course, in our case, the student is the AI. Second, the questions should be modular. This is a huge advantage because a large number of these questions can be generated procedurally by adding a different combination of subtasks, such as additions, function evaluations, and more. An additional advantage of this is that we can easily control the difficulty of these questions - the more modules we use, typically, the more difficult the question gets. Third, the questions and answers should be able to come in any form. This is an advantage, because the AI has to not only understand the mathematical expressions, but also focus on what exactly we wish to know about them. This also means that the question itself can be about factorization, where the answer is expected to be either true or false. And the algorithm is not told that we are looking for a true or false answer, it has to be able to infer this from the question itself. And to be able to tackle all this properly, with this paper, the authors released 2 million of these questions for training an AI free of charge to foster more future research in this direction. I wonder what percentage of these can a good learning algorithm answer today? Let\u2019s have a look at some results! A neural network model that goes by the name Transformer network produced the best results by being able to answer 50% of the questions. This you find in the extrapolation column here. When you look at the interpolation column, you see that it successfully answered 76% of the questions. So which one is it, 50% of 76%? Actually, both. The difference is that interpolation means that the numbers in these questions were within the bounds that was seen in the training data, where extrapolation means that some of these numbers are potentially much larger or smaller than others that the AI has seen in the training examples. I would say that given the difficulty of just even understanding what these questions are, these are really great results. Generally, in the future, we will be looking for algorithms that do well on the extrapolation tasks, because these are the AIs that have knowledge that generalizes well. So, which tasks were easy and which were difficult? Interestingly, the AI has had similar difficulties as we, fellow humans have, namely, rounding decimals and integers, comparisons, basic algebra was quite easy for it, whereas detecting primality and factorization were not very accurate. I will keep an eye out on improvements in this area, if you are interested to hear more about it, make sure to subscribe to this series. And if you just pushed the red button, you may think you are subscribed, but you are not. You are just kind of subscribed. Make sure to also click the bell icon to not miss these future episodes. Also, please make sure to read the paper, it is quite readable and contains a lot more really cool insights about this dataset and the experiments. As always, the link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=f9z1I_81_Q4",
        "paper_link": "https://arxiv.org/abs/1904.01557",
        "paper_title": "Analysing Mathematical Reasoning Abilities of Neural Models"
    },
    {
        "video_id": "goD36hVVl7M",
        "video_title": "How Can This Liquid Climb?",
        "position_in_playlist": 346,
        "description": "\ud83d\udcdd The paper \"On the Accurate Large-scale Simulation of Ferrofluids\" is available here:\nhttp://computationalsciences.org/publications/huang-2019-ferrofluids.html\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Ferrofluids",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. You\u2019re in for a real treat today, because today we\u2019re not going to simulate just plain regular fluids. No! We\u2019re going to simulate ferrofluids! These are fluids that have magnetic properties and respond to an external magnetic field and you will see in a moment that they are able to even climb things. You see in the reference footage here that this also means if there is no magnetic field, we have a regular fluid simulation. Nothing too crazy here. In this real-world footage, we have a tray of ferrofluid up in the air, and we have a magnet below it, so as the tray descends down and gets closer to the magnet, this happens. But the strength of the magnetic field is not the only factor that a simulation needs to take into account. Here\u2019s another real experiment that shows that the orientation of the magnet also makes a great deal of difference to the distortions of the fluid surface. And now, let\u2019s have a look at some simulations! This simulation reproduces the rotating magnet experiment that you\u2019ve seen a second ago. It works great, what is even more, if we are in a simulation, we can finally do things that would be either expensive, or impossible in the real life, so let\u2019s do exactly that! You see a steel sphere attracting the ferrofluid here, and, now, the strength of the magnet within is decreased, giving us the impression that we can bend this fluid to our will! How cool is that? In the simulation, we can also experiment with arbitrarily-shaped magnets. And here is the legendary real experiment where with magnetism, we can make a ferrofluid climb up on this steel helix. Look at that. When I\u2019ve first seen this video and started reading the paper, I was just giggling like a little girl. So good. Just imagine how hard it is to do something where have footage from the real world that keeps judging our simulation results, and we are only done when there is a near-exact match, such as the one you see here. Huge congratulations to the authors. You see here how the simulation output depends on the number of iterations. More iterations means that we redo the calculations over and over again, and get results closer to what would happen in real life at the cost of more computation time. However, as you see, we can get close to the real solution with even 1 iteration, which is remarkable. In my own fluid simulation experiments, when I tried to solve the pressure field, using 1 to 4 iterations gave me a result that\u2019s not only inaccurate, but singular, which blows up the simulation. Look at this. On this axis, you can see how the fluid disturbances get more pronounced as a response to a stronger magnetic field. And in this direction, you see how the effect of surface tension smooths out these shapes. What a visualization! The information density in this example is just out of this world\u2026 and it is still both informative, and beautiful. If only I could tell you how many times I have to remake each of the figures in my papers in pursuit of this\u2026I can only imagine how long it took to finish this one. Bravo. And if all that\u2019s not enough for you to fall out of your chair, get this. It is about Libo Huang, the first author of this paper. I became quite curious about his other works, and have found exactly zero of them. This was his first paper. My goodness. And of course, it takes a team to create such a work, so congratulations to all three authors, this is one heck of a paper. Check it out in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=goD36hVVl7M",
        "paper_link": "http://computationalsciences.org/publications/huang-2019-ferrofluids.html",
        "paper_title": "On the Accurate Large-scale Simulation of Ferrofluids"
    },
    {
        "video_id": "aJq6ygTWdao",
        "video_title": "This AI Makes Amazing DeepFakes\u2026and More",
        "position_in_playlist": 347,
        "description": "Check out Lambda Labs here: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Deferred Neural Rendering: Image Synthesis using Neural Textures\" is available here:\nhttps://niessnerlab.org/projects/thies2019neural.html\n\nMy earlier work on neural rendering in the first part of the video is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Deepfake",
        "transcript": "This episode has been supported by Lambda Labs. Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In our earlier paper, Gaussian Material Synthesis, we made a neural renderer, and what this neural renderer was able to do is reproduce the results of a light transport simulation within 4 to 6 milliseconds in a way that is almost pixel perfect. It took a fixed camera and scene, and we were able to come up with a ton of different materials, and it was always able to guess what the output would look like if we changed the physical properties of a material. This is a perfect setup for material synthesis where these restrictions are not too limiting. Trying to perform high-quality neural rendering has been a really important research problem lately, and everyone is asking the question, can we do more with this? Can we move around with the camera and have a neural network predict what the scene would look like? Can we do this with animations? Well, have a look at this new paper which is a collaboration between researchers at the Technical University of Munich and Stanford University, where all we need is some video footage of a person or object. It takes a close look at this kind of information, and can offer three killer applications. One, it can synthesize the object from new viewpoints. Two, it can create a video of this scene and imagine what it would look like if we reorganized it, or can even add more objects to it. And three, perhaps everyone\u2019s favorite, performing facial reenactment from a source to a target actor. As with many other methods, these neural textures are stored on top of the 3D objects, however, a more detailed, high-dimensional description is also stored and learned by this algorithm, which enables it to have a deeper understanding of intricate light transport effects create these new views. For instance, it is particularly good at reproducing specular highlights, which typically change rapidly as we change our viewpoint for the object. One of the main challenges was building a learning algorithm that can deal with this kind of complexity. The synthesis of mouth movements and teeth was always the achilles heel of these methods, so have a look at how well this one does with it! You can also see with the comparisons here that in general, this new technique smokes the competition. So how much training data do we need to achieve this? I would imagine that this would take hours and hours of video footage, right? No, not at all. This is what the results look like as a function of the amount of training data. On the left, you see that is already kinda works with 125 images, but contains artifacts, but if we can supply a 1000 images, we\u2019re good. Note that a 1000 images sounds like a lot, but it really isn\u2019t, it\u2019s just half a minute worth of video. How crazy is that? Some limitations still apply, you see one failure case right here, and the neural network typically needs to be retrained if we wish to use it on new objects, but this work finally generalizes to multiple viewpoints, animation, scene editing, lots of different materials and geometries, and I can only imagine what we\u2019ll get two more papers down the line. Respect to Justus for accomplishing this, and in general, make sure to have a look at Matthias Niessner\u2019s lab, who just got tenured as a full professor and he\u2019s only 32 years old. Congratulations! If you have AI-related ideas, and you would like to try them, but not do it in the cloud because you wish to own your own hardware, look no further than Lambda Labs. Lambda Labs offers sleek, beautifully designed laptops, workstations and servers that come pre-installed with every major learning framework and updates them for you, taking care of all the dependencies. Look at those beautiful and powerful machines! This way, you can spend more of your time with your ideas, and don\u2019t have to deal with all the software maintenance work. They have an amazing roster of customers that include Apple, Microsoft, Amazon, MIT and more. These folks really know what they are doing. Make sure to go to lambdalabs.com/papers or click their link in the video description and look around, and if you have any questions, you can even even call them for advice. Big thanks to Lambda Labs for supporting this video and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=aJq6ygTWdao",
        "paper_link": "https://niessnerlab.org/projects/thies2019neural.html",
        "paper_title": "Deferred Neural Rendering: Image Synthesis using Neural Textures"
    },
    {
        "video_id": "38ZXwJj6j8k",
        "video_title": "All Hail The Mighty Translatotron!",
        "position_in_playlist": 348,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\nMy talk and the full panel discussion at the NATO conference (I start at around the 12:30 minute mark):\n\u25b6\ufe0f https://www.facebook.com/StratComCOE/videos/698737203889068/\n\n\ud83d\udcdd The paper \"Direct speech-to-speech translation with a sequence-to-sequence model\" and the voice samples are available here:\nhttps://arxiv.org/abs/1904.06037\nhttps://google-research.github.io/lingvo-lab/translatotron/#conversational\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Translatotron",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Scientists at Google just released the Translatotron. This is an AI that is able to translate speech from one language into speech into another language, and here comes the first twist, without using text as an intermediate representation. You give it the soundwaves, and you get the translated soundwaves. And this neural network was trained approximately on a million voice samples. So let\u2019s see what learning on this one million samples gives us. Listen. This is the input sentence in Spanish. And here it is, translated to English, but using the voice of the same person. This is incredible. However, there is another twist, perhaps an even bigger one, believe it or not. This technique can not only translate, but can also perform voice transfer, so it can say the same thing using someone else\u2019s voice. This means that the AI not only has to learn what to say, but how to say it. This is immensely difficult. It\u2019s also not that easy to know what we need to listen to and when, so let me walk you through it. This is a source sentence in Spanish. This is the same sentence said by someone else, an actual person, and in English. And now, the same thing, but synthesized by the algorithm using both of their voices. Let\u2019s listen to them side by side some more. This is so good, let\u2019s have a look at some more examples. Wow. The method performs the learning by trying to map these Mel spectrograms between multiple speakers. You can see example sentences here and their corresponding spectrograms, which are concise representations of someone\u2019s voice and intonation. And of course, it is difficult to mathematically formalize what makes a good translation and a good mimicking of someone\u2019s voice, so in these cases, we let people be the judge and have them listen to a few speech signals and asking them to guess which was a real person, and which was the AI speaking. If you take a closer look at the paper, you will see that it smokes the competition. This is great progress on an immensely difficult task as we have to perform proper translation and voice transfer at the same time. It\u2019s quite a challenge. Of course, failure cases still exist. Listen. Just imagine that you are in a foreign country and all you need to do is use your phone to tell stories to people not only in their own language, but also using your own voice, even if you don\u2019t speak a word of their language. Beautiful. Even this video could perhaps be available in a variety of languages using my own voice within the next few years, although I wonder how these algorithms would pronounce my name. So far, that proved to be quite a challenge for humans and AIs alike. And for now, all hail the mighty Translatotron. In the meantime, I just got back from this year\u2019s NATO conference. It was an incredible honor to get an invitation to speak at such an event, and of course, I was happy to attend as a service to the public. The goal of the talk was to inform key political and military decision makers about recent advancements in AI so they can make better decisions for us. And I was SO nervous during the talk. My goodness. If you wish to watch it, I put a link to it in the video description and I may be able to upload a higher-quality version of this video here in the future. Attending the conference introduced delays in our schedule, my apologies for that, and normally, we would have to worry whether because of this, we\u2019ll have enough income to improve our recording equipment. However, with your support on Patreon, this is not at all the case, so I want to send you a big thank you for all your amazing support. This was really all possible thanks to you. If you wish to support us, just go to patreon.com/TwoMinutePapers or just click the link in the video description. Have fun with the video! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=38ZXwJj6j8k",
        "paper_link": "https://arxiv.org/abs/1904.06037",
        "paper_title": "Direct speech-to-speech translation with a sequence-to-sequence model"
    },
    {
        "video_id": "Y73iUAh56iI",
        "video_title": "We Can All Be Video Game Characters With This AI",
        "position_in_playlist": 349,
        "description": "\ud83d\udcdd The paper \"Vid2Game: Controllable Characters Extracted from Real-World Videos\" is available here:\nhttps://arxiv.org/abs/1904.08379\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Vid2Game",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The title of this paper is very descriptive, it says Controllable Characters Extracted from Real-World Videos. This sounds a little like science fiction, so let\u2019s pick this apart. If we forget about the controllable part, we get something that you\u2019ve seen in this series many times - pose estimation. Pose estimation means that we have a human character in an image or a video, have a computer program look at it, and tell us the current position this character is taking. This is useful for medical applications, such as detecting issues with motor functionality, fall detection, or we can also use it for motion capture for our video games and blockbuster movies. So just performing the pose estimation part is a great invention, but relatively old news. So what\u2019s really new here? Why is this work interesting? How does it go beyond pose estimation? Well, as a hint, the title contains an additional word, \u201ccontrollable\u201d, so, look at this! Woo-hoo! As you see, this technique is not only able to identify where a character is, but we can grab a controller, and move it around! This means making this character perform novel actions, and showing it from novel views. That\u2019s really remarkable, because this requires a proper understanding of the video we\u2019re watching. And this means that we can not only watch these real-world videos, as you see this small piece of footage used for the learning, but by performing these actions with a controller, we can make a video game out of it. Especially given that here, the background has also been changed. To achieve this, this work contains two key elements. Element number one is the pose2pose network that takes an input posture and the button we pushed on the controller, and creates the next step of the animation. And then, element number two, the pose2frame architecture then blends this new animation step into an already existing image. The neural network that performs this is trained in a way where it is encouraged to create these character masks in a way that is continuous and doesn\u2019t contain jarring jumps between the individual frames, leading to smooth and believable movements. Now, clearly, anyone who takes a cursory look sees that the animations are not perfect and still contain artifacts, but just imagine that this paper is among the first introductory works on this problem. Imagine what we\u2019ll have two more papers down the line. I can\u2019t wait. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Y73iUAh56iI",
        "paper_link": "https://arxiv.org/abs/1904.08379",
        "paper_title": "Vid2Game: Controllable Characters Extracted from Real-World Videos"
    },
    {
        "video_id": "pQA8Wzt8wdw",
        "video_title": "OpenAI's MuseNet Learned to Compose Mozart, Bon Jovi and More",
        "position_in_playlist": 350,
        "description": "\ud83d\udcdd The blog post on OpenAI MuseNet is available here:\nhttps://openai.com/blog/musenet/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#OpenAI #MuseNet",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Not so long ago, OpenAI has released GPT-2, an AI that was trained to look at a piece of text and perform common natural language processing operations on it, for instance, answering questions, summarization, and more. But today, we are going to be laser focused on only one of these tasks. And that task is continuation, where we give an AI a bunch of text and we ask it to continue it. However, as these learning algorithms are quite general by design, here comes the twist - who said that this can only work for text? Why not try it on composing music? So let\u2019s have a look at some results, where only the first 6 notes were given from a song, and the AI was asked to continue it. Love it. This is a great testament to the power of general learning algorithms. As you\u2019ve heard, this works great for a variety of different genres as well, and not only that, but it can also create really cool blends between genres. Listen as the AI starts out from the first 6 notes of a Chopin piece and transitions into a pop style with a bunch of different instruments entering a few seconds in. And, great news, because if you look here, we can try our own combinations through an online demo as well. On the left side, we can specify and hear the short input sample, and ask for a variety of different styles for the continuation. It is amazing fun, try it, I\u2019ve put a link in the video description. I was particularly impressed with this combination. Really cool. Now, this algorithm is also not without limitations as it has difficulties pairing instruments that either don\u2019t go too well together or there is lacking training data on how they should sound together. The source code is also either already available as of the publishing of this video, or will be available soon. If so, I will come back and update the video description with the link. OpenAI has also published an almost two-hour concert with ton of different genres so make sure to head to the video description and check it out yourself! I think these techniques are either already so powerful, or will soon be powerful enough to raise important copyright questions, and we\u2019ll need plenty of discussions on who really owns this piece of music. What do you think? Let me know in the comments! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=pQA8Wzt8wdw"
    },
    {
        "video_id": "S7HlxaMmWAU",
        "video_title": "Artistic Style Transfer, Now in 3D!",
        "position_in_playlist": 351,
        "description": "\ud83d\udcdd The paper \"Fast Example-Based Stylization with Local Guidance\" is available here:\nhttps://dcgi.fel.cvut.cz/home/sykorad/styleblit.html\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#StyleTransfer",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to super fun, and really good looking results. We have seen plenty of papers doing variations of style transfer, but can we can push this concept further? And the answer is, yes! For instance, few people know that style transfer can also be done in 3D! If you look here, you see an artist performing this style transfer by drawing on a simple sphere and get their artistic style to carry over to a complicated piece of 3D geometry. We talked about this technique in Two Minute Papers episode 94, and for your reference, we are currently at over episode 340. Leave a comment if you\u2019ve been around back then! And this previous technique led to truly amazing results, but still had two weak points. One, it took too long. As you see here, this method took around a minute or more to produce these results. And hold on to your papers, because this new paper is approximately a 1000 times faster than that, which means that it can produce 100 frames per second using a whopping 4K resolution. But of course, none of this matters\u2026 if the visual quality is not similar. And, if you look closely, you see that the new results are indeed really close to the reference results of the older method. So, what was the other problem? The other problem was the lack of temporal coherence. This means that when creating an animation, it seems like each of the individual frames of the animation were drawn separately by an artist. In this new work, this is not only eliminated as you see here, but the new technique even gives us the opportunity to control the amount of flickering. With these improvements, this is is now a proper tool to help artists perform this 3D style transfer and create these rich virtual worlds much quicker and easier in the future. It also opens up the possibility for novices to do that, which is an amazing value proposition. Limitations still apply, for instance, if we have a texture with some regularity, such as this brickwall pattern here, the alignment and continuity of the bricks on the 3D model may suffer. This can be fixed, but it is a little labor-intensive. However, you know our saying, two more papers down the line, and this will likely cease to be an issue. And what you\u2019ve seen here today is just one paper down the line from the original work, and we can do 4K resolution at a 100 frames per second. Unreal. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=S7HlxaMmWAU",
        "paper_link": "https://dcgi.fel.cvut.cz/home/sykorad/styleblit.html",
        "paper_title": "Fast Example-Based Stylization with Local Guidance"
    },
    {
        "video_id": "thQ7QjqNPlY",
        "video_title": "This AI Makes The Mona Lisa Come To Life",
        "position_in_playlist": 352,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo:\nhttps://www.wandb.com/papers\n\n\ud83d\udcdd The paper \"Few-Shot Adversarial Learning of Realistic Neural Talking Head Models\" is available here:\nhttps://arxiv.org/abs/1905.08233v1\nhttps://www.youtube.com/watch?v=p1b5aiTrGzY\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepFake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work presents a learning-based method that is able to take just a handful of photos, and use those to synthesize a moving virtual character. Not only that, but it can also synthesize these faces from new viewpoints that the AI hasn\u2019t seen before. These results are truly sublime, however, hold on to your papers, because it also works from as little as just one input image. This we refer to as 1-shot learning. You see some examples here, but wait a second\u2026really, just one image? If all it needs is really just one photo, this means that we can use famous photographs, and even paintings, and synthesize animations for them. Look at that! Of course, if we show multiple photos to the AI, it is able to synthesize better output results, you see such a progression here as a function of the amount of input data. The painting part I find to be particularly cool because it strays away from the kind of data the neural networks were trained on, which is photos, however, if we have proper intelligence, the AI can learn how different parts of the human face move, and generalize this knowledge to paintings as well. The underlying laws are the same, only the style of the output is different. Absolutely amazing. The paper also showcases an extensive comparison section to previous works, and, as you see here, nothing really compares to this kind of quality. I have heard the quote \u201cany sufficiently advanced technology is indistinguishable from magic\u201d so many times in my life, and I was like, OK, well, maybe, but I\u2019m telling you - this is one of those times when I really felt that I am seeing magic at work on my computer screen. So, I know what you\u2019re thinking - how can all this wizardry be done? This paper proposes a novel architecture where 3 neural networks work together. One, the Embedder takes colored images with landmark information and compresses it down into the essence of these images, two, the Generator takes a set of landmarks, a crude approximation of the human face, and synthesizes a photorealistic result from it. And three, the Discriminator looks at both real and fake images and tries to learn how to tell them apart. As a result, these networks learn together, and over time, they improve together, so much so that they can create these amazing animations from just one source photo. The authors also released a statement on the purpose and effects of this technology, which I\u2019ll leave here for a few seconds for our interested viewers. This work was partly done at the Samsung AI lab and Skoltech. Congratulations to both institutions. Killer paper. Make sure to check it out in the video description. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It is like a shared logbook for your team, and with this, you can compare your own experiment results, put them next to what your colleagues did and you can discuss your successes and failures much easier. It takes less than 5 minutes to set up and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It was also used in this OpenAI project that you see here, which we covered earlier in the series. They reported that experiment tracking was crucial in this project and that this tool saved them quite a bit of time and money. If only I had access to such a tool during our last research project where I had to compare the performance of neural networks for months and months. Well, it turns out, I will be able to get access to these tools, because, get this, it\u2019s free and will always be free for academics and open source projects. Make sure to visit them through wandb.com or just click the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=thQ7QjqNPlY",
        "paper_link": "https://arxiv.org/abs/1905.08233v1",
        "paper_title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models"
    },
    {
        "video_id": "9M18rc9-VWU",
        "video_title": "This Jello Simulation Uses Only ~88 Lines of Code",
        "position_in_playlist": 353,
        "description": "\ud83d\udcdd The paper \"Moving Least Squares MPM with Compatible Particle-in-Cell\" and its source code is available here:\nhttp://taichi.graphics/wp-content/uploads/2019/03/mls-mpm-cpic.pdf\nhttps://github.com/yuanming-hu/taichi_mpm\n\nThe Taichi framework: http://taichi.graphics/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\u20bf Crypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 1a5ttKiVQiDcr9j8JT2DoHGzLG7XTJccX\n\u203a Ethereum: 0xbBD767C0e14be1886c6610bf3F592A91D866d380\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we\u2019re going to talk about the material point method. This method uses both grids and particles to simulate the movement of snow, dripping honey, interactions of granular solids and a lot of other really cool phenomena on our computers. This can be used, for instance, in the movie industry to simulate what a city would look like if it were flooded. However, it has its own limitations, which you will hear more about in a moment. This paper showcases really cool improvements to this technique, for instance, it enables to run these simulations twice as fast, and can simulate new phenomena that were previously not supported by the material point method. One is the simulation of complex thin boundaries that enables us to cut things, so in this video, expect lots of virtual characters to get dismembered. I think this might be the only channel on YouTube where we can say this celebrate it as an amazing scientific discovery. And the other key improvement of this paper is introducing two-way coupling, which means the example that you see here as the water changes the movement of the wheel, but the wheel also changes with the movement of the water. It is also demonstrated quite aptly here by this elastoplastic jello scene, in which we can throw in a bunch of blocks of different densities, and it is simulated beautifully here how they sink into the jello deeper and deeper as a result. Here, you see a real robot running around in a granular medium, and here, we have a simulation of the same phenomenon and can marvel at how close the result is to what would happen in real life. Another selling point of this method is that this is easy to implement, which is demonstrated here, and what you see here is the essence of this algorithm implemented in 88 lines of code. Wow! Now, these methods still take a while as there is a lot of deformations and movement to compute and we can only advance time in very small steps, and as a result, the speed of such simulations is still measured in not frames per second, but in seconds per frame. These are the kinds of simulations that we like to leave on the machine overnight. If you want to see something that is done with a remarkable amount of love and care, please read this paper. And I don\u2019t know if you have heard about this framework called Taichi. This contains implementations for many amazing papers in computer graphics. Lots of paper implementations on animation, light transport simulations, you name it, a total of more than 40 papers are implemented here. And I was thinking, this is really amazing, \u201cI wonder which group made this\u201d, then I noticed it was all written by one person. And that person is Yuanming Hu, the scientist who is the lead author of this paper. This is insanity. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9M18rc9-VWU",
        "paper_link": "http://taichi.graphics/wp-content/uploads/2019/03/mls-mpm-cpic.pdf",
        "paper_title": "Moving Least Squares MPM with Compatible Particle-in-Cell"
    },
    {
        "video_id": "2xWnOL5bts8",
        "video_title": "Rewrite Videos By Editing Text",
        "position_in_playlist": 354,
        "description": "\ud83d\udcdd The paper \"Text-based Editing of Talking-head Video\" is available here:\nhttps://www.ohadf.com/projects/text-based-editing/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepFake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The last few years have been an amazing ride when it comes to research works for creating facial reenactments for real characters. Beyond just transferring our gestures to a video footage of an existing talking head, controlling their gestures like video game characters and full-body movement transfer are also a possibility. With WaveNet and its many variants, we can also learn someone\u2019s way of speaking, write a piece of text and make an audio waveform where we can impersonate them using their own voice. So, what else is there to do in this domain? Are we done? No-no, not at all! Hold on to your papers, because with this amazing new technique, what we can do is look at the transcript of a talking head video, remove parts of it or add to it, just as we would edit any piece of text - and, this technique produces both the audio and a matching video of this person uttering these words. Check this out. It works by looking through the video collecting small sounds that can be used to piece together this new word that we\u2019ve added to the transcript. The authors demonstrate this by adding the word \u201cfox\u201d to the transcript. This can be pieced together by the \u201cv\u201d which appears in the word \u201cviper\u201d, and taking \u201cox\u201d as a part of another word found in the footage. As a result, one can make the character say \u201cfox\u201d even without hearing her uttering this word before. Then, we can look for not only the audio occurrences for these sounds, but the video footage of how they are being said, and in the paper, a technique is proposed to blend these video assets together. Finally, we can provide all this information to a neural renderer that synthesizes a smooth video of this talking head. This is a beautiful architecture with lots of contributions, so make sure to have a look at the paper in the description for more details. And of course, as it is not easy to measure the quality of these results in a mathematical manner, a user study was made where they asked some fellow humans which is the real footage, and which one was edited. You will see the footage edited by this algorithm on the right. And, hm, it\u2019s not easy to tell which one is which, and it also shows in the numbers, which are not perfect, but they clearly show that the fake video is very often confused with the real one. Did you find any artifacts there that give the trick away? Perhaps the sentence was said a touch faster than expected. Found anything else? Let me know in the comments below. The paper also contains tons of comparisons against previous works. So in the last few years, the trend seems clear: the bar is getting lower, it is getting easier and easier to produce these kinds of videos, and it is getting harder and harder to catch them with our naked eyes, and now, we can edit the transcript of what is being said, which is super convenient. I would like to note that AIs also exist that can detect these edited videos with a high confidence. I put up the the ethical considerations of the authors here, it is definitely worthy of your attention as it discusses how they think about these techniques. The motivation for this work was mainly to enhance digital storytelling by removing filler words, potentially flubbed phrases or retiming sentences in talking head videos. There is much more to it, so make sure to pause the video and read their full statement. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2xWnOL5bts8",
        "paper_link": "https://www.ohadf.com/projects/text-based-editing/",
        "paper_title": "Text-based Editing of Talking-head Video"
    },
    {
        "video_id": "kie4wjB1MCw",
        "video_title": "Virtual Characters Learn To Work Out\u2026and Undergo Surgery \ud83d\udcaa",
        "position_in_playlist": 355,
        "description": "\ud83d\udcdd The paper \"Scalable Muscle-actuated Human\nSimulation and Control\" is available here:\nhttp://mrl.snu.ac.kr/research/ProjectScalable/Page.htm\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This work is about creating virtual characters with a skeletal system, adding more than 300 muscles and teaching them to use these muscles to kick, jump, move around and perform other realistic human movements. Throughout the video, you will see the activated muscles with red. I am loving the idea, which, turns out, comes lots of really interesting corollaries. For instance, this simulation realistically portrays how increasing the amount of weight to be lifted changes what muscles are being trained during a workout. These agents also learned to jump really high and you can see a drastic difference between the movement required for a mediocre jump and an amazing one. As we are teaching these virtual agents within a simulation, we can perform all kinds of crazy experiments by giving them horrendous special conditions, such as bone deformities, a stiff ankle, muscle deficiencies and watch them learn to walk despite these setbacks. For instance, here you see that the muscles in the left thigh are contracted, resulting in a stiff knee, and as a result, the agent learned an asymmetric gait. If the thigh-bones are twisted inwards, ouch, the AI shows that it is still possible to control the muscles to walk in a stable manner. I don\u2019t know about you, but at this point I am feeling quite sorry for these poor simulated beings, so let\u2019s move on, we have plenty of less gruesome, but equally interesting things to test here. In fact, if we are in a simulation, why not take it further? It doesn\u2019t cost anything! That\u2019s exactly what the authors did, and it turns out that we can even simulate the use of prosthetics. However, since we don\u2019t need to manufacture these prosthetics, we can try a large number of different designs and evaluate their usefulness without paying a dime. How cool is that? So far, we have hamstrung this poor character many-many times, so why not try to heal it? With this technique, we can also quickly test the effect of different kinds of surgeries on the movement of the patient. With this, you can see here how a hamstring surgery can extend the range of motion of this skeleton. It also tells us not to try our luck with one-legged squats. You heard it here first folks. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=kie4wjB1MCw",
        "paper_link": "http://mrl.snu.ac.kr/research/ProjectScalable/Page.htm",
        "paper_title": "Scalable Muscle-actuated Human\nSimulation and Control"
    },
    {
        "video_id": "hYWr67i8z5o",
        "video_title": "AI Discovers Sentiment By Writing Amazon Reviews",
        "position_in_playlist": 356,
        "description": "\u2764\ufe0f Support the show on Patreon: https://www.patreon.com/TwoMinutePapers\n\n\u20bf Crypto and PayPal links are available below. Thank you very much for your generous support!\n\u203a PayPal: https://www.paypal.me/TwoMinutePapers\n\u203a Bitcoin: 1a5ttKiVQiDcr9j8JT2DoHGzLG7XTJccX\n\u203a Ethereum: 0xbBD767C0e14be1886c6610bf3F592A91D866d380\n\u203a LTC: LM8AUh5bGcNgzq6HaV1jeaJrFvmKxxgiXg\n\n\ud83d\udcdd The paper \"Learning to Generate Reviews and Discovering Sentiment\" is available here:\nhttps://openai.com/blog/unsupervised-sentiment-neuron/\nhttps://arxiv.org/abs/1704.01444\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In 2017, scientists at OpenAI embarked on an AI project where they wanted to show a neural network a bunch of Amazon product reviews and wanted to teach it to be able to generate new ones, or continue a review when given one. Now, so far, this sounds like a nice hobby project, definitely not something that would justify an entire video on this channel, however, during this experiment, something really unexpected happened. Now, it is clear that when the neural network reads these reviews, it knows that it has to generate new ones, therefore it builds up a deep understanding of language. However, beyond that, it used surprisingly few neurons to continue these reviews and scientists were wondering \u201cwhy is that\u201d? Usually, the more neurons, the more powerful the AI can get, so why use so few neurons? The reason for that is that it has learned something really interesting. I\u2019ll tell you what in a moment. This neural network was trained in an unsupervised manner, therefore it was told to do what the task was, but was given no further supervision. No labeled datasets, no additional help, nothing. Upon closer inspection, they noticed that the neural network has built up a knowledge of not only language, but also built a sentiment detector as well. This means that the AI recognized that in order to be able to continue a review, it needs to be able to efficiently detect whether the review seems positive or not. And thus, it dedicated a neuron to this task, which we will refer to as the sentiment neuron. However, it was no ordinary sentiment neuron, it was a proper, state of the art sentiment detector. In this diagram, you see this neuron at work. As it reads through the review, it starts out detecting a positive outlook which you can see with green, and then, uh-oh, it detects that review has taken a turn and is not happy with the movie at all. And all this was learned on a relatively small dataset. Now, if we have this sentiment neuron, we don\u2019t just have to sit around and be happy for it. Let\u2019s play with it! For instance, by overwriting this sentiment neuron in the neural network, we can force it to create positive or negative reviews. Here is a positive example: \u201cJust what I was looking for. Nice fitted pants, exactly matched seam to color contrast with other pants I own. Highly recommended and also very happy!\u201d And, if we overwrite the sentiment neuron to negative, we get the following: \u201cThe package received was blank and has no barcode. A waste of time and money.\u201d There are some more examples here on the screen for your pleasure. Absolutely amazing. This paper teaches us that we should endeavor to not just accept these AI-based solutions, but look under the hood, and sometimes, a goldmine of knowledge can be found within. If you have enjoyed this episode and would like to help us make better videos for you in the future, please consider supporting us on Patreon.com/TwoMinutePapers or just click the link in the video description. In return, we can offer you early access to these episodes, or even add your name to our key supporters so you can appear in the desciption of every video and more. We also support cryptocurrencies like Bitcoin, Ethereum and Litecoin. The majority of these funds is used to improve the show and we use a smaller part to give back to the community and empower science conferences like the Central European Conference on Computer Graphics. This is a conference that teaches young scientists to present their work at bigger venues later, and with your support, it\u2019s now the second year we\u2019ve been able to sponsor them, which warms my heart. This is why every episode ends with, you know the drill\u2026 Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hYWr67i8z5o",
        "paper_link": "https://openai.com/blog/unsupervised-sentiment-neuron/",
        "paper_title": "Learning to Generate Reviews and Discovering Sentiment"
    },
    {
        "video_id": "tRHFQHYfAVc",
        "video_title": "Better Photorealistic Materials Are Coming!",
        "position_in_playlist": 357,
        "description": "\ud83d\udcdd The paper \"An Adaptive Parameterization for Efficient Material Acquisition and Rendering\" is available here:\nhttps://rgl.epfl.ch/publications/Dupuy2018Adaptive\n\nMy course on photorealistic rendering is available here:\n- Playlist: https://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n- Website: https://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we often marvel at light simulation programs that are able to create beautiful images by simulating the path of millions and millions of light rays. To make sure that our simulations look lifelike, we not only have to make sure that these rays of light interact with the geometry of the scene in a way that is physically plausible, but the materials within the simulation also have to reflect reality. Now that\u2019s an interesting problem. How do we create a convincing mathematical description of real-world materials? Well, one way to do that is taking a measurement device, putting in a sample of the subject material and measuring how rays of light bounce off of it. This work introduces a new database for sophisticated material models and includes interesting optical effects, such as iridescence, which gives the colorful physical appearance of bubbles and fuel-water mixtures, it can do colorful mirror-like specular highlights and more, so we can include these materials in our light simulation programs. You see this database in action in this scene that showcases a collection of these complex material models. However, creating such a database is not without perils, because normally, these materials take prohibitively many measurements to reproduce properly, and the interesting regions are often found at quite different places. This paper proposes a solution that adapts the location of these measurements to where the action happens, resulting in a mathematical description of these materials that can be measured in a reasonable amount of time. It also takes very little memory when we run the actual light simulation on them. So, as if light transport simulations weren\u2019t beautiful enough, they are about to get even more realistic in the near future. Super excited for this. Make sure to have a look at the paper, which is so good, I think I sank into a minor state of shock upon reading it. If you are enjoying learning about light transport, make sure to check out my course on this topic at the Technical University of Vienna. I still teach this at the University for about 20 Master students at a time and thought that the teachings shouldn't only be available for a lucky few people who can afford a college education. Clearly, the teachings should be available for everyone, so, we recorded it and put it online, and now everyone can watch it, free of charge. I was quite stunned to see that more than 25 thousand people decided to start it, so make sure to give it a go if you're interested! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=tRHFQHYfAVc",
        "paper_link": "https://rgl.epfl.ch/publications/Dupuy2018Adaptive",
        "paper_title": "An Adaptive Parameterization for Efficient Material Acquisition and Rendering"
    },
    {
        "video_id": "xlrGOfvYcQc",
        "video_title": "AI Creates Near Perfect Images Of People, Dogs and More",
        "position_in_playlist": 358,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo:\n- Run experiments with this paper here: https://app.wandb.ai/l2k2/sonnet-sonnet_examples/runs/jizpgd0o?workspace=user-l2k2\n- Free demo: https://www.wandb.com/papers\n\n\ud83d\udcdd The paper \"Generating Diverse High-Fidelity Images with VQ-VAE-2\" and its supplementary materials are available here:\nhttps://arxiv.org/abs/1906.00446\nhttps://drive.google.com/file/d/1H2nr_Cu7OK18tRemsWn_6o5DGMNYentM/view\n\nOur latent-space material synthesis paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nLearning a Manifold of Fonts:\nhttp://vecg.cs.ucl.ac.uk/Projects/projects_fonts/projects_fonts.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the last few years, we have seen a bunch of new AI-based techniques that were specialized in generating new and novel images. This is mainly done through learning-based techniques, typically a Generative Adversarial Network, a GAN in short, which is an architecture where a generator neural network creates new images, and passes it to a discriminator network, which learns to distinguish real photos from these fake, generated images. These two networks learn and improve together, so much so that many of these techniques have become so realistic that we often can\u2019t tell they are synthetic images unless we look really closely. You see some examples here from BigGAN, a previous technique that is based on this architecture. So in these days, many of us are wondering, is there life beyond GANs? Can they be matched in terms of visual quality by a different kind of a technique? Well, have a look at this paper, because it proposes a much simpler architecture that is able to generate convincing, high-resolution images quickly for a ton of different object classes. The results it is able to churn out is nothing short of amazing. Just look at that! To be able to proceed to the key idea here, we first have to talk about latent spaces. You can think of a latent space as a compressed representation that tries to capture the essence of the dataset that we have at hand. You can see a similar latent space method in action here that captures the key features that set different kinds of fonts apart and presents these options on a 2D plane, and here, you see our technique that builds a latent space for modeling a wide range of photorealistic material models. And now, onto the promised key idea! As you have guessed, this new technique uses a latent space, which means that instead of thinking in pixels, it thinks more in terms of these features that commonly appear in natural photos, which also makes the generation of these images up to 30 times quicker, which is super useful, especially in the case of larger images. While we are at that, it can rapidly generate new images with the size of approximately a thousand by thousand pixels. Machine learning is a research field that is enjoying a great deal of popularity these days, which also means that so many papers appear every day it\u2019s getting really difficult to keep track of all of them. The complexity of the average technique is also increasing rapidly over time, and what I like most about this paper is that it shows us that surprisingly simple ideas can still lead to breakthroughs. What a time to be alive! Make sure to have a look at the paper in the description as it also describes how this method is able to generate more diverse images than previous techniques and how we can measure diversity at all because that is no trivial matter. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It is like a shared logbook for your team, and with this, you can compare your own experiment results, put them next to what your colleagues did and you can discuss your successes and failures much easier. It takes less than 5 minutes to set up and is being used by OpenAI, Toyota Research, Stanford and Berkeley. In fact, it is so easy to add to your project, the CEO himself, Lukas instrumented it for you for this paper, and if you look here, you can see how the output images and the reconstruction error evolve over time and you can even add your own visualizations. It is a sight to behold, really, so make sure to check it out in the video description, and if you liked it, visit them through wandb.com/papers or just use the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=xlrGOfvYcQc",
        "paper_link": "https://arxiv.org/abs/1906.00446",
        "paper_title": "Generating Diverse High-Fidelity Images with VQ-VAE-2"
    },
    {
        "video_id": "prMk6Znm4Bc",
        "video_title": "This AI Learns About Movement By Watching Frozen People",
        "position_in_playlist": 359,
        "description": "\ud83d\udcdd The paper \"Learning the Depths of Moving People by Watching Frozen People\" is available here:\nhttps://mannequin-depth.github.io/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This paper is about endowing colored images with depth information, which is typically done through depth maps. Depth maps describe how far parts of the scene are from the camera and are given with a color coding where the darker the colors are, the further away the objects are. These depth maps can be used to apply a variety of effects to the image that require knowledge about the distance of the objects within - for instance, selectively defocusing parts of the image, or even removing people and inserting new objects to the scene. If we, humans look at an image, we have an intuitive understanding of its contents and have the knowledge to produce a depth map by pen and paper. However, this would, of course, be infeasible and would take too long, so we would prefer a machine to do it for us instead. But of course, machines don\u2019t understand the concept of 3D geometry so they probably cannot help us with this. Or, with the power of machine learning algorithms, can they? This new paper from scientists at Google Research attempts to perform this, but, with a twist. The twist is that a learning algorithm is unleashed on a dataset of what they call mannequins, or in other words, real humans are asked to stand around frozen in a variety of different positions while the camera moves around in the scene. The goal is that the algorithm would have a look at these frozen people and take into consideration the parallax of the camera movement. This means that the objects closer to the camera move more than other objects that are further away. And turns out, this kind of knowledge can be exploited, so much so that if we train our AI properly, it will be able to predict the depth maps of people that are moving around, even if it had only seen these frozen mannequins before. This is particularly difficult because if we have an animation, we have to make sure that the guesses are consistent across time, or else we get these annoying flickering effects that you see here with previous techniques. It is still there with the new method, especially for the background, but the improvement on the human part of the image is truly remarkable. Beyond the removal and insertion techniques we talked about earlier, I am also really excited for this method as it may open up the possibility of creating video versions of these amazing portrait mode images with many of the newer smartphones people have in their pockets. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=prMk6Znm4Bc",
        "paper_link": "https://mannequin-depth.github.io/",
        "paper_title": "Learning the Depths of Moving People by Watching Frozen People"
    },
    {
        "video_id": "OEQf0AtSSsc",
        "video_title": "Tighten the Towel! Simulating Liquid-Fabric Interactions",
        "position_in_playlist": 360,
        "description": "\ud83d\udcf8 We are now available on Instagram with short snippets of our new episodes. Check us out there! https://www.instagram.com/twominutepapers/\n\n\ud83d\udcdd The paper \"A Multi-Scale Model for Simulating Liquid-Fabric Interactions\" is available here:\nhttp://www.cs.columbia.edu/cg/wetcloth/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Ivelin Ivanov, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, I\u2019ve got some fluids for you! Most hobby projects with fluid simulations involve the simulation of a piece of sloshing liquid in a virtual container. However, if you have a more elaborate project at hand, the story is not so so simple anymore. This new paper elevates the quality and realism of these simulations through using mixture theory. Now, what is there to be mixed, you ask? Well, what mixture theory does for us is that it helps simulating how liquids interact with fabrics, including splashing, wringing, and more. These simulations have to take into account that the fabrics may absorb some of the liquids poured onto them and get saturated, how diffusion transports this liquid to nearby yarn strands, or, what you see here is a simulation with porous plastic, where water flows off of, and also through this material as well. Here you see how it can simulate honey dripping down on a piece of cloth. This is a real good one - if you are a parent with small children, you probably have lots of experience with this situation and can assess the quality of this simulation really well. The visual fidelity of these simulations is truly second to none. I love it. Now, the question naturally arises - how do we know if these simulations are close to what would happen in reality? We don\u2019t just make a simulation and accept it as true to life if it looks good, right? Well, of course not, the paper also contains comparisons against real world laboratory setups to ensure the validity of these results, so make sure to have a look at it in the video description. And if you\u2019ve been watching this series for a while, you notice that I always recommend that you check out the papers yourself. And even though it is true that these are technical write-ups that are meant to communicate results between experts, it is beneficial for everyone to also read at least a small part of it. If you do, you\u2019ll not only see beautiful craftsmanship, but you\u2019ll also learn how to make a statement and how to prove the validity of this statement. This is a skill that is necessary to find truth. So, please, read your papers. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=OEQf0AtSSsc",
        "paper_link": "http://www.cs.columbia.edu/cg/wetcloth/",
        "paper_title": "A Multi-Scale Model for Simulating Liquid-Fabric Interactions"
    },
    {
        "video_id": "fcnjHmBcLNQ",
        "video_title": "3D Style Transfer For Video is Now Possible!",
        "position_in_playlist": 361,
        "description": "\ud83d\udcdd The paper \"Stylizing Video by Example\" is available here:\nhttps://dcgi.fel.cvut.cz/home/sykorad/ebsynth.html\n\nThe app is available here: https://ebsynth.com\n\nOur earlier episode on StyLit:\nhttps://www.youtube.com/watch?v=S7HlxaMmWAU\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Neural style transfer just appeared four years ago, in 2015. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to the super fun results that you see here. However, most of these works are about photos. So what about video? Well, hold on to your papers because this new work does this for video and the results are marvelous. The process goes as follows: we take a few keyframes from the video, and the algorithm propagates our style to the remaining frames of the video, and, WOW, these are some silky smooth results. In specific, what I would like you to take a look look at is the temporal coherence of the results. Proper temporal coherence means that the individual images within this video are not made independently from each other, which would introduce a disturbing flickering effect. I see none of that here, which makes me very, very happy! And now, hold on to your papers again because this technique does not use any kind of AI. No neural networks and other learning algorithms were used here. Okay, great, no AI. But, is it any better than its AI-based competitors? Well, look at this! Hell yeah! This method does this magic through building a set of guide images, for instance, a mask guide highlights the stylized objects, and sure enough, we also have a temporal guide that penalizes the algorithm for making too much of a change from one frame to the next one, ensuring that the results will be smooth. Make sure to have a look at the paper for a more exhaustive description of these guides. Now if we make a carefully crafted mixture from these guide images and plug them in to a previous algorithm by the name StyLit, we talked about this algorithm before in the series, the link is in the video description, then, we get these results that made me fall out of my chair. I hope you will be more prepared and held on to your papers. Let me know in the comments! And you know what is even better? You can try this yourself because the authors made a standalone tool available free of charge, just go to ebsynth.com, or just click the link in the video description. Let the experiments begin! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fcnjHmBcLNQ",
        "paper_link": "https://dcgi.fel.cvut.cz/home/sykorad/ebsynth.html",
        "paper_title": "Stylizing Video by Example"
    },
    {
        "video_id": "u90TbxK7VEA",
        "video_title": "This Superhuman Poker AI Was Trained in 20 Hours",
        "position_in_playlist": 362,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo:\nhttps://www.wandb.com/papers\n\nWeights & Biases blog post with the 1 line of code visualization: https://www.wandb.com/articles/visualize-keras-models-with-one-line-of-code\n\n\ud83d\udcdd The paper \"Superhuman AI for multiplayer poker\" is available here:\n- https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/\n- https://www.cs.cmu.edu/~noamb/papers/19-Science-Superhuman.pdf\n- https://science.sciencemag.org/content/early/2019/07/10/science.aay2400\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Poker #PokerAI",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, the game we\u2019ll be talking about is the six-player no-limit Hold\u2019em poker, which is one of the more popular poker variants out there. And the goal of this project was to build a poker AI that never played against a human before and learns entirely through self-play, and is able to defeat professional human players. During these tests, two of the players that were tested against are former World Series of Poker Main Event winners. And of course, before you ask, yes, in a moment, we\u2019ll look at an example hand that shows how the AI traps a human player. Poker is very difficult to learn for AI bots because it is a game of imperfect information. For instance, chess is a game of perfect information where we see all the pieces and can make a good decision if we analyze the situation well. However, not so much in Poker, because only at the very end of the hand do the players show what they have. This makes it extremely difficult to train an AI to do well. And now, let\u2019s have a look at the promised example hand here. We talked about imperfect information just a moment ago, so I\u2019ll note that all the cards are shown face up for us to make the analysis of this hand easier, of course, this is not how the hands were played. You see the AI up there marked with P2 sittin\u2019 pretty with a Jack and a Queen, and before the flop happens, which is when the first three cards are revealed, only one human player seems to be interested in this hand. During the flop, the AI paired its Queen and has a Jack as a kicker, which, if played well is going to be disastrous for the human player. So, why is that? You see, the human player also paired their queen, but has a weaker kicker and will therefore lose to the AIs hand. In this case, this player thinks they have a strong hand and will get lots of value out of it\u2026 only to find out that they will be the one milked by the AI. So, how exactly does that happen? Well, look here carefully! The bot shows weakness by checking here, to which, the human player\u2019s answer is a small raise. The bot, again, shows weakness by just calling this raise, and checking again on the turn, essentially saying \u201cI am weak, don\u2019t hurt me!\u201d. By the time we get to the river, the AI, again, appears weak to the human player, who now tries to milk the bot with a mid-sized raise\u2026 and, the AI recognizes that now is the time to pounce, the confused player calls the bet and gets milked for almost all their money. An excellent slow play from the AI. Now, note that one hand is difficult to evaluate in isolation, this was a great hand indeed, but we need to look at entire games to get a better grasp of the capabilities of this AI. So if we look at the dollar-equivalent value of the chips in the game, the AI was able to win a thousand dollars from these 5 professional poker players\u2026every hour. It also uses very little resources, can be trained in the cloud for only several hundred dollars, and exceeds human-level performance within only 20 hours. What you see here is a decision tree that explains how the algorithm figures out whether to check or bet, and as you see here, this tree is traversed in a depth-first way, so first, it descends deep into one possible decision, and later, as more options are being unrolled and evaluated, the probability of these choices are updated above. In simpler words, first, the AI seems somewhat sure that checking would be a good choice here, but after carefully evaluating both decisions, it is able to further reinforce this choice. One of the professional players noted that the bot is a much more efficient bluffer than a human and always puts on a lot of pressure. Now note that this is also a general learning technique and is not tailored specifically for poker, and as a result, the authors of the paper noted that they will also try it on other imperfect information games in the future. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It is really easy to use, in fact, this blog post describes how you can visualize your Keras models with only one line of code. When you run this model, it will also start saving relevant metrics for you and here you can see the visualization of the mentioned model and these metrics as well. That\u2019s it. You\u2019re done! It can do a lot more than this, of course, and, you know what the best part is? The best part is that it\u2019s free and will always be free for academics and open source projects. Make sure to visit them through wandb.com/papers or just click the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u90TbxK7VEA",
        "paper_link": "https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/",
        "paper_title": "Superhuman AI for multiplayer poker"
    },
    {
        "video_id": "wVtOuvFlczg",
        "video_title": "Augmented Reality Presentations Are Coming!",
        "position_in_playlist": 363,
        "description": "\ud83d\udcdd The paper \"Interactive Body-Driven Graphics for Augmented Video Performance\" is available here:\nhttps://1iyiwei.github.io/ibg-chi19/\nhttps://hal.archives-ouvertes.fr/hal-02005318/document\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we talk about amazing research papers. However, when a paper is published, also, a talk often has to be given at a conference. And this paper is about the talk itself, or more precisely, how to enhance your presentation with dynamic graphics. Now, these effects can be added to music videos and documentary movies, however, they take a long time and cost a fortune. But not these ones, because this paper proposes a simple framework in which the presenter stands before a kinect camera and an AR mirror monitor, and can trigger these cool little graphical elements with simple gestures. A key part of the paper is the description of a user interface where we can design these mappings. This skeleton represents the presenter who is tracked by the kinect camera, and as you see here, we can define interactions between these elements and the presenter, such as grabbing this umbrella, pull up a chart, and more. As you see with the examples here, using such a system leads to more immersive storytelling, and note that again, this is an early implementation of this really cool idea. A few more papers down the line, I can imagine rotatable and deformable 3D models and photorealistic rendering entering the scene\u2026well, sign me up for that. If you have any creative ideas as to how this could be used or improved, make sure to leave a comment. In the meantime, we are also now available on Instagram, so if you wish to see cool little snippets of our latest episodes, including this one, make sure to check us out there. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wVtOuvFlczg",
        "paper_link": "https://1iyiwei.github.io/ibg-chi19/",
        "paper_title": "Interactive Body-Driven Graphics for Augmented Video Performance"
    },
    {
        "video_id": "CSQPD3oyvD8",
        "video_title": "Simulating Water and Debris Flows",
        "position_in_playlist": 364,
        "description": "\u2764\ufe0f You can support the show through Patreon: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"Animating Fluid Sediment Mixture in Particle-Laden Flows\" is available here:\nhttp://pages.cs.wisc.edu/~sifakis/papers/MPM-particle-laden-flow.pdf\nhttps://dl.acm.org/citation.cfm?id=3201309\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We recently talked about an amazing paper that uses mixture theory to simulate the interaction of liquids and fabrics. And this new work is about simulating fluid flows where we have some debris or other foreign particles in our liquids. This is really challenging. For example, one of the key challenges is incorporating two-way coupling into this process. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. Now, before you start wondering whether this is real footage or not, the fact that this is a simulation should become clear now, because what you see here in the background is where the movement of the two domains are shown in isolation. Just look at how much interaction there is between the two. Unbelievable. Beautiful simulation, ice cream for your eyes. This new method also contains a novel density correction step, and if you watch closely here, you\u2019ll notice why. Got it? Let\u2019s watch it again. If we try to run this elastoplastic simulation for these two previous methods, they introduce here a gain in density, or in other words, we end up with more stuff than we started with. These two rows show the number of particles in the simulation in the worst case scenario, and as you see, some of these incorporate millions of particles for the fluid and many hundreds of thousands for the sediment. Since this work uses the material point method, which is a hybrid simulation technique that uses both particles and grids, the delta x row denotes the resolution of the simulation grid. Now since these grids are often used for 3D simulations, we need to raise the 256 and the 512 to the third power, and with that, we get a simulation grid with up to hundreds of millions of points, and we haven\u2019t even talked about the particle representation yet! In the face of all of these challenges, the simulator is able to compute one frame in a matter of minutes, and not hours or days, which is an incredible feat. With this, I think it is easy to see that the computer graphics research is improving at a staggering pace. What a time to be alive! If you enjoyed this episode, please make consider supporting us through Patreon, our address is Patreon.com/TwoMinutePapers, or just click the link in the video description. With this, we can make better videos for you. You can also get your name immortalized in the video description as a key supporter or watch these videos earlier than others. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=CSQPD3oyvD8",
        "paper_link": "http://pages.cs.wisc.edu/~sifakis/papers/MPM-particle-laden-flow.pdf",
        "paper_title": "Animating Fluid Sediment Mixture in Particle-Laden Flows"
    },
    {
        "video_id": "IMZkLVBhcig",
        "video_title": "DeepMind\u2019s New AI Dreams Up Videos on Many Topics",
        "position_in_playlist": 365,
        "description": "\ud83d\udcdd The paper \"Efficient Video Generation on Complex Datasets\" is available here:\nhttps://arxiv.org/abs/1907.06571\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Zach Boldyga.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the last few years, the pace of progress in machine learning research has been staggering. Neural network-based learning algorithms are now able to look at an image and describe what\u2019s seen in this image, or even better, the other way around, generating images from a written description. You see here a set of results from BigGAN, a state of the art image generation technique and marvel at the fact that all of these images are indeed synthetic. The GAN part of this technique abbreviates the term Generative Adversarial Network - this means a pair of neural networks that battle each over time to master a task, for instance, to generate realistic looking images when given a theme. These detailed images are great, but, what about generating video? With the Dual Video Discriminator GAN, DVD-GAN in short, DeepMind\u2019s naming game is still as strong as ever, it is now possible to create longer and higher-resolution videos than was previously possible, the exact numbers are 256x256 in terms of resolution and 48 frames, which is about 2 seconds. It also learned the concept of changes in the camera view, zooming in on an object, and understands that if someone draws something with a pen, the ink has to remain on the paper unchanged. The Dual Discriminator part of the name reveals one of the key ideas of the paper. In a classical GAN, we have a discriminator network that looks at the images of the generator network and critiques them. As a result, this discriminator learns to tell fake and real images apart better, but at the same time, provides ample feedback for the generator neural network so it can come up with better images. In this work, we have not one, but two discriminators, one is called the spatial discriminator that looks at just one image and assesses how good it is structurally, while the second, temporal discriminator critiques the quality of the movement in these videos. This additional information provides better teaching for the generator, which will in return, be able to generate better videos for us. The paper contains all the details that you could possibly want to learn about this algorithm, in fact, let me give you two that I found to be particularly interesting: one, it does not get any additional information about where the foreground and the background is, and is able to leverage the learning capacity of these neural networks to learn these concepts by itself. And two, it does not generate the video frame by frame sequentially, but it creates the entire video in one go. That\u2019s wild. Now, 256x256 is not a particularly high video resolution, but if you have been watching this series for a while, you are probably already saying that two more papers down the line, and we may be watching HD videos that are also longer than we have the patience to watch. And all this through the power of machine learning research. For now, let\u2019s applaud DeepMind for this amazing paper, and I can\u2019t wait to have a look at more results and see some followup works on it. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=IMZkLVBhcig",
        "paper_link": "https://arxiv.org/abs/1907.06571",
        "paper_title": "Efficient Video Generation on Complex Datasets"
    },
    {
        "video_id": "Jnj7OmmOm2Y",
        "video_title": "This AI Hallucinates Images For You",
        "position_in_playlist": 366,
        "description": "\ud83d\udcf7 We are now available on Instagram: https://www.instagram.com/twominutepapers/\n\n\ud83d\udcdd The paper \"On the steerability of generative adversarial networks\" is available here:\nhttps://ali-design.github.io/gan_steerability/\n\nThe paper \"Learning a Manifold of Fonts\" and its demo are available here:\nhttp://vecg.cs.ucl.ac.uk/Projects/projects_fonts/projects_fonts.html\n\nOur material synthesis paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As machine learning research advances over time, learning-based techniques are getting better and better at generating images, or even creating videos when given a topic. A few episodes ago, we talked about a DeepMind\u2019s Dual Video Discriminator technique, in which, multiple neural networks compete against each other, teaching our machines to synthesize a collection of 2-second long videos. One of the key advantages of this method was that it also learned the concept of changes in the camera view, zooming in on an object, and understood that if someone draws something with a pen, the ink has to remain on the paper unchanged. However, generally, if we wish to ask an AI to synthesize assets for us, in many cases, we\u2019ll likely have an exact idea of what we are looking for. In these cases, we are looking for a little more artistic control than this technique offers us. So, can we get around this? If so, how? Well, we can! I\u2019ll tell you how in a moment, but to understand this solution, we first have to have a firm grasp on the concept of latent spaces. You can think of a latent space as a compressed representation that tries to capture the essence of the dataset that we have at hand. You can see a similar latent space method in action here that captures the key features that set different kinds of fonts apart and presents these options on a 2D plane, and here, you see our technique that builds a latent space for modeling a wide range of photorealistic material models that we can explore. And now to this new work. What this tries to do is find a path in the latent space of these images that relates to intuitive concepts like camera zooming, rotation or shifting. That\u2019s not an easy task, but if we can pull it off, we\u2019ll have more artistic control over these generated images, which would be immensely useful for many creative tasks. This new work can perform that, and not only that, but it is also able to learn the concept of color enhancement, and can even increase or decrease the contrast of these images. The key idea of this paper is that this can be done through trying to find crazy, non-linear trajectories in these latent spaces that happen to relate to these intuitive concepts. It is not perfect in a sense that we can indeed zoom in on the picture of this dog, but the posture of the dog also changes, and it even seems like we\u2019re starting out with a puppy that grows up frame by frame. This means that we have learned to navigate this latent space, but there is still some additional fat in these movements, which is a typical side effect of latent space-based techniques and also, don\u2019t forget that the training data the AI is given also has its own limits. However, as you see, we are now one step closer to not only having an AI that synthesizes images for us, but one that does it exactly with the camera setup, rotation, and colors that we are looking for. What a time to be alive! If you wish to see beautiful formulations of walks\u2026walks in latent spaces, that is, make sure to have a look at the paper in the video description. Also, note that we have now appeared on instagram with bite-sized pieces of our bite-sized videos. Yes, it is quite peculiar. Make sure to check it out, just search for two minute papers on instagram or click the link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Jnj7OmmOm2Y",
        "paper_link": "https://ali-design.github.io/gan_steerability/\n\nThe paper \"Learning a Manifold of Fonts\" and its demo are available here:\nhttp://vecg.cs.ucl.ac.uk/Projects/projects_fonts/projects_fonts.html",
        "paper_title": "On the steerability of generative adversarial networks"
    },
    {
        "video_id": "-ryF7237gNo",
        "video_title": "This Adorable Baby T-Rex AI Learned To Dribble \ud83e\udd96",
        "position_in_playlist": 367,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers \n\n\ud83d\udcdd The paper \"MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\" is available here:\nhttps://xbpeng.github.io/projects/MCP/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. About 350 episodes ago in this series, in episode number 8, we talked about an amazing paper in which researchers built virtual characters with a bunch of muscles and joints, and through the power of machine learning, taught them to actuate them just the right way so that they would learn to walk. Well, some of them anyway. Later, we\u2019ve seen much more advanced variants where we could even teach them to lift weights, jump really high, or even observe how their movements would change after they undergo surgery. This paper is a huge step forward in this area, and if you look at the title, it says that it proposes multiplicative composition policies to control these characters. What this means is that these complex actions are broken down into a sum of elementary movements. Intuitively you can imagine something similar when you see a child use small, simple lego pieces to build a huge, breathtaking spaceship. That sounds great, but what does this do for us? Well, the ability to properly combine these lego pieces is where the learning part of the technique shines, and you can see on the right that these individual lego pieces are as amusing as useless if they\u2019re not combined with others. To assemble efficient combinations that are actually useful, the characters are first required to learn to perform reference motions using combinations of these lego pieces. Here, on the right, the blue bars show which of these these lego pieces are used and when in the current movement pattern. Now, we\u2019ve heard enough of these legos, what is this whole compositional thing good for? Well, a key advantage of using these is that they are simple enough so that they can be transferred and reused for other types of movement. As you see here, this footage demonstrates how we can teach a biped, or even a T-Rex to carry and stack boxes or how to dribble, or, how to score a goal. Amusingly, according to the paper, it seems that this T-Rex weighs only 55 kilograms or 121 pounds. An adorable baby T-Rex, if you will. As a result of this transferability property, when we assemble a new agent or wish to teach an already existing character some new moves, we don\u2019t have to train them from scratch as they already have access to these lego pieces. I love seeing all these new papers in the intersection of computer graphics and machine learning. This is a similar topic to what I am working on as a full-time research scientist at the Technical University of Vienna, and in these projects, we train plenty of neural networks, which requires a lot of computational resources. Sometimes when we have to spend time maintaining the machines running these networks, buying new hardware or troubleshooting software issues, I wish we could use Linode. Linode is the world\u2019s largest independent cloud hosting and computing provider, and they have GPU instances that are tailor-made for AI, scientific computing and computer graphics projects. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To reserve your GPU instance and receive\u00a0a $20 free credit, visit\u00a0linode.com/papers\u00a0or click the link in the description and use the promo code \u201cpapers20\u201d during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-ryF7237gNo",
        "paper_link": "https://xbpeng.github.io/projects/MCP/",
        "paper_title": "MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies"
    },
    {
        "video_id": "AOZw1tgD8dA",
        "video_title": "Adversarial Attacks on Neural Networks - Bug or Feature?",
        "position_in_playlist": 368,
        "description": "\u2764\ufe0f Support us on Patreon: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"Adversarial Examples Are Not Bugs, They Are Features\" is available here:\nhttp://gradientscience.org/adv/\n\nThe Distill discussion article is available here:\nhttps://distill.pub/2019/advex-bugs-discussion/\n\nIf you wish to play with some of these Distill articles, look here:\n- https://distill.pub/2017/feature-visualization/\n- https://distill.pub/2018/building-blocks/\n\nAndrej Karpathy\u2019s image classifier - you can run this in your web browser: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-2981865/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This will be a little non-traditional video where the first half of the episode will be about a paper, and the second part will be about\u2026something else. Also a paper. Well, kind of. You\u2019ll see. We\u2019ve seen in the previous years that neural network-based learning methods are amazing at image classification, which means that after training on a few thousand training examples, they can look at a new, previously unseen image and tell us whether it depicts a frog or a bus. Earlier we have shown that we can fool neural networks by adding carefully crafted noise to an image, which we often refer to as an adversarial attack on a neural network. If done well, this noise is barely perceptible and, get this, can fool the classifier into looking at a bus and thinking that it is an ostrich. These attacks typically require modifying a large portion of the input image, so when talking about a later paper, we were thinking, what could be the lowest number of pixel changes that we have to perform to fool a neural network? What is the magic number? Based on the results of previous research works, an educated guess would be somewhere around a hundred pixels. A followup paper gave us an unbelievable answer by demonstrating the one pixel attack. You see here that by changing only one pixel in an image that depicts a horse, the AI will be 99.9% sure that we are seeing a frog. A ship can also be disguised as a car, or, amusingly, with a properly executed one-pixel attack, almost anything can be seen as an airplane by the neural network. And, this new paper discusses whether we should look at these adversarial examples as bugs or not, and of course, does a lot more than that! It argues that most datasets contain features that are predictive, meaning that they provide help for a classifier to find cats, but also non-robust, which means that they provide a rather brittle understanding that falls apart in the presence adversarial changes. We are also shown how to find and eliminate these non-robust features from already existing datasets and that we can build much more robust classifier neural networks as a result. This is a truly excellent paper that sparked quite a bit of discussion. And here comes the second part of the video with the something else. An interesting new article was published within the Distill journal, a journal where you can expect clearly worded papers with beautiful and interactive visualizations. But this is no ordinary article, this is a so-called discussion article where a number of researchers were asked to write comments on this paper and create interesting back and forth discussions with the original authors. Now, make no mistake, the paper we\u2019ve talked about was peer-reviewed, which means that independent experts have spent time scrutinizing the validity of the results, so this new discussion article was meant to add to it by getting others to replicate the results and clear up potential misunderstandings. Through publishing six of these mini-discussions, each of which were addressed by the original authors, they were able to clarify the main takeaways of the paper, and even added a section of non-claims as well. For instance, it\u2019s been clarified that they don\u2019t claim that adversarial examples arise from software bugs. A huge thanks to the Distill journal and all the authors who participated in this discussion, and Ferenc Husz\u00e1r, who suggested the idea of the discussion article to the journal. I\u2019d love to see more of this, and if you do too, make sure to leave a comment so we can show them that these endeavors to raise the replicability and clarity of research works are indeed welcome. Make sure to click the link to both works in the video description, and spend a little quality time with them. You\u2019ll be glad you did. I think this was a more complex than average paper to talk about, however, as you have noticed, the usual visual fireworks were not there. As a result, I expect this to get significantly fewer views. That\u2019s not a great business model, but no matter, I made this channel so I can share with you all these important lessons that I learned during my journey. This has been a true privilege and I am thrilled that I am still able to talk about all these amazing papers without worrying too much whether any of these videos will go viral or not. Videos like this one are only possible because of your support on Patreon.com/TwoMinutePapers. If you feel like chipping in, just click the Patreon link in the video description. This is why every video ends with, you know what\u2019s coming\u2026 Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=AOZw1tgD8dA",
        "paper_link": "http://gradientscience.org/adv/",
        "paper_title": "Adversarial Examples Are Not Bugs, They Are Features"
    },
    {
        "video_id": "qkHK1QdQ2Fk",
        "video_title": "This AI Clears Up Your Hazy Photos",
        "position_in_playlist": 369,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\n\ud83d\udcdd The paper \"Double-DIP: Unsupervised Image Decomposition via Coupled Deep-Image-Priors\" is available here:\nhttp://www.wisdom.weizmann.ac.il/~vision/DoubleDIP/\nhttps://github.com/yossigandelsman/DoubleDIP\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to talk about a paper that builds on a previous work by the name Deep Image Priors, DIP in short. This work was capable of performing JPEG compression artifact removal, image inpainting, or in other words, filling in parts of the image with data that makes sense, super resolution, and image denoising. It was quite the package. This new method is able to subdivide an image into a collection of layers, which makes it capable of doing many seemingly unrelated tasks, for instance, one, it can do image segmentation, which typically means producing a mask that shows us the boundaries between the foreground and background. As an additional advantage, it can also do this for videos as well. Two, it can perform dehazing, which can also be thought of as a decomposition task where the input is one image, and the output is an image with haze, and one with the objects hiding behind the haze. If you spend a tiny bit of time looking out the window on a hazy day, you will immediately see that this is immensely difficult, mostly because of the fact that the amount of haze that we see is non-uniform along the landscape. The AI has to detect and remove just the right amount of this haze and recover the original colors of the image. And three, it can also subdivide these crazy examples where two images are blended together. In a moment, I\u2019ll show you a better example with a complex texture where it is easier to see the utility of such a technique. And four, of course, it can also perform image inpainting, which, for instance, can help us remove watermarks or other unwanted artifacts from our photos. This case can also be thought of an image layer plus a watermark layer, and in the end, the algorithm is able to recover both of them. As you see here on the right, a tiny part of the content seems to bleed into the watermark layer, but the results are still amazing. It does this by using multiple of these DIPs, deep image prior networks, and goes by the name DoubleDIP. That one got me good when I\u2019ve first seen it. You see here how it tries to reproduce this complex textured pattern as a sum of these two, much simpler individual components. The supplementary materials are available right in your browser, and show you a ton of comparisons against other previous works. Here you see results from these earlier works on image dehazing and see that indeed, the new results are second to none. And all this progress within only two years. What a time to be alive! If like me, you love information theory, woo-hoo! Make sure to have a look at the paper and you\u2019ll be a happy person. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It is like a shared logbook for your team, and with this, you can compare your own experiment results, put them next to what your colleagues did and you can discuss your successes and failures much easier. It takes less than 5 minutes to set up and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It was also used in this OpenAI project that you see here, which we covered earlier in the series. They reported that experiment tracking was crucial in this project and that this tool saved them quite a bit of time and money. If only I had an access to such a tool during our last research project where I had to compare the performance of neural networks for months and months. Well, it turns out, I will be able to get access to these tools, because, get this, it\u2019s free and will always be free for academics and open source projects. Make sure to visit them through wandb.com/papers or just click the link in the video description and sign up for a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=qkHK1QdQ2Fk",
        "paper_link": "http://www.wisdom.weizmann.ac.il/~vision/DoubleDIP/",
        "paper_title": "Double-DIP: Unsupervised Image Decomposition via Coupled Deep-Image-Priors"
    },
    {
        "video_id": "duo-tHbSdMk",
        "video_title": "New Face Swapping AI Creates Amazing DeepFakes",
        "position_in_playlist": 370,
        "description": "\ud83d\udcdd The paper \"FSGAN: Subject Agnostic Face Swapping and Reenactment\" is available here:\nhttps://nirkin.com/fsgan/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepFakes #FaceSwap",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Recently, we have experienced an abundance of papers on facial reenactment in machine learning research. We talked about a technique by the name Face2Face back in 2016, approximately 300 videos ago. It was able to take a video of us and transfer our gestures to a target subject. This was kind of possible at the time with specialized depth cameras, until Face2Face appeared and took the world by storm as it was able to perform what you see here with a regular consumer camera. However, it only transferred gestures, so of course, scientists were quite excited about the possibility of transferring more than just that. But, that would require solving so many more problems - for instance, if we wish to turn the head of the target subject, we may need to visualize regions that we haven\u2019t seen in these videos, which also requires an intuitive understanding of hair, the human face and more. This is quite challenging. So, can this be really done? Well, have a look at this amazing new paper! You see here the left image, this is the source person, the video on the right is the target video, and our task is to transfer not just the gestures, but the pose, gestures and appearance of the face on the left to the video on the right. And, this new method works like magic. Look! It not only works like magic, but pulls it off on a surprisingly large variety of cases, many of which I haven\u2019t expected at all. Now, hold on to your papers, because this technique was not trained on these subjects, which means that this is the first time it is seeing these people. It has been trained on plenty of people, but not these people. Now, before we look at this example, you are probably saying, well, the occlusions from the microphone will surely throw the algorithm off, right? Well, let\u2019s have a look. Nope, no issues at all. Absolutely amazing, love it! So how does this wizardry work exactly? Well, it requires careful coordination between no less than four neural networks, where each of which specializes for a different task. The first two is a reenactment generator that produces a first estimation of the reenacted face, and a segmentation generator network that creates this colorful image that shows which region in the image corresponds to which facial landmark. These two are then handed over to the third network, the inpainting generator, which fills the rest of the image, and since we have overlapping information, in comes the fourth, blending generator to the rescue to combine all this information into our final image. The paper contains a detailed description of each of these networks, so make sure to have a look! And if you do, you will also find that there are plenty of comparisons against previous works, of course, Face2Face is one of them, which was already amazing, and you can see how far we\u2019ve come in only three years. Now, when we try to evaluate such a research work, we are curious as to how much these individual puzzle pieces, in this case, the generator networks contribute to the final results. Are really all of them needed? What if we remove some of them? Well, this is a good paper, so we can find the answer in Table 2, where all of these components are tested in isolation. The downward and upward arrows show which measure is subject to minimization and maximization, and if we look at this column, it is quite clear that all of them indeed improve the situation, and contribute to the final results. And remember, all this from just one image of the source person. Insanity. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=duo-tHbSdMk",
        "paper_link": "https://nirkin.com/fsgan/",
        "paper_title": "FSGAN: Subject Agnostic Face Swapping and Reenactment"
    },
    {
        "video_id": "uVC5WowQxD8",
        "video_title": "This is How You Simulate Making Pasta \ud83c\udf5c",
        "position_in_playlist": 371,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers \n\n\ud83d\udcf7 Check us out on Instagram: https://www.instagram.com/twominutepapers/\n\n\ud83d\udcdd The paper \"A Multi-Scale Model for Coupling Strands with Shear-Dependent Liquid \" is available here:\nhttp://www.cs.columbia.edu/cg/creamystrand/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Fluid simulation is a mature research field within computer graphics with amazing papers that show us how to simulate water flows with lots of debris, how to perform liquid-fabric interactions, and more. This new project further improves the quality of these works and shows us how thin, elastic strands interact with oil paint, mud, melted chocolate, and pasta sauce. There will be plenty of tasty and messy simulations ahead, not necessarily in that order, so, make sure to hold on to your papers, just in case. Here you see four scenarios of these different materials dripping off of a thin strand. So, why are these cases difficult to simulate? The reason why it\u2019s difficult, if not flat out impossible because the hair strands and the fluid layers are so thin, it would require a simulation grid that is so microscopic, or in other words, we would have to perform our computations of quantities like pressure and velocity on so many grid points, it would probably take not from hours to days, but from weeks to years to compute. I will show you a table in a moment where you will see that these amazingly detailed simulations can be done on a grid of surprisingly low resolution. As a result, our simulations also needn\u2019t be so tiny in scale with one hair strand and a few drops of mud or water. They can be done on a much larger scale, so we can marvel together at these tasty and messy simulations, you decide which is which. I particularly liked this animation with the oyster sauce because you can also see a breakdown of the individual elements of the simulation. Note that all of the interactions between the noodles, the sauce, the fork, and plate have to be simulated with precision. Love it. And now, the promised table. Here you can see the delta x that means how fine the grid resolution is, which is in the order of centimeters, and not micrometers. That is reassuring, and don\u2019t forget that this work is an extension to the Material Point Method, which is a hybrid simulation method that both uses grids and particles. And, sure enough, you can see here that it simulates up to tens of millions of particles as well, and the fact that the computation times are still only measured in a few minutes per frame is absolutely remarkable. Remember, the fact that we can simulate this at all is a miracle. Now, this was run on the processor, and a potential implementation on the graphics card could yield us significant speedups, so I really hope something like this appears in the near future. Also, make sure to have a look at the paper itself, which is outrageously well written. If you wish to see more from this paper, make sure to follow us on Instagram, just search for Two Minute Papers there or click the link in the description. Now, I am still working as a full-time research scientist at the Technical University of Vienna, and we train plenty of neural networks during our projects, which requires a lot of computational resources. Every time we have to spend our time maintaining these machines, I wish we could use Linode. Linode is the world\u2019s largest independent cloud hosting and computing provider. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To reserve your GPU instance and receive\u00a0a $20 free credit, visit\u00a0linode.com/papers\u00a0or click the link in the description and use the promo code \u201cpapers20\u201d during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=uVC5WowQxD8",
        "paper_link": "http://www.cs.columbia.edu/cg/creamystrand/",
        "paper_title": "A Multi-Scale Model for Coupling Strands with Shear-Dependent Liquid "
    },
    {
        "video_id": "zrF5_O92ELQ",
        "video_title": "These Are The 7 Capabilities Every AI Should Have",
        "position_in_playlist": 372,
        "description": "\u2764\ufe0f Thank you so much for your support on Patreon: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"Behaviour Suite for Reinforcement Learning\" is available here:\nhttps://arxiv.org/abs/1908.03568\nhttps://github.com/deepmind/bsuite\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A few years ago, scientists at DeepMind published a learning algorithm that they called deep reinforcement learning which quickly took the world by storm. This technique is a combination of a neural network that processes the visual data that we see on the screen, and a reinforcement learner that comes up with the gameplay-related decisions, which proved to be able to reach superhuman performance on computer games like Atari Breakout. This paper not only sparked quite a bit of mainstream media interest, but also provided fertile grounds for new followup research works to emerge. For instance, one of these followup papers infused these agents with a very human-like quality, curiosity, further improving many aspects of the original learning method, however, had a disadvantage, I kid you not, it got addicted to the TV and kept staring at it forever. This was perhaps a little too human-like. In any case, you may rest assured that this shortcoming has been remedied since, and every followup paper recorded their scores on a set of Atari games. Measuring and comparing is an important part of research and is absolutely necessary so we can compare new learning methods more objectively. It\u2019s like recording your time for the olympics at the 100 meter dash. In that case, it\u2019s quite easy to decide which athlete is the best. However, this is not so easy in AI research. In this paper, scientists at DeepMind note that just recording the scores doesn\u2019t give us enough information anymore. There\u2019s so much more to reinforcement learning algorithms than just scores. So, they built a behavior suite that also evaluates the 7 core capabilities of reinforcement learning algorithms. Among these 7 core capabilities, they list generalization, which tells us how well the agent is expected to do in previously unseen environments, how good it is at credit assignment, which is a prominent problem in reinforcement learning. Credit assignment is very tricky to solve because, for instance, when we play a strategy game, we need to make a long sequence of strategic decisions, and in the end, if we lose an hour later, we have to figure out which one of these many-many decisions led to our loss. Measuring this as one of the core capabilities, was, in my opinion, a great design decision here. How well the algorithm scales to larger problems also gets a spot as one of these core capabilities. I hope this testing suite will see widespread adoption in reinforcement learning research, and what I am really looking forward to is seeing these radar plots for newer algorithms, which will quickly reveal whether we have a new method that takes a different tradeoff than previous methods, or in other words, has the same area within the polygon, but with a different shape, or, in the case of a real breakthrough, the area of these polygons will start to increase. Luckily, a few of these charts are already available in the paper and they give us so much information about these methods, I could stare at them all day long and I cannot wait to see some newer methods appear here. Now note that there is a lot more to this paper, if you have a look at it in the video description, you will also find the experiments that are part of this suite, what makes a good environment to test these agents in, and that they plan to form a committee of prominent researchers to periodically review it. I loved that part. If you enjoyed this video, please consider supporting us on Patreon. If you do, we can offer you early access to these videos so you can watch them before anyone else, or, you can also get your name immortalized in the video description. Just click the link in the description if you wish to chip in. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=zrF5_O92ELQ",
        "paper_link": "https://arxiv.org/abs/1908.03568",
        "paper_title": "Behaviour Suite for Reinforcement Learning"
    },
    {
        "video_id": "0OtZ8dUFxXA",
        "video_title": "OpenAI\u2019s GPT-2 Is Now Available - It Is Wise as a Scholar! \ud83c\udf93",
        "position_in_playlist": 373,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nWeights & Biases blog post (the notebook is available too!)\n- https://www.wandb.com/articles/visualize-xgboost-in-one-line\n- https://colab.research.google.com/drive/1SPludDkpAPonmdDdRlIJ7d5jB2FwVfZn#scrollTo=kszahG2PBN5R\n\nTry GPT-2 yourself here and post your results in the comments if you've found anything interesting. https://talktotransformer.com/\n\nCheck out this GPT-2 implementation too (thanks Robert Miles for the link!) - write something, then tab, enter, tab, enter and so on: https://transformer.huggingface.co/doc/gpt2-large\n\nOpenAI's post: https://openai.com/blog/gpt-2-6-month-follow-up/\nTweet source: https://twitter.com/gdm3000/status/1151469462614368256\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bruno Brito, Bryan Learn, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#OpenAI #GPT2 #GPT3",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. OpenAI GPT-2 is a learning-based technique which can perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. What is absolutely amazing about this technique is that it is able to perform these tasks with as little supervision as possible. This means that we unleash the algorithm that they call GPT-2 to read the internet, and it will learn our language by itself. To perform this, of course, we need a lot of training data, and here, the AI reads 40 gigabytes of internet text, which is 40 gigs of non-binary plaintext data, which is a stupendously large amount of text. You see here an example of how it fares with text completion, and the results were absolutely amazing. And, not only that, because, hold on to your papers, we can now try it ourselves and my results show that it can be wise as a scholar, and it can also cheat like nobody\u2019s business. So, let\u2019s try something simple here. Well, first, as everyone is talking about job security, I\u2019d like to give it the start of this Two Minute Papers script and see if I am about to be replaced. Let\u2019s see! It continues the intro with \u201cThe history of Eastern Europe since 1989 has been an interesting and complex story spanning four decades. On one side you have a regime of Western-backed dictators from the mid-1990s\u201d and so on. Very scholarly indeed. However, I think I might be safe \u2026for now. And now, something that I wanted to try since the inception of GPT-2, which is, feed it a snippet of something that I said about Wavelet Turbulence, which is one of the best papers ever written, and have the AI finish it. I tried to frame it as an exam question and was not disappointed be the results. So the text is, \u201cThere was a similar technique by the name Wavelet Turbulence, which is\u201d, I hope you see what I did there\u2026and look what it did! \u201cThere was a similar technique by the name Wavelet Turbulence, which is not at all what we are talking about, but I'll let you decide whether it is worth your time to read it.\u201d Wow! GPT-2 has been caught cheating red handed. I love it. Let\u2019s continue what it had to say: \u201cAnyway, my aim here is to describe how we use a fluid simulation to simulate smoke and liquid effects using just a few simple equations: Solving a few functions gives us a set of derivatives of the particle trajectories, that is equations that describe how and where the particles will come and go across the liquid in the simulation. For simplicity (as a good teacher of high-school students should know) we can use a simple equation, which simulates two particles that collide.\u201d I am stunned. It recognized that we are talking about fluid simulations, which is already remarkable, but it went much further - The completion is not bad at all and is not only coherent, on topic, but has quite a bit of truth to it. I will have to rethink my previous claim about my job security. The even crazier thing is that the size of this model is about 750 million parameters, which is only half of the size of the original full model, which is expected to be even better. I put a link to this website in the video description for your pleasure, make sure to play with it, this is mad fun. And, GPT-2 will also see so many applications that we cannot even fathom yet. For instance, here you can see that one can train it on many source code files on GitHub and it will be able to complete the code that we write on the fly. Now, nobody should think of this as GPT-2 writing programs for us, this is, of course, unlikely, however, it will ease the process for novice and experts users alike. If you have any other novel applications in mind, make sure to leave a comment below. For now, bravo OpenAI, and a big thank you for Daniel King and the Hugging Face company for this super convenient public implementation. Let the  experiments begin! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=0OtZ8dUFxXA"
    },
    {
        "video_id": "Uk9p4Kk98_g",
        "video_title": "Google's AI Plays Football\u2026For Science! \u26bd\ufe0f",
        "position_in_playlist": 374,
        "description": "\ud83d\udcdd The paper \"Google Research Football: A Novel Reinforcement Learning Environment\" is available here:\nhttps://arxiv.org/abs/1907.11180\nhttps://github.com/google-research/football\nhttps://ai.googleblog.com/2019/06/introducing-google-research-football.html\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is an important subfield within machine learning research where we teach an agent to choose a set of actions in an environment to maximize a score. This enables these AIs to play Atari games at a superhuman level, control drones, robot arms, or even create self-driving cars. A few episodes ago, we talked about DeepMind\u2019s behavior suite that opened up the possibility of measuring how these AIs perform with respect to the 7 core capabilities of reinforcement learning algorithms. Among them were how well such an AI performs when being shown a new problem, how well or how much they memorize, how willing they are to explore novel solutions, how well they scale to larger problems, and more. In the meantime, the Google Brain research team has also been busy creating a physics-based 3D football, or for some of you, soccer simulation where we can ask an AI to control one, or multiple players in this virtual environment. This is a particularly difficult task because it requires finding a delicate balance between rudimentary short-term control tasks, like passing, and long-term strategic planning. In this environment, we can also test our reinforcement learning agents against handcrafted, rule-based teams. For instance, here you can see that DeepMind\u2019s Impala algorithm is the only one that can reliably beat the medium and hard handcrafted teams, specifically, the one that was run for 500 million training steps. The easy case is tuned to be suitable for single-machine research works, where the hard case is meant to challenge sophisticated AIs that were trained on a massive array of machines. I like this idea a lot. Another design decision I particularly like here is that these agents can be trained from pixels or internal game state. Okay, so what does that really mean? Training from pixels is easy to understand but very hard to perform - this simply means that the agents see the same content as what we see on the screen. DeepMind\u2019s Deep Reinforcement Learning is able to do this by training a neural network to understand what events take place on the screen, and passes, no pun intended all this event information to a reinforcement learner that is responsible for the strategic, gameplay-related decisions. Now, what about the other one? The internal game state learning means that the algorithm sees a bunch of numbers which relate to quantities within the game, such as the position of all the players and the ball, the current score and so on. This is typically easier to perform because the AI is given high-quality and relevant information and is not burdened with the task of visually parsing the entire scene. For instance, OpenAI\u2019s amazing DOTA2 team learned this way. Of course, to maximize impact, the source code for this project is also available. This will not only help researchers to train and test their own reinforcement learning algorithms on a challenging scenario, but they can extend it and make up their own scenarios. Now note that so far, I tried my hardest not to comment on the names of the players and the teams, but my will to resist just ran out. Go real Bayesians! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Uk9p4Kk98_g",
        "paper_link": "https://arxiv.org/abs/1907.11180",
        "paper_title": "Google Research Football: A Novel Reinforcement Learning Environment"
    },
    {
        "video_id": "leoRHsBsv6Q",
        "video_title": "Finally, Style Transfer For Smoke Simulations! \ud83d\udca8",
        "position_in_playlist": 375,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Transport-Based Neural Style Transfer for Smoke Simulations\" is available here:\nhttp://www.byungsoo.me/project/neural-flow-style/index.html\n\n \ud83d\udca8 My fluid control paper and its source code (and Blender implementation) is available here, pick it up if you're interested! If I remember correctly, you will have to be able to compile Blender. - https://users.cg.tuwien.ac.at/zsolnai/gfx/real_time_fluid_control_eg/\n(If you improved this in some way, please let me know!)\n\nWavelet Turbulence - one of the best papers ever written:\nhttp://www.tkim.graphics/WTURB/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I can confidently say that this is the most excited I\u2019ve been for a smoke simulation paper since Wavelet Turbulence. Wavelet Turbulence is a magical algorithm from 2008 that takes a low-quality fluid or smoke simulation and increases its quality by filling in the remaining details. And here we are, 11 years later, the results still hold up. Insanity. This is one of the best papers ever written and has significantly contributed to my decision to pursue a research career. And, this new work performs style transfer for smoke simulations. If your haven\u2019t fallen out of your chair yet, let me try to explain why this is amazing. Style transfer is a technique in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to the super fun results that you see here. An earlier paper had shown that the more sophisticated ones can make even art curators think that they are real. However, doing this for smoke simulations is a big departure from 2D style transfer, because that one takes an image, where this works in 3D, and does not deal with images, but with density fields. A density field means a collection of numbers that describe how dense a smoke plume is at a given spatial position. It is a physical description of a smoke plume, if you will. So how could we possibly apply artistic style from an image to a collection of densities? This doesn\u2019t sound possible at all. Unfortunately, the problem gets even worse. Since we typically don\u2019t just want to look at a still image of a smoke plume, but enjoy a physically correct simulation, not only the density fields, but the velocity fields and the forces that animate them over time also have to be stylized. Hmm. Again, that\u2019s either impossible, or almost impossible to do. You see, if we run a proper smoke simulation, we\u2019ll see what would happen in reality, but that\u2019s not stylized. However, if we stylize, we get something that would not happen in mother nature. I have spent my master\u2019s thesis trying to solve a problem called fluid control, which would try to coerce a smoke plume or a piece of fluid to take a given shape. Like a bunny, or a logo with letters. You can see some footage of what I came up with here. Here, both the simulation and the controlling force field is computed in real time on the graphics card and as you see, it can be combined with Wavelet Turbulence. If you wish to hear more about this work, make sure to leave a comment, but in any case, I had a wonderful time working on it, if anyone wants to pick it up, the paper and the source code, and even a Blender addon version are available in the video description. In any case, in a physics simulation, we\u2019re trying to simulate reality, and for style transfer, we\u2019re trying to depart from reality. The two are fundamentally incompatible, and we have to reconcile them in a way that is somehow still believable. Super challenging. However, back then when I wrote the fluid control paper, learning-based algorithms were not nearly as developed, so it turns out, they can help us perform style transfer for density fields, and also, animate them properly. Again, the problem definition is very easy, in comes a smoke plume, we add an image for style, and the style of this image is somehow applied to the density field to get these incredible effects. Just look at these marvelous results. Fire textures, starry night, you name it. It seems to be able to do anything! One of the key ideas is that even though style transfer is challenging on highly detailed density fields, but it becomes much easier if we first downsample the density field to a coarser version, perform the style transfer there, and upsample this density field again with already existing techniques. Rinse and repeat. The paper also describes a smoothing technique that ensures that the changes in the velocity fields that guide our density fields change slowly over time to keep the animation believable. There are a lot more new ideas in the paper, so make sure to have a look! It also takes a while, the computation time is typically around 10 to 15 minutes per frame, but who cares! Today, with the ingenuity of research scientists and the power of machine learning algorithms, even the impossible seems possible. If it takes 15 minutes per frame, so be it. What a time to be alive! Thanks for watching and for  your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=leoRHsBsv6Q",
        "paper_link": "http://www.byungsoo.me/project/neural-flow-style/index.html",
        "paper_title": "Transport-Based Neural Style Transfer for Smoke Simulations"
    },
    {
        "video_id": "RoGHVI-w9bE",
        "video_title": "DeepFake Detector AIs Are Good Too!",
        "position_in_playlist": 376,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"FaceForensics++: Learning to Detect Manipulated Facial Images\" is available here:\nhttp://www.niessnerlab.org/projects/roessler2019faceforensicspp.html\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepFake #DeepFakes #FaceSwap",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. We talked about a technique by the name Face2Face back in 2016, approximately 300 videos ago. It was able to take a video of us and transfer our gestures to a target subject. With techniques like this, it\u2019s now easier and cheaper than ever to create these deepfake videos of a target subject provided that we have enough training data, which is almost certainly the case for people who are the most high-value targets for these kinds of operations. Look here. Some of these videos are real, and some are fake. What do you think, which is which? Well, here are the results - this one contains artifacts and is hence easy to spot, but the rest\u2026it\u2019s tough. And it\u2019s getting tougher by the day. How many did you get right? Make sure to leave a comment below. However, don\u2019t despair, it\u2019s not all doom and gloom. Approximately a year ago, in came FaceForensics, a paper that contains a large dataset of original and manipulated video pairs. As this offered a ton of training data for real and forged videos, it became possible to train a deepfake detector. You can see it here in action as these green to red colors showcase regions that the AI correctly thinks were tampered with. However, this followup paper by the name FaceForensics++ contains not only not only an improved dataset, but provides many more valuable insights to help us detect these DeepFake videos, and even more. Let\u2019s dive in. Key insight number one. As you\u2019ve seen a minute ago, many of these DeepFake AIs introduce imperfections, or in other words, artifacts to the video. However, most videos that we watch on the internet are compressed, and the compression procedure\u2026you have guessed right, also introduces artifacts to the video. From this, it follows that hiding these DeepFake artifacts behind compressed videos sounds like a good strategy to fool humans and detector neural networks likewise, and not only that, but the paper also shows us by how much exactly. Here you see a table where each row shows the detection accuracy of previous techniques and a new proposed one, and the most interesting part is how this accuracy drops when we go from HQ to LQ, or in other words, from a high-quality video to a lower-quality one with more compression artifacts. Overall, we can get an 80-95% success rate, which is absolutely amazing. But, of course, you ask, amazing compared to what? Onwards to insight number two. This chart shows how humans fared in DeepFake detection, as you can see, not too well. Don\u2019t forget, the 50% line means that the human guesses were as good as a coinflip, which means that they were not doing well at all. Face2face hovers around this ratio, and if you look at NeuralTextures, you see that this is a technique that is extremely effective at fooling humans. And wait\u2026what\u2019s that? For all the other techniques, we see that the grey bars are shorter, meaning that it\u2019s more difficult to find out if a video is a DeepFake because its own artifacts are hidden behind the compression artifacts. But the opposite is the case for NeuralTextures, perhaps because its small footprint on the videos. Note that a state of the art detector AI, for instance, the one proposed in this paper does way better than these 204 human participants. This work does not only introduce a dataset, these cool insights, but also introduces a detector neural network. Now, hold on to your papers because this detection pipeline is not only so powerful that it practically eats compressed DeepFakes for breakfast, but it even tells us with remarkable accuracy which method was used to tamper with the input footage. Bravo! Now, it is of utmost importance that we let the people know about the existence of these techniques, this is what I am trying to accomplish with this video. But that\u2019s not enough, so I also went to this year\u2019s biggest NATO conference and made sure that political and military decision makers are also informed about this topic. Last year, I went to the European Political Strategy Center with a similar goal. I was so nervous before both of these talks and spent a long time rehearsing them, which delayed a few videos here on the channel. However, because of your support on Patreon, I am in a fortunate situation where I can focus on doing what is right and what is the best is for all of us, and not worry about the financials all the time. I am really grateful for that, it really is a true privilege. Thank you. If you wish to support us, make sure to click the Patreon link in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=RoGHVI-w9bE",
        "paper_link": "http://www.niessnerlab.org/projects/roessler2019faceforensicspp.html",
        "paper_title": "FaceForensics++: Learning to Detect Manipulated Facial Images"
    },
    {
        "video_id": "7SM816P5G9s",
        "video_title": "Is a Realistic Honey Simulation Possible? \ud83c\udf6f",
        "position_in_playlist": 377,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\n\ud83d\udcdd The paper \"A Geometrically Consistent Viscous Fluid Solver with Two-Way Fluid-Solid Coupling\" is available here:\nhttp://gamma.cs.unc.edu/ViscTwoway/\n\nThe Weights & Biases posts on the Witness (and code!) are available here:\nhttps://www.wandb.com/articles/i-trained-a-robot-to-play-the-witness\nhttps://github.com/wandb/witness\n\nMy earlier video on The Witness game:\nhttps://www.youtube.com/watch?v=Ee9vF5eChhU\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we study the laws of fluid motion from physics, and write a computer program that contains these laws, we can create wondrous fluid simulations like the ones you see here. The amount of detail we can simulate with these programs is increasing every year, not only due to the fact that hardware improves over time, but also, the pace of progress in computer graphics research is truly remarkable. So, it s there nothing else to do? Are we done with fluid simulation research? Oh, no. No, no, no. For instance, fluid-solid interaction still remains a challenging phenomenon to simulate. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. This is what we refer to as two-way coupling. Note that this previous work that you see here was built on the Material Point Method, a hybrid simulation technique that uses both particles and grids, whereas this new paper introduces proper fluid-solid coupling to the simpler, grid-based simulators. Not only that, but this new work also shows us that there are different kinds of two-way coupling. If we look at this footage with the honey and the dipper, it looks great, however, this still doesn\u2019t seem right to me. We are doing science here, so fortunately, we don\u2019t need to guess what seems or doesn\u2019t seem right. This is my favorite part, because this is when we let reality be our judge and compare to what exactly happens in the real world. So let\u2019s do that! Whoa! There is quite a bit of a difference, because in reality, the honey is able to support the dipper. One-way coupling, of course, cannot simulate this kind of back and forth interaction, and neither can weak two-way coupling pull this off. And now, let\u2019s see, YES! There we go, the new strong two-way coupling method finally gets this right. And not only that, but what I really love about this is that it also gets small nuances right. I will try to speed up the footage a little, so you can see that the honey doesn\u2019t only support the dipper, but the dipper still has some subtle movements both in reality and in the simulation. A+. Love it. So, what is the problem? Why is this so difficult to simulate? One of the key problems here is being able to have a simulation that has a fine resolution in the areas where a fluid and a solid intersect each other. If we create a super detailed simulation, it will take from hours to days to compute, but on the other hand, if we have a too coarse one, it will compute the required deformations in so few of these grid points that we\u2019ll get a really inaccurate simulation, and not only that, but we will even miss some of the interactions completely. This paper proposes a neat new volume estimation technique that focuses these computations to where the action happens, and only there, which means that we can get these really incredible results, even if we only run a relatively coarse simulation. I could watch these gooey, viscous simulations all day long. If you have a closer look at the paper in the description, you will find some hard data that shows that this technique executes quicker than other methods that are able to provide comparable results. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7SM816P5G9s",
        "paper_link": "http://gamma.cs.unc.edu/ViscTwoway/",
        "paper_title": "A Geometrically Consistent Viscous Fluid Solver with Two-Way Fluid-Solid Coupling"
    },
    {
        "video_id": "882O_7hsAms",
        "video_title": "AI Learns Human Movement From Unorganized Data \ud83c\udfc3\u200d\u2640\ufe0f",
        "position_in_playlist": 378,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Learning Predict-and-Simulate Policies From Unorganized Human Motion Data\" is available here:\nhttp://mrl.snu.ac.kr/publications/ProjectICC/ICC.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Last year, an amazing neural network-based technique appeared that was able to look at a bunch of unlabeled motion data, and learned to weave them together to control the motion of quadrupeds, like this wolf here. It was able to successfully address the shortcomings of previous works, for instance, the weird sliding motions have been eliminated, and it was also capable of following some predefined trajectories. This new paper continues research in this direction by proposing a technique that is also capable of interacting with its environment or other characters, for instance, they can punch each other, and after the punch, they can recover from undesirable positions, and more. The problem formulation is as follows - it is given the current state of the character and a goal, and you see here with blue how it predicts the motion to continue. It understands that we have to walk towards the goal, that we are likely to fall when hit by a ball, and it knows, that then, we have to get up and continue our journey, and eventually, reach our goal. Some amazing life advice from the AI right there. The goal here is also to learn something meaningful from lots of barely labeled human motion data. Barely labeled means that a bunch of videos are given almost as-is, without additional information on what movements are being performed in these videos. If we had labels for all this data that you see here, it would say that this sequence shows a jump, and these ones are for running. However, the labeling process takes a ton of time and effort, so if we can get away without it, that\u2019s glorious, but, in return, with this, we create an additional burden that the learning algorithm has to shoulder. Unfortunately, the problem gets even worse - as you see here, the number of frames contained in the original dataset is very scarce. To alleviate this, the authors decided to augment this dataset, which means trying to combine parts of this data to squeeze out as much information as possible. You see some examples here how this motion data can be combined from many small segments, and in the paper, they show that the augmentation helps us create even up to 10 to 30 times more training data for the neural networks. As a result of this augmented dataset, it can learn to perform zombie, gorilla movements, chicken hopping, even dribbling with a basketball, you name it. What\u2019s even more, we can give the AI high-level commands interactively, and it will try to weave the motions together appropriately. They can also punch each other. Ow. And all this was learned from a bunch of unorganized data. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=882O_7hsAms",
        "paper_link": "http://mrl.snu.ac.kr/publications/ProjectICC/ICC.html",
        "paper_title": "Learning Predict-and-Simulate Policies From Unorganized Human Motion Data"
    },
    {
        "video_id": "Lu56xVlZ40M",
        "video_title": "OpenAI Plays Hide and Seek\u2026and Breaks The Game! \ud83e\udd16",
        "position_in_playlist": 379,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\u2764\ufe0f Their blog post is available here: https://www.wandb.com/articles/better-paths-through-idea-space\n\n\ud83d\udcdd The paper \"Emergent Tool Use from Multi-Agent Interaction\" is available here:\nhttps://openai.com/blog/emergent-tool-use/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#OpenAI",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this project, OpenAI built a hide and seek game for their AI agents to play. While we look at the exact rules here, I will note that the goal of the project was to pit two AI teams against each other, and hopefully see some interesting emergent behaviors. And, boy, did they do some crazy stuff. The coolest part is that the two teams compete against each other, and whenever one team discovers a new strategy, the other one has to adapt. Kind of like an arms race situation, and it also resembles generative adversarial network a little. And the results are magnificent, amusing, weird - you\u2019ll see in a moment. These agents learn from previous experiences, and to the surprise of no one, for the first few million rounds, we start out with\u2026pandemonium. Everyone just running around aimlessly. Without proper strategy and semi-random movements, the seekers are favored and hence win the majority of the games. Nothing to see here. Then, over time, the hiders learned to lock out the seekers by blocking the doors off with these boxes and started winning consistently. I think the coolest part about this is that the map was deliberately designed by the OpenAI scientists in a way that the hiders can only succeed through collaboration. They cannot win alone and hence, they are forced to learn to work together. Which they did, quite well. But then, something happened. Did you notice this pointy, doorstop-shaped object? Are you thinking what I am thinking? Well, probably, and not only that, but about 10 million rounds later, the AI also discovered that it can be pushed near a wall and be used as a ramp, and, tadaa! Got\u2019em! Te seekers started winning more again. So, the ball is now back on the court of the hiders. Can you defend this? If so, how? Well, these resourceful little critters learned that since there is a little time at the start of the game when the seekers are frozen, apparently, during this time, they cannot see them, so why not just sneak out and steal the ramp, and lock it away from them. Absolutely incredible. Look at those happy eyes as they are carrying that ramp. And, you think it all ends here? No, no, no. Not even close. It gets weirder. Much weirder. When playing a different map, a seeker has noticed that it can use a ramp to climb on the top of a box, and, this happens. Do you think couchsurfing is cool? Give me a break! This is box surfing! And, the scientists were quite surprised by this move as this was one of the first cases where the seeker AI seems to have broken the game. What happens here is that the physics system is coded in a way that they are able to move around by exerting force on themselves, but, there is no additional check whether they are on the floor or not, because who in their right mind would think about that? As a result, something that shouldn\u2019t ever happen does happen here. And, we\u2019re still not done yet, this paper just keeps on giving. A few hundred million rounds later, the hiders learned to separate all the ramps from the boxes. Dear Fellow Scholars, this is proper box surfing defense\u2026then, lock down the remaining tools and build a shelter. Note how well rehearsed and executed this strategy is - there is not a second of time left until the seekers take off. I also love this cheeky move where they set up the shelter right next to the seekers, and I almost feel like they are saying \u201cyeah see this here? there is not a single thing you can do about it\u201d. In a few isolated cases, other interesting behaviors also emerged, for instance, the hiders learned to exploit the physics system and just chuck the ramp away. After that, the seekers go \u201cwhat?\u201d \u201cwhat just happened?\u201d. But don\u2019t despair, and at this point, I would also recommend that you hold on to your papers because there was also a crazy case where a seeker also learned to abuse a similar physics issue and launch itself exactly onto the top of the hiders. Man, what a paper. This system can be extended and modded for many other tasks too, so expect to see more of these fun experiments in the future. We get to do this for a living, and we are even being paid for this. I can\u2019t believe it. In this series, my mission is to showcase beautiful works that light a fire in people. And this is, no doubt, one of those works. Great idea, interesting, unexpected results, crisp presentation. Bravo OpenAI! Love it. So, did you enjoy this? What do you think? Make sure to leave a comment below. Also, if you look at the paper, it contains comparisons to an earlier work we covered about intrinsic motivation, shows how to implement circular convolutions for the agents to detect their environment around them, and more. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Lu56xVlZ40M",
        "paper_link": "https://openai.com/blog/emergent-tool-use/",
        "paper_title": "Emergent Tool Use from Multi-Agent Interaction"
    },
    {
        "video_id": "nSHU-4Yt4eQ",
        "video_title": "AIs Are Getting Too Smart - Time For A New \"IQ Test\u201d \ud83c\udf93",
        "position_in_playlist": 380,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\" is available here:\nhttps://super.gluebenchmark.com\nhttps://arxiv.org/abs/1905.00537\n\nOur earlier video, \"DeepMind's AI Takes An IQ Test\":\nhttps://www.youtube.com/watch?v=eSaShQbUJTQ\n\nOur earlier video on the OpenAI Retro Contest is available here:\nhttps://www.youtube.com/watch?v=2FHHuRTkr_Y\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In a world where learning-based algorithms are rapidly becoming more capable, I increasingly find myself asking the question: \u201cso, how smart are these algorithms, really?\u201d. I am clearly not alone with this. To be able to answer this question, a set of tests were proposed, and many of these tests shared one important design decision: they are very difficult to solve for someone without generalized knowledge. In an earlier episode, we talked about DeepMind\u2019s paper where they created a bunch of randomized mind-bending, or in the case of an AI, maybe silicon-bending questions that looked quite a bit like a nasty, nasty IQ test. And even in the presence of additional distractions, their AI did extremely well. I noted that on this test, finding the correct solution around 60% of the time would be quite respectable for a human, where their algorithm succeeded over 62% of the time, and upon removing the annoying distractions, this success rate skyrocketed to 78%. Wow. More specialized tests have also been developed. For instance, scientists at DeepMind also released a modular math test with over 2 million questions, in which their AI did extremely well at tasks like interpolation, rounding decimals, integers, whereas they were not too accurate at detecting primality and factorization. Furthermore, a little more than a year ago, the Glue benchmark appeared that was designed to test the natural language understanding capabilities of these AIs. When benchmarking the state of the art learning algorithms, they found that they were approximately 80% as good as the fellow non-expert human beings. That is remarkable. Given the difficulty of the test, they were likely not expecting human-level performance, which you see marked with the black horizontal line, which was surpassed within less than a year. So, what do we do in this case? Well, as always, of course, design an even harder test. In comes SuperGLUE, the paper we\u2019re looking at today, which is meant to provide an even harder challenge for these learning algorithms. Have a look at these example questions here. For instance, this time around, reusing general background knowledge gets more emphasis in the questions. As a result, the AI has to be able to learn and reason with more finesse to successfully address these questions. Here you see a bunch of examples, and you can see that these are anything but trivial little tests for a baby AI - not all, but some of these are calibrated for humans at around college-level education. So, let\u2019s have a look at how the current state of the art AIs fared in this one! Well, not as good as humans, which is good news, because that was the main objective. However, they still did remarkably well. For instance, the BoolQ package contains a set of yes and no questions, in these, the AIs are reasonably close to human performance, but on MultiRC, the multi-sentence reading comprehension package, they still do OK, but humans outperform them by quite a bit. Note that you see two numbers for this test, the reason for this is that there are multiple test sets for this package. Note that in the second one, even humans seem to fail almost half the time, so I can only imagine the revelation we\u2019ll have a couple more papers down the line. I am very excited to see that, and if you are too, make sure to subscribe and hit the bell icon to not miss future episodes. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=nSHU-4Yt4eQ",
        "paper_link": "https://super.gluebenchmark.com",
        "paper_title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
    },
    {
        "video_id": "atcKO15YVD8",
        "video_title": "AI Learns To Compute Game Physics In Microseconds! \u269b\ufe0f",
        "position_in_playlist": 381,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nTheir blog post and their CodeSearchNet system are available here:\nhttps://www.wandb.com/articles/codesearchnet\nhttps://app.wandb.ai/github/CodeSearchNet/benchmark\n\n\ud83d\udcdd The paper \"Subspace Neural Physics: Fast Data-Driven Interactive Simulation\" is available here:\nhttps://montreal.ubisoft.com/fr/deep-cloth-paper/\nhttp://theorangeduck.com/page/subspace-neural-physics-fast-data-driven-interactive-simulation\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "dear fellow scholars this is too many papers with casual knife hit in almost any kind of real time computer games where different objects interact with each other having some sort of physics engine is a requirement flags waving in the wind stroking bunnies with circular objects are among these cases and of course not all heroes wear capes but the ones that do require the presence of such a physics engine however the full physical simulation of these interactions is often not possible because it is orders of magnitude slower than what we are looking for in real time applications now hold on to your papers because this project proposes a new learning based methods that can speed up the standard physical simulations and make them three hundred to five thousand times faster and then we can give it all the positions forces and other information and it will be able to tell us the outcome and do all this faster than real time since this is in your own network based project I season fellow scholars know that will need many hours of simulation data to train on fortunately this information can be produced with one of those more accurate but slower methods we can wait arbitrarily long for a full physical simulation for this training set because it is only needed once for the training one of the key decisions in this project is that it also supports interaction with objects and we can even specify external forces like wind direction and speed controls in some papers the results are difficult to evaluate for instance when we produce any kind of deepfake we need to call in people and create a user study where we measure how often do people believe forced to be deals to be real the process has many pitfalls by choosing a good distribution of people asking the right questions and so on another great part of the design of this project is that evaluating this  is a breeze we can just give it another situation leaded gas the result Dan simulate the same thing with a full physical simulator and compared the two against each other and they are really close  but wait do you see what I see if you are worried about how can traditionally intensive do your own network based solution is don't be it only takes a few megabytes of memory which is nothing and it runs in the order of microseconds which is also nothing so much so that if you look here you see that the full simulation can be done at two frames per second why this new solution produces thousands and thousands of frames per second I think it is justified to say that this thing costs absolutely nothing I think I will take this one thank you very much we can even scale up the number of interactions as you see here and even in this case it can produce more than a hundred frames per second incredible we can also up or down scale the quality of the results and get different trade offs if the core simulation looks good enough for applications we can't even get up to tens of thousands of frames per second that cost nothing even compared to the previous nothing the key part of the solution is that it compresses the simulated data through a method called principal component analysis and the training takes place on this compressed representation which only needs to be unpacked when something new is happening in the game which leads to a significant speed up and it is also very gentle with memory loss and working with this compressed representation is the reason why you see this method referred to as subspace neural physics however as always some limitations apply for instance it can kind of extrapolate beyond the examples that it has been trained on but as you see here if the training data is lacking a given kind of application don't expect miracles yet if you have a look at the paper you'll actually find a user study but it is about the usefulness of the individual components of the system make sure to check it out in the video description  this episode has been supported by weights and biases weights and biases provides tools to track your experiments in your deep learning projects it can save you a ton of time and money in these projects and is being used by OpenAI Toyota research Stanford and Berkeley have a look at this project they launched to make computer cold semantically searchable where for example we could ask show me the best model on this dataset with the fewest parameters and get a piece of cold that does exactly that absolutely amazing make sure to visit them through Wendy B. dot com slash papers W. E. N. D. B. E. dot com slash papers or just click the link in the video description and you can get the free demo today I thank the weights and biases for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=atcKO15YVD8",
        "paper_link": "https://montreal.ubisoft.com/fr/deep-cloth-paper/",
        "paper_title": "Subspace Neural Physics: Fast Data-Driven Interactive Simulation"
    },
    {
        "video_id": "g1sAjtDoItE",
        "video_title": "Cubify All The Things! \ud83d\udc04",
        "position_in_playlist": 382,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Cubic Stylization\" is available here:\nhttp://www.dgp.toronto.edu/projects/cubic-stylization/\n\nErratum: I have misunderstood the \"fixing\" part. Instead of fixing as in \"repairing\", it rather fixes regions as in \"pinning down\" parts of it. (Thank you Liam Appelbe for noting it!)\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I apologize for my voice today, I am trapped in this frail human body, and sometimes it falters. But, the papers must go on. This is one of those papers where I find that more time I spend with it, the more I realize how amazing it is. It starts out with an interesting little value proposition that in and of itself, would likely not make it to a paper. So, what is this paper about? Well, as you see here, this one is about cubification of 3D geometry. In other words, we take an input shape, and it stylizes it to look more like a cube. Okay, that\u2019s cute, especially given that there are many-many ways to do this, and it\u2019s hard to immediately put into words what a good end result would be, you can see a comparison to previous works here. These previous works did not seem to preserve a lot of fine details, but if you look at this new one, you see that this one does that really well. Very nice indeed. But still\u2026when I read this paper, at this point, I was thinking\u2026I\u2019d like a little more. Well, I quickly found out that this work has more up its sleeve. So much more. Let\u2019s talk about 7 of these amazing features. For instance, one, we can control the strength of the transformation with this lambda parameter, as you see, the more we increase it, the more heavy-handed the smushing process is going to get. Please remember this part. Two, we can also cubify selectively along different directions, or, select parts of the object that should be cubified differently. Hmm. Okay. Three, we can even use it to fix flaws in the input 3D geometry. Four, this transformation procedure also takes into consideration the orientations - this means that we can perform it from different angles, which gives us a large selection of possible outputs for the same model. Five, it is fast and works on high-resolution geometry, and you see different settings for the lambda parameter here that is the same parameter as we have talked about before - the strength of the transformation. Six, we can also combine many of these features interactively until a desirable shape is found. Seven is about to come in a moment, but to appreciate what that is, we have to look at \u2026this. To perform what you have seen here so far, we have to minimize this expression. This first term says ARAP, as rigid as possible, which stipulates that whatever we do in terms of smushing, it should preserve the fine local features. The second part is called a regularization term that encourages sparser, more axis-aligned solutions so we don\u2019t destroy the entire model during this process. The stronger this term is, the bigger say it has in the final results, which, in return, become more cube-like. So, how do we do that? Well, of course, with our trusty little lambda parameter. Not only that, but if we look at the appendix, it tells us that we can generalize this second regularization term for many different shapes. So here we are, finally, seven, it doesn\u2019t even need to be cubification, we can specify all kinds of polyhedra. Look at those gorgeous results. I love this paper. It is playful, it is elegant, it has utility, and, it generalizes well. It doesn\u2019t care in the slightest what the current mainstream ideas are and invites us into its own little world. In summary, this will serve all your cubification needs, and turns out, it might even fix your geometry, and more. I would love to see more papers like this. In this series, I try to make people feel how I feel when I read these papers. I hope I have managed this time, but you be the judge. Let me know in the comments. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=g1sAjtDoItE",
        "paper_link": "http://www.dgp.toronto.edu/projects/cubic-stylization/\n\nErratum: I have misunderstood the \"fixing\" part. Instead of fixing as in \"repairing\", it rather fixes regions as in \"pinning down\" parts of it. (Thank you Liam Appelbe for noting it!)",
        "paper_title": "Cubic Stylization"
    },
    {
        "video_id": "Z6iTo7KY7lw",
        "video_title": "Can an AI Learn The Concept of Pose And Appearance? \ud83d\udc71\u200d\u2640\ufe0f",
        "position_in_playlist": 383,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"HoloGAN: Unsupervised learning of 3D representations from natural images\" is available here:\nhttps://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost,, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. I apologize for my voice today, I am trapped in this frail human body, and sometimes it falters. But you remember from the previous episode, the papers must go on. In the last few years, we have seen a bunch of new AI-based techniques that were specialized in generating new and novel images. This is mainly done through learning-based techniques, typically a Generative Adversarial Network, a GAN in short, which is an architecture where a generator neural network creates new images, and passes it to a discriminator network, which learns to distinguish real photos from these fake, generated images. These two networks learn and improve together, so much so that many of these techniques have become so realistic that we sometimes can\u2019t even tell they are synthetic images unless we look really closely. You see some examples here from BigGAN, a previous technique that is based on this architecture. Now, normally, if we are looking to generate a specific human face, we have to generate hundreds and hundreds of these images, and our best bet is to hope that sooner or later, we\u2019ll find something that we were looking for. So, of course, scientists were interested in trying to exert control over the outputs, and with followup works, we can now kind of control the appearance, but, in return, we have to accept the pose in which they are given. And, this new project is about teaching a learning algorithm to separate pose from identity. Now, that sounds kind of possible with proper supervision. What does this mean exactly? Well, we have to train these GANs on a large number of images so they can learn what a human face looks like, what landmarks to expect and how to form them properly when generating new images. However, when the input images are given with different poses, we will normally need to add additional information to the discriminator that describes the rotations of these people and objects. Well, hold on to your papers, because that is exactly what is not happening in this new work. This paper proposes an architecture that contains a 3D transform and a projection unit, you see them here with red and blue, and, these help us in separating pose and identity. As a result, we have much finer artistic control over these during image generation. That is amazing. So as you see here, it enables a really nice workflow where we can also set up the poses. Don\u2019t like the camera position for this generated bedroom? No problem. Need to rotate the chairs? No problem. And we are not even finished yet, because when we set up the pose correctly, we\u2019re not stuck with these images - we can also choose from several different appearances. And all this comes from the fact that this technique was able to learn the intricacies of these objects. Love it. Now, it is abundantly clear that as we rotate these cars, or change the camera viewpoint for the bedroom, a flickering effect is still present. And this, is how research works. We try to solve a new problem, one step at a time. Then, we find flaws in the solution, and improve upon that. As a result, we always say, two more papers down the line, and we\u2019ll likely have smooth and creamy transitions between the images. The Lambda sponsorship spot is coming in a moment, and I don\u2019t know if you have noticed at the start, but they were also part of this research project as well. I think that is as relevant of a sponsor as it gets. Thanks for watching and for  your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Z6iTo7KY7lw",
        "paper_link": "https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/",
        "paper_title": "HoloGAN: Unsupervised learning of 3D representations from natural images"
    },
    {
        "video_id": "FZZ9rpmVCqE",
        "video_title": "Ken Burns Effect, Now In 3D!",
        "position_in_playlist": 384,
        "description": "\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\udcdd The paper \"3D Ken Burns Effect from a Single Image\" is available here:\nhttps://arxiv.org/abs/1909.05483\n\nThe paper with the Microplanet scene at the start is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\nScene geometry: Marekv\n\nImage credits: Ian D. Keating, Kirk Lougheed (Link: https://www.flickr.com/photos/kirklougheed/36766944501 ), Leif Skandsen, Oliver Wang, Ben Abel, Aurel Manea, Jocelyn Erskine-Kellie, Jaisri Lingappa, and Intiaz Rahim.\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Have you heard of the Ken Burns effect? If you have been watching this channel, you have probably seen examples where a still image is shown, and a zooming and panning effect is added to it. It looks something like this. Familiar, right? The fact that there is some motion is indeed pleasing for the eye, but something is missing. Since we are doing this with 2D images, all the depth information is lost, so we are missing out on the motion parallax effect that we would see in real life when moving around a camera. So, this is only 2D. Can this be done in 3D? Well, to find out, have a look at this. Wow, I love it. Much better, right? Well, if we would try to perform something like this without this paper, we\u2019ll be met with bad news. And that bad news is that we have to buy an RGBD camera. This kind of camera endows the 2D image with depth information, which is specialized hardware that is likely not available in our phones as of the making of this video. Now, since depth estimation from these simple, monocular 2D images without depth data is a research field of its own, the first step sounds simple enough: take one of those neural networks then, ask it to try to guess the depth of each pixel. Does this work? Well, let\u2019s have a look! As we move our imaginary camera around, uh-oh. This is not looking good. Do you see what the problems are here? Problem number one is the presence geometric distortions, you see it if you look here. Problem number two is referred to as semantic distortion in the paper, or in other words, we now have missing data. Not only that, but this poor tiny human\u2019s hand is also\u2026ouch. Let\u2019s look at something else instead. If we start zooming in into images, which is a hallmark of the Ken Burns effect, it gets even worse. Artifacts. So how does this new paper address these issues? After creating the first, coarse depth map, an additional step is taken to alleviate the semantic distortion issue, and then, this depth information is upsampled to make sure that we have enough fine details to perform the 3D Ken Burns effect. Let\u2019s do that! Unfortunately, we are still nowhere near done yet. Previously occluded parts of the background suddenly become visible, and, we have no information about those. So, how can we address that? Do you remember image inpainting? I hope so, but if not, no matter, I\u2019ll quickly explain what it is. Both learning-based, and traditional handcrafted algorithms exist to try to fill in this missing information in images with sensible data by looking at its surroundings. This is also not as trivial as it might seem first, for instance, just filling in sensible data is not enough, because this time around, we are synthesizing videos, it has to be temporally coherent, which means that there mustn\u2019t be too much of a change from one frame to another, or else we\u2019ll get a flickering effect. As a result, we finally have these results that are not only absolutely beautiful, but the user study in the paper shows that they stack up against handcrafted results made by real artists. How cool is that! It also opens up really cool workflows that would normally be very difficult, if not impossible to perform. For instance, here you see that we can freeze this lightning bolt in time, zoom around and marvel at the entire landscape. Love it. Of course, limitations still apply. If we have really thin objects, such as this flagpole, it might be missing entirely from the depth map, or there are also cases where the image inpainter cannot fill in useful information. I cannot wait to see how this work evolves a couple papers down the line. One more interesting tidbit. If you have a look at the paper, make sure to open it in Adobe Reader you will likely be very surprised to see that many of these things that you think are still images\u2026are actually animations. Papers are not only getting more mind-blowing by the day, but also more informative, and beautiful as well. What a time to be alive! This video has been supported by you on Patreon. If you wish to support the series and also pick up cool perks in return, like early access to these episodes, or getting your name immortalized in the video description, make sure to visit us through Patreon.com/TwoMinutePapers. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=FZZ9rpmVCqE",
        "paper_link": "https://arxiv.org/abs/1909.05483",
        "paper_title": "3D Ken Burns Effect from a Single Image"
    },
    {
        "video_id": "0sR1rU3gLzQ",
        "video_title": "This AI Clones Your Voice After Listening for 5 Seconds \ud83e\udd10",
        "position_in_playlist": 385,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers \n\nThe shown blog post is available here: https://www.wandb.com/articles/fundamentals-of-neural-networks\n\n\ud83d\udcdd The paper \"Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis\" and audio samples are available here:\nhttps://arxiv.org/abs/1806.04558\nhttps://google.github.io/tacotron/publications/speaker_adaptation/\n\nAn unofficial implementation of this paper is available here. Note that this was not made by the authors of the original paper and may contain deviations from the described technique - please judge its results accordingly! https://github.com/CorentinJ/Real-Time-Voice-Cloning\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#VoiceCloning",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to listen to some amazing improvements in the area of AI-based voice cloning. For instance, if someone wanted to clone my voice, there are hours and hours of my voice recordings on Youtube and elsewhere, they could do it with previously existing techniques. But the question today is, if we had even more advanced methods to do this, how big of a sound sample would we really need for this? Do we need a few hours? A few minutes? The answer is no. Not at all. Hold on to your papers because this new technique only requires 5 seconds. Let\u2019s listen to a couple examples. Absolutely incredible. The timbre of the voice is very similar, and it is able to synthesize sounds and consonants that have to be inferred because they were not heard in the original voice sample. This requires a certain kind of intelligence and quite a bit of that. So, while we are at that, how does this new system work? Well, it requires three components. One, the speaker encoder is a neural network that was trained on thousands and thousands of speakers and is meant to squeeze all this learned data into a compressed representation. In other words, it tries to learn the essence of human speech from many many speakers. To clarify, I will add that this system listens to thousands of people talking to learn the intricacies of human speech, but this training step needs to be done only once, and after that, it was allowed just 5 seconds of speech data from someone they haven\u2019t heard of previously, and later, the synthesis takes place using this 5 seconds as an input. Two, we have a synthesizer that takes text as an input, this is what we would like our test subject to say, and it gives us a Mel Spectrogram, which is a concise representation of someone\u2019s voice and intonation. The implementation of this module is based on DeepMind\u2019s Tacotron 2 technique, and here you can see an example of this Mel spectrogram built for a male and two female speakers. On the left, we have the spectrograms of the reference recordings, the voice samples if you will, and on the right, we specify a piece of text that we would like the learning algorithm to utter, and it produces these corresponding synthesized spectrograms. But, eventually, we would like to listen to something, and for that, we need a waveform as an output. So, the third element is thus a neural vocoder that does exactly that, and this component is implemented by DeepMind\u2019s WaveNet technique. This is the architecture that led to these amazing examples. So how do we measure exactly how amazing it is? When we have a solution, evaluating it is also anything but trivial. In principle, we are looking for a result that is both close to the recording that we have of the target person, but says something completely different, and all this in a natural manner. This naturalness and similarity can be measured, but we\u2019re not nearly done yet, because the problem gets even more difficult. For instance, it matters how we fit the three puzzle pieces together, and then, what data we train on, of course, also matters a great deal. Here you see that if we train on one dataset and test the results against a different one, and then, swap the two, and\u2026the results in naturalness and similarity will differ significantly. The paper contains a very detailed evaluation section that explains how to deal with these difficulties. The mean opinion score is measured in this section, which is a number that describes how well a sound sample would pass as genuine human speech. And we haven\u2019t even talked about the speaker verification part, so make sure to have a look at the paper. So, indeed, we can clone each other\u2019s voice by using a sample of only 5 seconds. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. They also wrote a guide on the fundamentals of neural networks where they explain in simple terms how to train a neural network properly, what are the most common errors you can make, and how to fix them. It is really great, you got to have a look. So make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=0sR1rU3gLzQ",
        "paper_link": "https://arxiv.org/abs/1806.04558",
        "paper_title": "Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis"
    },
    {
        "video_id": "cTqVhcrilrE",
        "video_title": "This AI Learned To Animate Humanoids!\ud83d\udeb6",
        "position_in_playlist": 386,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Neural State Machine for Character-Scene Interactions\" is available here:\nhttps://github.com/sebastianstarke/AI4Animation\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#GameDev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we have an animation movie or a computer game with quadrupeds, and we are yearning for really high-quality, lifelike animations, motion capture is often the go-to tool for the job. Motion capture means that we put an actor, in our case, a dog in the studio, we ask it to perform sitting, trotting, pacing and jumping, record its motion, and transfer it onto our virtual character. In an earlier work, a learning-based technique was introduced by the name Mode-Adaptive Neural Network, and it was able to correctly weave together these previously recorded motions, and not only that, but it also addressed these unnatural sliding motions that were produced by previous works. As you see here, it also worked well on more challenging landscapes. We talked about this paper approximately a hundred videos, or in other words, a little more than a year ago, and I noted that it was scientifically interesting, it was evaluated well, it had all the ingredients for a truly excellent paper. But one thing was missing. So what is that one thing? Well, we haven\u2019t seen the characters interacting with the scene itself. If you liked this previous paper, you are going to be elated by this one because this new work is from the very same group, and goes by the name Neural State Machine, and introduces character-scene interactions for bipeds. Now, we suddenly jumped from a quadruped paper to a biped one, and the reason for this is because I was looking to introduce the concept of foot sliding, which will be measured later for this new method too. Stay tuned! So, in this new problem formulation, we need to guide the character to a challenging end state, for instance, sitting in a chair, while being able to maneuver through all kinds of geometry. We\u2019ll use the chair example a fair bit in the next minute or two, so I\u2019ll stress that this can do a whole lot more, the chair is just used as a vehicle to get a taste of how this technique works. But the end state needn\u2019t just be some kind of chair. It can be any chair! This chair may have all kinds of different heights and shapes, and the agent has to be able to change the animations and stitch them together correctly regardless of the geometry. To achieve this, the authors propose an interesting new data augmentation model. Since we are working with neural networks, we already have a training set to teach it about motion, and data augmentation means that we extend this dataset with lots and lots of new information to make the AI generalize better to unseen, real-world examples. So, how is this done here exactly? Well, the authors proposed a clever idea to do this. Let\u2019s walk through their five prescribed steps. One, let\u2019s use motion capture data, have the subject sit down and see what the contact points are when it happens. Two, we then record the curves that describe the entirety of the motion of sitting down. So far so good, but we are not interested in one kind of chair, we want it to sit into all kinds of chairs, so three, generate a large selection of different geometries and adjust the location of these contact points accordingly. Four, change the motion curves so they indeed end at the new, transformed contact points. And five, move the joints of the character to make it follow this motion curve and compute the evolution of the character pose. We then pair up this motion with the chair geometry and chuck it into the new, augmented training set. Now, make no mistake, the paper contains much, much more than this, so make sure to have a look in the video description. So what do we get for all this work? Well, have a look at this trembly character from a previous paper, and look at the new synthesized motions. Natural, smooth, creamy, and I don\u2019t see artifacts. Also, here you see some results that measure the amount of foot sliding during these animations, which is subject to minimization. That means that the smaller the bars are, the better. With NSM, you see how this Neural State Machine method produces much less than previous methods, and now we see how cool it is that we talked about the quadruped paper as well, because we see that it even beats the MANN, the mode-adaptive neural networks from the previous paper. That one had very little foot sliding, and apparently, it can still be improved by quite a bit. The positional and rotational errors in the animation it offers are also by far the lowest of the bunch. Since it works in real time, it can also be used for computer games and virtual reality applications. And all this improvement within a year of work. What a time to be alive! If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cTqVhcrilrE",
        "paper_link": "https://github.com/sebastianstarke/AI4Animation",
        "paper_title": "Neural State Machine for Character-Scene Interactions"
    },
    {
        "video_id": "9b2dxc1QalM",
        "video_title": "OpenAI\u2019s Robot Hand Won't Stop Rotating The Rubik\u2019s Cube \ud83d\udc4b",
        "position_in_playlist": 387,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nThe mentioned #OpenAI blog post on the gradients and its notebook are available here:\nPost: https://www.wandb.com/articles/exploring-gradients\nNotebook: https://colab.research.google.com/drive/1bsoWY8g0DkxAzVEXRigrdqRZlq44QwmQ\n\n\ud83d\udcdd The paper \"Solving Rubik\u2019s Cubewith a Robot Hand\" is available here:\nhttps://openai.com/blog/solving-rubiks-cube/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Today, we\u2019re going to talk about OpenAI\u2019s robot hand that dexterously manipulates and solves a Rubik\u2019s cube. Here you can marvel at this majestic result. Now, why did I use the term dexterously manipulate a Rubik\u2019s cube? In this project, there are two problems to solve. One, finding out what kind of rotation we need to get closer to a solved cube, and adjusting the finger positions to be able to execute these prescribed rotations. And this paper is about the latter, which means the rotation sequences are given by a previously existing algorithm, and OpenAI\u2019s method manipulates the hand to be able to follow this algorithm. To rephrase it, the robot hand doesn\u2019t really know how to solve the cube and is told what to do, and the contribution lies in the robot figuring out how to execute these rotations. If you take only one thing from this video, let it be this thought. Now, to perform all this, we have to first solve a problem in a computer simulation where we can learn and iterate quickly, and then, transfer everything the agent learned there to the real world, and hope that it obtained general knowledge that indeed can be applied there. This task is one of my favorites. However, no simulation is as detailed as the real world, and as every experienced student knows very well, things that are written in the textbook might not always work exactly the same in practice. So the problem formulation naturally emerges - our job is to prepare this AI in this simulation so it becomes good enough to perform well in the real world. Well, good news, first, let\u2019s think about the fact that in a simulation, we can train much faster as we are not bound by the physical limits of the robot hand - in a simulation, we are bound by our processing power, which is much, much more vast and is growing every day. So, this means that the simulated environments can be as grueling as we can make them be, what\u2019s even more, we can do something that OpenAI refers to as Automatic Domain Randomization. This is one of the key contributions of this paper. The domain randomization part means that it creates a large number of random environments, each of which are a little different, and the AI is meant to learn how to account for these differences and hopefully, as a result, obtain general knowledge about our world. The automatic part is responsible for detecting how much randomization the neural network can shoulder, and hence, the difficulty of these random environments is increased over time. So, how good are the results? Well, spectacular. In fact, hold on to your papers, because it can not only dexterously manipulate and solve the cube, but we can even hamstring the hand in many different ways and it will still be able to do well. And I am telling you, scientists at OpenAI got very creative in tormenting this little hand. They added a rubber glove, tied multiple fingers together, threw a blanket on it, and pushed it around with a plush giraffe and a pen. It still worked. This is a testament to the usefulness of the mentioned automatic domain randomization technique. What\u2019s more, if you have a look at the paper, you will even see how well it was able to recover from a randomly breaking joint. What a time to be alive! As always, some limitations apply. The hand is only able to solve the cube about 60% of the time for simpler cases, and the success rate drops to 20% for the most difficult ones. If it gets stuck, it typically does in the first few rotations. But so far, we have been able to do this 0% of the time, and given that the first steps towards cracking the problem are almost always the hardest, I have no doubt that two more papers down the line, this will become significantly more reliable. But you know what, we are talking about OpenAI, make it one paper. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Here you see a write-up of theirs where they explain how to visualize the gradients running through your models, and illustrate it through the example of predicting protein structure. They also have a live example that you can try! Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9b2dxc1QalM",
        "paper_link": "https://openai.com/blog/solving-rubiks-cube/",
        "paper_title": "Solving Rubik\u2019s Cubewith a Robot Hand"
    },
    {
        "video_id": "slJI5r9rltI",
        "video_title": "This AI Captures Your Hair Geometry...From Just One Photo! \ud83d\udc69\u200d\ud83e\uddb1",
        "position_in_playlist": 388,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd Links to the paper \"Dynamic Hair Modeling from Monocular Videos using Deep Neural Networks\" are available here:\nhttp://www.cad.zju.edu.cn/home/zyy/docs/dynamic_hair.pdf\nhttp://www.kunzhou.net/2019/dynamic-hair-capture-sa19.pdf\nhttps://www.youyizheng.net/research.html\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#GameDev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we talk about research on all kinds of physics simulations, including fluids, collision physics, and we have even ventured into hair simulations. If you look here at this beautiful footage, you may be surprised to know how many moving parts a researcher has to get right to get something like this. For instance, some of these simulations have to go down to the level of computing the physics between individual hair strands. If it is done well, like what you see here from our earlier episode, these simulations will properly show us how things should move, but that\u2019s not all - there is also an abundance of research works out there on how they should look. And even then, we\u2019re not done, because before that, we have to take a step back and somehow create these digital 3D models that show us the geometry of these flamboyant hairstyles. Approximately 300 episodes ago, we talked about a technique that took a photograph as an input, and created a digital 3D model that we can use in our simulations and rendering systems. It had a really cool idea where it initially predicted a coarse result, and then, this result was matched with the hairstyles found in public data repositories, and the closest match was presented to us. Clearly, this often meant that we got something that was similar to the photograph, but often not exactly the hairstyle we were seeking. And now, hold on to your papers because this work introduces a learning-based framework that can create a full reconstruction by itself, without external help, and now, squeeze that paper, because it works not only only for images, but for videos too! Woohoo! It works for shorter hairstyles, long hair, and even takes into consideration motion and external forces as well. The heart of the architecture behind this technique is this pair of neural networks, where the one above creates the predicted hair geometry for each frame, while the other tries to look backwards in the data and try to predict the appropriate motions that should be present. Interestingly, it only needs two consecutive frames to make these predictions, and adding more information does not seem to improve its results. That is very little data. Quite remarkable. Also, note that there are a lot of moving parts here in the full paper, for instance, this motion is first predicted in 2D, and is then extrapolated to 3D afterwards. Let\u2019s have a look at this comparison - indeed, it seems to produce smoother and more appealing results than this older technique. But if we look here, this other method seems even better, so what about that? Well, this method had access to multiple views of the model, which is significantly more information than what this new technique has that only needs a simple monocular 2D video from our phone, or from the internet. The fact that they are even comparable is absolutely amazing. If you have a look at the paper, you will see that it even contains a hair growing component in this architecture. And as you see, the progress in computer graphics research is also absolutely amazing. And we are even being paid  for doing this. Unreal. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=slJI5r9rltI"
    },
    {
        "video_id": "4J0cpdR7qec",
        "video_title": "This AI Makes The Mona Lisa Speak\u2026And More!",
        "position_in_playlist": 389,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Few-shot Video-to-Video Synthesis\" is available here:\nhttps://nvlabs.github.io/few-shot-vid2vid/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Matthias Jost, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#DeepFake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In an earlier episode, we covered a paper by the name Everybody Dance Now. In this stunning work, we could take a video of a professional dancer, then record a video of our own, let\u2019s be diplomatic - less beautiful moves, and then, transfer the dancer's performance onto our own body in the video. We called this process motion transfer. Now, look at this new, also learning-based technique, that does something similar\u2026where in goes a description of a pose, just one image of the target person, and on the other side, out comes a proper animation of this character according to our prescribed motions. Now, before you think that it means that we would need to draw and animate stick figures to use this, I will stress that this is not the case. There are many techniques that perform pose estimation, where we just insert a photo, or even a video, and it creates all these stick figures for us that represent the pose that people are taking in these videos. This means that we can even have a video of someone dancing, and just one image of the target person, and the rest is history. Insanity. That is already amazing and very convenient, but this paper works with a video to video problem formulation, which is a concept that is more general than just generating movement. Way more. For instance, we can also specify the input video of us, then add one, or at most a few images of the target subject, and we can make them speak and behave using our gestures. This is already absolutely amazing, however, the more creative minds out there are already thinking that if we are thinking about images, it can be a painting as well, right? Yes, indeed, we can make the Mona Lisa speak with it as well. It can also take a labeled image, this is what you see here, where the colored and animated patches show the object boundaries for different object classes, then, we take an input photo of a street scene, and we get photorealistic footage with all the cars, buildings, and vegetation. Now, make no mistake, some of these applications were possible before, many of which we showcased in previous videos, some of which you can see here, what is new and interesting here is that we have just one architecture here that can handle many of these tasks. Beyond that, this architecture requires much less data than previous techniques as it often needs just one or at most a few images of the target subject to do all this magic. The paper is ample in comparisons to these other methods, for instance, the FID measures the quality and the diversity of the generated output images, and is subject to minimization, and you see that it is miles beyond these previous works. Some limitations also apply, if the inputs stray too far away from topics that the neural networks were trained on, we shouldn\u2019t expect results of this quality, and we are also dependent on proper inputs for the poses and segmentation maps for it to work well. The pace of progress in machine learning research is absolutely incredible, and we are getting very close to producing tools that can be actively used to empower artists working in the industry. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4J0cpdR7qec",
        "paper_link": "https://nvlabs.github.io/few-shot-vid2vid/",
        "paper_title": "Few-shot Video-to-Video Synthesis"
    },
    {
        "video_id": "jtlrWblOyP4",
        "video_title": "DeepMind\u2019s AlphaStar: A Grandmaster Level StarCraft 2 AI!",
        "position_in_playlist": 390,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers \n\nTheir mentioned blog post: https://www.wandb.com/articles/ml-best-practices-test-driven-development\n\n\ud83d\udcdd The paper \"#AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning\" is available here:\nhttps://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n\nMatches versus Serral, casted by Artosis (this is a playlist): https://www.youtube.com/watch?v=OxseexGkv_Q&list=PLojXIrB9Xau29fR-ZSdbFllI-ZCuH6urt\n\nOne more incredible match to watch that I loved (note: explicit language): https://www.youtube.com/watch?v=pUEPsHojUdw\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. The paper that we are going to cover today in my view, is one of the more important things that happened in AI research lately. In the last few years, we have seen DeepMind\u2019s AI defeat the best Go players in the world, and after OpenAI\u2019s venture in the game of DOTA2, DeepMind embarked on a journey to defeat pro players in Starcraft 2, a real-time strategy game. This is a game that requires a great deal of mechanical skill, split-second decision making and we have imperfect information as we only see what our units can see. A nightmare situation for any AI. The previous version of AlphaStar we covered in this series was able to beat at least mid-grandmaster level players, which is truly remarkable, but, as with every project of this complexity, there were limitations and caveats. In our earlier video, the paper was still pending, and now, it has finally appeared, so my sleepless nights have officially ended, at least for this work, and now, we can look into some more results. One of the limitations of the earlier version was that DeepMind needed to further tune some of the parameters and rules to make sure that the AI and the players play on an even footing. For instance, the camera movement and the number of actions the AI can make per minute has been limited some more and are now more human-like. TLO, a professional StarCraft 2 player noted that this time around, it indeed felt very much like playing another human player. The second limitation was that the AI was only able to play Protoss, which is one of the three races available in the game. This new version can now play all three races, and here you see its MMR ratings, a number that describes the skill level of the AI, and for non-experts, win percentages for each individual race. As you see, it is still the best with Protoss, however, all three races are well over the 99% winrate mark. Absolutely amazing. In this version, there is also more emphasis on self-play, and the goal is to create a learning algorithm that is able to learn how to play really well by playing against previous versions of itself millions and millions of times. This is, again, one of those curious cases where the agents train against themselves in a simulated world, and then, when the final AI was deployed on the official game servers, it played against human players for the very first time. I promise to tell you about the results in a moment, but for now, please note that relying more on self-play is extremely difficult. Let me explain why. Self-play agents have the well-known drawback of forgetting, which means that as they improve, they might forget how to win against a previous version of themselves. Since StarCraft 2 is designed in a way that every unit and strategy has an antidote, we have a rock-paper-scissors kind of situation where the agent plays rock all the time because it encountered a lot of scissors lately. Then, when a lot of papers appear, it will start playing scissors more often, and completely forget about the olden times when the rock was all the rage. And, on and on this circle goes without any real learning or progress. This doesn\u2019t just lead to suboptimal results - this leads to disastrously bad learning, if any learning at all. But it gets even worse. This situation opens up the possibility for an exploiter to take advantage of this information and easily beat these agents. In concrete StarCraft terms, such an exploit could be trying to defeat the AlphaStar AI early by rushing it with workers and warping in photon cannons to their base. This strategy is also known as a cannon rush, and as you can see here the red agent performing this, it can quickly defeat the unsuspecting blue opponent. So, how do we defend against such exploits? DeepMind used a clever idea here, by trying to turn the whole thing around and use these exploits to its advantage. How? Well, they proposed a novel self-play method where they additionally insert these exploiter AIs to expose the main AI\u2019s flaws and create an overall, more knowledgeable and robust agent. So, how did it go? Well, as a result, you can see how the green agent has learned to adapt to this by pulling its worker line and successfully defended the cannon rush of the red AI. This is proper machine learning progress happening right before our eyes. Glorious! This is just one example of using exploiters to create a better main AI, but the training process continually creates newer and newer kinds of exploiters, for instance, you will see in a moment that it later came up with a nasty strategy including attacking the main base with cloaking units. One of the coolest parts of the work, in my opinion, is that this kind of exploitation is a general concept that will surely come useful for completely different test domains as well. We noted earlier that it finally started playing humans for the first time on the official servers. So, how did that go? In my opinion, given the difficulty and the vast search space we have in StarCraft 2, creating a self-learning AI that has the skills of an amateur player is already incredible. But that\u2019s not what happened. Hold on to your papers, because it quickly reached grandmaster level with all three races and ranked above 99.8% of the officially ranked human players. Bravo, DeepMind. Stunning work. Later, it also played Serral, a decorated, world champion Zerg player, one of the most dominant players of our time. I will not spoil the results, especially given there were limitations as Serral wasn\u2019t playing on his equipment, but I will note that Artosis, a well-known and beloved Starcraft player and commentator analyzed these matches and said \u201cThe results are so impressive and I really feel like we can learn a lot from it. I would be surprised if a non-human entity could get this good and there was nothing to learn\u201d. His commentary is excellent and is tailored towards people who don\u2019t know anything about the game. He\u2019ll often pause the game and slowly explain what is going on. In these matches, I loved the fact that so many times it makes so many plays that we consider to be very poor and somehow, overall, it still plays outrageously well. It has unit compositions that nobody in their right minds would play. It is kind of like a drunken kung fu master, but in StarCraft 2. Love it. But no more spoilers - I think you should really watch these matches and, of course, I put a link to his analysis videos in the video description. Even though both this video and the paper appears to be laser focused on playing StarCraft 2, it is of utmost importance to note that this is still just a testbed to demonstrate the learning capabilities of this AI. As amazing as it sounds, DeepMind wasn\u2019t just looking to spend millions and millions of dollars on research to just play video games. The building blocks of AlphaStar are meant to be reasonably general, which means that parts of this AI can be reused for other things, for instance, Demis Hassabis mentioned weather prediction and climate modeling as examples. If you take only one thought from this video, let it be this one. There is really so much to talk about, so make sure to head over to the video description, watch the matches and check out the paper as well. The evaluation section is as detailed as it can possibly get. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=jtlrWblOyP4",
        "paper_link": "https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning",
        "paper_title": "#AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning"
    },
    {
        "video_id": "NlZJlFCh8MU",
        "video_title": "This AI Creates A Moving Digital Avatar Of You",
        "position_in_playlist": 392,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Neural Volumes: Learning Dynamic Renderable Volumes from Images\" is available here: \nhttps://research.fb.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Erik de Bruijn, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers \n\nThumbnail background image credit: https://pixabay.com/images/id-1280538/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we talk about research on all kinds of physics simulations, including fluids, collision physics, and we have even ventured into hair simulations. We mostly talk about how the individual hair strands should move, and how they should look, in terms of color and reflectance. Creating these beautiful videos takes getting many-many moving parts right, for instance, before all of that, the very first step is not any of those steps. First, we have to get the 3D geometry of these hairstyles into our simulation system. In a previous episode, we have seen an excellent work that does this well for human hair. But what if we would like to model not human hair, but something completely different? Well, hold on to your papers, because this new work is so general, that it can look at an input image or video, and give us not only a model of the human hair, but human skin, garments, and of course, my favorite, smoke plumes, and more. But if you look here, this part begs the following question - the input is an image, and the output also looks like an image, and we need to make them similar - so what\u2019s the big deal here? A copying machine can do that, no? Well, not really. Here\u2019s why. On the output, we are working with something that indeed looks like an image, but it is not an image. It is a 3 dimensional cube, in which we have to specify color and opacity values everywhere. After that, we simulate rays of light passing through this volume, which is a technique that we call ray marching, and this process has to produce the same 2D image through ray marching as what was given as an input. That\u2019s much, much harder than building a copying machine. As you see here, normally, this does not work well at all, because, for instance, a standard algorithm sees lights in the background, and it assumes that these are really bright and dense points. That is kind of true, but they are usually not even part of the data we would like to reconstruct. To solve this issue, the authors propose learning to tell the foreground and background images apart, so they can be separated before we start the reconstruction of the human. And this is a good research paper, which means that if it contains multiple new techniques, each of them are tested separately to know how much they contribute to the final results. We get the previously seen, dreadful results without the background separation step, here are the results with the learned backgrounds, we can still see the lights due to the way the final image is constructed, and the fact that we have so little of this halo effect is really cool. Here you see the results with the true background data where the background learning step is not present. Note that this is cheating, because this data is not available for all cameras and backgrounds, however, it is a great way to test the quality of this learning step. The comparison of the learned method against this reveals that the two are very close, which is exactly what we are looking for. And finally, the input footage is also shown for reference. This is ultimately what we are trying to achieve, and as you see, the output is quite close to it. As you see here, the final algorithm excels at reconstructing volume data for toys, smoke plumes, and humans alike. And the coolest part is that it works for not only stationary inputs, but for animations as well. Wait, actually, there is something that is perhaps even cooler, with the magic of neural networks and latent spaces, we can even animate this data. Here you see an example of that where an avatar is animated in real-time by moving around this magenta dot. A limiting factor here is the resolution of this reconstruction - if you look closely, you see that some fine details are missing, but you know the saying\u2026given the rate of progress in machine learning research, two more papers down the line, and this will likely be orders of magnitude better. If you feel that you always need to take your daily dose of papers, my statistics show that many of you are subscribed, but didn\u2019t use the bell icon. If you click this bell icon, you will never miss a future episode and can properly engage in your paper addiction. This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=NlZJlFCh8MU",
        "paper_link": "https://research.fb.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/",
        "paper_title": "Neural Volumes: Learning Dynamic Renderable Volumes from Images"
    },
    {
        "video_id": "_s7Bg6yVOdo",
        "video_title": "OpenAI Safety Gym: A Safe Place For AIs To Learn \ud83d\udcaa",
        "position_in_playlist": 393,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Benchmarking Safe Exploration in Deep Reinforcement Learning\" is available here:\nhttps://openai.com/blog/safety-gym/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Reinforcement learning is a technique in the field of machine learning to learn how to navigate in a labyrinth, play a video game, or to teach a digital creature to walk. Usually, we are interested in a series of actions that are in some sense, optimal in a given environment. Despite the fact that many enormous tomes exist to discuss the mathematical details, the intuition behind the algorithm itself is remarkably simple. Choose an action, and if you get rewarded for it, try to find out which series of actions led to this and keep doing it. If the rewards are not coming, try something else. The reward can be, for instance, our score in a computer game or how far our digital creature could walk. Approximately a 300 episodes ago, OpenAI published one of their first major works by the name Gym, where anyone could submit their solutions and compete against each other on the same games. It was like Disneyworld for reinforcement learning researchers. A moment ago, I noted that in reinforcement learning, if the rewards are not coming we have to try something else. Hmm..is that so? Because there are cases where trying crazy new actions is downright dangerous. For instance, imagine that during the training of this robot arm, initially, it would try random actions and start flailing about, where it may damage itself, some other equipment, or even worse, humans may come to harm. Here you see an amusing example of DeepMind\u2019s reinforcement learning agent from 2017 that liked to engage in similar flailing activities. So, what could be a possible solution for this? Well, have a look at this new work from OpenAI by the name Safety Gym. In this paper, they introduce what they call the constrained reinforcement learning formulation, in which these agents can be discouraged from performing actions that are deemed potentially dangerous in an environment. You can see an example here where the AI has to navigate through these environments and achieve a task, such as reaching the green goal signs, push buttons, or move a box around to a prescribed position. The constrained part comes in whenever some sort of safety violation happens, which are, in this environment, collisions with the boxes or blue regions. All of these events are highlighted with this red sphere and a good learning algorithm should be instructed to try to avoid these. The goal of this project is that in the future, for reinforcement learning algorithms, not only the efficiency, but the safety scores should also be measured. This way, a self-driving AI would be incentivized to not only drive recklessly to the finish line, but respect our safety standards along the journey as well. While noting that clearly, self-driving cars may be achieved with other kinds of algorithms, many of which have been in the works for years, there are many additional applications for this work: for instance, the paper discusses the case of incentivizing recommender systems to not show psychologically harmful content to its users, or to make sure that a medical question answering system does not mislead us with false information. This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. They offer you virtual servers that make it easy and affordable to host your own app, site, project, or anything else in the cloud. Whether you\u2019re a Linux expert or just starting to tinker with your own code, Linode will be useful for you. A few episodes ago, we played with an implementation of OpenAI\u2019s GPT-2 where our excited viewers accidentally overloaded the system. With Linode's load balancing technology, and instances ranging from shared nanodes all the way up to dedicated GPUs you don't have to worry about your project being overloaded. To get 20 dollars of free credit, make sure to head over to\u00a0linode.com/papers\u00a0and sign up today using the promo code \u201cpapers20\u201d. Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_s7Bg6yVOdo",
        "paper_link": "https://openai.com/blog/safety-gym/",
        "paper_title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
    },
    {
        "video_id": "cpxtd-FKY1Y",
        "video_title": "These Natural Images Fool Neural Networks (And Maybe You Too)",
        "position_in_playlist": 394,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nTheir blog post on training a neural network is available here: https://www.wandb.com/articles/mnist \n\n\ud83d\udcdd The paper \"Natural Adversarial Examples\" and its dataset are available here:\nhttps://arxiv.org/abs/1907.07174\nhttps://github.com/hendrycks/natural-adv-examples \n\nAndrej Karpathy's image classifier: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n\nYou can also join us here to get early access to these videos: https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n \ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Anastasia Marchenkova, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers \n\nThumbnail background image credit: https://pixabay.com/images/id-4344997/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In the last few years, neural network-based learning algorithms became so good at image recognition tasks that they can often rival, and sometimes even outperform humans in these endeavors. Beyond making these neural networks even more accurate in these tasks, interestingly, there is also plenty of research work on how to attack and mislead these neural networks. I think this area of research is extremely exciting and I\u2019ll now try to show you why. One of the first examples of an adversarial attack can be performed as follows. We present such a classifier with an image of a bus, and it will successfully tell us that yes, this is indeed a bus. Nothing too crazy here. Now, we show it not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. I will stress that this is not any kind of noise, but the kind of noise that exploits biases in the neural network, which is, by no means easy or trivial to craft. However, if we succeed at that, this kind of adversarial attack can be pulled off on many different kinds of images. Everything that you see here on the right will be classified as an ostrich by the neural network these noise patterns were crafted for. In a later work, researchers of the Google Brain team found that we can not only coerce the neural network into making some mistake, but we can even force it to make exactly the kind of mistake we want! This example here reprograms an image classifier to count the number of squares in our images. However, interestingly, some adversarial attacks do not need carefully crafted noise, or any tricks for that matter. Did you know that many of them occur naturally in nature! This new work contains a brutally hard dataset with such images that throw off even the best neural image recognition systems. Let\u2019s have a look at an example. If I were the neural network, I would look at this squirrel and claim that \u201cwith high confidence, I can tell you that this is a sea lion\u201d. And you human, may think that this is a dragonfly, but you would be wrong. I am pretty sure that this is a manhole cover! Except that it\u2019s not. The paper shows many of these examples, some of which don\u2019t really occur in my brain. For instance, I don\u2019t see this mushroom as a pretzel at all, but there was something about that dragonfly that, upon a cursory look, may get registered as a manhole cover. If you look quickly, you see a squirrel here\u2026just kidding, it\u2019s a bullfrog. I feel that if I look at some of these with a fresh eye, sometimes I get a similar impression as the neural network. I\u2019ll put up a bunch of more examples for you here, let me know in the comments which are the ones that got you. Very cool project, I love it. What\u2019s even better, this dataset by the name ImageNet-A is now available for everyone, free of charge. And if you remember, at the start of the video, I said that it is brutally hard for neural networks to identify what is going on here. So what kind of success rates can we expect? 70%? Maybe 50%? Nope. 2%. Wow. In a world where some of these learning-based image classifiers are better than us at some datasets, they are vastly outclassed by us humans on these natural adversarial examples. If you have a look at the paper, you will see that the currently known techniques to improve the robustness of training show little to no improvement to this. I cannot wait to see some followup papers on how to crack this nut. We can learn so much from this paper, and will likely learn even more from these followup works. Make sure to subscribe and also hit the bell icon to never miss future episodes. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. In this post, they show you how to train a state of the art machine learning model with over 99% accuracy on classifying squiggly handwritten numbers and how to use their tools to get a crystal clear understanding of what your model exactly does and what part of the letters it is looking at. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=cpxtd-FKY1Y",
        "paper_link": "https://arxiv.org/abs/1907.07174",
        "paper_title": "Natural Adversarial Examples"
    },
    {
        "video_id": "O8l4Kn-j-5M",
        "video_title": "This Robot Arm Learned To Assemble Objects It Hasn\u2019t Seen Before",
        "position_in_playlist": 395,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly\" is available here:\nhttps://form2fit.github.io/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Have a look and marvel at this learning-based assembler robot that is able to put together simple contraptions. Since this is a neural-network based learning method, it needs to be trained to be able to do this. So, how is it trained? Normally, to train such an algorithm, we would have to show it a lot of pairs of the same contraption, and tell is that this is what it looks like when it\u2019s disassembled, and what you see here is the same thing, assembled. If we did this, this method would be called supervised learning. This would be very time consuming, and potentially expensive as it would require the presence of a human as well. A more convenient way would be to go for unsupervised learning, where we just chuck a lot of things on the table and say, \u201cwell, robot, you figure it out\u201d. However, this would be very inefficient, if at all possible because we would have to provide it many-many contraptions that wouldn\u2019t fit on the table. But this paper went for none of these solutions, as they opted for a really smart self-supervised technique. So what does that mean? Well, first, we give the robot an assembled contraption, and ask it to disassemble it. And therein lies the really cool idea, because disassembling it is easier, and by rewinding the process, it also gets to know how to assemble it later. And, the training process takes place by assembling, disassembling, and doing it over and over again, several hundred times per object. Isn\u2019t this amazing? Love it. However, what is the point of all this? Instead of all this, we could just add explicit instructions to a non-learning based robot to assemble the objects. Why not just do that? And the answer lies in one of the most important aspects within machine learning - generalization. If we program a robot to be able to assemble one thing, it will be able to do exactly that - assemble one thing. And whenever we have a new contraption on our hands, we\u2019ll need to reprogram it. However, with this technique, after the learning process took place, we will be able to give it a new, previously unseen object and it will have a chance to assemble it. This requires intelligence to perform. So, how good is it at generalization? Well, get this, the paper reports that when showing it new objects, it was able to successfully assemble them 86% of the time. Incredible. So what about the limitations? This technique works on a 2D planar surface, for instance, this table, and while it is able to insert most of these parts vertically - it does not deal well with more complex assemblies that require inserting screws and pegs in a 45 degree angle. As we always say, two more papers down the line, and this will likely be improved significantly. I you have ever bought a bed or a cupboard and said, well, it just looks like a block, how hard can it be to assemble? Wait, does this thing have more than a 100 screws and pegs? I wonder why? And then, 4.5 hours later, you find out yourself. I hope techniques like these will help us save time by enabling us to buy many of these contraptions pre-assembled, and it can be used for much, much more. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=O8l4Kn-j-5M",
        "paper_link": "https://form2fit.github.io/",
        "paper_title": "Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly"
    },
    {
        "video_id": "hYV4-m7_SK8",
        "video_title": "MuZero: DeepMind\u2019s New AI Mastered More Than 50 Games",
        "position_in_playlist": 396,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\" is available here:\nhttps://arxiv.org/abs/1911.08265\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers \n\nThumbnail background image credit: https://pixabay.com/images/id-1215079/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Some papers come with an intense media campaign and a lot of nice videos, and some other amazing papers are at the risk of slipping under the radar because of the lack of such a media presence. This new work from DeepMind is indeed absolutely amazing, you\u2019ll see in a moment why, and is not really talked about. So in this video, let\u2019s try to reward such a work. In many episodes, you get ice cream for your eyes, but today, you get ice cream for your mind. Buckle up. In the last few years, we have seen DeepMind\u2019s AI defeat the best Go players in the world, and after OpenAI\u2019s venture in the game of DOTA2, DeepMind embarked on a journey to defeat pro players in Starcraft 2, a real-time strategy game. This is a game that requires a great deal of mechanical skill, split-second decision making and we have imperfect information as we only see what our units can see. A nightmare situation for any AI. You see some footage of its previous games here on the screen. And, in my opinion, people seem to pay too much attention to how good a given algorithm performs, and too little to how general it is. Let me explain. DeepMind has developed a new technique that tries to rely more on its predictions of the future, and generalizes to many many more games than previous techniques. This includes AlphaZero, a previous technique also from them that was able to play Go, Chess, and Japanese Chess or Shogi as well and beat any human player at these games confidently. This new method is so general, that it does as well as AlphaZero at these games, however, it can also play a wide variety of Atari games as well. And that is the key here: writing an algorithm that plays chess well has been a possibility for decades. For instance, if you wish to know more, make sure to check out Stockfish, which is an incredible open-source project and a very potent algorithm. However, Stockfish cannot play anything else - whenever we look at a new game, we have to derive a new algorithm that solves it. Not so much with these learning methods, that can generalize to a wide variety of games! This is why I would like to argue that the generalization capability of these AIs is just as important as their performance. In other words, if there were a narrow algorithm that is the best possible Chess algorithm that ever existed, or a somewhat below world-champion level AI that can play any game we can possibly imagine, I would take the latter in a heartbeat. Now, speaking about generalization, let\u2019s see how well it does at these Atari games, shall we? After 30 minutes of time on each game, it significantly outperforms humans on nearly all of these games, the percentages show you here what kind of outperformance we are talking about. In many cases, the algorithm outperforms us several times, and up to several hundred times. Absolutely incredible. As you see, it has a more than formidable score on almost all of these games, and therefore it generalizes quite well. I\u2019ll tell you in a moment about the games it falters at, but for now, let\u2019s compare it to three other competing algorithms. You see one bold number per row, which always highlights the best performing algorithm for your convenience. The new technique beats the others on about 66% of the games, including the Recurrent Experience Replay technique, in short, R2D2. Yes, this is another one of those crazy paper names. And even when it falls short, it is typically very close. As a reference, humans triumphed on less than 10% of the games. We still have a big fat zero on Pitfall and Montezuma\u2019s Revenge games. So why is that? Well, these games require long-term planning, which is one of the more difficult cases for reinforcement learning algorithms. In an earlier episode, we discussed how we can infuse an AI agent with a curiosity to go out there and explore some more with success. However, note that these algorithms are more narrow than the one we\u2019ve been talking about today. So there is still plenty of work to be done, but I hope you see that this is incredibly nimble progress on AI research. Bravo DeepMind! What a time to be alive! This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. They offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. Exactly the kind of works you see here in this series. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To spin up your own GPU instance and receive\u00a0a $20 free credit, visit\u00a0linode.com/papers\u00a0or click the link in the description and use the promo code \u201cpapers20\u201d during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hYV4-m7_SK8",
        "paper_link": "https://arxiv.org/abs/1911.08265",
        "paper_title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"
    },
    {
        "video_id": "SIGQSgifs6s",
        "video_title": "Baking And Melting Chocolate Simulations Are Now Possible! \ud83c\udf6b",
        "position_in_playlist": 397,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nTheir blog post is available here:\nhttps://www.wandb.com/tutorial/build-a-neural-network\n\n\ud83d\udcdd The paper \"A Thermomechanical Material Point Method for Baking and Cooking \" is available here:\nhttps://www.math.ucla.edu/~myding/papers/baking_paper_final.pdf\nhttps://dl.acm.org/doi/10.1145/3355089.3356537\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. This is one of those simulation papers where you can look at it for three seconds and immediately know what it\u2019s about. Let\u2019s try that! Clearly, expansion and baking is happening. And now, let\u2019s look inside. Mmmm! Yup, this is done. Clearly, this is a paper on simulating the process of baking! Loving the idea. So how comprehensive is it? Well, other than these phenomena, for a proper baking procedure, the simulator also has to be able to deal with melting, solidification, dehydration, coloring and much, much more. This requires developing a proper thermomechanical model where these materials are modeled as a collection of solids, water, and gas. Let\u2019s have a look at some more results. And we have to stop right here, because I\u2019d like to tell you that the information density on this deceivingly simple scene is just stunning. In the x axis, from the left to the right we have a decreasing temperature in the oven, left being the hottest, and chocolate chip cookies above are simulated with an earlier work from 2014. The ones in the bottom row are made with the new technique. You can see a different kind of shape change as we increase the temperature, discoloration if we crank the oven up even more, and\u2026look there! Even the chocolate chips are melting. Oh my goodness! What a paper! Talking about information density, you can also see here how these simulated pieces of dough of different viscosities react to different amounts of stress. Viscosity means the amount of resistance against deformation, therefore, as we go up, you can witness this kind of resistance increasing. Here you can see a cross section of the bread which shows the amount of heat everywhere. This not only teaches us why crust forms on the outside layer, but you can see how the amount of heat diffuses slowly into the inside. This is a maxed out paper. By this, I mean the execution quality is through the roof, and the paper is considered done not when it looks alright, but when the idea is being pushed to the limit and the work is as good as it can be without trivial ways to improve it. And the results are absolute witchcraft. Huge congratulations to the authors. In fact, double congratulations because it seems to me that this is only the second paper of Mengyuan Ding, the lead author, and it has been accepted to the SIGGRAPH ASIA conference, which is one of the greatest achivements a computer graphics researcher can dream of. A paper of such quality for the second try. Wow. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. They have excellent tutorial videos, in this one, the CEO himself teaches you how to build your own neural network, and more. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SIGQSgifs6s",
        "paper_link": "https://www.math.ucla.edu/~myding/papers/baking_paper_final.pdf",
        "paper_title": "A Thermomechanical Material Point Method for Baking and Cooking "
    },
    {
        "video_id": "wsFgrzYwchQ",
        "video_title": "This Beautiful Fluid Simulator Warps Time\u2026Kind Of \ud83c\udf0a",
        "position_in_playlist": 398,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"A Temporally Adaptive Material Point Method with Regional Time Stepping\" is available here:\nhttp://taichi.graphics/wp-content/uploads/2018/06/asyncmpm.pdf\n\nDisney\u2019s \u201cA Material Point Method For Snow Simulation\u201d is available here:\nVideo: https://www.youtube.com/watch?v=O0kyDKu8K-k\nPaper: https://www.math.ucla.edu/~jteran/papers/SSCTS13.pdf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "the fellow scholars this is too many papers that casual if I hit the opening video sequence of these paper immediately starts with a beautiful smile simulation which I presume is an homage to a legendary Disney paper from twenty thirteen by the name a material point method for snow simulation which to the best of my knowledge was used for the first frozen movie I was super excited to showcase that papers when this series started however unfortunately I was unable to get the rights to do it but I make sure to put a link to the original paper with the same scene in the video description if you're interested now typically we are looking to produce high resolution simulations with lots of detail however this takes from hours to days to compute so how can we deal with this kind of complexity well approximately four hundred videos ago into many papers episode ten we talked about this technique that introduced spatial add up to the tee to this process the adaptive part means that it made the simulation finer and coarser depending on the what parts of the simulation are visible the parts that we don't see it can be run through the course of simulation because we won't be able to see the difference very smart the special part means that we use particles and subdivide the three D. space into great points in which we compute the necessary quantities like velocities and pressures this was a great paper on adaptive fluid simulations but now look at this new paper this one says that it is about temporal at up to eighty there are two issues that immediately arise first we don't know what their productivity means and even if we did we'll find out that this is something that is almost impossible to pull off let me explain there is a great deal of difficulty in choosing the right time steps for such a simulation these simulations are run in a way that we check and resolve all the collisions and then we can advance  the time forward by a tiny amount this amount is called a time step in choosing the appropriate time step has always been a challenge you see if we set it to two large we will be done faster and computer last however we will almost certainly miss some collisions because we skipped over them it gets even worse because the simulation may end up in the state that is so incorrect that it is impossible to recover from and we have to throw the entire thing out if we set it to too low we get a more robust simulation however it will take for many hours to days to compute so what does this temporal activity mean exactly well it means that there is not one global times that for the simulation but time is advanced differently at different places you see here is that the T. this means the numbers chosen for the time steps and a blue color coding means a simple region where there isn't much going on so we can get away with bigger time steps and less computation without missing important events the red regions have to be simulated with smaller time steps because there's a lot going on and we would miss out on that hence the new technique is called an asynchronous method because it is a crazy simulation where time and lance's in different amounts at different spatial regions so how do we test the solution well of course ideally this should look the same as the synchronize simulation so that's it you bet your papers it does look at that absolutely fantastic and since we can get away with less computation it is faster how much faster in the worst cases forty percent faster in the better ones ten times faster so ten of my all nighter fluid simulations can be done in one night sign me up what a time to be alive this episode has been supported by lambda if you're a researcher or stock  top looking for GPGPU compute to run these algorithms check out lambda GPO cloud I've talked about lambdas GPU workstations in other videos and I'm happy to tell you that they are offering GPU cloud services as well the lambda GPU cloud can train image not to ninety three percent accuracy for less than nineteen dollars lambda's web based IDE lets you easily access your instance right in your browser and finally hold on to your papers because the land the GPU cloud cost less than half of a WS and danger make sure to go to land the lab dot com slash papers and sign up for one of their amazing GPU instances today I thank the lambda for helping us make better videos for you thanks for watching out for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=wsFgrzYwchQ",
        "paper_link": "http://taichi.graphics/wp-content/uploads/2018/06/asyncmpm.pdf",
        "paper_title": "A Temporally Adaptive Material Point Method with Regional Time Stepping"
    },
    {
        "video_id": "eTUmmW4ispA",
        "video_title": "This Neural Network Performs Foveated Rendering",
        "position_in_playlist": 399,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos\" is available here:\nhttps://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers \n\nThumbnail background image credit: https://pixabay.com/images/id-1893783/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#vr",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. As humans, when looking at the world, our eyes and brain does not process the entirety of the image we have in front of us, but plays an interesting trick on us. We can only see fine details in a tiny, tiny foveated region that we are gazing at, while our peripheral or indirect vision only sees a sparse, blurry version of the image, and the rest of the information is filled in by our brain. This is a very efficient system, because our vision system only has to process a tiny fraction of the visual data that is in front of us, and it still enables us to interact with the world around us. So what if we would take a learning algorithm that does something similar for digital videos? Imagine that we would need to render a sparse video only every tenth pixel filled with information, and some kind of neural network-based technique would be able to reconstruct the full image similarly to what our brain does. Yes, but that is very little information to reconstruct an image from. So, is it possible? Well, hold on to your papers, because this new work can reconstruct a near-perfect image by looking at less than 10% of the input pixels. So we have this as an input, and we get this. Wow. What is happening here is called a neural reconstruction of foveated rendering data, or you are welcome to refer to it as foveated reconstruction in short during your conversations over dinner. The scrambled text part here is quite interesting, one might think that, well, it could be better, however, given the fact that if you look at the appropriate place in the sparse image, I not only cannot read the text, I am not even sure if I see anything that indicates that there is text there at all! So far, the example assumed that we are looking at a particular point in the middle of the screen, and the ultimate question is, how does this deal with a real-life case where the user is looking around? Let\u2019s see! This is the input\u2026.and the reconstruction. Witchcraft. Let\u2019s have a look at some more results. Note that this method is developed for head-mounted displays, where we have information on where the user is looking over time, and this can make all the difference in terms of optimization. You see a comparison here against a method labeled as \u201cMultiresolution\u201d, this is from a paper by the name \u201cFoveated 3D Graphics\u201d, and you can see that the difference in the quality of the reconstruction is truly remarkable. Additionally, it has been trained on 350 thousand short natural video sequences, and the whole thing runs in real time! Also, note that we often discuss image inpainting methods in this series, for instance, what you see here is the legendary PatchMatch algorithm that is one of these, and it is able to fill in missing parts of an image. However, in image inpainting, most of the image is intact, with smaller regions that are missing. This is even more difficult than image inpainting, because the vast majority of the image is completely missing. The fact that we can now do this with learning-based methods is absolutely incredible. The first author of the paper is Anton Kaplanyan, who is a brilliant and very rigorous mathematician, so of course, the results are evaluated in detail, both in terms of mathematics, and with a user study. Make sure to have a look at the paper for more on that! We got to know each other with Anton during the days when all we did was light transport simulations, all day, every day, and were always speculating about potential projects, and to my great sadness, somehow, unfortunately we never managed to work together for a full project. Again, congratulations Anton! Stunning, beautiful work. What a time to be alive! This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. They offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. Exactly the kind of works you see here in this series. If you feel inspired by these works and you wish to run your experiments or deploy your already existing works through a simple and reliable hosting service, make sure to join over 800,000 other happy customers and choose Linode. To spin up your own GPU instance and receive\u00a0a $20 free credit, visit\u00a0linode.com/papers\u00a0or click the link in the description and use the promo code \u201cpapers20\u201d during signup. Give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=eTUmmW4ispA",
        "paper_link": "https://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/",
        "paper_title": "DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos"
    },
    {
        "video_id": "O-52enqUSNw",
        "video_title": "Is a Realistic Water Bubble Simulation Possible?",
        "position_in_playlist": 400,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nTheir blog post and report on 3D segmentation is available here:\nhttps://app.wandb.ai/nbaryd/SparseConvNet-examples_3d_segmentation/reports?view=nbaryd%2FSemantic%20Segmentation%20of%203D%20Point%20Clouds\n\n\ud83d\udcdd The paper \"Unified Spray, Foam and Bubbles for Particle-Based Fluids\" is available here:\n- https://pdfs.semanticscholar.org/72ec/134f3c87c543be5f95330f73f4eb383c5511.pdf\n- https://cg.informatik.uni-freiburg.de/publications/2012_CGI_sprayFoamBubbles.pdf\n\nThe FLIP Fluids plugin is available here:\nhttps://blendermarket.com/products/flipfluids\n\nIf you wish to understand and implement a simple, real-time fluid simulator, you can check out my thesis here. It runs on your GPU and comes with source code:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/fluid_control_msc_thesis/\n\nIf you are yearning for more, Doyub Kim's Fluid Engine Development is available here:\nhttps://doyub.com/fluid-engine-development/ \n\nRyan Guy's notes on their FLIP Fluids implementation differences from the paper:\n- The paper mentions gathering data from nearby particles in some calculations. Our simulator does not perform a nearest-neighbour particle search so we've made some modifications to to avoid this search. \n- Our whitewater generator uses the Signed Distance Field (SDF) of the liquid surface to derive the surface curvature for locating wavecrests. \n- The SDF is also used to test the depth of the particles, such as if they are located inside or outside of the liquid volume. \n- Instead of using the particles to calculate velocity difference (trapped air potential), we are using velocity values that are stored on the grid.\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers \n\nThumbnail background image credit: Ryan Guy and Dennis Fassbaender\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. If we study the laws of fluid motion from physics, and write a computer program that contains these laws, we can create beautiful fluid simulations like the one you see here. The amount of detail we can simulate with these programs is increasing every year, not only due to the fact that hardware improves over time, but also, the pace of progress in computer graphics research is truly remarkable. However, when talking about fluid simulations, we often see a paper produce a piece of geometry that evolves over time, and of course, the more detailed this geometry is, the better. However, look at this. It is detailed, but something is really missing here. Do you see it? Well, let\u2019s look at the revised version of this simulation to find out what it is. Yes, foam, spray and bubble particles are now present, and the quality of the simulation just got elevated to the next level. Also, if you look at the source text, you see that this is a paper from 2012, and it describes how to add these effects to a fluid simulation. So, why are we talking about a paper that\u2019s about 8 years old? Not only that, but this work was not published at one of the most prestigious journals. Not even close. So, why? Well, you\u2019ll find out in a moment, but I have to tell you that I just got to know about this paper a few days ago, and it is so good it has singlehandedly changed the way I think about research. Note that a variant of this paper has been implemented in a Blender plugin called FLIP Fluids. Blender is a free and open-source modeler program, which is a complete powerhouse, I love it. And this plugin embeds this work into a modern framework, and boy, does it come to life in there. I have rerun one of their simulations and rendered a high-resolution animation with light transport. The fluid simulation took about 8 hours, and as always, I went a little overboard with the light transport, that took about 40 hours. Have a look. It is unreal how good it looks. My goodness. It is one of the miracles of the world that we can take a piece of silicon in our machines, and through the power of science, explain fluid dynamics to it so well that such a simulation can come out of it. I have been working on these for many years now, and I am still shocked by the level of progress in computer graphics research. So, let\u2019s talk about three important aspects of this work. First, it proposes one unified technique to add foam, spray and bubbles in one go to the fluid simulation. One technique to model all three. In the paper, they are collectively called diffuse particles, and if these particles are deeply underwater, they will be classified as bubbles. If they are on the surface of the water, they will be foam particles, and if they are further above the surface, we will call them spray particles. With one method, we get all three of those. Lovely. Two, when I had shown you this footage with and without the diffuse particles, normally I would need to resimulate the whole fluid domain to add these advanced effects, but this is not the case at all. These particles can be added as a post-processing step, which means that I was able to just run the simulation once, and then decide whether to use them or not. Just one click, and here it is, with the particles removed. Absolutely amazing. And three, perhaps the most important part, the technique is so simple I could hardly believe the paper when I saw it. You see, normally, to be able to simulate the formation of bubbles or foam, we would need to compute the Weber numbers, which requires expensive surface tension computations and more. Instead, the paper forfeits that and goes with the notion that bubbles and foam appear at regions where air gets trapped within the fluid. On the back of this knowledge, they note that wave crests are an example of that, and propose a method to find these wave crests by looking for regions where the curvature of the fluid geometry is high and locally convex. Both of these can be found through very simple expressions. Finally, air is also trapped when fluid particles move rapidly towards each other, which is also super simple to compute and evaluate. The whole thing can be implemented in a day and it leads to absolutely killer fluid animations. You see, I have a great deal of admiration for a 20-page long technique that models something very difficult perfectly, but I have at least as much admiration for an almost trivially simple method that gets us to 80% of the perfect solution. This paper is the latter. I love it. This really changed my thinking not only about fluid simulation papers, but this paper is so good, it challenged how I think about research in general. It is an honor to be able to talk about beautiful works like this to you, so thank you so much for coming and listening to these videos. Note that the paper does more than what we\u2019ve talked about here - it also proposes a method to compute the lifetime of these particles, tells us how they get advected by water and more. Make sure to check out the paper in the description for more on that. If you are interested, go and try Blender, that tool is completely free for everyone to use, I have been using it for around a decade now and it truly is incredible that something like this exists as a community effort. The FLIP Fluids plugin is a paid addition. If one pays for it, it can be used immediately, or, if you spend a little time, you can compile it yourself, and this way, you can get it for free. Respect for the plugin authors for making such a gentle business model. If you don\u2019t want to do any of those, even Blender has a usable built-in fluid simulator. You can do incredible things with it, but it can\u2019t produce diffuse particles. I am still stunned by how simple and powerful this technique is. You can really find gems anywhere, not just around the most prestigious research venues. I hope you got inspired by this, and if you wish to understand how these fluids work some more, or write your own simulator, I put a link to my Master\u2019s thesis where I try to explain the whole thing as intuitively as possible, and it also comes with the full source code free of charge for a simulator that runs on your graphics card. If you feel so voracious that even that\u2019s not enough, I\u2019ll also highly recommend Doyub Kim\u2019s book on Fluid Engine Development. That one also comes with free source code. This episode has been supported by Weights & Biases. Here you see their beautiful final report on a point cloud classification project of theirs and see how using different learning rates and other parameters influences the final results. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=O-52enqUSNw",
        "paper_link": "https://pdfs.semanticscholar.org/72ec/134f3c87c543be5f95330f73f4eb383c5511.pdf",
        "paper_title": "Unified Spray, Foam and Bubbles for Particle-Based Fluids"
    },
    {
        "video_id": "o_DhNqHazKY",
        "video_title": "This Neural Network Combines Motion Capture and Physics",
        "position_in_playlist": 401,
        "description": "\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\udcdd The paper \"DReCon: Data-Driven responsive Control of Physics-Based Characters\" is available here:\n- https://montreal.ubisoft.com/en/drecon-data-driven-responsive-control-of-physics-based-characters/\n- https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2019/11/13214229/DReCon.pdf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we often talk about computer animation and physical simulations, and these episodes are typically about one or the other. You see, it is possible to teach a simulated AI agent to lift weights and jump really high using physical simulations to make sure that the movements and forces are accurate. The simulation side is always looking for correctness. However, let\u2019s not forget that things also have to look good. Animation studios are paying a fortune to record motion capture data from real humans and sometimes even dogs to make sure that these movements are visually appealing. So, is it possible to create something that reacts to our commands with the controller, looks good, and also adheres to physics? Well, have a look! This work was developed at Ubisoft La Forge. It responds to our input via the controller and the output animations are fluid and natural. Since it relies on a technique called deep reinforcement learning, it requires training. You see that early on, the blue agent is trying to imitate the white character, and it is not doing well at all. It basically looks like me when going to bed after reading papers all night. The white agent\u2019s movement is not physically simulated and was built using a motion database with only 10 minutes of animation data. This is the one that is in the \u201clooks good\u201d category. Or, it would look really good if it wasn\u2019t pacing around like a drunkard, so the question naturally arises, who in their right minds would control a character like this? Well, of course, no one! This sequence was generated by an artificial, worst-case player which is a nightmare situation for any AI to reproduce. Early on, it indeed is a nightmare. However\u2026after 30 hours of training, the blue agent learned to reproduce the motion of the white character, while being physically simulated. So, what is the advantage of that? Well, for instance, it can interact with the scene better, and is robust against perturbations. This means that it can rapidly recover from undesirable positions. This can be validated via something that the paper calls impact testing. Are you thinking what I am thinking? I hope so, because I am thinking about throwing blocks at this virtual agent, one of our favorite pastimes at Two Minute Papers, and it will be able to handle them. Whoops! Well, most of them anyway. It also reacts to a change in direction much quicker than previous agents. If all that was not amazing enough, the whole control system is very light, and takes only a few microseconds, most of which is spent by not even the control part, but the physics simulation. So, with the power of computer graphics and machine learning research, animation and physics can now be combined beautifully, it does not limit controller responsiveness, looks very realistic, and it is very likely that we\u2019ll see this technique in action in future Ubisoft games. Outstanding. This video was supported by you on Patreon. If you wish to watch these videos in early access, or get your name immortalized in the video description, make sure to go to Patreon.com/TwoMinutePapers and pick up one of those cool perks, or, we are also test driving the early access program here on YouTube, just go ahead and click the join button, or use the link in the description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=o_DhNqHazKY",
        "paper_link": "https://montreal.ubisoft.com/en/drecon-data-driven-responsive-control-of-physics-based-characters/",
        "paper_title": "DReCon: Data-Driven responsive Control of Physics-Based Characters"
    },
    {
        "video_id": "T7w7QuYa4SQ",
        "video_title": "Finally, Differentiable Physics is Here!",
        "position_in_playlist": 402,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nTheir instrumentation for this paper is available here:\nhttps://app.wandb.ai/lavanyashukla/difftaichi\n\n\ud83d\udcdd The paper \"DiffTaichi: Differentiable Programming for Physical Simulation\" is available here:\n- https://arxiv.org/abs/1910.00935\n- https://github.com/yuanming-hu/difftaichi\n\nMy thesis on fluid control (with source code) is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/fluid_control_msc_thesis/\n\n \ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers \n\nThumbnail background image credit: https://pixabay.com/images/id-407081/\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. A few episodes ago, we discussed a new research work that performs something that they call differentiable rendering. The problem formulation is the following: we specify a target image that is either rendered by a computer program, or even better, a photo. The input is a pitiful approximation of it, and now, because it progressively changes the input materials, textures, and even the geometry of this input in a 3D modeler system, it is able to match this photo. At the end of the video, I noted that I am really looking forward for more differentiable rendering and differentiable everything papers. So, fortunately, here we go, this new paper introduces differentiable programming for physical simulations. So what does that mean exactly? Let\u2019s look at a few examples and find out together! Imagine that we have this billiard game, where we would like to hit the white ball with just the right amount of force and from the right direction, such that the blue ball ends up close to the black spot. Let\u2019s try it. Well, this example shows that this doesn\u2019t happen by chance, and we have to engage in a fair amount of trial and error to make this happen. What this differentiable programming system does for us is that we can specify an end state, which is the blue ball on the black dot, and it is able to compute the required forces and angles to make this happen. Very close. But the key point here is that this system is general, and therefore can be applied to many-many more problems. We\u2019ll have a look at a few that are much more challenging than this example. For instance, it can also teach this gooey object to actuate itself in a way so that it would start to walk properly within only 2 minutes. The 3D version of this simulation learned so robustly, so that it can even withstand a few extra particles in the way. The next example is going to be obscenely powerful. I\u2019ll try to explain what this is to make sure we can properly appreciate it. Many years ago, I was trying to solve a problem called fluid control, where we would try to coerce a smoke plume or a piece of fluid to take a given shape. Like a bunny, or a logo with letters. You can see some footage of this project here. The key difficulty of this problem is that this is not what typically happens in reality, of course, a glass of spilled water is very unlikely to suddenly take the shape of a human face, so we have to introduce changes to the simulation itself, but at the same time, it still has to look as if it could happen in nature. If you wish to know more about my work here, the full thesis and the source code is available in the video description, and one of my kind students has even implemented it in Blender. So, this problem is obscenely difficult. So you can now guess what\u2019s next for this differentiable technique\u2026it starts out with a piece of simulated ink with a checkerboard pattern, and it exerts just the appropriate forces so that it forms exactly the Yin-Yang symbol shortly after. I am shocked by how such a general system can perform something of this complexity. Having worked on this problem for a while, I can tell you that this is immensely difficult. Amazing. And hold on to your papers, because it can do even more. In this example, it adds carefully crafted ripples to the water, to make sure that it ends up in a state that distorts the image of the squirrel in a way that a powerful and well-known neural network sees it not as a squirrel, but as a goldfish. This thing is basically a victory lap in the paper. It is so powerful, it\u2019s not even funny. You can just make up some problems that sound completely impossible and it rips right through them. The full source code of this work is also available. By the way, the first author of this paper is Yuanming Hu, his work was showcased several times in this series, in one of the earlier videos, we showcased his amazing Jello simulation that was implemented in so few lines of code, it almost fits on a business card. I said it in a previous episode, and I will say it again. I can\u2019t wait to see more and more papers in differentiable rendering and simulations. And as this work leaves plenty of room for creativity for novel problem definitions, I\u2019d love to hear what you think about it. What else could this be used for? Solving video games faster than other learning-based techniques? Anything else? Let me know in the comments below. What a time to be alive! This episode has been supported by Weights & Biases. Here you see a beautiful final report on one of their projects on classifying parts of street images, and see how these learning algorithms evolve over time. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. It is really easy to set up, so much so that they have made an instrumentation for this exact paper we have talked about in this episode. Have a look here! Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=T7w7QuYa4SQ",
        "paper_link": "https://arxiv.org/abs/1910.00935",
        "paper_title": "DiffTaichi: Differentiable Programming for Physical Simulation"
    },
    {
        "video_id": "SWoravHhsUU",
        "video_title": "StyleGAN2: Near-Perfect Human Face Synthesis...and More",
        "position_in_playlist": 403,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers\n\nTheir blog post on street scene segmentation is available here:\nhttps://app.wandb.ai/borisd13/semantic-segmentation/reports/Semantic-Segmentation-on-Street-Scenes--VmlldzoxMDk2OA\n\n\ud83d\udcdd The paper \"Analyzing and Improving the Image Quality of #StyleGAN\" and its source code is available here:\n- http://arxiv.org/abs/1912.04958\n- https://github.com/NVlabs/stylegan2 \n\nYou can try it here: - https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH#scrollTo=4_s8h-ilzHQc\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#StyleGAN2",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. Neural network-based learning algorithms are on the rise these days, and even though it is common knowledge that they are capable of image classification, or in other words, looking at an image and saying whether it depicts a dog or a cat, nowadays, they can do much, much more. In this series, we covered a stunning paper that showcased a system that could not only classify an image, but write a proper sentence on what is going on, and could cover even highly non-trivial cases. You may be surprised, but this thing is not recent at all. This is 4 year old news! Insanity. Later, researchers turned this whole problem around, and performed something that was previously thought to be impossible. They started using these networks to generate photorealistic images from a written text description. We could create new bird species by specifying that it should have orange legs and a short yellow bill. Later, researchers at NVIDIA recognized and addressed two shortcomings: one was that the images were not that detailed, and two, even though we could input text, we couldn\u2019t exert too much artistic control over the results. In came StyleGAN to the rescue, which was then able to perform both of these difficult tasks really well. These images were progressively grown, which means that we started out with a coarse image, and go over it over and over again, adding new details. This is what the results look like and we can marvel at the fact that none of these people are real. However, some of these images were still contaminated by unwanted artifacts. Furthermore, there are some features that are highly localized as we exert control over these images, you can see how this part of the teeth and eyes are pinned to a particular location and the algorithm just refuses to let it go, sometimes to the detriment of its surroundings. This new work is titled StyleGAN2 and it addresses all of these problems in one go. Perhaps this is the only place on the internet where we can say that finally, teeth and eyes are now allowed to float around freely, and mean it with a positive sentiment. Here you see a few hand-picked examples from the best ones, and I have to say, these are eye-poppingly detailed and correct looking images. My goodness! The mixing examples you have seen earlier are also outstanding. Way better than the previous version. Also, note that as there are plenty of training images out there for many other things beyond human faces, it can also generate cars, churches, horses, and of course, cats. Now that the original StyleGAN 1 work has been out for a while, we have a little more clarity and understanding as to how it does what it does, and the redundant parts of architecture have been revised and simplified. This clarity comes with additional advantages beyond faster and higher-quality training and image generation. Interestingly, despite the fact that the quality has improved significantly, images made with the new method can be detected more easily. Note that the paper does much, much more than this, so make sure to have a look in the video description! In this series, we always say that two more papers down the line, and this technique will be leaps and bounds beyond the first iteration. Well, here we are, not two, only one more paper down the line. What a time to be alive! The source code of this project is also available. What\u2019s more, it even runs in your browser. This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Here you see a beautiful final report on one of their projects on classifying parts of street images, and see how these learning algorithms evolve over time. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SWoravHhsUU",
        "paper_link": "http://arxiv.org/abs/1912.04958",
        "paper_title": "Analyzing and Improving the Image Quality of #StyleGAN"
    },
    {
        "video_id": "9IqRdEs4_JU",
        "video_title": "Simulating Breaking Bread \ud83c\udf5e",
        "position_in_playlist": 404,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"CD-MPM: Continuum Damage Material Point Methods for Dynamic Fracture Animation\" and its source code is available here:\n- https://www.seas.upenn.edu/~cffjiang/research/wolper2019fracture/wolper2019fracture.pdf\n- https://github.com/squarefk/ziran2019\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In a recent video, we showcased a computer graphics technique that simulated the process of baking, and now, it\u2019s time to discuss a paper that is about simulating how we can tear this loaf of bread apart. This paper aligns well with the favorite pastimes of a computer graphics researcher, which is, of course, destroying virtual objects in a spectacular fashion. Like the previous work, this new paper also builds on top of the Material Point Method, a hybrid simulation technique that uses both particles and grids to create these beautiful animations, however, it traditionally does not support simulating cracking and tearing phenomena. Now, have a look at this new work, and marvel at how beautifully this phenomenon is simulated. With this, we can smash oreos, candy crabs, pumpkins, and much, much more. This jelly fracture scene is my absolute favorite. Now, when an artist works with these simulations, the issue of artistic control often comes up. After all, this method is meant to compute these phenomena by simulating physics, and we can\u2019t just instruct physics to be more beautiful\u2026or can we? Well, this technique offers us plenty of parameters to tune the simulation to our liking, two that we\u2019ll note today are alpha, which means the hardening, and beta is the cohesion parameter. So what does that mean exactly? Well, beta was cohesion, which is the force that holds matter together, so as we go to the right, the objects stay more intact, and as we go down, the objects shatter into more and more pieces. The method offers us more parameters than these, but even with these two, we can really make the kind of simulation we are looking for. Ah, what the heck, let\u2019s do two more. We can even control the way the cracks form with the Mc parameter, which is the speed of crack propagation, and G is the energy release which, as we look to the right, increases the object\u2019s resistance to damage. So how long does this take? Well, the technique takes its sweet time, the execution timings range from 17 seconds to about 10 minutes per frame. This is one of those methods that does something that wasn\u2019t possible before, and it is about doing things correctly. And after a paper appears on something that makes the impossible possible, followup research works get published later that further refine and optimize it. So, as we say, two more papers down the line, this will run much faster. Now, a word about the first author of the paper, Joshuah Wolper. Strictly speaking, it is his third paper, but only the second within computer graphics, and my goodness, did he come back with guns blazing. This paper was accepted to the SIGGRAPH conference, which is one of the biggest honors a computer graphics researcher can get, perhaps equivalent to the olympic gold medal for an athlete. It definitely is worthy of a gold medal. Make sure to have a look at the paper in the video description, it is an absolutely beautifully crafted piece of work. Congratulations Joshuah! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9IqRdEs4_JU",
        "paper_link": "https://www.seas.upenn.edu/~cffjiang/research/wolper2019fracture/wolper2019fracture.pdf",
        "paper_title": "CD-MPM: Continuum Damage Material Point Methods for Dynamic Fracture Animation"
    },
    {
        "video_id": "EjVzjxihGvU",
        "video_title": "This Neural Network Restores Old Videos",
        "position_in_playlist": 405,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo: https://www.wandb.com/papers \n\nTheir blog post on training neural networks is available here: https://www.wandb.com/articles/fundamentals-of-neural-networks\n\n\ud83d\udcdd The paper \"DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement\" is available here:\nhttp://iizuka.cs.tsukuba.ac.jp/projects/remastering/en/index.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we often discuss a class of techniques by the name image inpainting. Image inpainting methods are capable of filling in missing details from a mostly intact image. You see the legendary PatchMatch algorithm at work here, which is more than 10 years old, and it is a good old computer graphics method with no machine learning in sight, and after so much time, 10 years is an eternity in research years, it still punches way above its weight. However, with the ascendancy of neural network-based learning methods, I am often wondering whether it would be possible to take a more difficult problem, for instance, inpainting not just still images, but movies as well. For instance, let\u2019s take and old old black and white movie that suffers from missing data, flickering, blurriness, and interestingly, even the contrast of the footage has changed as it faded over time. Well, hold on to your papers, because this learning-based approach fixes all of these, and even more! Step number one is restoration, which takes care of all of these artifacts and contrast issues. You can not only see how much better the restored version is, but it is also reported what the technique did exactly. However, it does more. What more could we possibly ask for? Well, colorization! What it does is that it looks at only 6 colorized reference images that we have to provide, and uses this as art direction and propagate it to the remainder of the frames. And it does an absolutely amazing work at that. It even tells us which reference image it is looking at when colorizing some of these frames, so if something does not come out favorably, we know which image to recolor. The architecture of the neural network that is used for all this also has to follow the requirements appropriately. For instance, beyond the standard spatial convolution layers, it also makes ample use of these blue temporal convolution layers, which helps \u201csmearing out\u201d the colorization information from one reference image to multiple frames. However, in research, a technique is rarely the very first at doing something, and sure enough, this is not the first technique that does this kind of restoration and colorization. So how does it compare to previously published methods? Well, quite favorably. With previous methods, in some cases, the colorization just appears and disappears over time, while it is much more stable here. Also, fewer artifacts make it to the final footage, and since cleaning these up is one of the main objectives of these methods, that\u2019s also great news. If we look at some quantitative results, or in other words, numbers that describe the difference, you see here that we get a 3-4 decibels cleaner image, which is outstanding. Note that the decibel scale is not linear, but a logarithmic scale, therefore if you read 28 instead of 24, it does not mean that it\u2019s just approximately 15% better. It is a much, much more pronounced difference than that. I think these results are approaching a state where they are becoming close to good enough so that we can revive some of these old masterpiece movies and give them a much-deserved facelift. What a time to be alive! This episode has been supported by Weights & Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. They also wrote a guide on the fundamentals of neural networks where they explain in simple terms how to train a neural network properly, what are the most common errors you can make, and how to fix them. It is really great, you got to have a look. So make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=EjVzjxihGvU",
        "paper_link": "http://iizuka.cs.tsukuba.ac.jp/projects/remastering/en/index.html",
        "paper_title": "DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement"
    },
    {
        "video_id": "62Q1NL4k8cI",
        "video_title": "OpenAI Performs Surgery On A Neural Network to Play DOTA 2",
        "position_in_playlist": 406,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Dota 2 with Large Scale Deep Reinforcement Learning\" from #OpenAI is available here:\nhttps://arxiv.org/abs/1912.06680\nhttps://openai.com/projects/five/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#DOTA2",
        "transcript": "dear fellow scholars this is two minute papers with k\u00e1roly if a hit finally the full research paper has appeared on open e I 5 which is an AI that plays dota 2 a multiplayer online battle arena game with a huge cult following and you may not expect this it is not only as good as some of the best players in the world but it also describes a surgery technique that sounds quite unexpected and I promise to tell you what it is later during this video this game is a nightmare for any AI to play because of three main reasons one it requires long-term strategic planning where it is possible that we make one bad decision then a thousand good ones and we still lose the game in the end finding out which decision led to this loss is immensely difficult often even for humans too we have imperfect information meaning that we can only see what our units and buildings can see and three even though these learning agents don't look at the pixels of the game but they see the world as a big bunch of numbers there is just too much information to look at and too many decisions to make compared to chess or go or almost anything else despite these difficulties in 2017 open AI showed us an initial version of their agent that was able to play 1 versus 1 games with only one hero and was able to reliably be dandy a world champion player that was quite an achievement however of course this was meant to be a stepping stone towards something much bigger that is playing the real dota 2 and just two years later a newer version named open AI 5 has appeared defeated the dota 2 world champions and beat 99.4% of human players during an online event that ran for multiple days many voices said that this would never happen so two years to pull this off after the first version I think was an absolute miracle Bravo now note that even this version has two key limitations one in a normal game we can choose from a pool of 117 heroes where this system supports seven dean of them and two items that allow the player to control multiple characters at once have been disabled if I remember correctly from a previous post of theirs invisibility effects are also neglected because the algorithm is not looking at pixels it would either always have this information shown as a bunch of numbers or never neither of these would be good design decisions so thus invisibility is not part of this technique fortunately the paper is now available so I was really excited to look under the hood for some more details so first as I promised what is this surgery thing about you see the training of the neural network part of this algorithm took no less than 10 months now just imagine forgetting to feed an important piece of information into the system or finding a bug what training is underway in cases like this normally we would have to abort the training and start again if we have a new idea as to how to improve the system again we have to abort the training and start again if a new version of dota 2 comes out with some changes you guessed right we start again this would be ok if the training took from the order of minutes to hours but we are talking 10 months here this is clearly not practical so what if there would be a technique that would be able to apply all of these changes to a training process that is already underway well this is what the surgery technique is about here with the blue curve you see the agents key rating improving over time and the red lines with the black triangles show us the dates for the surgeries the authors note that over the 10 month training process they have performed approximately one surgery per two weeks it seems that getting a doctorate in machine learning research is getting a whole new meaning some of them indeed made an immediate difference while others seemingly not so much so how do we assess how potent these surgeries were the they give the agent superpowers well have a look at the rerun part here which is the final Frankenstein's monster agent containing the result of all the surgeries retrained from scratch and just look at how quickly it is trained and not only that but it shoots even higher than the original agent absolute madness apparently open AI is employing some proper surgeons over there at their lab I love it interestingly this is not the only time I've seen the word surgery used in the computer sciences outside of Medicine a legendary mathematician named Grigori Perelman who proved the Poincare conjecture also performed a mathematical technique that he called surgery what's more we even talked about simulating weightlifting and how a simulated AI agent will walk after getting hamstrung and you guessed it right undergoing surgery to fix it what a time to be alive and again an important lesson is that in this project open AI is not spending so much money and resources just to play video games dota 2 is a wonderful testbed to see how their AI compares to humans and complex tasks that involve strategy and teamwork however the ultimate goal is to reuse parts of this system for other complex problems outside of video games for instance the algorithm that you've seen here today can also do this this episode has been supported by Linode Linode is the world's largest independent cloud computing provider unlike entry-level hosting services Linode gives you full back-end access to your server which is your step up to powerful fast fully configurable cloud computing Linode also has one-click apps the streamlined your ability to deploy websites personal VPNs game servers and more if you need something as small as a personal online portfolio Linode has your back and if you need to manage tons of clients websites and reliably serve them to millions of visitors Linode can do that too what's more they offer affordable GPU instances featuring the quadrille r-tx 6000 which is tailor-made for AI scientific computing and computer graphics projects if only I had access to a tool like this while I was working on my PhD studies to receive $20 credit in your new Linode account visit linda.com slash papers or just click the link in the video description and give it a try today our thanks to Lee node for supporting the series and helping us make better videos for you thanks for watching and for your generous support and I'll see you next time",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=62Q1NL4k8cI",
        "paper_link": "https://arxiv.org/abs/1912.06680",
        "paper_title": "Dota 2 with Large Scale Deep Reinforcement Learning"
    },
    {
        "video_id": "Ks7wDYsN4yM",
        "video_title": "Neural Portrait Relighting is Here!",
        "position_in_playlist": 407,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir blog post and example project are available here:\n- https://www.wandb.com/articles/exploring-gradients\n- https://colab.research.google.com/drive/1bsoWY8g0DkxAzVEXRigrdqRZlq44QwmQ\n\n\ud83d\udcdd The paper \"Deep Single Image Portrait Relighting\" is available here:\nhttps://zhhoper.github.io/dpr.html\n\n\u2600\ufe0f Our \"Separable Subsurface Scattering\" paper with source code is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/ \n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. In computer graphics, when we are talking about portrait relighting, we mean a technique that is able to look at an image and change the lighting, and maybe even the materials or geometry after this image has been taken. This is a very challenging endeavor. So can neural networks put a dent into this problem and give us something new and better? You bet! The examples that you see here are done with this new work that uses a learning-based technique, and is able to change the lighting for human portraits, and only requires one input image. You see, normally, using methods in computer graphics to relight these images would require trying to find out what the geometry of the face, materials, and lighting is from the image, and then, we can change the lighting or other parameters, run a light simulation program, and hope that the estimations are good enough to make it realistic. However, if we wish to use neural networks to learn the concept of portrait relighting, of course, we need quite a bit of training data. Since this is not trivially available, the paper contains a new dataset with over 25 thousand portrait images that are relit in 5 different ways. It also proposes a neural network structure that can learn this relighting operation efficiently. It is shaped a bit like an hourglass and contains an encoder and decoder parts. The encoder part takes an image as an input and estimates what lighting could have been used to produce it, while the decoder part is where we can play around with changing the lighting, and it will generate the appropriate image that this kind of lighting would produce. However, there is more to it. What you see here are skip connections that are useful to save insights from different abstraction levels and transfer them from the encoder to the decoder network. So what does this mean exactly? Intuitively, it is a bit like using the lighting estimator network to teach the image generator what it has learned. So, do we really lose a lot if we skip the skip connections? Well, quite a bit, have a look here. The image on the left shows the result using all skip connections, while as we traverse to the right, we see the results omitting them. These connections indeed make a profound difference. Let\u2019s be thankful for the authors of the paper as putting together such a dataset and trying to get an understanding as to what network architectures it would require to get great results like these takes quite a bit of work. However, I\u2019d like to make a note about modeling subsurface light transport. This is a piece of footage from our earlier paper that we wrote as a collaboration with the Activision Blizzard company, and you can see here that including this indeed makes a profound difference in the looks of a human face. I cannot wait to see some followup papers that take more advanced effects like this into consideration for relighting as well. If you wish to find out more about this work, make sure to click the link in the video description. This episode has been supported by Weights & Biases. Here you see a write-up of theirs where they explain how to visualize the gradients running through your models, and illustrate it through the example of predicting protein structure. They also have a live example that you can try! Weights & Biases provides tools to track your experiments in your deep learning projects. It can save you a ton of time and money in these projects and is being used by OpenAI, Toyota Research, Stanford and Berkeley. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Ks7wDYsN4yM",
        "paper_link": "https://zhhoper.github.io/dpr.html\n\n\u2600\ufe0f Our \"Separable Subsurface Scattering\" paper with source code is available here:",
        "paper_title": "Deep Single Image Portrait Relighting"
    },
    {
        "video_id": "B1Dk_9k6l08",
        "video_title": "This Neural Network Turns Videos Into 60 FPS!",
        "position_in_playlist": 408,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir blog post on hyperparameter optimization is available here:\nhttps://www.wandb.com/articles/find-the-most-important-hyperparameters-in-seconds\n\n\ud83d\udcdd The paper \"Depth-Aware Video Frame Interpolation\" and its source code are available here:\nhttps://sites.google.com/view/wenbobao/dain\n\nThe promised playlist with a TON of interpolated videos:\nhttps://www.youtube.com/playlist?list=PLDi8wAVyouYNDl7gGdSbWKdRxIogfeD3H\n\n \ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nFar Cry video source by N00MKRAD: https://www.youtube.com/watch?v=tW0cvyut7Gk&list=PLDi8wAVyouYNDl7gGdSbWKdRxIogfeD3H&index=20\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DainApp",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with K\u00e1roly Zsolnai-Feh\u00e9r. With today's camera and graphics technology, we can enjoy smooth and creamy videos on our devices that were created with 60 frames per second. I also make each of these videos using 60 frames per second, however, it almost always happens that I encounter the paper videos that have anything from 24 to 30 frames per second. In this case, I put them in my video editor that has a 60 fps timeline, so half or even more of these frames will not provide any new information. As we try to slow down the videos for some nice slow-motion action, this ratio is even worse, creating an extremely choppy output video because we have huge gaps between these frames. So, does this mean that there is nothing we can do and have to put up with this choppy footage? No, not at all! Earlier, we discussed two potential techniques to remedy this issue. One was frame blending, which simply computes the average of two consecutive images and presents that as a solution. This helps a little for simpler cases, but this technique is unable to produce new information. Optical flow is a much more sophisticated method that is very capable as it tries to predict the motion that takes place between these frames. This can kind of produce new information and I use this in the video series on a regular basis, but the output footage also has to be carefully inspected for unwanted artifacts. Which are a relatively common occurrence. Now, our seasoned Fellow Scholars will immediately note that we have a lot of high-framerate videos on the internet, why not delete some of the in-between frames, give the choppy and the smooth videos to a neural network, and teach it to fill in the gaps! After the lengthy training process, it should be able to complete these choppy videos properly. So, is that true? Yes, but note that there are plenty of techniques out there that already do this, so what is new in this paper? Well, this work does that, \u2026and\u2026 much more! We will have a look at the results, which are absolutely incredible, but to be able to appreciate what is going on, let me quickly show you this. The design of this neural network tries to produce four different kinds of data to fill in these images. One is optical flows, which is part of previous solutions too, but two, it also produces a depth map that tells us how far different parts of the image are from the camera. This is of utmost importance, because if we rotate the camera around, previously occluded objects suddenly become visible, and we need proper intelligence to be able to recognize this and to fill in this kind of missing information. This is what the contextual extraction step is for, which drastically improves the quality of the reconstruction, and finally, the interpolation kernels are also learned, which gives it more knowledge as to what data to take from the previous and the next frame. Since it also has a contextual understanding of these images, one would think that it needs a ton of neighboring frames to understand what is going on, which, surprisingly, is not the case at all! All it needs is just the two neighboring images. So, after doing all this work, it better be worth it, right? Let\u2019s have a look at some results! Hold on to your papers, and in the meantime, look at how smooth and creamy the outputs are! Love it! Because it also deals with contextual information, if you wish to feel like real Scholar, you can gaze at regions where the occlusion situation changes rapidly and see how well it fills in this kind of information. Unreal. So how does one show that the technique is quite robust? Well, by producing and showing it off on tons and tons of footage - and that is exactly what the authors did! I put a link to a huge playlist with 33 different videos in the description so you can have a look at how well this works on a wide variety of genres. Now, of course, this is not the first technique for learning-based frame interpolation, so let\u2019s see how it stacks up against the competition! Wow, this is quite a value proposition, because depending on the dataset, it comes out first and second place on most examples. The PSNR is the peak signal to noise ratio, while the SSIM is the structural similarity metric, both of which measure how well the algorithm reconstructs these details compared to the ground truth, and both are subject to maximization. Note that none of them are linear, therefore even a small difference in these numbers can mean a significant difference. I think we are now at a point where these tools are getting so much better than their handcrafted optical flow rivals that I think they will quickly find their way to production software. I cannot wait. What a time to be alive! This episode has been supported by Weights & Biases. In this post, they show you which hyperparameters to tweak to improve your model performance. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. They don\u2019t lock you in, and if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=B1Dk_9k6l08",
        "paper_link": "https://sites.google.com/view/wenbobao/dain",
        "paper_title": "Depth-Aware Video Frame Interpolation"
    },
    {
        "video_id": "TbWQ4lMnLNw",
        "video_title": "The Story of Light! \u2600\ufe0f",
        "position_in_playlist": 409,
        "description": "\ud83d\udcdd The paper \"Unifying points, beams, and paths in volumetric light transport simulation\" and its implementation are available here:\n- https://cs.dartmouth.edu/~wjarosz/publications/krivanek14upbp.html\n- http://www.smallupbp.com/\n\nEric Veach's thesis with Multiple Importance Sampling is available here:\nhttps://graphics.stanford.edu/papers/veach_thesis/\n\nMy Light Transport course at the TU Wien is available here:\nhttps://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi\n\nWe are hiring! I recommend the topic \"Lighting Simulation For Architectural Design\":\nhttp://gcd.tuwien.ac.at/?page_id=2404\n\nMy educational light transport program and 1D MIS implementation is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/smallpaint/\n\nWojciech Jarosz's Beams paper is available here:\nhttps://cs.dartmouth.edu/~wjarosz/publications/jarosz11comprehensive.html\n\nErrata:\nWojciech Jarosz is at the Dartmouth College, or simply Dartmouth (not the Dartmouth University). He also started at Disney Research in 2009, a little earlier than I noted. Apologies!\n\n \u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Whenever we look at these amazing research papers on physical simulations, it is always a joy seeing people discussing them in the comments section. However, one thing that caught my attention is that some people comment about how things look, and not on how things move in these papers. Which is fair enough, and to this end, I will devote this episode to talk a little about a few amazing techniques used in light transport simulations. But first things first: when talking about physics simulations, we are talking about a technique that computes how things move. Then, we typically run a light simulation program that computes how things look. The two are completely independent, which means that it is possible that the physical behavior of bread breaking here is correct, but the bread itself does not look perfectly realistic. This second part depends on the quality of the light simulation and the materials used there. We can create such an image by simulating the path of millions and millions of light rays. And initially, this image will look noisy, and as we add more and more rays, this image will slowly clean up over time. If we don\u2019t have a well-optimized program, this can take from hours to days to compute. We can speed up this process by carefully choosing where to shoot these rays, and this is a technique that is called importance sampling. But then, around 1993, an amazing paper appeared by the name Bidirectional Path Tracing, that proposed that we don\u2019t just start building light paths from one direction, but two instead - one from the camera, and one from a light source and then, connect them. This significantly improved the efficiency of these light simulations, however, it opened up a new can of worms. There are many different ways of connecting these paths which leads to mathematical difficulties. For instance, we have to specify the probability of a light path forming, but what do we do if there are multiple ways of producing this light path? There will be multiple probabilities. What do we do with all this stuff? To address this, Eric Veach described a magical algorithm in this thesis, and thus, multiple importance sampling was born. I can say without exaggeration that this is one of the most powerful techniques in all photorealistic rendering research. What multiple importance sampling, or from now on, MIS in short does, is combine these multiple sampling techniques in a way that accentuates the strengths of each of them. For instance, you can see the image created by one sampling technique here, and the image from a different one here. Both of them are quite noisy, but if we combine them with MIS, we get this instead in the same amount of time. A much smoother, less noisy image. In many cases, this can truly bring down the computation times from several hours to several minutes. Absolute witchcraft. Later, even more advanced techniques appeared to accelerate the speed of these light simulation programs. For instance, it is now not only possible to compute light transport between points in space, but between a point and a beam instead. You see the evolution of an image using this photon beam-based technique. This way, we can get rid of the point-based noise and get a much, much more appealing rendering process. The lead author of this beam paper is Wojciech Jarosz, who, three years later, ended up being the head of the Rendering group at the amazing Disney Reseach lab. Around that time he also hired me to work with him at Disney on a project I can\u2019t talk about, which was an incredible and life-changing experience and I will be forever grateful for his kindness. By the way, he is now a professor at the Darthmouth University and just keeps pumping out one killer paper after another. So, as you might have guessed, if it is possible to compute light transport between two points, a point and a beam, later, it became possible to do this between two beams. None of these are for the faint of the heart, but it works really well. But, there is a huge problem. These techniques work with different dimensionalities, or in other words, they estimate the final result so differently, that they cannot be combined with multiple importance sampling. That is indeed a problem, because all of these have completely different strengths and weaknesses. And now, hold on to your papers, because we have finally arrived to the main paper of this episode. It bears the name UPBP, which stands for unifying points, beams and paths, and it formulates multiple importance sampling between all of these different kinds of light transport simulations. Basically, what we can do with this is throw every advanced simulation program we can think of together, and out comes a super powerful version of them that combines all their strengths and nullifies nearly all of their weaknesses. It is absolutely unreal. Here you see four completely different algorithms running, and as you can see, they are noisy and smooth at very different places. They are good at computing different kinds of light transport. And now, hold on to your papers, because the final result with the UPBP technique is this. Wow! Light transport on steroids. While we look at some more results, I will note that in my opinion, this is one of the best papers ever written in light transport research. The crazy thing is that I hardly ever hear anybody talk about it. If any paper would deserve a bit more attention, it is this one, so I hope this video will help with that. I would like to dedicate this video to Jaroslav Krivanek, the first author of this absolutely amazing paper, who has tragically passed away a few months ago. In my memories, I think of him as the True King of Multiple Importance Sampling and I hope that now, you do too. Note that MIS is not limited to light transport algorithms, it is a general concept that can be used together with a mathematical technique called Monte Carlo integration, which is used pretty much everywhere, from finding out what an electromagnetic field looks like, to financial modeling, and much, much more. If you have anything to do with Monte Carlo integration, please read Eric Veach\u2019s thesis and this paper, and if you feel that it is a good fit, try to incorporate Multiple Importance Sampling into your system. You\u2019ll be glad you did. Also, we have recorded my lectures of a Master-level course on light transport simulations at the Technical University of Vienna. In this course, we write write such a light simulation program from scratch, and it is available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. Additionally, I have implemented a small 1 dimensional example of MIS, if you wish to pick it up and try it, that\u2019s also available in the video description. While talking about the Technical University of Vienna - we are hiring for a PhD and a PostDoc position. The call here about \u201cLighting Simulation For Architectural Design\u201d is advised by my PhD advisor, Michael Wimmer, who I highly recommend. Apply now if you feel qualified, the link is in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=TbWQ4lMnLNw",
        "paper_link": "https://cs.dartmouth.edu/~wjarosz/publications/krivanek14upbp.html\n- http://www.smallupbp.com/\n\nEric Veach's thesis with Multiple Importance Sampling is available here:",
        "paper_title": "Unifying points, beams, and paths in volumetric light transport simulation"
    },
    {
        "video_id": "548sCh0mMRc",
        "video_title": "This Neural Network Creates 3D Objects From Your Photos",
        "position_in_playlist": 410,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer\" is available here:\nhttps://nv-tlabs.github.io/DIB-R/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Claudio Fernandes, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1232435/\nThumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In computer graphics research, we spend most of our time dealing with images. An image is a bunch of pixels put onto a 2D plane, which is a tiny window into reality, but reality is inherently 3D. This is easy to understand for us, because if we look at a flat image, we see the geometric structures that it depicts. If we look at this image, we know that this is not a sticker, but a three dimensional fluid domain. If I would freeze an image and ask a human to imagine rotating around this fluid domain, that human would do a pretty good job at that. However, for a computer algorithm, it would be extremely difficult to extract the 3D structure out from this image. So, can we use these shiny new neural network-based learning algorithms to accomplish something like this? Well, have a look at this new technique that takes a 2D image as an input, and tries to guess three things. The cool thing is that the geometry problem we talked about is just the first one. Beyond that, two, it also guesses what the lighting configuration is that leads to an appearance like this, and three, it also produces the texture map for an object as well. This would already be great, but wait, there is more. If we plug all this into a rendering program, we can also specify a camera position, and this position can be different from the one that was used to take this input image. So what does that mean exactly? Well, it means that maybe, it can not only reconstruct the geometry, light and texture of the object, but even put this all together and make a photo of it from a novel viewpoint! Wow. Let\u2019s have a look at an example! There is a lot going on in this image, so let me try to explain how to read it. This image is the input photo, and the white silhouette image is called a mask, which can either be given with the image, or be approximated by already existing methods. This is the reconstructed image by this technique, and then, this is a previous method from 2018 by the name category-specific mesh reconstruction, CMR in short. And, now, hold on to your papers, because in the second row, you see this technique creating images of this bird from different, novel viewpoints! How cool is that! Absolutely amazing. Since we can render this bird from any viewpoint, we can even create a turntable video of it. And all this from just one input photo. Let\u2019s have a look at another example! Here, you see how it puts together the final car rendering in the first column from the individual elements, like geometry, texture, and lighting. The other comparisons in the paper reveal that this technique is indeed a huge step up from previous works. Now, this all sounds great, but what is all this used for? What are some example applications of this 3D object from 2D image thing? Well, techniques like this can be a great deal of help in enhancing the depth perception capabilities of robots, and of course, whenever we would like to build a virtual world, creating a 3D version of something we only have a picture of can get extremely laborious. This could help a great deal with that too. For this application, we could quickly get a starting point with some texture information, and get an artist to fill in the fine details. This might get addressed in a followup paper. And if you are worried about the slight discoloration around the beak area of this bird, do not despair. As we always say, two more papers down the line, and this will likely be improved significantly. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=548sCh0mMRc",
        "paper_link": "https://nv-tlabs.github.io/DIB-R/",
        "paper_title": "Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer"
    },
    {
        "video_id": "mjl4NEMG0JE",
        "video_title": "Can We Detect Neural Image Generators?",
        "position_in_playlist": 411,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation of this paper: https://app.wandb.ai/lavanyashukla/cnndetection/reports/Detecting-CNN-Generated-Images--Vmlldzo2MTU1Mw\n\n\ud83d\udcdd The paper \"CNN-generated images are surprisingly easy to spot...for now\" is available here:\nhttps://peterwang512.github.io/CNNDetection/\n\nOur Discord server is now available here and you are all invited!\nhttps://discordapp.com/invite/hbcTJu2\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepFake #DeepFakes",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, we have an abundance of neural network-based image generation techniques. Every image that you see here and throughout this video is generated by one of these learning-based methods. These can offer high-fidelity synthesis, and not only that, but we can often even exert artistic control over the outputs. We can truly do so much with these. And if you are wondering, there is a reason why we will be talking about an exact set of techniques, and you will see that in a moment. So the first one is a very capable technique by the name CycleGAN! This was great at image translation, or in other words, transforming apples into oranges, zebras into horses, and more. It was called CycleGAN because it introduced a cycle consistency loss function. This means that if we convert a summer image to a winter image, and then back to a summer image, we should get the same input image back. If our learning system obeys to this principle, the output quality of the translation is going to be significantly better. Later, a technique by the name BigGAN appeared, which was able to create reasonably high quality images and not only that, but it also gave us a little artistic control over the outputs. After that, StyleGAN and even its second version appeared, which, among many other crazy good features, opened up the possibility to lock in several aspects of these images, for instance, age, pose, some facial features and more, and then, we could mix them with other images to our liking, while retaining these locked-in aspects. And of course, DeepFake creation provides fertile grounds for research works, so much so that at this point, it seems to be a subfield of its own where the rate of progress is just stunning. Now that we can generate arbitrarily many beautiful images with these learning algorithms, they will inevitably appear in many corners of the internet, so an important new question arises - can we detect if an image was made by these methods? This new paper argues that the answer is a resounding yes. You see a bunch of synthetic images above, and real images below here, and if you look carefully for the labels, you\u2019ll see many names that ring a bell to our Scholarly minds. CycleGAN, BigGAN, StyleGAN\u2026nice! And now, you know that this is exactly why we briefly went through what these techniques do at the start of the video. So, all of these can be detected by this new method. And now, hold on to your papers, because I kind of expected that, but what I didn\u2019t expect is that this detector was trained on only one of these techniques, and leaning on that knowledge, it was able to catch all the others! Now that\u2019s incredible. This means that there are foundational elements that bind together all of these techniques. Our seasoned Fellow Scholars know that this similarity is none other than the that the fact that they are all built on convolutional neural networks. They are vastly different, but they use very similar building blocks. Imagine the convolutional layers as lego pieces, and think of the techniques themselves to be the objects that we build using them. We can build anything, but what binds these all together is that they are all but a collection of lego pieces. So, this detector was only trained on real images and synthetic ones created by the ProGAN technique, and you see with the blue bars that the detection ratio is quite close to perfect for a number of techniques, save for these two. The AP label means average precision. If you look at the paper in the description, you will get a lot more insights as to how robust it is against compression artifacts, a little frequency analysis of the different synthesis techniques, and more. Let\u2019s send a huge thank you to the authors of the paper, who also provide the source code and training data for this technique. For now, we can all breathe a sigh of relief that there are proper detection tools that we can train ourselves at home. In fact, you will see such an example in a second. What a time to be alive! Good news! We now have an unofficial discord server where all of you Fellow Scholars are welcome to discuss ideas and learn together in a kind and respectful environment. Look, some connections and discussions are already being made - thank you so much for our volunteering Fellow Scholars for making this happen! The link is available in the video description, it is completely free, if you have joined, make sure to leave a short introduction! Meanwhile, what you see here is an instrumentation of this exact paper we have talked about, which was made by Weights and Biases. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mjl4NEMG0JE",
        "paper_link": "https://peterwang512.github.io/CNNDetection/",
        "paper_title": "CNN-generated images are surprisingly easy to spot...for now"
    },
    {
        "video_id": "yX84nGi-V7E",
        "video_title": "Transferring Real Honey Into A Simulation \ud83c\udf6f",
        "position_in_playlist": 412,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Video-Guided Real-to-Virtual Parameter Transfer for Viscous Fluids\" is available here:\nhttp://gamma.cs.unc.edu/ParameterTransfer/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. It\u2019s time for some fluid simulations again! Writing fluid simulations is one of the most fun things we can do within computer graphics, because we can create a virtual scene, add the laws of physics for fluid motion, and create photorealistic footage with an absolutely incredible amount of detail and realism. Note that we can do this ourselves, so much so, that for this scene, I ran the fluid and light simulation myself here at the Two Minute Papers studio, and, on consumer hardware. However, despite this amazing looking footage, we are not nearly done yet! There is still so much to explore! For instance, a big challenge these days is trying to simulate fluid-solid interactions. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. This is what we refer to as two-way coupling. We also know that there are different kinds of two-way coupling, and only the more advanced ones can correctly simulate how real honey supports the dipper and there is barely any movement. This may be about the only place on the internet where we are super happy that nothing at all is happening. However, many of you astute Fellow Scholars immediately ask, okay, but what kind of honey are we talking about? We can buy tens, if not hundreds of different kinds of honey at the market. If we don\u2019t know what kind of honey we are using, how do we know if this simulation is too viscous, or not viscous enough? Great question! Just to make sure we don\u2019t get lost, viscosity means the amount of resistance against deformation, therefore, as we go up, you can witness this kind of resistance increasing. And now, hold on to your papers because this new technique comes from the same authors as the previous one with the honey dipper, and enables us to import our real-world honey into our simulation. That sounds like science fiction. Importing real-world materials into a computer simulation? How is that even possible? Well, with this solution, all we need to do is point a consumer smartphone camera at the phenomenon and record it. The proposed technique does all the heavy lifting by first, extracting the silhouette of the footage, and then, creating a simulation that tries to reproduce this behavior. The closer it is, the better. However, at first, of course, we don\u2019t know the exact parameters that would result in this, however, now we have an objective we can work towards. The goal is to re-run this simulation with different parameter sets, in a way to minimize the difference between the simulation and reality. This is not just working by trial and error but through a technique that we refer to as mathematical optimization. As you see, later, the technique was able to successfully identify the appropriate viscosity parameter. And when evaluating these results, note that this work does not deal with how things look - for instance, whether the honey has the proper color or translucency is not the point here. What we are trying to reproduce is not how it looks, but how it moves. It works on a variety of different fluid types. I have slowed down some of these videos to make sure we can appreciate together how amazingly good these estimations are. And we\u2019re not even done yet! If we wish to, we can even set up a similar scene as the real-world one with our simulation as a proxy for the real honey or caramel flow. After that, we can perform anything we want with this virtual piece of fluid, even including putting it into novel scenarios, like this scene which would otherwise be difficult to control, and quite wasteful, or even creating the perfect honey dipper experiment. Look at how perfect the symmetry is there down below! Yum! Normally, in a real-world environment, we cannot pour the honey and apply forces this accurately, but in a simulation, we can do anything we want! And now, we can also import the exact kind of materials from our real world repertoire. If you can buy it, you can simulate it. What a time to be alive! This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client\u2019s websites and reliably serve them to millions of visitors, Linode can do that too. What\u2019s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit\u00a0linode.com/papers\u00a0or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=yX84nGi-V7E",
        "paper_link": "http://gamma.cs.unc.edu/ParameterTransfer/",
        "paper_title": "Video-Guided Real-to-Virtual Parameter Transfer for Viscous Fluids"
    },
    {
        "video_id": "HcB3ImpYeQU",
        "video_title": "Deformable Simulations\u2026Running In Real Time! \ud83d\udc19",
        "position_in_playlist": 413,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers\n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/visualize-lightgbm-performance-in-one-line-of-code\n\n\ud83d\udcdd The paper \"A Scalable Galerkin Multigrid Method for Real-time Simulation of Deformable Objects\" is available here:\nhttp://tiantianliu.cn/papers/xian2019multigrid/xian2019multigrid.html\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. With the power of modern computer graphics and machine learning techniques, we are now able to teach virtual humanoids to walk, sit, manipulate objects, and we can even make up new creature types and teach them new tricks\u2026if we are patient enough, that is. But, even with all this knowledge, we are not done yet, are we? Should we just shut down all the research facilities, because there is nothing else to do? Well, if you have spent any amount of time watching Two Minute Papers, you know that the answer is, of course not! There is so much to do I don\u2019t even know where to start! For instance, let\u2019s consider the case of deformable simulations. Not so long ago, we talked about Yuanming Hu\u2019s amazing paper with which, we can engage in the favorite pastime of a computer graphics researcher, which is, of course, destroying virtual objects in a spectacular manner. It can also create remarkably accurate jello simulations, where we can even choose our physical parameters. Here you see how we can drop in blocks of different densities into the jello, and as a result, they sink in deeper and deeper. Amazing. However, note that this is not for real-time applications and computer games because the execution time is measured not in frames per second, but in seconds per frame. If we are looking for somewhat coarse results, but in real time, we have covered a paper approximately 300 episodes ago, which performed something that is called a Reduced Deformable Simulation. Leave a comment if you were already a Fellow Scholar back then! This technique could be trained on a number of different representative cases, which, in computer graphics research, is often referred to as precomputation, which means that we have to do a ton of work before starting a task, but only once, and then, all our subsequent simulations can be sped up. Kind of like a student studying before an exam, so when the exam itself happens, the student, in the ideal case, will know exactly what to do. Imagine trying to learn the whole subject during the exam! Note that this training in this technique is not the same kind of training we are used to see with neural networks, and its generalization capabilities were limited, meaning that if we strayed too far from the training examples, the algorithm did not work so reliably. And now, hold on to your papers, because this new method contains a ton of optimizations, runs on your graphics card, and hence, can perform these deformable simulations at close to 40 frames per second. And in the following examples in a moment, you will see something even better. A killer advantage of this method is that this is also scalable. This means that the resolution of the object geometry can be changed around, here, the upper left is a coarse version of the object, where the lower right is the most refined version of it. Of course, the number of frames we can put out per second depends a great deal on the resolution of this geometry, and if you have a look, this looks very close to the one below it, but is still more than 3 to 6 times faster than real time. Wow. And whenever we are dealing with collisions, lots of amazing details appear. Just look at this! Let\u2019s look at a little more formal measurement of the scalability of this method. Note that this is a log-log plot, since the number of tetrahedra used for the geometry and the execution time spans many orders of magnitude. In other words, we can see how it works from the coarsest piece of geometry to the most detailed models we can throw at it. If we look at something like this, we are hoping that the lines are not too steep, which is the case for both the memory and execution timings. So, finally, real-time deformable simulations, here we come! What a time to be alive! This episode has been supported by Weights & Biases. Here, they show you how to make it to the top of Kaggle leaderboards by using their tool to find the best model faster than everyone else. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HcB3ImpYeQU",
        "paper_link": "http://tiantianliu.cn/papers/xian2019multigrid/xian2019multigrid.html",
        "paper_title": "A Scalable Galerkin Multigrid Method for Real-time Simulation of Deformable Objects"
    },
    {
        "video_id": "-IbNmc2mTz4",
        "video_title": "This Neural Network Learned The Style of Famous Illustrators",
        "position_in_playlist": 414,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers\n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/better-models-faster-with-weights-biases\n\n\ud83d\udcdd The paper \"#GANILLA: Generative Adversarial Networks for Image to Illustration Translation\" is available here:\nhttps://github.com/giddyyupp/ganilla\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background image credit: https://pixabay.com/images/id-3651473/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In the last few years, we have seen a bunch of new AI-based techniques that were specialized in generating new and novel images. This is mainly done through learning-based techniques, typically a Generative Adversarial Network, a GAN in short, which is an architecture where a generator neural network creates new images, and passes it to a discriminator network, which learns to distinguish real photos from these fake, generated images. These two networks learn and improve together, and generate better and better images over time. What you see here is a set of results created with the technique by the name CycleGAN. This could even translate daytime into nighttime images, reimagine a picture of a horse as if it were a zebra, and more. We can also use it for style transfer, a problem where we have two input images, one for content, and one for style, and as you see here, the output would be a nice mixture of the two. However, if we use CycleGAN for this kind of style transfer, we\u2019ll get something like this. The goal was to learn the style of a select set of famous illustrators of children\u2019s books by providing an input image with their work. So, what do you think about the results? Well, the style is indeed completely different, but the algorithm seems a little too heavy-handed and did not leave the content itself intact. Let\u2019s have a look at another result with a previous technique. Maybe this will do better. This is DualGAN, which refers to a paper by the name Unsupervised dual learning for image-to-image translation. This uses two GANs to perform image translation, where one GAN learns to translate, for instance, from day to night, while the other one learns the opposite, night to day translation. This, among other advantages, makes things very efficient, but as you see here, in these cases, it preserves the content of the image, but perhaps a little too much, because the style itself does not appear too prominently in the output images. So CycleGAN is good at transferring style, but a little less so for content, and DualGAN is good at preserving the content, but sometimes adds too little of the style to the image. And now, hold on to your papers, because this new technique by the name GANILLA offers us these results. The content is intact, checkmark, the style goes through really well, checkmark. It preserves the content and transfers the style at the same time! Excellent! One of the many key reasons as to why this happens is the usage of skip connections, which help preserve the content information as we travel deeper into the neural network. So, finally, let\u2019s put our money where our mouth is and take a bunch of illustrators, marvel at their unique style, and then, apply it to photographs and see how the algorithm stacks up against other previous works. Wow. I love these beautiful results. These comparisons really show how good the new GANILLA technique is at preserving content. And note that these are distinct artistic styles that are really difficult to reproduce, even for humans. It is truly amazing that we can perform such a thing algorithmically. Don\u2019t forget that the first style transfer paper appeared approximately 3-3.5 years ago, and now, we have come a long-long way! The pace of progress in machine learning research is truly stunning! While we are looking at some more amazing results, this time around, only from GANILLA, I will note that the authors also made a user study with 48 people who favored this against previous techniques. And, perhaps leaving the best for last, it can even draw in the style of Hayao Miyazaki. I bet there are a bunch of Miyazaki fans watching, so let me know in the comments what you think about these results! What a time to be alive! This episode has been supported by Weights & Biases. In this post they show you how to easily iterate on models by visualizing and comparing experiments in real time. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-IbNmc2mTz4",
        "paper_link": "https://github.com/giddyyupp/ganilla",
        "paper_title": "#GANILLA: Generative Adversarial Networks for Image to Illustration Translation"
    },
    {
        "video_id": "bXzauli1TyU",
        "video_title": "This Neural Network Regenerates\u2026Kind Of \ud83e\udd8e",
        "position_in_playlist": 415,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers\n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/visualize-xgboost-in-one-line\n\n\ud83d\udcdd The paper \"Growing Neural Cellular Automata\" is available here: \nhttps://distill.pub/2020/growing-ca/\n\nGame of Life source: https://copy.sh/life/\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to play with a cellular automaton. You can imagine these automata as small games where we have a bunch of cells, and a set of simple rules that describe when a cell should be full, and when it should be empty. These rules typically depend on the state of the neighboring cells. For instance, perhaps the most well-known form of this cellular automaton is John Horton Conway\u2019s Game of Life, which a simulates a tiny world where each cell represents a little life form. The rules, again, depend on the neighbors of this cell - if there are too many neighbors, they will die due to overpopulation, if too few, they will die due to underpopulation, and if they have just the right amount of neighbors, they will thrive, and reproduce. So why is this so interesting? Well, this cellular automaton shows us that a small set of simple rules can give rise to remarkably complex life forms, such as gliders, spaceships, and even John von Neumann\u2019s universal constructor, or in other words, self-replicating machines. I hope you think that\u2019s quite something, and in this paper today, we are going to take this concept further. Way further! This cellular automaton is programmed to evolve a single cell to grow into a prescribed kind of life form. Apart from that, there are many other key differences from other works, and we will highlight two of them them today. One, the cell state is a little different because it can either be empty, growing, or mature, and even more importantly, two, the mathematical formulation of the problem is written in a way that is quite similar to how we train a deep neural network to accomplish something. This is absolutely amazing. Why is that? Well, because it gives rise to a highly-useful feature, namely that we can teach it to grow these prescribed organisms. But wait, over time, some of them seem to decay, some of them can\u2019t stop growing\u2026and, some of them will be responsible for your nightmares, so, from this point on, proceed with care. In the next experiment, the authors describe an additional step in which it can recover from these undesirable states. And now, hold on to your papers, because this leads to the one of the major points of this paper. If it can recover from undesirable states, can it perhaps..regenerate when damaged? Well, here, you will see all kinds of damage\u2026and then, this happens. Wow! The best part is that this thing wasn\u2019t even trained to be able to perform this kind of regeneration! The objective for training was that it should be able to perform its task of growing and maintaining shape, and it turns out, some sort of regeneration is included in that. It can also handle rotations as well, which will give rise to a lot of fun, as noted a moment ago, some nightmarish experiments. And, note that this is a paper in the Distill journal, which not only means that it is excellent, but also interactive, so you can run many of these experiments yourself right in your web browser. If Alexander Mordvintsev, the name of the first author rings a bell, he worked on Google\u2019s Deep Dreams approximately 5 years ago. How far we can some since, my goodness. Loving these crazy, non-traditional research papers and am looking forward to seeing more of these. This episode has been supported by Weights & Biases. Here, they show you how you can visualize the training process for your boosted trees with XGBoost using their tool. If you have a closer look, you\u2019ll see that all you need is one line of code. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bXzauli1TyU",
        "paper_link": "https://distill.pub/2020/growing-ca/",
        "paper_title": "Growing Neural Cellular Automata"
    },
    {
        "video_id": "3Wppf_CNvD0",
        "video_title": "Google\u2019s Chatbot: Almost Perfect \ud83e\udd16",
        "position_in_playlist": 416,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Towards a Human-like Open-Domain Chatbot\" is available here:\nhttps://arxiv.org/abs/2001.09977\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When I was growing up, IQ tests were created by humans to test the intelligence of other humans. If someone told me just 10 years ago that algorithms will create IQ tests to be taken by other algorithms, I wouldn\u2019t have believed a word of it. Yet, just a year ago, scientists at DeepMind created a program that is able to generate a large amount of problems that test abstract reasoning capabilities. They are inspired by human IQ-tests with all the these questions about sizes, colors and progressions. And then, they wrote their own neural network to take these tests, which performed remarkably well. How well exactly? In the presence of nasty distractor objects, it was able to find out the correct solution about 62% of the time, and, if we removed these distractors, which, I will note that are good at misdirecting humans too, the AI was correct 78% of the time! Awesome. But today, we are capable of writing even more sophisticated learning algorithms that can even complete our sentences! Not so long ago, the OpenAI lab published GPT-2, a technique that they unleashed to read the internet, and it learned our language by itself. A few episodes ago, we gave it a spin, and I almost fell out of the chair when I saw that it could finish my sentences about fluid simulations in such a scholarly way, that I think, could easily fool a layperson. Have a look here and judge for yourself! This GPT-2 technique was a neural network variant that was trained using 1.5 billion parameters. At the risk of oversimplifying what that means, it roughly refers to the internal complexity of the networks, or in other words, how many weights and connections are there. And now, the Google Brain team has released Meena, an open-domain chatbot that uses 2.6 billion parameters, and shows remarkable human-like properties. The chatbot part means a piece of software or a machine that we can talk to, and the open-domain part refers to the fact that we can try any topic, hotels, movies, the ocean, favorite movie characters, or pretty much anything we can think of and expect the bot to do well. So how do we know that it\u2019s really good? Well, let\u2019s try to evaluate it in two different ways. First, let\u2019s try the super fun, but less scientific way, or, in other words, what we are already doing, looking at chat logs! You see a Meena writing on the left, and a human being on the right, and it not only answers questions sensibly and coherently, but is even capable of cracking a joke. Of course, if you consider a pun to be a joke, that is. You see a selection of topics here, where the user talks with Meena about movies, and the bot expresses the desire to see The Grand Budapest Hotel, which is indeed a very humanlike quality. It can also try to come up with a proper definition of philosophy. And now, since we are scholars, we would also like to measure how humanlike this is in a more scientific manner as well! Now is a good time to hold on to your papers, because this is measured with the Sensibleness and Specificity Average score, from now on, SSA in short, in which, humans are here, previous chatbots are down there, and Meena is right there, close by, which means that it is easy to be confused for a real human. That already sounds like science fiction, however, let\u2019s be a little nosy here and also ask, how do we know that this SSA is any good in predicting what is humanlike and what isn\u2019t? Excellent question. When measuring human-likeness for these chatbots, plugging in this SSA, again, the Sensibleness and Specificity Average, we see that they correlate really strongly, which means that the two seem to measure very similar things, and in this case, SSA can indeed be used as a proxy for human likeness. The coefficient of determination is 0.96. To put this into perspective, this is a several times stronger correlation than we can measure between the intelligence and the grades of a student, which is already a great correlation. This is a remarkable result. Now, what we get out of this is that the SSA is much easier and precise to measure than human likeness, and is hence, used throughout the paper. So, chatbots eh? What are all these things useful for? Well, remember Google\u2019s technique that would automatically use an AI to talk to your callers and screen your calls? Or even make calls on your behalf. When connected to a text to speech synthesizer, something that Google already does amazingly well, Meena could really come alive in our daily lives soon. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=3Wppf_CNvD0",
        "paper_link": "https://arxiv.org/abs/2001.09977",
        "paper_title": "Towards a Human-like Open-Domain Chatbot"
    },
    {
        "video_id": "eTYcMB6Yhe8",
        "video_title": "Can Self-Driving Cars Learn Depth Perception? \ud83d\ude98",
        "position_in_playlist": 417,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers\n\nThe showcased instrumentation post is available here:\nhttps://app.wandb.ai/stacey/sfmlearner/reports/See-3D-from-Video%3A-Depth-Perception-for-Self-Driving-Cars--Vmlldzo2Nzg2Nw\n\n\ud83d\udcdd The paper \"Unsupervised Learning of Depth and Ego-Motion from Video\" is available here:\nhttps://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#SelfDrivingCars",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When we, humans look at an image, or a piece of video footage, such as this one, we all understand that this is just a 2D projection of the world around us. So much so, that if we have the time and patience, we could draw a depth map that describes the distance of each object from the camera. This information is highly useful, because we can use it to create real-time defocus effects for virtual reality and computer games, or even perform this Ken Burns effect in 3D, or in other words, zoom and pan around in a photograph, but, with a beautiful twist, because in the meantime, we can reveal the depth of the image. However, when we show the same images to a machine, all it sees is a bunch of numbers. Fortunately, with the ascendancy of neural network-based learning algorithms, we now have a chance to do this reasonably well. For instance, we discussed this depth perception neural network in an earlier episode, which was trained using large number input-output pairs, where the inputs are a bunch of images, and the outputs are their corresponding depth maps for the neural network to learn from. The authors implemented this with a random scene generator, which creates a bunch of these crazy configurations with a lot of occlusions and computes via simulation the appropriate depth map for them. This is what we call supervised learning, because we have all these input-output pairs. The solutions are given in the training set to guide the training of the neural network. This is supervised learning, machine learning with crutches. We can also use this depth information to enhance the perception of self-driving cars, but this application is not like previous two I just mentioned. It is much, much harder, because in the earlier, supervised learning example, we have trained the network in a simulation, and then, we also use it later in a computer game, which is, of course, another simulation. We control all the variables and the environment here. However, self-driving cars need to be deployed in the real world. These cars also generate a lot of video footage with their sensors, which could be fed back to the neural networks as additional training data\u2026if we had the depth maps for them, which, of course, unfortunately, we don\u2019t. And now, with this, we have arrived to the concept of unsupervised learning. Unsupervised learning is proper machine learning, where no crutches are allowed. We just unleash the algorithm on a bunch of data, with no labels, and if we do it well, the neural network will learn something useful from it. It is very convenient, because any video we have may be used as training data. That would be great. But we have a tiny problem, and that tiny problem is that that this sounds impossible. Or it may have sounded impossible, until this paper appeared. This work promises us no less than unsupervised depth learning from videos. Since this is unsupervised, it means that during training, all it sees is unlabeled videos from different viewpoints, and somehow, figures out a way to create these depth maps from it. So how is this even possible? Well, it is possible by adding just one ingenious idea. The idea is that since we don\u2019t have the labels, we can\u2019t teach the algorithm how to be right, but instead, we can teach it to be consistent. That doesn\u2019t sound like much, does it? Well, it makes all the difference, because if we ask the algorithm to be consistent, it will find out that a good way to be consistent is to be right! While we are looking at some results, to make this clearer, let me add one more real-world example that demonstrates how cool this idea is. Imagine that you are a university professor overseeing an exam in mathematics, and someone tells you that for one of the problems, most of the students gave the same answer. If this is the case, there is good chance that this was the right answer. It is not a 100% chance that this is the case, but if most of the students have the same answer, it is much more unlikely that they all failed the same way. There are many different ways to fail, but there is only one way to succeed. Therefore, if there is consistency, often there is success. And this simple, but powerful thought leads to far-reaching conclusions. Let\u2019s have a look at some more results! Wo-hoo! Now this is something. Let me explain why I am so excited for this. This is the input image, and this is the perfect depth map that is concealed from our beloved algorithm and is there for us to be able to evaluate its performance. These are two previous works, both use crutches, the first was trained via supervised learning by showing it input-output image pairs with depth maps, and does reasonably well, while the other one gets even less supervision, a worse crutch if you will, and it came up with this. Now, the unsupervised new technique was not given any crutches and came up with this. Holy mother of papers. It looks like a somewhat coarser, but still, very accurate version of the true depth maps. So what do you know! This neural network-based method just looks at unlabeled videos, and finds a way to create depth maps by not trying to be right, but trying to be consistent. This is one of those amazing papers where one simple, brilliant idea can change everything and make the impossible possible. What a time to be alive! What you see here is an instrumentation of this depth learning paper we have talked about, which was made by Weights and Biases. I think organizing these experiments really showcases the usability of their system. Also, Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=eTYcMB6Yhe8",
        "paper_link": "https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/",
        "paper_title": "Unsupervised Learning of Depth and Ego-Motion from Video"
    },
    {
        "video_id": "mUfJOQKdtAk",
        "video_title": "Everybody Can Make Deepfakes Now!",
        "position_in_playlist": 418,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers\n\nTheir blog post is available here:\nhttps://www.wandb.com/articles/hyperparameter-tuning-as-easy-as-1-2-3\n\n\ud83d\udcdd The paper \"First Order Motion Model for Image Animation\" and its source code are available here:\n- Paper: https://aliaksandrsiarohin.github.io/first-order-model-website/\n- Colab notebook: https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepFakes",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. It is important for you to know that everybody can make deepfakes now. You can turn your head around, mouth movements are looking great, and eye movements are also translated into the target footage. And, of course, as we always say, two more papers down the line, and it will be even better and cheaper than this. As you see, some papers are so well done, and are so clear, that they just speak for themselves. This is one of them. To use this technique, all you need to do is record a video of yourself, add just one image of the target subject, run this learning-based algorithm, and there you go. If you stay until the end of this video, you will see even more people introducing themselves as me. As noted, many important gestures are being translated, such as head, mouth and eye movement, but what\u2019s even better, is that even full-body movement works. Absolutely incredible. Now, there are plenty of techniques out there that can create DeepFakes, many of which we have talked about in this series, so what sets this one apart? Well, one, most previous algorithms required additional information, for instance, facial landmarks or a pose estimation of the target subject. This one requires no knowledge of the image. As a result, this technique becomes so much more general. We can create high quality DeepFakes with just one photo of the target subject, make ourselves dance like a professional, and what\u2019s more, hold on to your papers, because it also works on non-humanoid and cartoon models, and even that\u2019s not all, we can even synthesize an animation of a robot arm by using another one as a driving sequence. So, why is it that it doesn\u2019t need all this additional information? Well, if we look under the hood, we see that it is a neural-network based method that generates all this information by itself! It identifies what kind of movements and transformations are taking place in our driving video. You can see that the learned keypoints here follow the motion of the videos really well. Now, we pack up all this information, and send it over to the generator to warp the target image appropriately, taking into consideration possible occlusions that may occur. This means that some parts of the image may now be uncovered where we don\u2019t know what the background should look like. Normally, we would do this by hand, with an image inpainting technique, for instance, you see the legendary PatchMatch algorithm here that does it, however, in this case, the neural network does it automatically, by itself! If you are seeking for flaws in the output, these will be important regions to look at. And it not only requires less information than previous techniques, but it also outperforms them\u2026significantly. Yes, there is still room to improve this, for instance, the sudden head rotation here seems to generate an excessive amount of visual artifacts. The source code and even an example Colab notebook is available, I think it is one of the most accessible papers in this area. Don\u2019t miss out and make sure to have a look in the video description, and try to run your own experiments! Let me know in the comments how they went or feel free to drop by at our discord server, where all of you Fellow Scholars are welcome to discuss ideas and learn together in a kind and respectful environment. The link is available in the video description, it is completely free, if you have joined, make sure to leave a short introduction! Now, of course, beyond the many amazing use-cases of this in reviving deceased actors, creating beautiful visual art, redubbing movies, and more, unfortunately, there are people around the world who are rubbing their palms together in excitement to use this to their advantage. So, you may ask, why make these videos on DeepFakes? Why spread this knowledge, especially now, with the source codes? Well, I think step number one is to make sure to inform the public that these DeepFakes can now be created quickly and inexpensively, and they don\u2019t require a trained scientist anymore. If this can be done, it is of utmost importance that we all know about it! Then, beyond that, step number two, as a service to the public, I attend to EU and NATO conferences, and inform key political and military decision makers about the existence and details of these techniques to make sure that they also know about these and using that knowledge, they can make better decisions for us. You see me doing it here. \u2026and again, you see this technique in action here to demonstrate that it works really well for video footage in the wild. Note that these talks and consultations all happen free of charge, and if they keep inviting me, I\u2019ll keep showing up to help with this in the future as a service to the public. The cool thing is that later, over dinner, they tend to come back to me with a summary of their understanding of the situation, and I highly appreciate the fact that they are open to what we, scientists have to say. And now, please enjoy the promised footage. Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. It is important for you to know that everybody can make deepfakes now. You can turn your head around, mouth movements are looking great, and eye movements are also translated into the target footage. And, of course, as we always say, two more papers down the line, and it will be even better and cheaper than this. This episode has been supported by Weights & Biases. Here, they show you how you can use Sweeps, their tool to search through high-dimensional parameter spaces and find the best performing model. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mUfJOQKdtAk",
        "paper_link": "https://aliaksandrsiarohin.github.io/first-order-model-website/",
        "paper_title": "First Order Motion Model for Image Animation"
    },
    {
        "video_id": "-O7ZJ-AJGRE",
        "video_title": "Is Visualizing Light Waves Possible? \u2600\ufe0f",
        "position_in_playlist": 419,
        "description": "\u2764\ufe0f Check out Weights & Biases here and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir blog post is available here:\nhttps://www.wandb.com/articles/intro-to-keras-with-weights-biases\n\n\ud83d\udcdd The paper \"Progressive Transient Photon Beams\" is available here:\nhttp://webdiis.unizar.es/~juliom/pubs/2019CGF-PTPB/\n\n\ud83d\udcdd The paper \"Femto-Photography: Capturing and Visualizing the Propagation of Light\" is available here: \nhttp://giga.cps.unizar.es/~ajarabo/pubs/femtoSIG2013/\n\nMy light transport course is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\nThe paper with the image of the shown caustics is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\n\nErratum: people see a \"slightly\" younger, not older version of you. Apologies!\n\n \ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dan Kennedy, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Have you heard the saying that whenever we look into the mirror, strictly speaking, we don\u2019t really see ourselves, but we see ourselves from the past\u2026from a few nanoseconds ago. Is that true? If so, why? This is indeed true, and the reason for this is that the speed of light is finite, and it has to travel back from the mirror to our eyes. If you feel that this is really hard to imagine, you are in luck, because a legendary paper from 2013 by the name Femto-photography captured this effect. I would say it is safe to start holding on to your papers from this point basically until the end of this video. Here you can see a super high-speed camera capturing how a wave of light propagates through a bottle, most makes it through, and some gets absorbed by the bottle cap. But this means that this mirror example we talked about shall not only be a thought experiment, but we can even witness it ourselves. Yup, toy first, mirror image second. Approximately a nanosecond apart. So if someone says that you look old, you have an excellent excuse now. The first author of this work was Andreas Velten, who worked on this at MIT, and he is now a professor leading an incredible research group at the University of Wisconsin, Madison. But wait\u2026since it is possible to create light transport simulations, in which we simulate the path of many-many millions of light rays to create a beautiful, photorealistic image, Adri\u00e1n Jarabo thought that he would create a simulator that wouldn\u2019t just give us the final image, but he would show us the propagation of light in a digital, simulated environment. As you see here, with this, we can create even crazier experiments because we are not limited to the real-world light conditions and the limitations of the camera. The beauty of this technique is just unparalleled. He calls this method transient rendering, and this particular work is tailored to excel at rendering caustic patterns. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. I hope that you are not surprised when I say that it is the favorite phenomenon of most light transport researchers. Now, a word about these caustics. We need a super efficient technique to be able to pull this off. For instance, back in 2013, we showcased a fantastic scene made by Vlad Miller that was a nightmare to compute, and it took a community effort and more than a month to accomplish it. Beyond that, the transient renderer only uses very little memory, builds on the photon beams technique we talked about a few videos ago, and always arrives to a correct solution, given enough time. Bravo! And we can do all this, through the power of science. Isn\u2019t it incredible? And if you feel a little stranded at home and are yearning to learn more about light transport, I held a Master-level course on light transport simulations at the Technical University of Vienna. Since I was always teaching it to a handful of motivated students, I thought that the teachings shouldn\u2019t only be available for the privileged few who can afford a college education, but the teachings should be available for everyone. So, the course is now available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. We write a full light simulation program from scratch there, and learn about physics, the world around us, and more. This episode has been supported by Weights & Biases. In this post, they show you how to build and track a simple neural network in Keras to recognize characters from the Simpsons series. You can even fork this piece of code and start right away. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-O7ZJ-AJGRE",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\nThe paper with the image of the shown caustics is available here:",
        "paper_title": "Progressive Transient Photon Beams"
    },
    {
        "video_id": "higGxGmwDbs",
        "video_title": "Muscle Simulation...Now In Real Time! \ud83d\udcaa",
        "position_in_playlist": 420,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"VIPER: Volume Invariant Position-based Elastic Rods\" is available here:\nhttps://arxiv.org/abs/1906.05260\nhttps://github.com/vcg-uvic/viper\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. We have showcased this paper just a few months ago, which was about creating virtual characters with a skeletal system, adding more than 300 muscles and teaching them to use these muscles to kick, jump, move around, and perform other realistic human movements. It came with really cool insights as it could portray how increasing the amount of weight to be lifted changes what muscles are being trained during a workout. These agents also learned to jump really high and you can see a drastic difference between the movement required for a mediocre jump and an amazing one. Beyond that, it showed us how these virtual characters would move if they were hamstrung by bone deformities, a stiff ankle, or muscle deficiencies and watch them learn to walk despite these setbacks. We could even have a look at the improvements after a virtual surgery takes place. So now, how about an even more elaborate technique that focuses more on the muscle simulation part? The ropes here are simulated in a way that the only interesting property of the particles holding them together is position. Cosserat rod simulations are an improvement because they also take into consideration the orientation of the particles, and hence, can simulate twists as well. And this new technique is called VIPER, and adds a scale property to these particles, and hence, takes into consideration stretching and compression. What does that mean? Well, it means that this can be used for a lot of muscle-related simulation problems that you will see in a moment. However, before that, an important part is inserting these objects into our simulations. The cool thing is that we don\u2019t need to get an artist to break up these surfaces into muscle fibers. That would not only be too laborious, but of course, would also require a great deal of anatomical knowledge. Instead, this technique does all this automatically, a process that the authors call\u2026viperization. So, in goes the geometry, and out comes a nice muscle model. This really opens up a world of really cool applications. For instance, one such application is muscle movement simulation. When attaching the muscles to bones, as we move the character, the muscles move and contract accurately. Two, it can also perform muscle growth simulations. And three, we get more accurate soft body physics. Or, in other words, we can animate gooey characters, like this octopus. Okay, that all sounds great, but how expensive is this? Do we have to wait a few seconds to minutes to get this? No, no, not at all! This technique is really efficient and runs in milliseconds, so we can throw in a couple more objects. And by couple, a computer graphics researcher always means a couple dozen more, of course. And in the meantime, let\u2019s look carefully at the simulation timings! It starts from around 8-9 milliseconds per frame, and with all these octopi, we\u2019re still hovering around 10 milliseconds per frame. That\u2019s a hundred frames per second, which means that the algorithm scales with the complexity of these scenes really well. This is one of those rare papers that is written both very precisely, and it is absolutely beautiful. Make sure to have a look in the video description. The source code of the project is also available. With this, I hope that we\u2019ll get even more realistic characters with real muscle models in our computer games and real-time applications. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=higGxGmwDbs",
        "paper_link": "https://arxiv.org/abs/1906.05260",
        "paper_title": "VIPER: Volume Invariant Position-based Elastic Rods"
    },
    {
        "video_id": "VQgYPv8tb6A",
        "video_title": "This AI Makes \"Audio Deepfakes\"",
        "position_in_playlist": 421,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir blog post on #deepfakes is available here:\nhttps://www.wandb.com/articles/improving-deepfake-performance-with-data\n\n\ud83d\udcdd The paper \"Neural Voice Puppetry: Audio-driven Facial Reenactment\" and its online demo are available here:\nPaper: https://justusthies.github.io/posts/neural-voice-puppetry/\nDemo - **Update: seems to have been disabled in the meantime, apologies!** : http://kaldir.vc.in.tum.de:9000/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#audiodeepfake #voicedeepfake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with this guy's name that is impossible to pronounce. My name is Dr. K\u00e1roly Zsolnai-Feh\u00e9r, and indeed, it seems that pronouncing my name requires some advanced technology. So what was this? I promise to tell you in a moment, but to understand what happened here, first, let\u2019s have a look at this deepfake technique we showcased a few videos ago. As you see, we are at the point where our mouth, head, and eye movements are also realistically translated to a chosen target subject, and perhaps the most remarkable part of this work was that we don\u2019t even need a video of this target person, just one photograph. However, these deepfake techniques mainly help us in transferring video content. So what about voice synthesis? Is it also as advanced as this technique we\u2019re looking at? Well, let\u2019s have a look at an example, and you can decide for yourself. This is a recent work that goes by the name Tacotron 2, and it performs AI-based voice cloning. All this technique requires is a 5-second sound sample of us, and is able to synthesize new sentences in our voice, as if we uttered these words ourselves. Let\u2019s listen to a couple examples. Wow, these are truly incredible. The timbre of the voice is very similar, and it is able to synthesize sounds and consonants that have to be inferred because they were not heard in the original voice sample. And now, let\u2019s jump to the next level, and use a new technique that takes a sound sample and animates the video footage as if the target subject said it themselves. This technique is called Neural Voice Puppetry, and even though the voices here are synthesized by this previous Tacotron 2 method that you heard a moment ago, we shouldn\u2019t judge this technique by its audio quality, but how well the video follows these given sounds. Let\u2019s go! If you decide to stay until the end of this video, there will be another fun video sample waiting for you there. Now, note that this is not the first technique to achieve results like this, so I can\u2019t wait to look under the hood and see what\u2019s new here. After processing the incoming audio, the gestures are applied to an intermediate 3D model, which is specific to each person since each speaker has their own way of expressing themselves. You can see this intermediate 3D model here, but we are not done yet, we feed it through a neural renderer, and what this does is apply this motion to the particular face model shown in the video. You can imagine the intermediate 3D model as a crude mask that models the gestures well, but does not look like the face of anyone, where the neural renderer adapts this mask to our target subject. This includes adapting it to the current resolution, lighting, face position and more, all of which is specific to what is seen in the video. What is even cooler is that this neural rendering part runs in real time. So, what do we get from all this? Well, one, superior quality, but at the same time, it also generalizes to multiple targets. Have a look here! And the list of great news is not over yet, you can try it yourself, the link is available in the video description. Make sure to leave a comment with your results! To sum up, by combining multiple existing techniques, it is important that everyone knows about the fact that we can both perform joint video and audio synthesis for a target subject. This episode has been supported by Weights & Biases. Here, they show you how to use their tool to perform faceswapping and improve your model that performs it. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=VQgYPv8tb6A",
        "paper_link": "https://justusthies.github.io/posts/neural-voice-puppetry/",
        "paper_title": "Neural Voice Puppetry: Audio-driven Facial Reenactment"
    },
    {
        "video_id": "nCpGStnayHk",
        "video_title": "This Neural Network Learned To Look Around In Real Scenes!",
        "position_in_playlist": 422,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir amazing instrumentation is available here:\nhttps://app.wandb.ai/sweep/nerf/reports/NeRF-%E2%80%93-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis--Vmlldzo3ODIzMA\n\n\ud83d\udcdd The paper \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\" is available here:\nhttp://www.matthewtancik.com/nerf\n\n\ud83d\udcdd The paper \"Gaussian Material Synthesis\" is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. About two years ago, we worked on a neural rendering system, which would perform light transport on this scene and guess how it would change if we would change the material properties of this test object. It was able to closely match the output of a real light simulation program, and, it was near instantaneous as it took less than 5 milliseconds instead of the 40-60 seconds the light transport algorithm usually requires. This technique went by the name Gaussian Material Synthesis, and the learned quantities were material properties. But this new paper sets out to learn something more difficult, and also, more general. We are talking about a 5D neural radiance field representation. So what does this mean exactly? What this means is that we have 3 dimensions for location and two for view direction, or in short, the input is where we are in space and what are we looking at, and the resulting image of this view. So here, we take a bunch of this input data, learn it, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. And here, we are talking not only digital environments, but also, real scenes as well! Now that\u2019s quite a value proposition, let\u2019s see if it can live up to this promise! Wow! So good. Love it! But, what is it really that we should be looking at? What makes a good output here? The most challenging part is writing an algorithm that is able to reproduce delicate, high-frequency details while having temporal coherence. So what does that mean? Well, in simpler words, we are looking for sharp and smooth image sequences. Perfectly matte objects are easier to learn here because they look the same from all directions, while glossier, more reflective materials are significantly more difficult, because they change a great deal as we move our head around, and this highly variant information is typically not present in the learned input images. If you read the paper, you\u2019ll see these referred to as non-Lambertian materials. The paper and the video contains a ton of examples of these view-dependent effects to demonstrate that these difficult scenes are handled really well by this technique. Refractions also look great. Now, if we define difficulty as things that change a lot when we change our position or view direction a little, not only the non-Lambertian materials are going to give us headaches, occlusion can be challenging as well. For instance, you can see here how well it handles the complex occlusion situation between the ribs of the skeleton here. It also has an understanding of depth, and this depth information is so accurate, that we can do these nice augmented reality applications where we put a new, virtual object in the scene and it correctly determines whether it is in front of, or behind the real objects in the scene. Kind of what these new iPads do with their LiDAR sensors, but without the sensor. As you see, this technique smokes the competition. So what do you know, entire real-world scenes can be reproduced from only a few views by using neural networks. And the results are just out of this world. Absolutely amazing. What you see here is an instrumentation of this exact paper we have talked about, which was made by Weights and Biases. I think organizing these experiments really showcases the usability of their system. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=nCpGStnayHk",
        "paper_link": "https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/",
        "paper_title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
    },
    {
        "video_id": "hFZlxpJPI5w",
        "video_title": "Sure, DeepFake Detectors Exist - But Can They Be Fooled?",
        "position_in_playlist": 423,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples\" is available here:\nhttps://arxiv.org/abs/2002.12749\nhttps://adversarialdeepfakes.github.io/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Deepfake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. With the ascendancy of neural network-based learning algorithms, we are now able to take on, and defeat problems that sounded completely impossible just a few years ago. For instance, now, we can create deepfakes, or, in other words, we can record a short video of ourselves, and transfer our gestures to a target subject, and this particular technique is so advanced, that we don\u2019t even need a video of our target, just one still image. So, we can even use paintings, images of sculptures, so yes, even the Mona Lisa works! However, don\u2019t despair, it\u2019s not all doom and gloom. A paper by the name FaceForensics contains a large dataset of original and manipulated video pairs. As this offered a ton of training data for real and forged videos, it became possible to use these to train a deepfake detector. You can see it here in action as these green to red colors showcase regions that the AI correctly thinks were tampered with. However, if we have access to a deepfake detector, we can also use it to improve our deepfake creating algorithms. And with this, an arms race has begun. The paper we are looking at today showcases this phenomenon. If you look here, you see this footage which is very visibly fake, and the algorithm correctly concludes that. Now, if you look at this video, which, for us, looks like if it were the same video, yet it suddenly became real, at least the AI thinks that, of course, incorrectly. This is very confusing. So what really happened here? To understand what is going on here, we first have to talk about ostriches. So what do ostriches have to do with this insanity? Let me try to explain that. An adversarial attack on a neural network can be performed as follows. We present such a classifier network with an image of a bus, and it will successfully tell us that yes, this is indeed a bus. Nothing too crazy here. Now, we show it not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. I will stress that this is not any kind of noise, but the kind of noise that exploits biases in the neural network, which is, by no means trivial to craft. However, if we succeed at that, this kind of adversarial attack can be pulled off on many different kinds of images. Everything that you see here on the right will be classified as an ostrich by the neural network these noise patterns were crafted for. And, this can now be done not only on images, but videos as well, hence, what happened a minute ago is that the deepfake video has been adversarially modified with noise to bypass such a detector. If you look here, you see that the authors have chosen excellent examples, because some of these are clearly forged videos, which is initially recognized by the detector algorithm, but after adding the adversarial noise to it, the detector fails spectacularly. To demonstrate the utility of their technique, they have chosen the other examples to be much more subtle. Now, let\u2019s talk about one more question. We are talking about a detector algorithm. But there is not one detector out there, there are many, and we can change the wiring of these neural networks to have even more variation. So what does it mean to fool a detector? Excellent question. The success rate of these adversarial videos indeed depends on the deepfake detector we are up against, but, hold on to your papers, because this success rate on uncompressed videos is over 98%, which is amazing, but, note that when using video compression, this success rate may drop to 58 to 92% depending on the detector. This means that video compression and some other tricks involving image transformations still help us in defending against these adversarial attacks. What I also really like about the paper is that it discusses white and black-box attacks separately. In the white-box case, we know everything about the inner workings of the detector, including the neural network architecture and parameters, this is typically the easier case. But, the technique also does really well in the black-box case, where we are not allowed to look under the hood of the detector, but we can show it a few videos and see how it reacts to them. This is a really cool work that gives us a more nuanced view about the current state of the art around deepfakes and deepfake detectors. I think it is best if we all know about the fact that these tools exist. If you wish to help us with this endeavor, please make sure to share this with your friends. Thank you. This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=hFZlxpJPI5w",
        "paper_link": "https://arxiv.org/abs/2002.12749",
        "paper_title": "Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples"
    },
    {
        "video_id": "bVXPnP8k6yo",
        "video_title": "This AI Learned to Summarize Videos \ud83c\udfa5",
        "position_in_playlist": 424,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"CLEVRER: CoLlision Events for Video REpresentation and Reasoning\" is available here:\nhttp://clevrer.csail.mit.edu/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-95032/\nNeural network image credit: https://en.wikipedia.org/wiki/Neural_network\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Neural network-based learning algorithms are making great leaps in a variety of areas. And many of us are wondering whether it is possible that one day we\u2019ll get a learning algorithm, show it a video, and ask it to summarize it, and we can then decide whether we wish to watch it or not? Or just describe what we are looking for and it would fetch the appropriate videos for us. I think today\u2019s paper has a good pointer whether we can expect this to happen, and in a few moments, we\u2019ll find out together why. A few years ago, these neural networks were mainly used for image classification, or in other words, they would tell us what kinds of objects are present in an image. But they are capable of so much more, for instance, these days, we can get a recurrent neural network write proper sentences about images, and it would work well for even highly non-trivial cases. For instance, it is able to infer that work is being done here, or that a ball is present in this image even if the vast majority of the ball itself is concealed. The even crazier thing about this is that this work is not recent at all, this is from a more than 4 year old paper! Insanity. The first author of this paper was Andrej Karpathy, one of the best minds in the game who is currently the director of AI at Tesla and works on making these cars able to drive themselves. So, as amazing as this work was, the progress in machine learning research keeps on accelerating, so let\u2019s have a look at this newer paper that takes it a step further, and has a look at not an image, but a video, and explains what happens therein. Very exciting. Let\u2019s have a look at an example! This was the input video, and let\u2019s stop right at the first statement. The red sphere enters the scene. So, it was able to correctly identify not only what we are talking about in terms of color and shape, but also knows what this object is doing as well. That\u2019s a great start. Let\u2019s proceed further. Now, it correctly identifies the collision event with the cylinder, then this cylinder hits another cylinder, very good\u2026 and look at that. It identifies that the cylinder is made of metal, I like that a lot, because this particular object is made of a very reflective material, which shows us more about the surrounding room than the object itself. But we shouldn\u2019t only let the AI tell us what is going on on its own terms - let\u2019s ask questions and see if it can answer them correctly. So, first, let\u2019s ask - what is the material of the last object that hit the cyan cylinder? And it correctly finds that the answer is Metal. Awesome. Now let\u2019s take it a step further and stop the video here - can it predict what is about to happen after this point? Look, it indeed can! This is remarkable because of two things. If we look under the hood, we see that to be able to pull this off, it not only has to understand what objects are present in the video and predict how they will interact, but also has to parse our questions correctly, put it all together, and form an answer based on all this information. If any of these tasks works unreliably, the answers will be incorrect. And two, there are many other techniques that are able to do some of these tasks, so why is this one particularly interesting? Well, look here! This new method is able to do all of these tasks at the same time. So there we go, if this improves further, we might become able to search Youtube videos by just typing something that happens in the video and it would be able to automatically find it for us. That would be absolutely amazing. What a time to be alive! This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client\u2019s websites and reliably serve them to millions of visitors, Linode can do that too. What\u2019s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit\u00a0linode.com/papers\u00a0or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bVXPnP8k6yo",
        "paper_link": "http://clevrer.csail.mit.edu/",
        "paper_title": "CLEVRER: CoLlision Events for Video REpresentation and Reasoning"
    },
    {
        "video_id": "EWKAgwgqXB4",
        "video_title": "This AI Creates Beautiful Time Lapse Videos \u2600\ufe0f",
        "position_in_playlist": 425,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/intro-to-cnns-with-wandb \n\n\ud83d\udcdd The paper \"High-Resolution Daytime Translation Without Domain Labels\" is available here:\n- https://saic-mdal.github.io/HiDT/ \n- https://github.com/saic-mdal/HiDT\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nhttps://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. A few years ago, we have mainly seen neural network-based techniques being used for image classification. This means that they were able to recognize objects, for instance, animals and traffic signs in images. But today, with the incredible pace of machine learning research, we now have a selection of neural network-based techniques for not only classifying images, but, also, synthesizing them! The images that you see here and throughout this video is generated by one of these learning-based methods. But of course, in this series, we are always obsessed with artistic control, or, in other words, how much of a say we have in the creation of these images. After all, getting thousands and thousands of images without any overarching theme or artistic control is hardly useful for anyone. One way of being able to control the outputs is to use a technique that is capable of image translation. What you see here is a work by the name CycleGAN! It could transform apples into oranges, zebras into horses, and more. It was called CycleGAN because it introduced a cycle consistency loss function. This means that if we convert a summer image to a winter image, and then back to a summer image, we should get the same input image back, or at least, something very similar. If our learning system obeys this principle, the output quality of the translation is going to be significantly better. Today, we are going to study a more advanced image translation technique that takes this further. This paper is amazingly good at daytime image translation. It looks at a selection of landscape images, and then, as you see here, it learns to reimagine our input photos as if they were taken at different times of the day. I love how clouds form, and move over time in the synthesized images, and the night sky with the stars is also truly a sight to behold. But wait, CycleGAN and many other followup works did image translation, this also does image translation, so, what\u2019s really new here? Well, one, this work proposes a novel upsampling scheme that helps creating output images with lots and lots of detail. Two, it can also create not just a bunch of images a few hours apart, but it can also make beautiful timelapse videos, where the transitions are smooth. Oh my goodness. I love this. And three, the training happens by shoveling 20 thousand landscape images into the neural network, and it becomes able to perform this translation task without labels. This means that we don\u2019t have to explicitly search for all the daytime images and tell the learner that these are daytime images, and these other images are not. This is amazing because the algorithm is able to learn by itself, without labels, but it is also easier to use because we can feed in lots and lots more training data without having to label these images correctly. As a result, we now know that this daytime translation task is used as a testbed to demonstrate that this method can be reused for other kinds of image translation tasks. The fact that it can learn on its own and still compete with other works in this area is truly incredible. Due to this kind of generality, it can also perform other related tasks, for instance, it can perform style transfer, or in other words, not just change the time of day, but reimagine our pictures in the style of famous artists. I think with this paper, we have a really capable technique on our hands that is getting closer and closer to the point where they can see use in mainstream software packages and image editors. That would be absolutely amazing. If you have a closer look at the paper, you will see that it tries to minimize 7 things at the same time. What a time to be alive! This episode has been supported by Weights & Biases. Here, they show you how to build a proper Convolutional Neural Network for image classification, and how to visualize the performance of your model. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=EWKAgwgqXB4",
        "paper_link": "https://saic-mdal.github.io/HiDT/",
        "paper_title": "High-Resolution Daytime Translation Without Domain Labels"
    },
    {
        "video_id": "54YvCE8_7lM",
        "video_title": "Is Simulating Soft and Bouncy Jelly Possible? \ud83e\udd91",
        "position_in_playlist": 426,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"A Hybrid Material Point Method for Frictional Contact with Diverse Materials\" is available here:\n- https://www.math.ucla.edu/~jteran/papers/HGGWJT19.pdf\n- https://www.math.ucla.edu/~qiguo/Hybrid_MPM.pdf\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Benji Rabhan, Brian Gilman, Bryan Learn, Christian Ahlin, Daniel Hasegan, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, James Watt, Javier Bustamante,  Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. After reading a physics textbook on the laws of fluid motion, with a little effort, we can make a virtual world come alive by writing a computer program that contains these laws, resulting in beautiful fluid simulations like the one you see here. The amount of detail we can simulate with these programs is increasing every year, not only due to the fact that hardware improves over time, but also, the pace of progress in computer graphics research is truly remarkable. To simulate all these, many recent methods build on top of a technique called the Material Point Method. This is a hybrid simulation technique that uses both particles and grids to create these beautiful animations, however, when used by itself, we can come up with a bunch of phenomena that it cannot simulate properly. One such example is cracking and tearing phenomena, which has been addressed in a previous paper that we covered a few videos ago. With this, we can smash oreos, candy crabs, pumpkins, and much, much more. In a few minutes, I will show you how to combine some of these aspects of a simulation. It is going to be glorious\u2026or, maybe, not so much! Just give me a moment and you\u2019ll see! Beyond that, when using this material point method, coupling problems frequently arise. This means that the sand is allowed to have an effect on the fluid, but at the same time, as the fluid sloshes around, it also moves the sand particles within. This is what we refer to as two-way coupling. If it is implemented correctly, our simulated honey will behave as real honey in the footage here, and support the dipper. These are also not trivial to compute with the Material Point Method and require specialized extensions to do so. So what else is there to do? This amazing new paper provides an extension to handle simulating elastic objects, such as hair, rubber, and you will see that it even works for skin simulations, and it can handle their interactions with other materials. So, why is this useful? Well, we know that we can pull off simulating a bunch of particles, and a jello simulation separately, so it\u2019s time for some experimentation! This is the one I promised earlier, so let\u2019s try to put these two things together and see what happens. It seems to start our okay, particles are bouncing off of the jello\u2026and then\u2026uh-oh\u2026look! Many of them seem to get stuck. So can we fix this somehow? Well, this, is where this new paper comes into play. Look here! It starts out somewhat similarly, most of the particles get pushed away from the jello, and then\u2026look! Some of them indeed keep bouncing for a long-long time, and, none of them are stuck to the jello. Glorious! We can see the same phenomenon here with three jello blocks of different stiffness values. With this, we can also simulate more than 10 thousand bouncy hair strands, and to the delight of a computer graphics researcher, we can even throw snow into it and expect it to behave correctly. Braids work well too. And if you remember, I also promised some skin simulation. And this demonstration is not only super fun, for instance, the ones around this area are perhaps the most entertaining, but the information density of this screen is just absolutely amazing. As we go from bottom to top, you can see the effect of the stiffness parameters, or in other words, the higher we are, the stiffer things become, and as we go from left to right, the effect of damping increases. And you can see not only a bunch of combinations of these two parameters, but you can also compare many configurations against each other at a glance, on the same screen. Loving it. So how long does it take to simulate all this? Well, given that we are talking about an off-line simulation technique, this is not designed to run in real-time games, as the execution time is typically not measured in frames per second, but seconds per frame, and sometimes even minutes per frame. However, having run simulations that contained much fewer interactions than this that took me several days to compute, I would argue that these numbers are quite appealing for a method of this class. Also, note that this is one of those papers that makes the impossible possible for us, and of course, as we always say around here, two more papers down the line, and it will be significantly improved. For now, I am very impressed. Time to fire up some elaborate jello simulations! What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=54YvCE8_7lM",
        "paper_link": "https://www.math.ucla.edu/~jteran/papers/HGGWJT19.pdf",
        "paper_title": "A Hybrid Material Point Method for Frictional Contact with Diverse Materials"
    },
    {
        "video_id": "rGOy9rqGX1k",
        "video_title": "What\u2019s Inside a Neural Network?",
        "position_in_playlist": 427,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/visualize-xgboost-in-one-line\n\n\ud83d\udcdd The paper \"Zoom In: An Introduction to Circuits\" is available here: \nhttps://distill.pub/2020/circuits/zoom-in/\n\nFollowup article: https://distill.pub/2020/circuits/early-vision/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#ai #machinelearning",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This paper is not your usual paper, but it does something quite novel. It appeared in the Distill journal, one of my favorites, which offers new and exciting ways of publishing beautiful, but unusual works aiming for exceptional clarity and readability. And of course, this new paper is no different. It claims that despite the fact that these neural network-based learning algorithms look almost unfathomably complex inside, if we look under the hood, we can often find meaningful algorithms in there. Well, I am quite excited for this, so, sign me up! Let\u2019s have a look at an example! At the risk of oversimplifying the explanation, we can say that a neural network is given as a collection of neurons and connections. If you look here, you see the visualization of three neurons. At first glance, they look like an absolute mess, don\u2019t they? Well, kind of, but upon closer inspection, we see that there is quite a bit of structure here! For instance, the upper part looks like a car window, the next one resembles a car body, and the bottom of the third neuron clearly contains a wheel detector. However, no car looks exactly like these neurons, so what does the network do with all this? Well, in the next layer, the neurons arise as a combination of neurons in the previous layers, where we cherry pick parts of each neuron that we wish to use. So here, with red, you see that we are exciting the upper part of this neuron to get the window, use roughly the entirety of the middle one, and use the bottom part of the third one to assemble\u2026this! And now, we have a neuron in the next layer that will help us detect whether we see a car in an image or not. So cool! I love this one! Let\u2019s look at another example. Here you see a dog head detector, but it kind of looks like a crazy Picasso painting where he tried to paint a human from not one angle like everyone else, but from all possible angles on one image. But this is a neural network, so why engage in this kind of insanity? Well, if we have a picture of a dog, the orientation of the head of the dog can be anything. It can be a frontal image, look from the left to right, right to left, and so on. So this, is a pose invariant dog head detector! What this means is that it can detect many different orientations, and look here! You see that it gets very excited by all of these good boys. I think we even have a squirrel in here. Good thing this is not the only neuron we have in the network to make a decision! I hope that it already shows that this is truly an ingenious design. If you have a look at the paper in the video description, which you should absolutely do, you\u2019ll see exactly how these neurons are built from the neurons in the previous layers. The article contains way, way more than this, you\u2019ll see a lot more dog snouts, curve detectors, and even a followup article that you can have a look at and even comment on before it gets finished! A huge thank you to Chris Olah, who devotes his time away from research and uses his own money to run this amazing journal, I cannot wait to cover more of these articles in future episodes, so make sure to subscribe and hit the bell icon to never miss any of those. So, finally, we understand a little more how neural networks do all the amazing things they are able to do. What a time to be alive! This episode has been supported by Weights & Biases. Here, they show you how you can visualize the training process for your boosted trees with XGBoost using their tool. If you have a closer look, you\u2019ll see that all you need is one line of code. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rGOy9rqGX1k",
        "paper_link": "https://distill.pub/2020/circuits/zoom-in/",
        "paper_title": "Zoom In: An Introduction to Circuits"
    },
    {
        "video_id": "MPdj8KGZHa0",
        "video_title": "Neural Network Dreams About Beautiful Natural Scenes",
        "position_in_playlist": 428,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/better-models-faster-with-weights-biases\n\n\ud83d\udcdd The paper \"Manipulating Attributes of Natural Scenes via Hallucination\" is available here:\nhttps://hucvl.github.io/attribute_hallucination/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nNeural network image source: https://en.wikipedia.org/wiki/File:Neural_network_example.svg\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#ai #machinelearning",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In the last few years, the pace of progress in machine learning research has been staggering. Neural network-based learning algorithms are now able to look at an image and describe what\u2019s seen in this image, or even better, the other way around, generating images from a written description. You see here a set of results from BigGAN, a state of the art image generation technique and marvel at the fact that all of these images are indeed synthetic. The GAN part of this technique abbreviates the term Generative Adversarial Network - this means a pair of neural networks that battle each other over time to master a task, for instance, to generate realistic looking images when given a theme. After that, StyleGAN and even its second version appeared, which, among many other crazy good features, opened up the possibility to lock in several aspects of these images, for instance, age, pose, some facial features and more, and then, we could mix them with other images to our liking, while retaining these locked-in aspects. I am loving the fact that these newer research works are moving in the direction of more artistic control, and the paper we\u2019ll discuss today also takes a step in this direction. With this new work, we can ask to translate our image into different seasons, weather conditions, time of day, and more! Let\u2019s have a look! Here, we have our input, and imagine that we\u2019d like to add more clouds, and translate it into a different time of the day, and\u2026there we go! Wow. Or, we can take this snowy landscape image and translate it into a blooming flowery field. This truly seems like black magic, so I can\u2019t wait to look under the hood and see what is going on! The input is our source image, and, a set of attributes where we can describe our artistic vision. For instance, here, let\u2019s ask the AI to add some more vegetation to this scene. That will do! Step number one, this artistic description is routed to a scene generation network, which hallucinates an image that fits our description. Well, that\u2019s great, as you see here, it kind of resembles the input image, but still, it is substantially different! So, why is that? If you look here, you see that it also takes the layout of our image as an input, or in other words, the colors and the silhouettes describe what part of the image contains a lake, vegetation, clouds, and more. It creates the hallucination according to that, so we have more clouds, that\u2019s great, but the road here has been left out. So now, are we stuck with an image that only kind of resembles what we want. What do we do now? Now, step number two, let\u2019s not use this hallucinated image directly, but, apply its artistic style to our source image. Brilliant! Now we have our content, but, with more vegetation. However, remember that we have the layout of the input image. That is a gold mine of information! So, are you thinking what I am thinking? Yes, including this indeed opens up a killer application. We can even change the scene around by modifying the labels on this layout, for instance, by adding some mountains, make it a grassy field, and add a lake. Making a scene from scratch from a simple starting point is also possible. Just add some mountains, trees, a lake, and you are good to go! And then, you can use the other part of the algorithm to transform it into a different season, time of day, or make it foggier. What a time to be alive! Now, as with every research work, there is still room for improvements! For instance, I find that it is hard to define what it means to have a cloudier image. For instance, the hallucination here works according to the specification, it indeed has more clouds than this. But, for instance, here, I am unsure if we have more clouds in the output - you see that perhaps it is even less than in the input. It seems that not all of them made it to the final image. Also, do fewer, but denser clouds qualify as cloudier? Nonetheless, I think this is going to be an awesome tool as is, and I can only imagine how cool it will become two more papers down the line. This episode has been supported by Weights & Biases. In this post they show you how to easily iterate on models by visualizing and comparing experiments in real time. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MPdj8KGZHa0",
        "paper_link": "https://hucvl.github.io/attribute_hallucination/",
        "paper_title": "Manipulating Attributes of Natural Scenes via Hallucination"
    },
    {
        "video_id": "i4KWiq3guRU",
        "video_title": "Finally, A Blazing Fast Fluid Simulator! \ud83c\udf0a",
        "position_in_playlist": 429,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nThe shown blog post is available here:\nhttps://www.wandb.com/articles/visualize-lightgbm-performance-in-one-line-of-code\n\n\ud83d\udcdd The paper \"Fast Fluid Simulations with Sparse Volumes on the GPU\" and some code samples are  available here:\n- https://people.csail.mit.edu/kuiwu/gvdb_sim.html\n- https://www.researchgate.net/publication/325488464_Fast_Fluid_Simulations_with_Sparse_Volumes_on_the_GPU\n\n\ud83d\udcf8 Our Instagram page is available here: https://www.instagram.com/twominutepapers/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. With the nimble progress we are seeing in computer graphics research, it is now not only possible to perform beautiful fluid simulations, but we can also simulate more advanced effects, such as honey coiling, ferrofluids climbing up on other objects, and a variety of similar advanced effects. However, due to the complexity of these techniques, we often have to wait for several seconds, or even minutes for every single one of these images, which often means that we have to leave our computer crunching these scenes overnight. Or even wait several days for the results to appear. But what about real-time applications? Can we perform these fluid simulations in a more reasonable timeframe? Well, this technique offers detailed fluid simulations, like the one here, and is blazing fast. The reason for this is that one, it uses a sparse volume representation and two, it supports parallel computation and runs on your graphics card. So what do these terms really mean? Let\u2019s start with the sparse part. With classical fluid simulation techniques, the simulation domain has to be declared in advance and is typically confined to a cube. This comes with several disadvantages. For instance, if we wish to have a piece of fluid or smoke coming out of this cube, we are out of luck. The simulation domain stays, so we would have to know in advance how the simulation pans out, which we don\u2019t. Now, the first thing you are probably thinking about, well, of course, make the simulation domain bigger! Yes, but unless special measures are taken, the bigger the domain, the more we have to compute. Even the empty parts take some computation! Ouch. This means that we have to confine the simulation to as small a domain as we can. So, this is where this technique comes into play - the sparse representation that it uses means that the simulation domain can take any form, as you see here, it just starts altering the shape of the simulation domain as the fluid splashes out of it. Furthermore, we are not only not doing work in the empty parts of the domain, which is a huge efficiency increase, but we don\u2019t need to allocate too much additional memory for these regions, which, you will see in a minute, will be a key part of the value proposition of this technique. We noted that it supports parallel computation and runs on your graphics card. The graphics card part is key, because otherwise, it would run on your processor, like most of the techniques that require \u201cminutes per frame\u201d. The more complex the technique is, typically, the more likely that it runs on your processor, which has a few cores to a few tens of cores. However, your graphics card, comparably, is almost a supercomputer as it has up to a few hundred, or, even a few thousand cores to compute on. So why not use that? Well, it\u2019s not that simple, and here is where the word \u201cparallel\u201d is key. If the problem can be decomposed into smaller, independent problems, they can be allocated to many-many cores that can work independently, and much more efficiently. This, is exactly what this paper does with the fluid simulation. It runs it on your graphics card, and hence, it is typically 10 to 20 times faster than the equivalent techniques running on your processor. Let me try to demonstrate this with an example. Let\u2019s talk about coffee. Making coffee is not a very parallel task. If you ask a person to make coffee, it can typically be done in a few seconds. However, if you suddenly put 30 people in the kitchen and ask them to make coffee, it not only will not be a faster process, but may even be slower than one person because of two reasons: one, it is hard to coordinate 30 people and there will be miscommunication, and two, there are very few tools, and lots of people, so they won\u2019t be able to help each other, or much worse, will just hold each other up. If we could formulate the coffee making problem such that we need 30 units of coffee and we have 30 kitchens, we could just place one person into each kitchen and then, they could work efficiently and independently. At the risk of oversimplifying the situation, this is an intuition of what this technique does, and hence, it runs on your graphics card, and is incredibly fast. Also, note that your graphics card typically has a limited amount of memory, and, remember, we noted that the sparse representation makes it very gentle on memory usage, making this the perfect algorithm for creating detailed, large-scale fluid simulations quickly. Excellent design. I plan to post slowed down versions of some of the footage that you see here to our Instagram page, if you feel that it is something that you would enjoy, make sure to follow us there. Just search for Two Minute Papers on Instagram to find it, or also, as always, the link is in the video description. And, finally! Hold on to your papers, because if you look here, you see that the dam break scene can be simulated with about 5 frames per second, not seconds per frame, while the water drop scene can run at about 7 frames per second with a few million particles. We can, of course, scale up the simulation, and then, we are back at seconds per frame land, but it is still blazing fast. If you look here, we can go up to 27 times faster, so in one all-nighter simulation, I can simulate what I could simulate in nearly a month. Sign me up! What a time to be alive! Now, note that in the early days of Two Minute Papers, about 3-400 episodes ago, I covered plenty of papers of fluid simulations, however, nearly no one really showed up to watch them. Before publishing any of these videos, I was like \u201chere we go again\u201d, I knew that almost nobody would watch it, but this is a series where I set out to share the love for these papers. I believe we can learn a lot from these works, and if no one watches them, then so be it, I still love doing this. But, I was surprised to find out that over the years, something has changed. You Fellow Scholars somehow started to love the fluids, and I am delighted to see that. So, thank you so much for trusting the process, showing up and watching these videos. I hope you are enjoying watching these as much as I enjoyed making them. This episode has been supported by Weights & Biases. Here, they show you how to make it to the top of Kaggle leaderboards by using Weights & Biases to find the best model faster than everyone else. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=i4KWiq3guRU",
        "paper_link": "https://people.csail.mit.edu/kuiwu/gvdb_sim.html",
        "paper_title": "Fast Fluid Simulations with Sparse Volumes on the GPU"
    },
    {
        "video_id": "u5wtoH0_KuA",
        "video_title": "This AI Does Nothing In Games\u2026And Still Wins!",
        "position_in_playlist": 430,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation for this paper is available here:\nhttps://app.wandb.ai/stacey/aprl/reports/Adversarial-Policies-in-Multi-Agent-Settings--VmlldzoxMDEyNzE\n\n\ud83d\udcdd The paper \"Adversarial Policies\" is available here:\nhttps://adversarialpolicies.github.io\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, it is almost taken for granted that neural network-based learning algorithms are capable of identifying objects in images, or even write full, coherent sentences about them, but fewer people know that there is also parallel research on trying to break these systems. For instance, some of these image detectors can be fooled by adding a little noise to the image, and in some specialized cases, we can even perform something that is called the one pixel attack. Let\u2019s have a look at some examples. Changing just this one pixel can make a classifier think that this ship is a car, or that this horse is a frog, and amusingly, be quite confident about its guess. Note that the choice of this pixel and the color is by no means random and it needs solving a mathematical optimization problem to find out exactly how to perform this. Trying to build better image detectors, while other researchers are trying to break them is not the only arms race we\u2019re experiencing in machine learning research. For instance, a few years ago, DeepMind introduced an incredible learning algorithm that looked at the screen, much like a human would, but was able to reach superhuman levels in playing a few Atari games. It was a spectacular milestone in AI research. They also have just published a followup paper on this that we\u2019ll cover very soon, so make sure to subscribe and hit the bell icon to not miss it when it appears in the near future. Interestingly, while these learning algorithms are being improved at a staggering pace, there is a parallel subfield where researchers endeavor to break these learning systems by slightly changing the information they are presented with. Let\u2019s have a look at OpenAI\u2019s example. Their first method adds a tiny bit of noise to a large portion of the video input, where the difference is barely perceptible, but it forces the learning algorithm to choose a different action than it would have chosen otherwise. In the other one, a different modification was used, that has a smaller footprint, but is more visible. For instance, in pong, adding a tiny fake ball to the game can coerce the learner into going down when it was originally planning to go up. It is important to emphasize that the researchers did not do this by hand. The algorithm itself is able to pick up game-specific knowledge by itself and find out how to fool the other AI using it. Both attacks perform remarkably well. However, it is not always true that we can just change these images or the playing environment to our desire to fool these algorithms. So, with this, an even more interesting question arises. Is it possible to just enter the game as a player, and perform interesting stunts that can reliably win against these AIs? And with this, we have arrived to the subject of today\u2019s paper. This is the \u201cYou Shall Not Pass\u201d game, where the red agent is trying to hold back the blue character and not let it cross the line. Here you see two regular AIs duking it out, sometimes the red wins, sometimes the blue is able to get through. Nothing too crazy here. This is the reference case which is somewhat well balanced. And now, hold on to your papers, because this adversarial agent that this new paper proposes, does this. You may think this was some kind of glitch, and I put the incorrect footage here by accident. No, this is not an error, you can believe your eyes, it basically collapses and does absolutely nothing. This can\u2019t be a useful strategy, can it? Well, look at that! It still wins the majority of the time. This is very confusing. How can that be? Let\u2019s have a closer look. This red agent is normally a somewhat competent player, as you can see here, it can punch the blue victim and make it fall. We now replaced this red player with the adversarial agent, which collapsed, and it almost feels like it hypnotized the blue agent to also fall. And now, squeeze your papers, because the normal red opponent\u2019s winrate was 47% percent, and this collapsing chap wins 86% of the time. It not only wins, but it wins much, much more reliably than a competent AI. What is this wizardry? The answer is that the adversary induces off-distribution activations. To understand what that exactly means, let\u2019s have a look at this chart. This tells us how likely it is that the actions of the AI against different opponents are normal. As you see, when this agent named Zoo plays against itself, the bars are in the positive region, meaning that normal things are happening. Things go as expected. However, that\u2019s not the case for the blue lines, which are the actions when we play against this adversarial agent, in which case, the blue victim\u2019s actions are not normal in the slightest. So, the adversarial agent is really doing nothing, but it is doing nothing in a way that reprograms its opponent to make mistakes and behave close to a completely randomly acting agent! This paper is absolute insanity. I love it! And if you look here, you see that the more the blue curve improves, the better this scheme works for a given game. For instance, it is doing real good on Kick and Defend, fairly good on Sumo Humans, and that there is something about the Sumo Ants game that prevents this interesting kind of hypnosis from happening. I\u2019d love to see a followup paper that can pull this off a little more reliably. What a  time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u5wtoH0_KuA",
        "paper_link": "https://adversarialpolicies.github.io",
        "paper_title": "Adversarial Policies"
    },
    {
        "video_id": "9gX24m3kcjA",
        "video_title": "Can We Teach a Robot Hand To Keep Learning?",
        "position_in_playlist": 431,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Efficient Adaptation for End-to-End Vision-Based Robotic Manipulation\" is available here:\nhttps://sites.google.com/view/efficient-ft/home\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In 2019, researchers at OpenAI came up with an amazing learning algorithm that they deployed on a robot hand that was able to dexterously manipulate a Rubik\u2019s cube\u2026even when it was severely hamstrung. A good game plan to perform such a thing, is to first, solve the problem in a computer simulation where we can learn and iterate quickly, and then, transfer everything the agent learned there to the real world, and hope that it obtained general knowledge that indeed can be applied to real tasks. Papers like these are some of my favorites. If you are one of our core Fellow Scholars, you may also remember that we talked about walking robots about 200 episodes ago. In this amazing paper, we witnessed a robot not only learning to walk, but, it could also adjust its behavior and keep walking, even if one or multiple legs lose power, or get damaged. In this previous work, the key idea was to allow the robot to learn tasks such as walking not only in one, optimal way, but to explore and build a map of many alternative motions relying on different body parts. Both of these papers teach us that working in the real world often shows us new, unexpected challenges to overcome. And, this new paper offers a technique to adapt a robot arm to these challenges after it has been deployed into the real world. It is supposed to be able to pick up objects, which sounds somewhat simple these days\u2026until we realize that new, previously unseen objects may appear in the bin with different shapes, or material models. For example, reflective and refractive objects are particularly perilous because they often show us more about their surroundings than about themselves, lighting conditions may also change after deployment, the gripper\u2019s length or shape may change, and many, many other issues are likely to arise. Let\u2019s have a look at the lighting conditions part. Why would that be such an issue? The objects are the same, the scene looks nearly the same, so, why is this a challenge? Well, if the lighting changes, the reflections change significantly, and since the robot arm sees its reflection and thinks that it is a different object, it just keeps trying to grasp it. After some fine-tuning, this method was able to increase the otherwise not too pleasant 32% success rate to 63%. Much, much better. Also, extending the gripper used to be somewhat of a problem, but as you see here, with this technique, it is barely an issue anymore. Also, if we have a somewhat intelligent system, and we move position of the gripper around, nothing really changes, so we would expect it to perform well. Does it? Well, let\u2019s have a look! Unfortunately, it just seems to be rotating around without too many meaningful actions. And now, hold on to your papers, because after using this continual learning scheme, yes! It improved substantially and makes very few mistakes, and can even pick up these tiny objects that are very challenging to grasp with this clumsy hand. This fine-tuning step typically takes an additional hour, or at most a few hours of extra training, and can used to help these AIs learn continuously after they are deployed in the real world, thereby updating and improving themselves. It is hard to define what exactly intelligence is, but an important component of it is being able to reuse knowledge and adapt to new, unseen situations. This is exactly what this paper helps with. Absolute witchcraft. What a time to be alive! This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client\u2019s websites and reliably serve them to millions of visitors, Linode can do that too. What\u2019s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit\u00a0linode.com/papers\u00a0or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9gX24m3kcjA",
        "paper_link": "https://sites.google.com/view/efficient-ft/home",
        "paper_title": "Efficient Adaptation for End-to-End Vision-Based Robotic Manipulation"
    },
    {
        "video_id": "sTe_-YOccdM",
        "video_title": "Two Shots of Green Screen Please!",
        "position_in_playlist": 432,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation for this paper is available here:\nhttps://app.wandb.ai/stacey/greenscreen/reports/Two-Shots-to-Green-Screen%3A-Collage-with-Deep-Learning--VmlldzoxMDc4MjY\n\n\ud83d\udcdd The paper \"Background Matting: The World is Your Green Screen\" is available here:\nhttps://grail.cs.washington.edu/projects/background-matting/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When shooting feature-length movies, or just trying to hold meetings from home through Zoom or Skype, we can make our appearance a little more professional by hiding the mess we have in the background by changing it to something more pleasing. Of course, this can only happen if we have an algorithm at hand that can detect what the foreground and background is, which, typically, is easiest when we have a green screen behind us that is easy to filter for even the simpler algorithms out there. However, of course, not everyone has a green screen at home, and even for people who do, may need to hold meetings out there in the wilderness. Unfortunately, this would mean that the problem statement is the exact opposite of what we\u2019ve said, or in other words, the background is almost anything else but a green screen. So, it is possible to apply some of these newer neural-network based learning methods to tackle this problem? Well, this technique promises to make this problem much, much easier to solve. All we need to do is take two photographs, one with, and one without the test subject, and it will automatically predict an alpha matte that isolates the test subject from the background. If you have a closer look, you\u2019ll see the first part of why this problem is difficult. This matte is not binary, so the final compositing process is not given as only foreground or only background for every pixel in the image, but there are parts, typically around the silhouettes and hair that need to be blended together. This blending information is contained in the grey parts of the image, and are especially difficult to predict. Let\u2019s have a look at some results! You see the captured background here, and the input video below, and you see that it is truly a sight to behold. It seems that this person is really just casually hanging out in front of a place that is definitely not a whiteboard. It even works in cases where the background or the camera itself is slightly in motion. Very cool! It really is much, much better than these previous techniques, where you see that temporal coherence is typically a problem. This is the flickering that you see here, which arises from the vastly different predictions for the alpha matte between neighboring frames in the video. Opposed to previous methods, this new technique shows very little of that. Excellent! Now, we noted that a little movement in the background is permissible, but it really means just a little\u2026if things get too crazy back there, the outputs are also going to break down. This wizardry all works through a generative adversarial network, in which one neural network generates the output results. This, by itself, didn\u2019t work all that well, because images used to train this neural network can differ greatly from the backgrounds that we record out there in the wild. In this work, the authors bridged the gap by introducing a detector network, that tries to find faults in the output and tell the generator if it has failed to fool it. As the two neural networks duke it out, they work and improve together, yielding these incredible results. Note that there are plenty of more contributions in the paper, so please make sure to have a look for more details! What a time to be alive! What you see here is an instrumentation of this exact paper we have talked about, which was made by Weights and Biases. I think organizing these experiments really showcases the usability of their system. This episode has been supported by Weights & Biases. Here, they show you how you can use Sweeps, their tool to search through high-dimensional parameter spaces and find the best performing model. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it is actively used in projects at prestigious labs, such as OpenAI, Toyota Research, GitHub, and more. And, the best part is that if you are an academic or have an open source project, you can use their tools for free. It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long-standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=sTe_-YOccdM",
        "paper_link": "https://grail.cs.washington.edu/projects/background-matting/",
        "paper_title": "Background Matting: The World is Your Green Screen"
    },
    {
        "video_id": "ihYsJpibNRU",
        "video_title": "Now We Can Relight Paintings\u2026and Turns Out, Photos Too! \ud83c\udfa8",
        "position_in_playlist": 433,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation for this paper is available here:\nhttps://app.wandb.ai/ayush-thakur/paintlight/reports/Generate-Artificial-Lightning-Effect--VmlldzoxMTA2Mjg\n\n\ud83d\udcdd The paper \"Generating Digital Painting Lighting Effects via RGB-space Geometry\" is available here:\nhttps://lllyasviel.github.io/PaintingLight/\n\nThe brush synthesizer project is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/procedural-brush-synthesis-paper/\n\nImage credits:\nBJPentecost - https://www.deviantart.com/bjpentecost\nStyle2Paints Team\nPepper and Carrot\nDavid Revoy - https://www.davidrevoy.com/tag/cc-by\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear fellow scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When I was a bachelor student and took on my first bigger undertaking in computer graphics in 2011 it was a research project for a feature length movie where the goal was to be able to learn the brush strokes of an artist; you see the sample brush stroke here. And what it could do is change the silhouette of a digital 3D object to appear, as if it were drawn with this style. This way, we could use an immense amount of perfectly modeled geometry, and make them look as if they were drawn by an artist. The project was a combination of machine learning and computer graphics and got me hooked on this topic for life. So this was about silhouettes but what about being able to change the lighting? To address this problem, this new work promises something that sounds like science fiction. The input is a painting which is thought of as a collection of brush strokes. First, the algorithm is trying to break down the image into these individual strokes. Here on the left, with (a), you see the painting itself; and (b), is the real collection of strokes that were used to create it. This is what the algorithm is trying to estimate it with. And this colorful image visualizes the difference between the two. The blue color denotes regions where these brush strokes are estimated well; and we can find more differences as we transition into the red colored regions. So, great, now that we have a bunch of these brush strokes, but what do we do with them? Well let's add one more assumption into this system, which is: that the densely packed regions are going to be more effected by the lighting effects, while the sparser regions will be less impacted. This way, we can make the painting change as if we were to move our imaginary light source around. No painting skills or manual labor required. Wonderful. But, some of the skeptical fellow scholars out there would immediately ask the question: It looks great, but how do we know if this really is good enough to be used in practice? The authors thought of that too, and asked an artist to create some of these views by hand; and what do you know, they are extremely good. Very close to the real deal, and all this comes for free. Insanity. Now, we noted that the input for this algorithm is just one image. So, what about a cheeky experiment where we would add, not a painting, but a photo; and pretend that it is a painting, can it relight it properly? Well hold onto your papers and let's have a look. Here is the photo; the break down of the brush strokes, if this were a painting; and, wow! here are the lighting effects. It worked! and if you enjoyed these results, and would like to see more, make sure to have a look at this beautiful paper in the video description. For instance here, you see a comparison against previous works, and it seems quite clear that it smokes the competition, on a variety of test cases. And these papers they're comparing to are also quite recent. The pace of progress in computer graphics research is absolutely incredible; more on that in a moment. Also, just look at the information density here. This tiny diagram shows you exactly where the light source positions are; I remember looking at a paper on a similar topic that did not have this thing, and it made the entirety of the work a great deal more challenging to evaluate properly. This kind of attention to detail might seem like a small thing, but it makes all the difference for a great paper; which, this one is. The provided user study shows that these outputs can be generated within a matter of seconds, and reinforces our hunch that most people prefer the outputs of the new technique to the previous ones; so much improvement in so little time. With this, we can now create digital lighting effects from a single image, for paintings; and even photographs in a matter of seconds. What a time to be alive! What you see here, is an instrumentation of this exact paper we have talked about, which was made by Weights & Biases. I think organizing these experiments really showcases the usability of their system. Weights & Biases provides tools to track your experiments in your deep learning projects. Their system is designed to save you a ton of time and money, and it's actively used in projects at prestigious labs, such as: OpenAI, Toyota Research, GitHub, and more. And, the best part is, that if you're an academic, or have an open source project, you can use their tools for free. (wandb.com/papers) It really is as good as it gets. Make sure to visit them through wandb.com/papers or just click the link in the video description and you can get a free demo today. Our thanks to Weights & Biases for their long standing support, and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ihYsJpibNRU",
        "paper_link": "https://lllyasviel.github.io/PaintingLight/",
        "paper_title": "Generating Digital Painting Lighting Effects via RGB-space Geometry"
    },
    {
        "video_id": "dJ4rWhpAGFI",
        "video_title": "DeepMind Made A Superhuman AI For 57 Atari Games! \ud83d\udd79",
        "position_in_playlist": 434,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Agent57: Outperforming the Atari Human Benchmark\" is available here:\nhttps://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark\nhttps://arxiv.org/abs/2003.13350\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\nApologies and special thanks to Owen Skarpness!\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/ \n\n#Agent57 #DeepMind",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Between 2013 and 2015, DeepMind worked on an incredible learning algorithm by the name Deep Reinforcement Learning. This technique looked at the pixels of the game, was given a controller and played much like a human would\u2026 with the exception that it learned to play some Atari games on a superhuman level. I have tried to train it a few years ago and would like to invite you for a marvelous journey to see what happened. When it starts learning to play an old game, Atari breakout, at first, the algorithm loses all of its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing the game, roughly matching the skill level of an adept player. But here's the catch, if we wait for longer, we get something absolutely spectacular. Over time, it learns to play like a pro, and finds out that the best way to win the game is digging a tunnel through the bricks and hit them from behind. This technique is combination of a neural network that processes the visual data that we see on the screen, and a reinforcement learner that comes up with the gameplay-related decisions. This is an amazing algorithm, a true breakthrough in AI research. However, it had its own issues. For instance, it did not do well on Montezuma\u2019s revenge or Pitfall because these games require more long-term planning. Believe it or not, the solution in a followup work was to infuse these agents with a very human-like property\u2026 curiosity. That agent was able to do much, much better at these games\u2026and then got addicted to the TV. But that\u2019s a different story. Note that this has been remedied since. And believe it or not, as impossible as it may sound, all of this has been improved significantly. This new work is called Agent57, and it plays better than humans, on all 57 Atari games. Absolute insanity. Let\u2019s have a look at it in action and then in a moment, I\u2019ll try to explain how it does what it does. You see Agent57 doing really well at the Solaris game here. This space battle game is one of the most impressive games on the Atari as it contains 16 quadrants, 48 sectors, space battles, warp mechanics, pirate ships, fuel management, and much more, you name it. This game is not only quite complex, but it also is a credit assignment nightmare for an AI to play. This credit assignment problem means that it can happen that we choose an action, and we only win or lose hundreds of actions later, leaving us with no idea as to which of our actions led to this win or loss, thus, making it difficult to learn from our actions. This Solaris game is a credit assignment nightmare. Let me try to bring this point to life by talking about school. In school, when we take an exam, we hand it in, and the teacher gives us feedback for every single one of our solutions and tells us whether we were correct or not. We know exactly where we did well, and what we need to practice to do better next time. Clear, simple, easy. Solaris, on the other hand, not so much! If this were a school project, the Solaris game would be a brutal, merciless teacher. Would you like to know your grade? No grades, but he tells you that you failed. Well, that\u2019s weird, okay. Where did we fail? He won\u2019t say. What should we do better next time to improve? You\u2019ll figure it out bucko! Also, we wrote this exam 10 weeks ago, why do we only get to know about the results now? No answer. I think in this case, we can conclude that this would be a challenging learning environment even for a motivated human, so just imagine how hard it is for an AI! Hopefully this puts into perspective how incredible it is that Agent57 performs well on this game. It truly looks like science fiction. To understand what Agent57 adds to this, it was given something called a meta-controller that can decide when to prioritize short and long term planning. On the short term, we typically have mechanical challenges, like avoiding a skull in Montezuma\u2019s revenge or dodging the shots of an enemy ship in Solaris. The long term part is also necessary to explore new parts of the game, and have a good strategic plan to eventually win the game. This is great because this new technique can now deal with the brutal and merciless teacher who we just introduced. Alternatively, this agent can be thought of someone who has a motivation to explore the game and do well at mechanical tasks at the same time and can also prioritize these tasks. With this, for the first time, scientists at DeepMind found a learning algorithm that exceeds human performance on all 57 Atari games. And please, do not forget about the fact that DeepMind tries to solve general intelligence, and then, use general intelligence to solve everything else. This is their holy grail. In other words, they are seeking an algorithm that can learn by itself and achieve human-like performance on a wide variety of tasks. There is still plenty to do, but, we are now one step closer to that. If you learn only one thing from this video, let it be the fact that there are not 57 different methods, but one general learning algorithm that plays 57 games better than humans. What a time to be alive! I would like to show you a short message from a few days ago that melted my heart. This I got from Nathan, who has been inspired by these incredible works and he decided to turn his life around, and go back to study more. I love my job, and reading messages like this is one of the absolute best parts of it. Congratulations Nathan and note that you can take this inspiration and greatness can materialize in every aspect of life, not only in computer graphics or machine learning research. Good luck! If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dJ4rWhpAGFI",
        "paper_link": "https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark",
        "paper_title": "Agent57: Outperforming the Atari Human Benchmark"
    },
    {
        "video_id": "qk4cz0B5kK0",
        "video_title": "Can We Make An Image Synthesis AI Controllable?",
        "position_in_playlist": 435,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nThe showcased post is available here:\nhttps://app.wandb.ai/lavanyashukla/visualize-sklearn/reports/Visualize-Sklearn-Model-Performance--Vmlldzo0ODIzNg\n\n\ud83d\udcdd The paper \"Semantically Multi-modal Image Synthesis\" is available here:\nhttps://seanseattle.github.io/SMIS/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Not so long ago, we talked about a neural image generator that was able to dream up beautiful natural scenes. It had a killer feature where it would take as an input, not only the image itself, but the labeled layout of this image as well. That is a gold mine of information, and including this indeed opens up a killer application. Look! We can even change the scene around by modifying the labels on this layout, for instance, by adding some mountains, make a grassy field, and add a lake. Making a scene from scratch from a simple starting point was also possible with this technique. This is already a powerful, learning-based tool for artists to use as-is, but can we go further? For instance, would it be possible to choose exactly what to fill these regions with? And this, is what today\u2019s paper excels at, and it turns out, it can do, much, much more. Let\u2019s dive in. One, we can provide it this layout, which they refer to as a semantic mask, and it can synthesize clothes, pants, and hair in many, many different ways. Heavenly. If you have a closer look, you see that fortunately, it doesn\u2019t seem to change any other part of the image. Nothing too crazy here, but please remember this, and now would be a good time to hold on to your papers, because two, it can change the sky or the material properties of the floor. And\u2026wait! Are you seeing what I am seeing? We cannot just change the sky because we have a lake there, reflecting it, therefore, the lake has to change too. Does it? Yes, it does! It indeed changes other parts of the image when it is necessary, which is the hallmark of a learning algorithm that truly understands what it is synthesizing. You can see this effect especially clearly at the end of the looped footage when the sky is the brightest. Loving it. So what about the floor? This is one of my favorites! It doesn\u2019t just change the color of the floor itself, but it performs proper material modeling. Look, the reflections also became glossier over time. A proper light transport simulation for this scenario would take a very, very long time, we are likely talking from minutes to hours. And this thing has never been taught about light transport and learned about these materials by itself! Make no mistake, these may be low-resolution, pixelated images, but this still feels like science fiction. Two more papers down the line, and we will see HD videos of this I am sure. The third application is something that the authors refer to as appearance mixture, where we can essentially select parts of the image to our liking and fuse these aspects together into a new image. This could, more or less be done with traditional, handcrafted methods too, but four, it can also do style morphing, where we start from image A, change it until it looks like image B, and back. Now, normally, this can be done very easily with a handcrafted method called image interpolation, however, to make this morphing really work, the tricky part is that all of the intermediate images have to be meaningful. And as you can see, this learning method does a fine job at that. Any of these intermediate images can stand on their own. I\u2019ll try to stop the morphing process at different points so you can have a look and decide for yourself. Let me know in the comments if you agree. I am delighted to see that these image synthesis algorithms are improving at a stunning pace, and I think these tools will rapidly become viable to aid the work of artists in the industry. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=qk4cz0B5kK0",
        "paper_link": "https://seanseattle.github.io/SMIS/",
        "paper_title": "Semantically Multi-modal Image Synthesis"
    },
    {
        "video_id": "8GVHuGCH2eM",
        "video_title": "DeepMind\u2019s New AI Helps Detecting Breast Cancer",
        "position_in_playlist": 436,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"International evaluation of an AI system for breast cancer screening\" is available here:\nhttps://deepmind.com/research/publications/International-evaluation-of-an-artificial-intelligence-system-to-identify-breast-cancer-in-screening-mammography\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Marcin Dukaczewski, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Rob Rowe, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. These days, we see so many amazing uses for learning-based algorithms, from enhancing computer animations, teaching virtual animals to walk, to teaching self-driving cars depth perception, and more. It truly feels like no field of science is left untouched by these new techniques, including the medical sciences. You see, in medical imaging, a common problem is that we have so many diagnostic images out there in the wild that it makes it more and more infeasible for doctors to look at all of them. What you see here is a work from scientists at DeepMind Health that we covered a few hundred episodes ago. The training part takes about 14 thousand optical coherence tomography scans, this is the OCT label you see on the the left, these images are cross sections of the human retina. We first start out with this OCT scan, then, a manual segmentation step follows, where a doctor marks up this image to show where the most relevant parts, like the retinal fluids or the elevations of retinal pigments are. After the learning process, this method can reproduce these segmentations really well by itself, without the doctor\u2019s supervision, and you see here that the two images are almost identical in these tests. Now that we have the segmentation map, it is time to perform classification. This means that we look at this map and assign a probability to each possible condition that may be present. Finally, based on these, a final verdict is made whether the patient needs to be urgently seen, or just a routine check, or perhaps no check is required. This was an absolutely incredible piece of work. However, it is of utmost importance to evaluate these tools together with experienced doctors, and hopefully, on international datasets. Since then, in this new work, DeepMind has knocked the evaluation out of the park for a system they developed to detect breast cancer as early as possible. Let\u2019s briefly talk about the technique, and then I\u2019ll try to explain why it is sinfully difficult to evaluate it properly. So, onto the new problem. These mammograms contain four images that show the breasts from two different angles, and the goal is to predict whether the biopsy taken later will be positive for cancer or not. This is especially important because early detection is key for treating these patients. And the key question is, how does it compare to the experts? Have a look here. This is a case of cancer that was missed by all six experts in the study, but was correctly identified by the AI. And what about this one? This case didn\u2019t work so well \u2014 it was caught by all six experts but was missed by the AI. So, one reassuring sample, and one failed sample. And with this, we have arrived to the central thesis of the paper, which asks the question, \u201cwhat does it really take to say that an AI system surpassed human experts\u201d? To even have a fighting chance in tackling this, we have to measure false positives and false negatives. A false positive means that the AI mistakenly predicts that the sample is positive, when in reality, it is negative. A false negative means that the AI thinks that the sample is negative, whereas it is positive in reality. The key is that in every decision domain, the permissible rates for false negatives and positives is different. Let me try to explain this through this example. In cancer detection, if we have a sick patient who gets classified as healthy is a grave mistake that can lead to serious consequences. But, if we have a healthy patient who is misclassified as sick, the positive cases get a second look from a doctor, who can easily identify the mistake. The consequences, in this case, are much less problematic, and can be remedied by spending a little time checking the samples that the AI was less confident about.The bottom line is that there are many different ways to interpret the data, and, it is by no means trivial to find out which one is the right way to do so. And now, hold on to your papers because here comes the best part. If we compare the predictions of the AI to the human experts, we see that the false positive cases in the US have been reduced by 5.7 percent, while the false negative cases have been reduced by 9.7%. That is the holy grail! That is the holy grail! We don\u2019t need to consider the cost of false positives or negatives here, because it reduced false positives and false negatives at the same time. Spectacular! Another important detail is that these numbers came out of an independent evaluation. It means that the results did not come from the scientists who wrote the algorithm and have been thoroughly checked by independent experts who have no vested interest in this project. This is the reason why you see so many authors on this paper. Excellent. Another interesting tidbit is that the AI was trained on subjects from the UK, and the question was how well does this knowledge generalize for subjects from other places, for instance, the United States. Is this UK knowledge reusable in the US? I have been quite surprised by the answer, because it never saw a sample from anyone in the US, and still did better than the experts on US data. This is a very reassuring property, and I hope to see some more studies that show how general the knowledge is that these systems are able to obtain through training. And, perhaps the most important. If you remember one thing from this video, let it be the following. This work, much like most other AI-infused medical solutions are not made to replace human doctors. The goal is, instead, to empower them, and take off as much weight from their shoulders as possible. We have hard numbers for this, as the results concluded that this work reduces this workload of the doctors by 88%, which is an incredible result. Among other far-reaching consequences, I would like to mention that this would substantially help not only the work of doctors in wealthier, more developed countries, but it may single-handedly enable proper cancer detection in more developing countries who can not afford to check these scans. And note that in this video, we truly have just scratched the surface, whatever we talk about here in a few minutes cannot be a description as rigorous and accurate as the paper itself, so make sure to check it out in the video description. And with that, I hope you now have a good feel of the pace of progress in machine learning research. The retina fluid project was the state of the art in 2018, and now, less than two years later, we have a proper, independently evaluated AI-based detection for breast cancer. Bravo DeepMind. What a time to be alive! This episode has been supported by Linode. Linode is the world\u2019s largest independent cloud computing provider. Unlike entry-level hosting services, Linode gives you full backend access to your server, which is your step up to powerful, fast, fully configurable cloud computing. Linode also has One-Click Apps that streamline your ability to deploy websites, personal VPNs, game servers, and more. If you need something as small as a personal online portfolio, Linode has your back, and if you need to manage tons of client\u2019s websites and reliably serve them to millions of visitors, Linode can do that too. What\u2019s more, they offer affordable GPU instances featuring the Quadro RTX 6000 which is tailor-made for AI, scientific computing and computer graphics projects. If only I had access to a tool like this while I was working on my last few papers! To receive $20 in credit on your new Linode account, visit\u00a0linode.com/papers\u00a0or click the link in the description and give it a try today! Our thanks to Linode for supporting the series and helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=8GVHuGCH2eM",
        "paper_link": "https://deepmind.com/research/publications/International-evaluation-of-an-artificial-intelligence-system-to-identify-breast-cancer-in-screening-mammography",
        "paper_title": "International evaluation of an AI system for breast cancer screening"
    },
    {
        "video_id": "b8sCSumMUvM",
        "video_title": "Is Style Transfer For Fluid Simulations Possible? \ud83c\udf0a",
        "position_in_playlist": 437,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir showcased post is available here:\nhttps://app.wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-model-to-generate-tweets--VmlldzoxMTY5MjI\n\n\ud83d\udcdd The paper \"Lagrangian Neural Style Transfer for Fluids\" is available here:\nhttp://www.byungsoo.me/project/lnst/index.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Style transfer is a technique in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to the super fun results that you see here. An earlier paper had shown that the more sophisticated ones can sometimes make even art curators think that they are real. This previous work blew me away as it could perform style transfer for smoke simulations. I almost fell out of the chair when I have first seen these results. It could do fire textures, starry night, you name it. It seems that it is able to do anything we can think of. Now let me try to explain two things. One, why is this so difficult, and two, the results are looking really good, so, are there any shortcomings? Doing this for smoke simulations is a big departure from 2D style transfer, because that one takes an image, where this works in 3D, and does not deal with images, but with density fields. A density field means a collection of numbers that describe how dense a smoke plume is at a given spatial position. It is a physical description of a smoke plume, if you will. So how could we possibly apply artistic style from an image to a collection of densities? The solution in this earlier paper was to first, downsample the density field to a coarser version, perform the style transfer there, and upsample this density field again with already existing techniques. This technique was called Transport-based Neural Style Transfer, TNST in short, please remember this. Now, let\u2019s look at some results from this technique. This is what our simulation would look like normally, and then, all we have to do is show this image to the simulator, and, what does it do with it? Wow. My goodness. Just look at those heavenly patterns! So what does today\u2019s new, followup work offer to us that the previous one doesn\u2019t? How can this seemingly nearly perfect technique be improved? Well, this new work takes an even more brazen vantage point to this question. If style transfer on density fields is hard, then try a different representation. The title of the paper says Lagrangian style neural style transfer. So what does that mean? It means particles! This was made for particle-based simulations, which comes with several advantages. One, because the styles are now attached to particles, we can choose different styles for different smoke plumes, and they will remember what style they are supposed to follow. Because of this advantageous property, we can even ask the particles to change their styles over time, creating these heavenly animations. In these 2D examples, you also see how the texture of the simulation evolves over time, and that the elements of the style are really propagated to the surface and the style indeed follows how the fluid changes. This is true, even if we mix these styles together. Two, it not only provides us these high-quality results, but, it is fast. And by this, I mean blazing fast. You see, we talked about TNST, the transport-based technique approximately 7 months ago, and in this series, I always note that two more papers down the line, and it will be much, much faster. So here is the Two Minute Papers Moment of Truth, what do the timings say? For the previous technique, it said more than 1d. What could that 1D mean? Oh, goodness! This thing took an entire day to compute. So, what about the new one? What, really? Just one hour? That is insanity. So, how detailed of a simulation are we talking about? Let\u2019s have a look together. M slash f means minutes per frame, and as you see, if we have tens of thousands of particles, we have 0.05, or in other words, three seconds per frame, and we can go up to hundreds of thousands, or even millions of particles, and end up around thirty seconds per frame. Loving it. Artists are going to do miracles with this technique, I am sure. The next step is likely going to be a real-time algorithm, which may appear as soon as one or two more works down the line, and you can bet your papers that I\u2019ll be here to cover it, so make sure to subscribe, and hit the bell icon to not miss it when it appears. The speed of progress in computer graphics research is nothing short of amazing. Also, make sure to have a look at the full paper in the video description, not only because it is a beautiful paper and a lot of fun to read, but because you will also know what this regularization step here does exactly to the simulation. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=b8sCSumMUvM",
        "paper_link": "http://www.byungsoo.me/project/lnst/index.html",
        "paper_title": "Lagrangian Neural Style Transfer for Fluids"
    },
    {
        "video_id": "qwAiLBPEt_k",
        "video_title": "This AI Controls Virtual Quadrupeds! \ud83d\udc15",
        "position_in_playlist": 438,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation for this previous work is available here:\nhttps://app.wandb.ai/sweep/nerf/reports/NeRF-%E2%80%93-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis--Vmlldzo3ODIzMA\n\n\ud83d\udcdd The paper \"CARL: Controllable Agent with Reinforcement Learning for Quadruped Locomotion\" is available here:\nhttps://inventec-ai-center.github.io/projects/CARL/index.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If we have an animation movie or a computer game with quadrupeds, and we are yearning for really high-quality, lifelike animations, motion capture is often the go-to tool for the job. Motion capture means that we put an actor, in our case, a dog in the studio, we ask it to perform sitting, trotting, pacing and jumping, record its motion, and transfer it onto our virtual character. There are two key challenges with this approach. One, we have to try to weave together all these motions, because we cannot record all the possible transitions between sitting and pacing, jumping and trotting, and so on. We need some filler animations to make these transitions work. This was addressed by this neural network-based technique here. The other one is trying to reduce these unnatural foot sliding motions. Both of these have been addressed by learning-based algorithms in the previous works that you see here. Later, bipeds were also taught to maneuver through complex geometry, and sit in not one kind of chair, but any chair, regardless of geometry. This already sounds like science fiction. So, are we done, or can these amazing techniques be further improved? Well, we are talking about research, so the answer is, of course, yes! Here, you see a technique that reacts to its environment in a believable manner. It can accidentally step on the ball, stagger a little bit, and then flounder on this slippery surface, and it doesn\u2019t fall! And it can do much, much more. The goal is that we would be able to do all this without explicitly programming all of these behaviors by hand. But, unfortunately, there is a problem. If we write an agent that behaves according to physics, it will be difficult to control properly. And this is where this new technique shines - it gives us physically appealing motion, and we can grab a controller and play with the character like in a video game. The first step we need to perform is called imitation learning. This means looking at real, reference movement data and trying to reproduce it. This is going to be motion that looks great, is very natural, however, we are nowhere near done, because we still don\u2019t have any control over this agent. Can we improve this somehow? Well, let\u2019s try something and see if it works! This paper proposes that in step number two, we try an architecture by the name generative adversarial network. Here, we have a neural network that generates motion, and a discriminator that looks at these motions and tries to tell what is real, and what is fake. However, to accomplish this, we need lots of real and fake data, that we then use to train the discriminator to be able to tell which one is which. So how do we do that? Well, let\u2019s try to label the movement that came from the user controller inputs as fake, and the reference movement data from before as real. Remember that this makes sense as we concluded that the reference motion looked natural. If we do this, over time, we will have a discriminator neural network that is able to look at a piece of animation data and tell whether it is real or fake. So, after doing all this work, how does this perform? Does this work? Well, sort of\u2026 but it does not react well if we try to control the simulation. If we let it run undisturbed, it works beautifully, and now, when we try to stop it with the controller\u2026well, this needs some more work, doesn\u2019t it? So, how do we adapt this architecture to the animation problem we have here? And here comes one of the key ideas of the paper. In step number three, we can rewire this whole thing to originate from the controller, and introduce a deep reinforcement learning-based fine tuning stage. This was the amazing technique that DeepMind used to defeat Atari Breakout. So what good does all this for us? Well, hold on to your papers, because it enables true user control, while synthesizing motion that is very robust against tough, previously unseen scenarios. And if you have been watching this series for a while, you know what is coming\u2026of course, throwing blocks at it and see how well it can take the punishment. As you see, the AI is taking it like a champ. We can also add pathfinding to the agent, and, of course, being computer graphics researchers, throw some blocks into the mix for good measure. It performs beautifully. This is so realistic. We can also add sensors to the agent to allow them to navigate in this virtual world in a realistic manner. Just a note on how remarkable this is. So this quadruped behaves according to physics, lets us control it with a controller, which is already somewhat of a contradiction. And, it is robust against these perturbations at the same time. This is absolute witchcraft, and no doubt, it has earned to be accepted to SIGGRAPH, which is perhaps the most prestigious research venue in computer graphics. Congratulations. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=qwAiLBPEt_k",
        "paper_link": "https://inventec-ai-center.github.io/projects/CARL/index.html",
        "paper_title": "CARL: Controllable Agent with Reinforcement Learning for Quadruped Locomotion"
    },
    {
        "video_id": "6oQ0Obi14rM",
        "video_title": "OpenAI\u2019s Jukebox AI Writes Amazing New Songs \ud83c\udfbc",
        "position_in_playlist": 439,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation of this paper is available here:\nhttps://app.wandb.ai/authors/openai-jukebox/reports/Experiments-with-OpenAI-Jukebox--VmlldzoxMzQwODg\n\n\ud83d\udcdd The paper \"Jukebox: A Generative Model for Music\" is available here:\nhttps://openai.com/blog/jukebox/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1261793/\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, I will attempt to tell you a glorious tale about AI-based music generation. You see, there is no shortage of neural network-based methods that can perform physics simulations, style transfer, deepfakes, and a lot more applications where the training data is typically images, or video. If the training data for a neural network is in pure text, it can learn about that. If the training data is waveforms and music, it can learn that too! Wait, really? Yes! In fact, let\u2019s look at two examples and then, dive into today\u2019s amazing paper. In this earlier work by the name Look, Listen and Learn, two scientists at DeepMind set out to look at a large number of videos with sound. You see here that there is a neural network for processing the vision, and one for the audio information. That sounds great, but what are these heatmaps? These were created by this learning algorithm, and they show us, which part of the image is responsible for the sounds that we hear in the video. The hotter the color, the more sounds are expected from a given region. It was truly amazing that it didn\u2019t automatically look for humans and colored them red in the heatmap - there are cases where the humans are expected to be the source of the noise, for instance, in concerts, whereas in other cases, they don\u2019t emit any noise at all. It could successfully identify these cases. This still feels like science fiction to me, and we covered this paper in 2017, approximately 250 episodes ago. You will see that we have come a long, long way since. We often say that these neural networks should try embody general learning concepts. That\u2019s an excellent, and in this case, testable statement, so let\u2019s go ahead and have a look under the hood of these vision and audio processing neural networks\u2026and\u2026yes, they are almost identical! Some parameters are not same because they have been adapted to the length and dimensionality of the incoming data, but the key algorithm that we run for the learning is the same. Later, in 2018 DeepMind published a followup work that looks at performances on the piano from the masters of the past and learns play in their style. A key differentiating factor here was that it did not do what most previous techniques do, which was looking at the score of the performance. These older techniques knew what to play, but not how to play these notes, and these are the nuances that truly make music come alive. This method learned from raw audio waveforms and thus, could capture much, much more of the artistic style. Let\u2019s listen to it, and in the meantime, you can look at the composers it has learned from to produce these works. However, in 2019, OpenAI recognized that text-based music synthesizers can not only look at a piece of score, but can also continue it, thereby composing a new piece of music, and what\u2019s more, they could even create really cool blends between genres. Listen as their AI starts out from the first 6 notes of a Chopin piece and transitions into a pop style with a bunch of different instruments entering a few seconds in. Very cool! The score-based techniques are a little lacking in nuance, but can do magical genre mixing and more, whereas the waveform-based techniques are more limited, but can create much more sophisticated music. Are you thinking what I am thinking? Yes, you have guessed right, hold on to your papers, because in OpenAI\u2019s new work, they tried to fuse the two concepts together, or, in other words, take a genre, an artist, and even lyrics as an input, and it would create a song for us. Let\u2019s marvel at these few curated samples together. The genre, artist and lyrics information will always be on the screen. Wow, I am speechless. Loved the AI-based lyrics too. This has the nuance of waveform-based techniques, with the versatility of the score-based methods. Glorious! If you look in the video description, you will find a selection of uncurated music samples as well. It does what it does by compressing the raw audio waveform into a compact representation. In this space, it is much easier to synthesize new patterns, after which, we can decompress it to get the output waveforms. It has also learned to group up and cluster a selection of artists which reflects how the AI thinks about them. There is so much cool stuff in here that it would be worthy of a video of its own. Note that it currently takes 9 hours to generate one minute of music, and the network was mainly trained on Western music and only speaks English, but you know, as we always say around here, two more papers down the line, and it will be improved significantly. I cannot wait to report on them should any followup works appear, so make sure to subscribe and hit the bell icon to not miss it. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6oQ0Obi14rM",
        "paper_link": "https://openai.com/blog/jukebox/",
        "paper_title": "Jukebox: A Generative Model for Music"
    },
    {
        "video_id": "2Bw5f4vYL98",
        "video_title": "How Well Can an AI Learn Physics? \u269b",
        "position_in_playlist": 440,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"Learning to Simulate Complex Physics with Graph Networks\" is available here:\nhttps://arxiv.org/abs/2002.09405\nhttps://sites.google.com/view/learning-to-simulate/home#h.p_hjnaJ6k8y0wo\n\n\ud83c\udf0a The thesis on fluids is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/fluid_control_msc_thesis/\n\n\u00a0\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If you have been watching this series for a while, you know very well that I love learning algorithms and fluid simulations. But do you know what I like even better? Learning algorithms applied to fluid simulations, so I couldn\u2019t be happier with today\u2019s paper. We can create wondrous fluid simulations like the ones you see here by studying the laws of fluid motion from physics, and write a computer program that contains these laws. As you see, the amount of detail we can simulate with these programs is nothing short of amazing. However, I just mentioned neural networks. If we can write a simulator that runs the laws of physics to create these programs, why would we need learning-based algorithms? The answer is in this paper that we have discussed about 300 episodes ago. The goal was to show a neural network video footage of lots and lots of fluid and smoke simulations, and have it learn how the dynamics work, to the point that it can continue and guess how the behavior of a smoke puff would change over time. We stop the video and it would learn how to continue it, if you will. This definitely is an interesting take as normally, we use neural networks to solve problems that are otherwise close to impossible to tackle. For instance, it is very hard, if not impossible to create a handcrafted algorithm that detects cats reliably because we cannot really write down the mathematical description of a cat. However, these days, we can easily teach a neural network to do that. But this task here is fundamentally different. Here, the neural networks are applied to solve something that we already know how to solve. Especially given that if we use a neural network to perform this task, we have to train it, which is a long and arduous process. I hope to have convinced you that this is a bad, bad idea. Why would anyone bother to do that? Does this make any sense? Well, it does make a lot of sense! And the reason for that is that this training step only has to be done once, and afterwards, querying the neural network, that is, predicting what happens next in the simulation runs almost immediately. This takes way less time than calculating all the forces and pressures in the simulation while retaining high quality results. So, we suddenly went from thinking that an idea is useless to being amazing. What are the weaknesses of the approach? Generalization. You see, these techniques, including a newer variant that you see here can give us detailed simulations in real time or close to real time, but if we present them with something that is far outside of the cases that they had seen in the training domain, they will fail. This does not happen with our handcrafted techniques, only to AI-based methods. So, onwards to this new technique, and you will see in just a moment that a key differentiator here is that its generalization capabilities are just astounding. Look here. The predicted results match the true simulation quite well. Let\u2019s look at it in slow motion too so we can evaluate it a little better. Looking great. But, we have talked about superior generalization, so what about that? Well, it can also handle sand and goop simulations, so that\u2019s a great step beyond just water and smoke. And now, have a look at this one. This is a scene with the boxes it has been trained on. And now, let\u2019s ask it to try to simulate the evolution of significantly different shapes. Wow. It not only does well with these previously unseen shapes, but it also handles their interactions really well. But there is more! We can also train it on a tiny domain with only a few particles, and then, it is able to learn general concepts that we can reuse to simulate a much bigger domain, and also, with more particles. Fantastic! But there is even more! We can train it by showing how water behaves on these water ramps, and then, let\u2019s remove the ramps and see if it understands what it has to do with all these particles? Yes, it does! Now, let\u2019s give it something more difficult. I want more ramps! Yes! And now, even more ramps! Yes! I love it! Let\u2019s see if it can do it with sand too. Here is the ramp for training, and let\u2019s try an hourglass now. Absolute witchcraft. And we are even being paid to do this. I can hardly believe this! The reason why you see so many particles in many of these views, is because if we look under the hood, we see that the paper proposes a really cool graph-based method that represents the particles and they can pass messages to each other over these connections between them. This leads to a simple, general and accurate model that truly is a force to be reckoned with. Now, this is a great leap in neural network-based physics simulations, but of course, not everything is perfect here. Its generalization capabilities have their limits. For instance, over longer timeframes, solids may get incorrectly deformed. However, I will quietly note that during my college years, I was also studying the beautiful Navier-Stokes equations and even as a highly motivated student, it took several months to understand the theory and write my first fluid simulator. You can check out the thesis and the source code in the video description if you are interested. And to see that these neural networks could learn something very similar in a matter of days\u2026 every time I think about this, shivers run down my spine. Absolutely amazing. What a time to be alive! This episode has been supported by Lambda. If you're a researcher or a startup looking for cheap GPU compute to run these algorithms, check out Lambda GPU Cloud. I've talked about Lambda's GPU workstations in other videos and am happy to tell you that they're offering GPU cloud services as well. The Lambda GPU Cloud can train Imagenet to 93% accuracy for less than $19! Lambda's web-based IDE lets you easily access your instance right in your browser. And finally, hold on to your papers, because the Lambda GPU Cloud costs less than half of AWS and Azure. Make sure to go to lambdalabs.com/papers and sign up for one of their amazing GPU instances today. Our thanks to Lambda for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2Bw5f4vYL98",
        "paper_link": "https://arxiv.org/abs/2002.09405",
        "paper_title": "Learning to Simulate Complex Physics with Graph Networks"
    },
    {
        "video_id": "oYtwCZx5rsU",
        "video_title": "Surprise Video With Our New Paper On Material Editing! \ud83d\udd2e",
        "position_in_playlist": 441,
        "description": "\ud83d\udcdd Our \"Photorealistic Material Editing Through Direct Image Manipulation\" paper and its source code are available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-editing/\n\nThe previous paper with the microplanet scene is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#NeuralRendering",
        "transcript": "Dear Fellow Scholars, this is not Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r, due to popular demand, this is a surprise video with the talk of our new paper that we just published. This was the third and last paper in my PhD thesis and hence, this is going to be a one-off video that is longer and a tiny bit more technical, I am keenly aware of it, but I hope you\u2019ll enjoy it. Let me know in the comments when you have finished the video. And worry not, all the upcoming videos are going to be in the usual Two Minute Papers format. The paper and the source code are all available in the video description. And now, let\u2019s dive in. In a previous paper, our goal was to populate this scene with over a hundred materials with a learning-based technique, and create a beautiful planet with rich vegetation. The results looked like this. One of the key elements to accomplish this was to use a neural renderer, or in other words, a decoder network that you see here which took a material shader description as an input and predicted its appearance, thereby replacing the renderer we used in the project. It had its own limitations, for instance, it was limited to this scene with a fixed lighting setup, and only the material properties were subject to change. But in return, it mimiced the global illumination renderer rapidly and faithfully. And, in this new work, our goal was to take a different vantage point, and help artists with general image processing knowledge to perform material synthesis. Now, this sounds a little nebulous, so let me explain. One of the key ideas is to achieve this with a system that is meant to take images from its own renderer, like the ones you see here. But, of course, we produced these ourselves, so obviously, we know how to do it, so this is not very useful yet. However, the twist is that we only start out with an image of this source material, and then, load it into a raster image editing program like Photoshop, and edit it to our liking, and just pretend that this is achievable with our renderer. As you see, many of these target images in the middle are results of poorly-executed edits. For instance, the stitched specular highlight in the first example isn\u2019t very well done, and neither is the background of the gold target image in the middle. However, in the next step, our method proceeds to find a photorealistic material description that, when rendered, resembles this target image, and works well even in the presence of these poorly executed edits. The whole process executes in 20 seconds To produce a mathematical formulation for this problem, we started with this. We have an input image t, and edit it to our liking to get the target image, t with a tilde. Now, we are looking for a shader parameter set x, that, when rendered with the phi operator, approximates the edited image. The constraint below stipulates that we should remain within the physical boundaries for each parameter, for instance, albedos between 0 and 1, proper indices of refraction and so on. So how do we deal with phi? We use the previously mentioned neural renderer to implement it, otherwise this optimization process would take 25 hours. Later, we made an equivalent, unconstrained reformulation of this problem to be able to accommodate a greater set of optimizers. This all sounds great on paper, and works reasonably well for materials that can be exactly matched with this shader, like this one. This optimizer-based solution can achieve it reasonably well. But, unfortunately, for more challenging cases, as you see the target image on the lower right, the optimizer\u2019s output leaves much to be desired. Note again that the result on the upper right is achievable with the shader, while the lower right is a challenging imaginary material that we are trying to achieve. The fact that this is quite difficult is not a surprise because we have non-linear and non-convex optimization problem, which is also high-dimensional. So, this optimization solution is also quite slow, but it can start inching towards the target image. As an alternative solution, we also developed something that we call an inversion network, this addresses the adjoint problem of neural rendering, or in other words, we show it the edited input image, and out comes the shader that would produce this image. We have trained 9 different neural network architectures for this problem, which sounds great, so how well did it work? Well, we found out that none of them are really satisfactory for more difficult edits because all of the target images are far-far outside of the training domain. We just cannot prepare the networks to be able to handle the rich variety of edits that come from the artists. However, some of them are, one could say, almost usable, for instance, number one and five are not complete failures. And note that these solutions are provided instantly. So, we have two techniques, none of them are perfect for our task: a fast and approximate solution with the inversion networks, and a slower optimizer that can slowly inch towards the target image. Our key insight here is that we can produce a hybrid method that fuses the two solutions together. The workflow goes as follows: we take an image of the initial source material, and edit it to our liking to get this target image. Then, we create a coarse prediction with a selection of inversion networks, to initialize the optimizer with the prediction of one of these neural networks. Preferably a good one so the optimizer can start out from a reasonable initial guess. So, how well does this hybrid method work? I\u2019ll show you in a moment. Here, we start out with an achievable target image, and then, try two challenging image editing operations. This image can be reproduced perfectly as long as the inversion process works reliably. Unfortunately, as you see here, this is not the case. In the first row, using the optimizer and the inversion networks separately, we get results that fail to capture the specular highlight properly. In the second row, we have deleted the specular highlight on the target image on the right and replaced it with a completely different one. I like to call this the FrankenBRDF and it would be amazing if we could do this, but unfortunately, both the optimizer and the inversion networks flounder. Another thing that would be really nice to do is deleting the specular highlight and filling the image via image inpainting. This kind of works with the optimizer, but you\u2019ll see in a moment that it\u2019s not nearly as good as it could be. And now, if you look carefully, you see that our hybrid method outperforms both of these techniques in each of the three cases. In the paper, we report results on a dozen more cases as well. We make an even stronger claim in the paper, where we say that these results are close to the global optimum. You see the results of this hybrid method if you look at the intersection of Nelder-Mead and NN, they are highlighted with the red ellipses. The records in the table show the RMS errors and are subject to minimization. With this, you see that this goes neck and neck with a global optimizer, which is highlighted with green. In summary, our technique runs in approximately 20 seconds, works for specular highlight editing, image blending, stitching, inpainting and more. The proposed method is robust, works even in the presence of poorly edited images, and can be easily deployed in already existing rendering systems and allows for rapid material prototyping for artists working in the industry. It is also independent of the underlying principled shader, so you can also add your own and expect it to work well as long as the neural renderer works reliably. A key limitation of the work is that it only takes images in this canonical scene with the carved sphere material sample, but we conjecture that it can be extended to be more general and propose a way to do it in the paper. Make sure to have a closer look if you are interested. The teaser image of this paper is showcased in the 2020 Computer Graphics Forum cover page. The whole thing is also quite simple to implement, and we provide the source code, pre-trained networks on our website, all of them are under a permissive license. Thank you so much for watching this and a big thanks to Peter Wonka and Michael Wimmer for advising this work.",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=oYtwCZx5rsU"
    },
    {
        "video_id": "3UZzu4UQLcI",
        "video_title": "NVIDIA\u2019s AI Recreated PacMan! \ud83d\udc7b",
        "position_in_playlist": 442,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nThe mentioned blog post is available here:\nhttps://www.wandb.com/articles/visualizing-molecular-structure-with-weights-biases\n\n\ud83d\udcdd The paper \"Learning to Simulate Dynamic Environments with GameGAN\" is available here:\nhttps://nv-tlabs.github.io/gameGAN/\n\nOur paper with the neural renderer is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#NVIDIA #GameGAN",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Neural network-based learning methods are capable of wondrous things these days. They can do classification, which means that they look at an image, and the output is a decision, whether we see a dog or a cat, or a sentence that describes an image. In the case of DeepMind\u2019s AI playing Atari games, the input is the video footage of the game, and the output is an action that decides what we do with our character next. In OpenAI\u2019s amazing Jukebox paper, the input was a style of someone, a music genre, and lyrics, and the output was a waveform, or in other words, a song we can listen to. But, a few hundred episodes ago we covered a paper from 2015, where scientists at DeepMind asked the question, what if we would get these neural networks to output not sentences, decisions, waveforms or any of that sort\u2026 what if the output would be a computer program? Can we teach a neural network programming? I was convinced that the answer is no, until I saw these results. So what is happening here? The input is a scratch pad where we are performing multi-digit addition in front of the curious eyes of the neural network. And if it has looked for long enough, it was indeed able to produce a computer program that could, eventually, perform addition. It could also perform sorting, and would even be able to rotate the images of these cars into a target position. It was called a neural programmer-interpreter, and of course, it was slow and a bit inefficient, but no matter, because it could finally make something previously impossible possible. That is an amazing leap. So, why are we talking about this work from 2015? Well, apart from the fact that there are many amazing works that are timeless, and this is one of them, in this series, I always say, two more papers down the line, and it will be improved significantly. So here is the Two Minute Papers Moment of Truth. How has this area improved with this followup work? Let\u2019s have a look at this paper from scientists at NVIDIA that implements a similar concept for computer games. So how is that even possible? Normally, if we wish to write a computer game, we first, envision the game in our mind, then, we sit down and do the programming. But this new paper does this completely differently. Now, hold on to your papers, because this is a neural network-based method, that first, looks at someone playing the game, and then, it is able to implement the game so that it not only looks like it, but it also behaves the same way to our keypresses. You see it at work here. Yes, this means that we can even play with it and it learns the internal rules of the game and the graphics, just by looking at some gameplay. Note that the key part here is that we are not doing any programming by hand - the entirety of the program is written by the AI. We don\u2019t need access to the source code or the internal workings of the game, as long as we can just look at it, it can learn the rules. Everything truly behaves as expected, we can even pick up the capsule and eat the ghosts as well. This sounds like science fiction, and we are not nearly done yet! There are additional goodies. It has memory and uses it consistently, or, in other words, things don\u2019t just happen arbitrarily. If we return to a state of the game that we visited before, it will remember to present us with very similar information. It also has an understanding of foreground and background, dynamic and static objects as well, so we can experiment with replacing these parts, thereby reskinning our games. It still needs quite a bit of data to perform all this as it has looked at approximately 120 hours of footage of the game being played, however, now, something is possible that was previously impossible. And of course, two more papers down the line, this will be improved significantly I am sure. I think this work is going to be one of those important milestones that remind us that many of the things that we had handcrafted methods for will, over time, be replaced with these learning algorithms. They already know the physics of fluids, or in other words, they are already capable of looking at videos of these simulations and learn the underlying physical laws, and they can demonstrate having learned general knowledge of the rules by being able to continue these simulations, even if we change the scene around quite a bit. In light transport research, we also have decades of progress in simulating how rays of light interact with scenes and we can create these beautiful images. Parts of these algorithms, for instance, noise filtering are already taken over by AI-based techniques, and I can\u2019t help but feel that a bigger tidal wave is coming. This tidal-wave will be an entirely AI-driven technique that will write the code for the entirety of the system. Sure the first ones will be limited, for instance, this is a neural renderer from one of our papers that is limited to this scene and lighting setup, but you know the saying, two more papers down the line, and it will be an order of magnitude better. I can\u2019t wait to tell all about it to you with a video when this happens. Make sure to subscribe and hit the bell icon to not miss any followup works. Goodness, I love my job. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=3UZzu4UQLcI",
        "paper_link": "https://nv-tlabs.github.io/gameGAN/",
        "paper_title": "Learning to Simulate Dynamic Environments with GameGAN"
    },
    {
        "video_id": "BQQxNa6U6X4",
        "video_title": "An AI Made All of These Faces! \ud83d\udd75\ufe0f\u200d\u2640\ufe0f",
        "position_in_playlist": 443,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation of this paper is available here:\nhttps://app.wandb.ai/authors/alae/reports/Adversarial-Latent-Autoencoders--VmlldzoxNDA2MDY\n\nYou can even play with their notebook below!\nhttps://colab.research.google.com/drive/1XWlXN7Oi_5UWqUXjX66z4TD859dwo3UN?usp=sharing\n\n\ud83d\udcdd The paper \"Adversarial Latent Autoencoders\" is available here:\nhttps://github.com/podgorskiy/ALAE\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today I am going to try to tell you the glorious tale of AI-based human face generation and showcase an absolutely unbelievable new paper in this area. Early in this series, we covered a stunning paper that showcased a system that could not only classify an image, but write a proper sentence on what is going on, and could cover even highly non-trivial cases. You may be surprised, but this thing is not recent at all. This is 4 year old news! Insanity. Later, researchers turned this whole problem around, and performed something that was previously thought to be impossible. They started using these networks to generate photorealistic images from a written text description. We could create new bird species by specifying that it should have orange legs and a short yellow bill. Then, researchers at NVIDIA recognized and addressed two shortcomings: one was that the images were not that detailed, and two, even though we could input text, we couldn\u2019t exert too much artistic control over the results. In came StyleGAN to the rescue, which was then able to perform both of these difficult tasks really well. Furthermore, there are some features that are highly localized as we exert control over these images, you can see how this part of the teeth and eyes were pinned to a particular location and the algorithm just refuses to let it go, sometimes to the detriment of its surroundings. A followup work titled StyleGAN2 addresses all of these problems in one go. So, StyleGAN2 was able to perform near-perfect synthesis of human faces, and remember, none of these people that you see here really exist. Quite remarkable. So, how can we improve this magnificent technique? Well, this new work can do so many things, I don\u2019t even know where to start. First, and most important, we now have much much more intuitive artistic control over the output images. We can add or remove a beard, make the subject younger or older, change their hairstyle, make their hairline recede, put a smile on their face, or even make their nose pointier. Absolute witchcraft. So why can we do all this with this new method? The key idea is that it is not using a Generative Adversarial Network, a GAN, in short. A GAN means two competing neural networks, where one is trained to generate new images, and the other one is used to tell whether the generated images are real or fake. GANs dominated this field for a long while because of their powerful generation capabilities, but, on the other hand, they are quite difficult to train and we have only limited control over its output. Among other changes, this work disassembles this generator network into F and G, and the discriminator network into E and D, or in other words, adds an encoder and decoder network here. Why? The key idea is that the encoder compresses the image data down into a representation that we can edit more easily. This is the land of beards and smiles, or in other words, all of these intuitive features that we can edit exist here, and when we are done, we can decompress the output with the decoder network and produce these beautiful images. This is already incredible, but what else can we do with this new architecture? A lot more. For instance, two, if we add a source and destination subjects, their coarse, middle, or fine styles can also be mixed. What does that mean exactly? The coarse part means that high-level attributes, like pose, hairstyle and face shape will resemble the source subject, in other words, the child will remain a child and inherit some of the properties of the destination subjects. However, as we transition to the \u201cfine from source\u201d part, the effect of the destination subject will be stronger, and the source will only be used to change the color scheme and microstructure of this image. Interestingly, it also changes the background of the subject. Three, it can also perform image interpolation. This means that we have these four images as starting points, and it can compute intermediate images between them. You see here that as we slowly become Bill Gates, somewhere along the way, glasses appear. Now note that interpolating between images is not difficult in the slightest and has been possible for a long-long time \u2014 all we need to do is just compute average results between these images. So what makes a good interpolation process? Well, we are talking about good interpolation, when each of the intermediate images make sense and can stand on their own. I think this technique does amazingly well at that. I\u2019ll stop the process at different places, you can see for yourself and let me know in the comments if you agree or not. I also kindly thank the authors for creating more footage just for us to showcase in this series. That is a huge honor, thank you so much! And note that StyleGAN2 appeared around December of 2019, and now, this paper by the name \u201cAdversarial Latent Autoencoders\u201d appeared only four months later. Four months later. My goodness! This is so much progress in so little time it truly makes my head spin. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=BQQxNa6U6X4",
        "paper_link": "https://github.com/podgorskiy/ALAE",
        "paper_title": "Adversarial Latent Autoencoders"
    },
    {
        "video_id": "N6wn8zMRlVE",
        "video_title": "How Do Neural Networks Learn? \ud83e\udd16",
        "position_in_playlist": 444,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation of a previous work we covered is available here:\nhttps://app.wandb.ai/stacey/aprl/reports/Adversarial-Policies-in-Multi-Agent-Settings--VmlldzoxMDEyNzE\n\n\ud83d\udcdd The paper \"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization\" is available here:\nhttps://github.com/poloclub/cnn-explainer\n\nLive web demo: https://poloclub.github.io/cnn-explainer/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. We have recently explored a few new neural network-based learning algorithms that could perform material editing, physics simulations, and more. As some of these networks have hundreds of layers, and often thousands of neurons within these layers, they are almost unfathomably complex. At this point, it makes sense to ask, can we understand what is going on inside these networks? Do we even have a fighting chance? Luckily, today, visualizing the inner workings of neural networks is a research subfield of its own, and the answer is, yes, we learn more and more every year. But there is also plenty more to learn. Earlier, we talked about a technique that we called activation maximization, which was about trying to find an input that makes a given neuron as excited as possible. This gives us some cues as to what the neural network is looking for in an image. A later work that proposes visualizing spatial activations gives us more information about these interactions between two, or even more neurons. You see here with the dots that it provides us a dense sampling of the most likely activations, and, this leads to a more complete bigger-picture view of the inner workings of the neural network. This is what it looks like if we run it on one image. It also provides us with way more extra value, because so far, we have only seen how the neural network reacts to one image, but this method can be extended to see its reaction to not one, but one million images! You can see an example of that here. Later, it was also revealed that some of these image detector networks can assemble something that we call a pose invariant dog head detector! What this means is that it can detect a dog head in many different orientations, and\u2026look! You see that it gets very excited by all of these good boys\u2026plus, this squirrel. Today\u2019s technique offers us an excellent tool to look into the inner workings of a convolutional neural network, a learning method that is very capable of image-related operations, for instance, image classification. The task here is that we have an input image of a mug or a red panda, and the output should be a decision from the network that yes, what we are seeing is indeed a mug or a panda or not. They apply something that we call a convolutional filter over an image which tries to find interesting patterns that differentiate objects from each other. You can see how the outputs are related to the input image here. As you see, the neurons in the next layer will be assembled as a combination of the neurons from the previous layer. When we use the term deep learning, we typically refer to neural networks that have two or more of these inner layers. Each subsequent layer is built by taking all the neurons in the previous layer, which select for the features relevant to what the next neuron represents, for instance, the handle of the mug and inhibits everything else. To make this a little clearer, this previous work tried to detect whether we have a car in an image by using these neurons. Here, the upper part looks like a car window, the next one resembles a car body, and the bottom of the third neuron clearly contains a wheel detector. This is the information that the neurons in the next layer are looking for. In the end, we make the final decision as to whether this is a panda or a mug by adding up all the intermediate results, the bluer this part is, the more relevant this neuron is in the final decision. Here, the neural network concludes that this doesn\u2019t look like a lifeboat or a ladybug at all, but it looks like pizza. If we look at the other sums, we see that the school bus and orange are not hopeless candidates, but still, the neural network does not have much doubt whether this is a pizza or not. And, the best part is that you can even try it yourself in your browser if you click the link in the video description, run these simulations, and even upload your own image. Make sure that you upload or link something that belongs to one of these classes on the right to make this visualization work. So, clearly, there is plenty more work to do for us to properly understand what is going on under the hood of neural networks, but I hope this quick rundown showcased how many facets there are to this neural network visualization subfield and how exciting it is. Make sure to post your experience in the comments section whether the classification worked well for you or not. And if you wish to see more videos like this, make sure to subscribe and hit the bell icon to not miss future videos. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=N6wn8zMRlVE",
        "paper_link": "https://github.com/poloclub/cnn-explainer",
        "paper_title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization"
    },
    {
        "video_id": "QSVrKK_uHoU",
        "video_title": "Amazing AR Effects Are Coming!",
        "position_in_playlist": 445,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir mentioned post is available here:\nhttps://app.wandb.ai/latentspace/published-work/The-Science-of-Debugging-with-W%26B-Reports--Vmlldzo4OTI3Ng\n\n\ud83d\udcdd The paper \"Consistent Video Depth Estimation\" is available here:\nhttps://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nader S., Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When we, humans look at an image, or a piece of video footage, we understand the geometry of the objects in there so well, that if we have the time and patience, we could draw a depth map that describes the distance of each object from the camera. This goes without saying. However, what does not go without saying, is that if we could teach computers to do the same, we could do incredible things. For instance, this learning-based technique creates real-time defocus effects for virtual reality and computer games, and this one performs this Ken Burns effect in 3D, or in other words, zoom and pan around in a photograph, but, with a beautiful twist, because in the meantime, it also reveals the depth of the image. With this data, we can even try to teach self-driving cars about depth perception to enhance their ability to navigate around safely. However, if you look here, you see two key problems: one, it is a little blurry, and there are lots of fine details that it couldn\u2019t resolve, and, it is flickering. In other words, there are abrupt changes from one image to the next one, which shouldn\u2019t be there as the objects in the video feed are moving smoothly. Smooth motion should mean smooth depth maps, and it is getting there, but it still is not the case here. So, I wonder, if we could teach a machine to perform this task better? And more importantly, what new wondrous things can we do if we pull this off? This new technique is called Consistent Video Depth Estimation, and it promises smooth and detailed depth maps that are of much higher quality than what previous works offer. And, now, hold on to your papers, because finally, these maps contain enough detail to open up the possibility of adding new objects to the scene, or even flood the room with water, or add many other, really cool video effects. All of these will take the geometry of the existing real-world objects, for instance, cats into consideration. Very cool! The reason why we need such a consistent technique to pull this off, is because if we have this flickering in time that we\u2019ve seen here, then, the depth of different objects suddenly bounces around over time, even for a stationary object. This means, that in one frame, the ball would be in front of the person, when in the next one, it would suddenly think that it has to put the ball behind them, and then, in the next one, front again, creating a not only jarring, but quite unconvincing animation. What is really remarkable is that due to the consistency of the technique, none of that happens here. Love it! Here are some more results where you can see that the outlines of the objects in the depth map are really crisp, and follow the changes really well over time. The snowing example here is one of my favorites, and it is really convincing. However, there are still a few spots where we can find some visual artifacts. For instance, as the subject is waving, there is lots of fine, high-frequency data around the fingers there, and if you look at the region behind the head closely, you find some more issues, or you can find that some balls are flickering on the table as we move the camera around. Compare that to previous methods that could not do nearly as good as this, and now, we have something that is quite satisfactory. I can only imagine how good this will get to more papers down the line. And for the meantime, we\u2019ll be able to run these amazing effects even without having a real depth camera. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=QSVrKK_uHoU",
        "paper_link": "https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/",
        "paper_title": "Consistent Video Depth Estimation"
    },
    {
        "video_id": "oHLR287rDRA",
        "video_title": "This is Geometry Processing Made Easy",
        "position_in_playlist": 446,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83d\udcdd The paper \"Monte Carlo Geometry Processing: A Grid-Free Approach to PDE-Based Methods on Volumetric Domains\" is available here:\nhttps://www.cs.cmu.edu/~kmcrane/Projects/MonteCarloGeometryProcessing/index.html \n\nImplementations:\n- https://twitter.com/iquilezles/status/1258218688726962183 \n- https://twitter.com/iquilezles/status/1258237114958802944\n- https://www.shadertoy.com/view/wdffWj\n\nOur mega video on Multiple Importance Sampling: https://www.youtube.com/watch?v=TbWQ4lMnLNw\n\nKoiava\u2019s MIS implementation: https://www.shadertoy.com/view/4sSXWt\n\nMy course at the Vienna University of Technology on light transport is available here. It is completely free for everyone:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I think this might be it. This paper is called Monte Carlo Geometry Processing, and in my opinion, this may one of the best, if not the best computer graphics paper of the year. There will be so many amazing results, I first thought this one paper could be the topic of ten Two Minute Papers videos, but I will attempt to cram everything into this one video. It is quite challenging to explain, so please bear with me, I\u2019ll try my best. To even have a fighting chance at understanding what is going on here, first, we start out one of the most beautiful topics in computer graphics, which is none other than light transport. To create a beautiful light simulation, we have to solve something that we call the rendering equation. For practical cases, it is currently impossible to solve it like we usually solve any other equation. However, what we can do, is that we can choose a random ray, simulate its path as it bounces around in the scene and compute how it interacts with the objects and material within this scene. As we do it with more and more rays, we get more information about what we see in the scene, that\u2019s good, but, look, it is noisy. As we add even more rays, this noise slowly evens out, and in the end, we get a perfectly clean image. This takes place with the help of a technique that we call Monte Carlo integration, which involves randomness. At the risk of oversimplifying the situation, in essence, this technique says that we cannot solve the problem, but we can take samples from the problem, and if we do it in a smart way, eventually, we will be able to solve it. In light transport, the problem is the rendering equation, which we cannot solve, but we can take samples, one sample is simulating one ray. If we have enough rays, we have a solution. However, it hasn\u2019t always been like this. Before Monte Carlo integration, light transport was done through a technique called Radiosity. The key issue with Radiosity was that the geometry of the scene had to be sliced up into many small pieces, and the light scattering events had to be evaluated between these small pieces. It could not handle all light transport phenomena and the geometry processing part was a major headache, and Monte Carlo integration was a revelation that breathed new life into this field. However, there are still many geometry processing problems that include these headaches, and, hold on to your papers, because this paper shows us that we can apply Monte Carlo integration to many of these problems too. For instance, one, it can resolve the rich external and internal structure of this ant. With traditional techniques, this would normally take more than 14 hours and 30 gigabytes of memory, but, if we apply Monte Carlo integration to this problem, we can get a somewhat noisy preview of the result in less than one minute. Of course, over time, as we compute more samples, the noise clears up and we get this beautiful final result. And the concept can be used for so much more, it truly makes my head spin. Let\u2019s discuss six amazing applications, while noting that there are so many more in the paper, which you can and should check out in the video description. For instance, two, it can also compute a CT scan of the infamous shovelnose frog that you see here, and instead of creating the full 3D solution, we only have to compute a 2D slice of it, which is much, much cheaper. Three, it can also edit these curves, and note that the key part is that we can do that without the major headache of creating an intermediate triangle mesh geometry for it. Four, it also supports denoising techniques, so we don\u2019t have to compute too many samples to get a clear image or piece of geometry. Five, performing a Helmholtz-Hodge decomposition with this method is also possible. This is a technique that is used widely in many domains, for instance, it is responsible to ensure the stability of many fluid simulation programs, and this technique can also produce these decompositions. And interestingly, here it is used to represent 3D objects without the usual triangle meshes that we use in computer graphics. Six, it supports multiple importance sampling as well. This means that if we have multiple sampling strategies, multiple ways to solve a problem that have different advantages and disadvantages, it combines them in a way that we get the best of all of them. We had a mega-episode on multiple importance sampling, it has lots of amazing uses in light transport simulations, so if you would like to hear more, make sure to check that video out in the video description. But wait, these are all difficult problems. One surely needs a PhD and years of experience in computer graphics to implement this, right? When seeing a work like this, we often ask, okay, it does something great, but how complex is it? How many days do I have to work to reimplement it? Please take a guess and let me know what the guess was in the comments section. And now, what you see here is none other than the source code for the core of the method, and what\u2019s even more, a bunch of implementations of it already exist. And, if you see that a paper has been reimplemented around day 1, you know it\u2019s good. So, no wonder, this paper has been accepted to SIGGRAPH, perhaps the most prestigious computer graphics conference. It is immensely difficult to get a paper accepted there, and I would say this one more than earned it. Huge congratulations to the first author of the paper, Rohan Sawhney, he is currently a PhD student, and note that this was his second published paper. Unreal. Such a great leap for the field in just one paper. Also congratulations to professor Keenan Crane who advised this project and many other wonderful works in the last few years. I cannot wait to see what they will be up to next and I hope that now, you are just as excited. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=oHLR287rDRA",
        "paper_link": "https://www.cs.cmu.edu/~kmcrane/Projects/MonteCarloGeometryProcessing/index.html",
        "paper_title": "Monte Carlo Geometry Processing: A Grid-Free Approach to PDE-Based Methods on Volumetric Domains"
    },
    {
        "video_id": "wg3upHE8qJw",
        "video_title": "Can an AI Learn Lip Reading?",
        "position_in_playlist": 447,
        "description": "\u2764\ufe0f Check out Snap's Residency Program and apply here: https://lensstudio.snapchat.com/snap-ar-creator-residency-program/?utm_source=twominutepapers&utm_medium=video&utm_campaign=tmp_ml_residency\n\u2764\ufe0f Try Snap's Lens Studio here: https://lensstudio.snapchat.com/\n\n\ud83d\udcdd The paper \"Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\" is available here:\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/speaking-by-observing-lip-movements\n\nOur earlier video on the \"bag of chips\" sound reconstruction is available here:\nhttps://www.youtube.com/watch?v=2i1hrywDwPo\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-4814562/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Lipreading",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When watching science fiction movies, we often encounter crazy devices and technologies that don\u2019t really exist, or sometimes, ones that are not even possible to make. For instance, reconstructing sound from vibrations would be an excellent example of that, and could make a great novel with the secret service trying to catch dangerous criminals. Except that it has already been done in real life research. I think you can imagine how surprised I was when I saw this paper in 2014 that showcased a result where a camera looks at this bag of chips, and from these tiny-tiny vibrations, it could reconstruct the sounds in the room. Let\u2019s listen. Yes, this indeed sounds like science fiction. But 2014 was a long-long time ago, and since then, we have a selection of powerful learning algorithms, and the question is, what\u2019s the next idea that sounded completely impossible a few years ago, which is now possible? Well, what about looking at silent footage from a speaker and trying to guess what they were saying? Checkmark, that sounds absolutely impossible to me, yet, this new technique is able to produce the entirety of this speech after looking at the video footage of the lip movements. Let\u2019s listen. Wow. So the first question is, of course, what was used as the training data? It used a dataset with lecture videos and chess commentary from 5 speakers, and make no mistake, it takes a ton of data from these speakers, about 20 hours from each, but it uses video that was shot in a natural setting, which is something that we have in abundance on Youtube and other places on the internet. Note that the neural network works on the same speakers it was trained on and was able to learn their gestures and lip movements remarkably well. However, this is not the first work attempting to do this, so let\u2019s see how it compares to the competition. The new one is very close to the true spoken sentence. Let\u2019s look at another one. Note that there are gestures, a reasonable amount of head movement and other factors at play and the algorithm does amazingly well. Potential applications of this could be video conferencing in zones where we have to be silent, giving a voice to people with the inability to speak due to aphonia or other conditions, or, potentially fixing a piece of video footage where parts of the speech signal are corrupted. In these cases, the gaps could be filled in with such a technique. Look! Now, let\u2019s have a look under the hood. If we visualize the activations within this neural network, we see that it found out that it mainly looks at the mouth of the speaker. That is, of course, not surprising. However, what is surprising is that the other regions, for instance, around the forehead and eyebrows are also important to the attention mechanism. Perhaps this could mean that it also looks at the gestures of the speaker, and uses that information for the speech synthesis. I find this aspect of the work very intriguing and would love to see some additional analysis on that. There is so much more in the paper, for instance, I mentioned giving a voice to people with aphonia, which should not be possible because we are training these neural networks for a specific speaker, but with an additional speaker embedding step, it is possible to pair up any speaker with any voice. This is another amazing work that makes me feel like we are living in a science fiction world. I can only imagine what we will be able to do with this technique two more papers down the line. If you have any ideas, feel free to speculate in the comments section below. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=wg3upHE8qJw",
        "paper_link": "https://www.youtube.com/watch?v=2i1hrywDwPo",
        "paper_title": "Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis"
    },
    {
        "video_id": "MrIbQ0pIFOg",
        "video_title": "This AI Creates Beautiful 3D Photographs!",
        "position_in_playlist": 448,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation of this paper is available here:\nhttps://app.wandb.ai/authors/3D-Inpainting/reports/3D-Image-Inpainting--VmlldzoxNzIwNTY\n\n\ud83d\udcdd The paper \"3D Photography using Context-aware Layered Depth Inpainting\" is available here:\nhttps://shihmengli.github.io/3D-Photo-Inpainting/\n\nTry it out! Weights & Biases notebook: https://colab.research.google.com/drive/1yNkew-QUtVQPG8PbwWWMLKmnVlLOIfTs?usp=sharing\nOr try it out here - Author notebook: https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz#scrollTo=wPvkMT0msIJB\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. We hear more and more about RGBD images these days. These are photographs that are endowed with depth information, which enable us to do many wondrous things. For instance, this method was used to endow self-driving cars with depth information and worked reasonably well, and this other one provides depth maps that are so consistent, we can even add some AR effects to it, and today\u2019s paper is going to show what 3D photography is. However, first, we need not only color, but depth information in our images to perform these. You see, phones with depth scanners already exist, and even more are coming as soon as this year, but even if you have a device that only gives you 2D color images, don\u2019t despair. There is plenty of research on how we can estimate these depth maps, even if we have very limited information. And, with proper depth information, we can now create these 3D photographs, where we get even more information out of one still image. We can look behind objects and see things that we wouldn\u2019t see otherwise. Beautiful parallax effects appear as objects at different distances move different amounts as we move the camera around. You see that the foreground changes a great deal, the buildings in the background, less so, and the hills behind them, even less so. These photos truly come alive with this new method. An earlier algorithm, the legendary PatchMatch method from more than a decade ago could perform something that we call image inpainting. Image inpainting means looking at what we see in these images, and trying to fill in missing information with data that makes sense. The key difference here is that this new technique uses a learning method, and does this image inpainting in 3D, and it not only fills in color, but depth information as well. What a crazy, amazing idea. However, this is not the first method to perform this, so how does it compare to other research works? Let\u2019s have a look together. Previous methods have a great deal of warping and distortions on the bathtub here, and if you look at the new method, you see that it is much cleaner. There is still a tiny bit of warping, but, it is significantly better. The dog head here with this previous method seems to be bobbing around a great deal, while the other methods also have some problems with it\u2026look at this too. And if you look at how the new method handles it\u2026it is significantly more stable. And you see that these previous techniques are from just one or two years ago. It is unbelievable how far we have come since. Bravo. So this was a qualitative comparison, or in other words, we looked at the results, what about the quantitative differences? What do the numbers say? Look at the PSNR column here, this means the Peak Signal to Noise Ratio, this is subject to maximization, as the up arrow denotes here. The higher, the better. The difference is between one half to 2.5 points when compared to previous methods, which does not sound like a lot at all. So, what happened here? Note that PSNR is not a linear, but a logarithmic scale, so this means that a small numeric difference typically translates to a great deal of difference in the images, even if the numeric difference is just 0.5 points on the PSNR scale. However, if we look at SSIM, the structural similarity metric, all of them are quite similar, and a previous technique appears to be winning here. But this was the method that warped the dog head, and in the visual comparisons, the new method came out significantly better than this. So what is going on here? Well, have a look at this metric, LPIPS, which was developed at UC Berkeley, OpenAI and Adobe research. At the risk of simplifying the situation, this uses a neural network to look at an image, and uses its inner representation to decide how close the two images are to each other. Loosely speaking, it kind of thinks about differences as we, humans do, and is an excellent tool to compare images. And, sure enough, this also concludes that the new method performs best. However, this method is still not perfect. There is some flickering going on behind these fences, the transparency of the glass here isn\u2019t perfect, but witnessing this huge a leap in the quality of results in such little time is truly a sight to behold. What a time to be alive! I started this series to make people feel how I feel when I read these papers, and I really hope that it goes through with this paper. Absolutely amazing. What is even more amazing is that with a tiny bit of technical knowledge, you can run the source code in your browser, so make sure to have a look at the link in the video description. Let me know in the comments how it went! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MrIbQ0pIFOg",
        "paper_link": "https://shihmengli.github.io/3D-Photo-Inpainting/",
        "paper_title": "3D Photography using Context-aware Layered Depth Inpainting"
    },
    {
        "video_id": "TrdmCkmK3y4",
        "video_title": "This AI Creates Dogs From Cats\u2026And More!",
        "position_in_playlist": 449,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\nTheir instrumentation of this paper is available here:\nhttps://app.wandb.ai/stacey/stargan/reports/Cute-Animals-and-Post-Modern-Style-Transfer%3A-Stargan-V2-for-Multi-Domain-Image-Synthesis---VmlldzoxNzcwODQ\n\n\ud83d\udcdd The paper \"StarGAN v2: Diverse Image Synthesis for Multiple Domains\" is available here:\n- Paper:\u00a0https://arxiv.org/abs/1912.01865\n- Code:\u00a0https://github.com/clovaai/stargan-v2\n- Youtube Video:\u00a0https://youtu.be/0EVh5Ki4dIY\n\nThe paper with the latent space material synthesis is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, we have a selection of learning-based techniques that can generate images of photorealistic human faces for people that don\u2019t exist. These techniques have come a long way over the last few years, so much so that we can now even edit these images to our liking, by, for instance, putting a smile on their faces, making them older or younger, adding or removing a beard, and more. However, most of these techniques are still lacking in two things. One is diversity of outputs, and two, generalization to multiple domains. Typically, the ones that work on multiple domains don\u2019t perform too well on most of them. This new technique is called StarGAN 2 and addresses both of these issues. Let\u2019s start with the humans. In the footage here, you see a lot of interpolation between test subjects, which means that we start out from a source person, and generate images that morph them into the target subjects, not in any way, but in a way that all of the intermediate images are believable. In these results, many attributes from the input subject, such as pose, nose type, mouth shape and position are also reflected on the output. I like how the motion of the images on the left reflects the state of the interpolation. As this slowly takes place, we can witness how the reference person grows out a beard. But we're not nearly done yet. We noted that another great advantage of this technique is that it works for multiple domains, and this means, of course, none other than us looking at cats morphing into dogs and other animals. In these cases, I see that the algorithm picks up the gaze direction, so this generalizes to even animals. That's great. What is even more great is that the face shape of the tiger appears to have been translated to the photo of this cat, and, if we have a bigger cat as an input, the output will also give us\u2026 this lovely, and a little plump creature. And...look! Here, the cat in the input is occluded in this target image, but that is not translated to the output image. The AI knows that this is not part of the cat, but an occlusion. Imagine what it would take to prepare a handcrafted algorithm to distinguish these features. My goodness. And now, onto dogs. What is really cool is that in this case, bendy ears have their own meaning and we get several versions of the same dog breed, with, or without them. And it can handle a variety of other animals too. I could look at these all day. And now, to understand why this works so well, we first have to understand what a latent space is. Here you see an example of a latent space that was created to be able to browse through fonts, and even generate new ones. This method essentially tries to look at a bunch of already existing fonts and tries to boil them down into the essence of what makes them different. It is a simpler, often incomplete, but, more manageable representation for a given domain. This domain can be almost anything, for instance, you see another technique that does something similar with material models. Now, the key difference in this new work compared to previous techniques, is that it creates not one latent space, but several of these latent spaces for different domains. As a result, it can not only generate images in all of these domains, but can also translate different features, for instance, ears, eyes, noses from a cat to a dog or a cheetah in a way that makes sense. And the results look like absolute witchcraft. Now, since the look on this cheetah\u2019s face indicates that it has had enough of this video, just one more example before we go. As a possible failure case, have a look at the ears of this cat. It seems to be in a peculiar midway-land between a pointy and a bent ear, but it doesn\u2019t quite look like any of them. What do you think? Maybe some of you cat people can weigh in on this. Let me know in  the comments. Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=TrdmCkmK3y4",
        "paper_link": "https://arxiv.org/abs/1912.01865",
        "paper_title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains"
    },
    {
        "video_id": "ICr6xi9wA94",
        "video_title": "An AI Learned To See Through Obstructions! \ud83d\udc40",
        "position_in_playlist": 450,
        "description": "\u2764\ufe0f Check out Snap's Residency Program and apply here: https://lensstudio.snapchat.com/snap-ar-creator-residency-program/?utm_source=twominutepapers&utm_medium=video&utm_campaign=tmp_ml_residency\n\u2764\ufe0f Try Snap's Lens Studio here: https://lensstudio.snapchat.com/\n\n\ud83d\udcdd The paper \"Learning to See Through Obstructions\" is available here:\nhttps://alex04072000.github.io/ObstructionRemoval/\nhttps://github.com/alex04072000/ObstructionRemoval\n\n\ud83d\udcdd Try it out here: https://colab.research.google.com/drive/1iOKknc0dePekUH2TEh28EhcRPCS1mgwz\n\n\u00a0\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-451982/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Approximately two years ago, we covered a work where a learning-based algorithm was able to read the wifi signals in a room to not only locate a person in a building, but even estimate their pose. An additional property of this method was that, as you see here, it does not look at images, but radio signals, which also traverse in the dark, and therefore, this pose estimation also works well in poor lighting conditions. Today\u2019s paper offers a learning-based method for a different, more practical data completion problem, and it mainly works on image sequences. You see, we can give this one a short image sequence with obstructions, for instance, the fence here. And it is able to find and remove this obstruction, and not only that, but it can also show us what is exactly behind the fence! How is that even possible? Well, note that we mentioned that the input is not an image, but an image sequence, a short video if you will. This contains the scene from different viewpoints, and is one of the typical cases where if we would give all this data to a human, this human would take a long-long time, but would be able to reconstruct what is exactly behind the fence, because this data was visible from other viewpoints. But of course, clearly, this approach would be prohibitively slow and expensive. The cool thing here is that this learning-based method is capable of doing this, automatically! But it does not stop there. I was really surprised to find out that it even works for video outputs as well, so if you did not have a clear sight of that tiger in the zoo, do not despair! Just use this method, and there you go. When looking at the results of techniques like this, I always try to only look at the output, and try to guess where the fence was obstructing it. With many simpler, image inpainting techniques, this is easy to tell if you look for it, but here, I can\u2019t see a trace. Can you? Let me know in the comments. Admittedly, the resolution of this video is not very high, but the results look very reassuring. It can also perform reflection removal, and some of the input images are highly contaminated by these reflected objects. Let\u2019s have a look at some results! You can see here how the technique decomposes the input into two images, one with the reflection, and one without. The results are clearly not perfect, but they are easily good enough to make my brain focus on the real background without being distracted by the reflections. This was not the case with the input at all. Bravo! This use case can also be extended for videos, and I wonder how much temporal coherence I can expect in the output. In other words, if the technique solves the adjacent frames too differently, flickering is introduced to the video, and this effect is the bane of many techniques that are otherwise really good on still images. Let\u2019s have a look! There is a tiny bit of flickering, but the results are surprisingly consistent. It also does quite well when compared to previous methods, especially when we are able to provide multiple images as an input. Now note that it says \u201cours, without online optimization\u201d. What could that mean? This online optimization step is a computationally expensive way to further improve separation in the outputs, and with that, the authors propose a quicker and a slower version of the technique. The one without the on-line optimization step runs in just a few seconds, and if we add this step, we will have to wait approximately 15 minutes. I had to read the table several times, because researchers typically bring the best version of their technique to the comparisons, and it is not the case here. Even the quicker version smokes the competition! Loving it. Note if you have a look at the paper in the video description, there are, of course, more detailed comparisons against other methods as well. If these AR glasses that we hear so much about come to fruition in the next few years, having an algorithm for real-time glare, reflection and obstruction removal would be beyond amazing. We truly live in a science fiction world. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ICr6xi9wA94",
        "paper_link": "https://alex04072000.github.io/ObstructionRemoval/",
        "paper_title": "Learning to See Through Obstructions"
    },
    {
        "video_id": "pBkFAIUmWu0",
        "video_title": "These AI-Driven Characters Dribble Like Mad! \ud83c\udfc0",
        "position_in_playlist": 451,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/cayush/pytorchlightning/reports/How-to-use-Pytorch-Lightning-with-Weights-%26-Biases--Vmlldzo2NjQ1Mw\n\n\ud83d\udcdd The paper \"Local Motion Phases for Learning Multi-Contact Character Movements\" is available here:\nhttps://github.com/sebastianstarke/AI4Animation\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In computer games and all kinds of applications where we are yearning for realistic animation, we somehow need to tell our computers how the different joints and body parts of these virtual characters are meant to move around over time. Since the human eye is very sensitive to even the tiniest inaccuracies, we typically don\u2019t program these motions by hand, but instead, we often record a lot of real-life motion capture data in a studio, and try to reuse that in our applications. Previous techniques have tackled quadruped control, and we can even teach bipeds to interact with their environment in a realistic manner. Today we will have a look at an absolutely magnificent piece of work, where the authors carved out a smaller subproblem, and made a solution for it that is truly second to none. And this subproblem is simulating virtual characters playing basketball. Like with previous works, we are looking for realism in the movement, and for games, it is also a requirement that the character responds to our controls well. However, the key challenge is that all we are given, is 3 hours of unstructured motion capture data. That is next to nothing, and from this next to nothing, a learning algorithm has to learn to understand these motions so well, that it can weave them together, even when a specific movement combination is not present in this data. That is quite a challenge. Compared to many other works, this data is really not a lot, so I am excited to see what value we are getting out of these three hours. At first I thought we\u2019d get only very rudimentary motions, and boy, was I dead wrong on that one. We have control over this character and can perform these elaborate maneuvers, and it remains very responsive even if we mash the controller like a madman, producing these sharp turns. As you see, it can handle these cases really well. And not only that, but it is so well done, we can even dribble through a set of obstacles, leading to a responsive, and enjoyable gameplay. About these dribbling behaviors. Do we get only one, boring motion, or not? Not at all, it was able to mine out not just one, but many kinds of dribbling motions, and is able to weave them into other moves as soon as we interact with the controller. This is already very convincing, especially from just three hours of unstructured motion capture data. But this paper is just getting started. Now, hold on to your papers, because we can also shoot and catch the ball, move it around, that is very surprising, because it has looked at so little shooting data, let\u2019s see\u2026yes, less than 7 minutes. My goodness. And it keeps going, what I have found even more surprising is that it can handle unexpected movements, which I find to be even more remarkable given the limited training data. These crazy corner cases are typically learnable when they are available in abundance in the training data, which is not the case here. Amazing. When we compare these motions to a previous method, we see that both the character and the ball\u2019s movement is much more lively. For instance, here, you can see that the Phase-Functioned Neural Network, PFNN in short, almost makes it seem like the ball has to stick to the hand of the player for an unhealthy amount of time to be able to create these motions. It doesn\u2019t happen at all with the new technique. And remember, this new method is also much more responsive to the player\u2019s controls, and thus, more enjoyable not only to look at, but to play with. This is an aspect that hard is hard to measure, but it is not to be underestimated in the general playing experience. Just imagine what this research area will be capable of not in a decade, but just two more papers down the line. Loving it. Now, at the start of the video, I noted that the authors carved out a small use-case, which is training an AI to weave together basketball motion capture data in a manner that is both realistic and controllable. However, many times in research, we look at a specialized problem, and during that journey we learn general concepts that can be applied to other problems as well. That is exactly what happened here, as you see, parts of this technique can be generalized for quadruped control as well. This good boy is pacing and running around beautifully. And\u2026you guessed right, our favorite biped from the previous paper is also making an introduction. I am absolutely spellbound by this work, and I hope that now, you are too. Can\u2019t wait to see this implemented in newer games and other real-time applications. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=pBkFAIUmWu0",
        "paper_link": "https://github.com/sebastianstarke/AI4Animation",
        "paper_title": "Local Motion Phases for Learning Multi-Contact Character Movements"
    },
    {
        "video_id": "nkHL1GNU18M",
        "video_title": "Physics in 4 Dimensions\u2026How?",
        "position_in_playlist": 452,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://www.wandb.com/articles/how-to-visualize-models-in-tensorboard-with-weights-and-biases\n\n\ud83d\udcdd The paper \"N-Dimensional Rigid Body Dynamics\" is available here:\nhttps://marctenbosch.com/ndphysics/\n\nCheck out these two 4D games here:\n4D Toys: https://4dtoys.com/\nMiegakure (still in the works): https://miegakure.com/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nMore info if you would like to appear here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "the fellow scholars this is too many papers with doctor casually for hit I recommend that you immediately start to hold on to your papers because this work is about creating physics simulations in higher than three spatial dimensions the research in this paper is used to create this game that takes place in a forty world and it is just as beautiful as it is confusing and I'll be honest the more we look the last this world seems to make sense for us so let's try to understand what is happening here together you see an example of a four dimensional simulation here the footage seems very cool but I can't help but notice that sometimes things just seem to disappear into the ether in other cases they suddenly appear from nowhere  how can that happen less decree that I mentioned earlier the of the problem and suddenly we will easily understand how this is possible here we take that to the slice of our three D. world start the simulation and imagine that we only see what happens on the slice we see nothing else just the slice that sounds fine things move around freely nothing crazy going on and suddenly take things slowly disappear the key in this example is that luckily we can also look at the corresponding three D. simulation on your display we can see not only the to the slice but everything around it so the playbook is as follows if something disappears it means that it went back or forward in the third dimension so what we see on the slide is not the object itself but it intersection with the three D. object the smaller the intersection the more we see the side of the sphere and the object appears to be smaller or even better  look the ball is even floating in the air when we look at the three D. world we see that this only appears like that but in reality what happens is that we see the side of the sphere really cool we now understand that colliding can create a Mirage it seems as if objects were suddenly receding disappearing hand even floating just imagine how confusing this would be if we were this to the character actually you don't even need to imagine that because this piece of research work was made to be able to create computer games where we are three D. character in a forty world so we can experience all of these confusing phenomena however we get some help in decoding the situation because even though this example runs in three D. when something disappears into the ether we can move along the fourth dimension and find it so good  floaty behavior can also happen and now we know exactly why but wait in this three D. domain even more confusing new things happen as you see here in three D. cubes seem to be changing shape and the reason for this is the same we just see a higher dimensional object intersection with our lower dimensional space but the paper does not only extend three D. rigid body dynamics simulations to not only to the bot to any higher dimension you see this kind of footage can only be simulated because it also proposes a way to compute collisions static and kinetic friction and similar physical forces in arbitrate I mentions and now hold on to your papers because this framework can also be used to make amazing mind bending game puzzles where we can miraculously unbind seemingly impossible configurations by reaching into the fourth dimension this is one of those crazy research works the truly cannot be bothered by recent trends and hot topics in a minute in the best possible way it creates its own little world and invites us to experience it and it truly is a sight to behold I can only imagine how one of the soft body or fluid simulations would look in higher dimensions and boy would I love to see some follow up papers perform something like this if you wish to see more make sure to have a look at the paper in the video description and I also put a link to one forty game that is under development and one that you can buy and play today huge congratulations to mark ten bosh who got the single author paper accepted to see graph perhaps the most prestigious computer graphics conference not only that but I am pretty sure that I have never seen a work from an indie game developer presented as a C. graph technical paper Bravo this episode has been supported by weights and biases in this post they show you how to use their system with tens of board and the  July's your results with it you can even try an example in an interactive notebook through the link in the video description weights and biases provides tools to track your experiments in your deep learning projects their system is designed to save you a ton of time and money and it is actively used in projects at prestigious labs such as OpenAI do your research get hub and more and the best part is that if you have an open source academic or personal project you can use that tools for free it really is as good as it gets make sure to visit them through W. and B. E. dot com slash papers or click the link in the video description to start tracking your experiments in five minutes our thanks to weights and biases for the longstanding support and for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=nkHL1GNU18M",
        "paper_link": "https://marctenbosch.com/ndphysics/",
        "paper_title": "N-Dimensional Rigid Body Dynamics"
    },
    {
        "video_id": "_x9AwxfjxvE",
        "video_title": "OpenAI GPT-3 - Good At Almost Everything! \ud83e\udd16",
        "position_in_playlist": 453,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their instrumentation of a previous OpenAI paper is available here: https://app.wandb.ai/authors/openai-jukebox/reports/Experiments-with-OpenAI-Jukebox--VmlldzoxMzQwODg\n\n\ud83d\udcdd The paper \"Language Models are Few-Shot Learners\" is available here:\n- https://arxiv.org/abs/2005.14165\n- https://openai.com/blog/openai-api/\n\nCredits follow for the tweets of the applications. Follow their authors if you wish to see more!\nWebsite layout: https://twitter.com/sharifshameem/status/1283322990625607681\nPlots: https://twitter.com/aquariusacquah/status/1285415144017797126?s=12\nTypesetting math: https://twitter.com/sh_reya/status/1284746918959239168\nPopulation data: https://twitter.com/pavtalk/status/1285410751092416513\nLegalese: https://twitter.com/f_j_j_/status/1283848393832333313\nNutrition labels: https://twitter.com/lawderpaul/status/1284972517749338112\nUser interface design: https://twitter.com/jsngr/status/1284511080715362304\n\nMore cool applications:\nGenerating machine learning models - https://twitter.com/mattshumer_/status/1287125015528341506?s=12\nCreating animations - https://twitter.com/ak92501/status/1284553300940066818\nCommand line magic - https://twitter.com/super3/status/1284567835386294273?s=12\nAnalogies: https://twitter.com/melmitchell1/status/1291170016130412544?s=12\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#GPT3 #GPT2",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In early 2019, a learning-based technique appeared that could perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. This method was developed by scientists at OpenAI, and they called it GPT-2. The goal was to be able to perform this task with as little supervision as possible. This means that they unleashed this algorithm to read the internet, and the question is, what would the AI learn during this process? That is a tricky question. And to be able to answer it, have a look at this paper from 2017, where an AI was given a bunch of Amazon product reviews and the goal was to teach it to be able to generate new ones, or continue a review when given one. Then, something unexpected happened. The finished neural network used surprisingly few neurons to be able to continue these reviews, and upon closer inspection, they noticed that the neural network has built up a knowledge of not only language, but also built a sentiment detector as well. This means that the AI recognized that in order to be able to continue a review, it not only needs to learn English, but also needs to be able to detect whether the review seems positive or not. If we know that we have to complete a review that seems positive from a small snippet, we have a much easier time doing it well. And now, back to GPT-2. As it was asked to predict the next character in sentences of not reviews, but of any kind, we asked what this neural network would learn? Well, now we know that of course, it learns whatever it needs to learn to perform the sentence completion properly. And to do this, it needs to learn English by itself, and that\u2019s exactly what it did! It also learned about a lot of topics to be able to discuss them well. What topics? Let\u2019s see. We gave it a try, and I was somewhat surprised when I saw that it was able to continue a Two Minute Papers script, even though it seems to have turned into a history lesson. What was even more surprising is that it could shoulder the Two Minute Papers test, or in other words, I asked it to talk about the nature of fluid simulations, and it was caught cheating red handed. But then, it continued in a way that was not only coherent, but had quite a bit of truth to it. Note that there was no explicit instruction for the AI apart from it being unleashed on the internet and reading it. And now, the next version appeared, by the name GPT-3. This version is now more than a 100 times bigger, so our first question is, how much better can an AI get if we increase the size of a neural network? Let\u2019s have a look together. These are the results on a challenging reading comprehension test as a function of the number of parameters. As you see, around 1.5 billion parameters, which is roughly equivalent to GPT-2, it learned a great deal, but its understanding is nowhere near the level of human comprehension. However, as we grow the network, something incredible happens. Non-trivial capabilities start to appear as we approach a hundred billion parameters. Look! It nearly matched the level of humans. My goodness! This was possible before, but only with neural networks that are specifically designed for a narrow task. In comparison, GPT-3 is much more general. Let\u2019s test that generality and have a look at 5 practical applications together! One, OpenAI made this AI accessible to a lucky few people, and it turns out, it has read a lot of things on the internet, which contains a lot of code, so it can generate website layouts from a written description. Two, it also learned how to generate properly formatted plots from a tiny prompt written in plain English. Not just one kind - many kinds! Perhaps to the joy of technical PhD students around the world, three, it can properly typeset mathematical equations from a plain English description as well. Four, it understands the kind of data we have in a spreadsheet, in this case, population, and fills the missing parts correctly. And five, it can also translate a complex legal text into plain language, or, the other way around, in other words, it can also generate legal text from our simple descriptions. And as you see here, it can do much, much more, I left a link to all of these materials in the video description. However, of course, this iteration of GPT also has its limitations. For instance, we haven\u2019t seen the extent to which these examples are cherrypicked, or in other words, for every good output that we marvel at, there might have been one, or a dozen tries that did not come out well. We don\u2019t exactly know. But the main point is that working with GPT-3 is a really peculiar process where we know that a vast body of knowledge lies within, but it only emerges if we can bring it out with properly written prompts. It almost feels like a new kind of programming that is open to everyone, even people without any programming or technical knowledge. If a computer is a bicycle for the mind, then GPT-3 is a fighter jet. Absolutely incredible. And to say that the paper is vast would be an understatement - we only scratched the surface of what it can do here, so make sure to have a look if you wish to know more about it. The link is available in the video description. I can only imagine what we will be able to do with GPT-4 and GPT-5 in the near future! What a  time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_x9AwxfjxvE",
        "paper_link": "https://arxiv.org/abs/2005.14165",
        "paper_title": "Language Models are Few-Shot Learners"
    },
    {
        "video_id": "fE9BqmJrrW0",
        "video_title": "Can We Simulate Tearing Meat? \ud83e\udd69",
        "position_in_playlist": 454,
        "description": "\u2764\ufe0f Check out Snap's Residency Program and apply here: https://lensstudio.snapchat.com/snap-ar-creator-residency-program/?utm_source=twominutepapers&utm_medium=video&utm_campaign=tmp_ml_residency\n\u2764\ufe0f Try Snap's Lens Studio here: https://lensstudio.snapchat.com/\n\n\ud83c\udfacOur Instagram page with the slow-motion videos is available here:\nhttps://www.instagram.com/twominutepapers/\n\n \ud83d\udcdd The paper \"AnisoMPM: Animating Anisotropic Damage Mechanics\" is available here: \nhttps://joshuahwolper.com/anisompm\n\n\u2757Erratum: At 4:17, I should have written \"Anisotropic damage (new method)\". Apologies!\n\n \ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Perhaps the best part of being a computer graphics researcher is creating virtual worlds on a daily basis and computing beautiful simulations in these worlds. And what you see here is not one, but two kinds of simulations. One is a physics simulation that computes how these objects move, and a light transport simulation that computes how these objects look. In this video, we will strictly talk about the physics simulation part of what you see on the screen. To simulate these beautiful phenomena, many recent methods build on top of a technique called the Material Point Method. This is a hybrid simulation technique that uses both particles and grids to create these beautiful animations, however, when used by itself, we can come up with a bunch of cases that it cannot simulate properly. One such example is cracking and tearing phenomena, which has been addressed in this great paper that we discussed earlier this year. With this method, we could smash oreos, candy crabs, pumpkins, and much, much more. It even supported tearing this piece of bread apart. This already looks quite convincing, and in this series, I always say, two more papers down the line, and it will be improved significantly. Today, we are going to have a Two Minute Papers moment of truth, because this is from a followup work by Joshuah Wolper, the first author of the previous bread paper, and you can immediately start holding on to your papers, because this work is one of the finest I have seen as of late. With this, we can enrich our simulations with anisotropic damage and elasticity. So what does that mean exactly? This means that it supports more extreme topological changes in these virtual objects. This leads to better material separation when the damage happens. For instance, if you look here on the right, this was done by a previous method. For the first sight, it looks good, there is some bouncy behavior here, but the separation line is a little too clean. Let\u2019s have a look at the new method! Woo-hoo! Now that\u2019s what I am talking about! Let\u2019s have another look. I hope you now see what I meant by the previous separation line being a little too clean. Remarkably, it also supports changing a few intuitive parameters, like eta, the crack propagation speed, which we can use to further tailor the simulation to our liking. Artists are going to love this. We can also play with the Young modulus, which describes the material\u2019s resistance against fractures. On the left, it is quite low, and makes the material tear apart easily, much like a sponge. As we increase it bit, we get a stiffer material, which gives us this glorious floppy behavior. Let\u2019s increase it even more, and see what happens! Yes, it is more resistant against damage, however, in return, it gives us some more vibrations after the break. It is not only realistic, but it also gives us the choice with these parameters to tailor our simulation results to our liking. Absolutely incredible. Now then, if you have been holding on to your papers so far, now squeeze that paper, because previous methods were only capable of tearing off a small piece, or only a strip of this virtual pork, let\u2019s see what this new work will do. Yes, it can also simulate peeling off an entire layer. Glorious! But that\u2019s not the only thing we can peel. It can also deal with small pieces of this mozzarella cheese. I must admit that I have never done this myself, so this will be the official piece of homework for me, and for the more curious minds out there after watching this video. Let me know in the comments if it went the same way in your kitchen as it did in the simulation here! You get extra credit if you post a picture too. And finally, if we tear this piece of meat apart, you see that it takes into consideration the location of the fibers, and the tearing takes place not in an arbitrary way, but much like in reality, it tears along the muscle fibers. So, how fast is it? We still have to wait a few seconds for each frame in these simulations. None of them took too long, there is a fish tearing experiment in the paper that went very quickly, half a second for each frame is a great deal, the pork experiment took nearly 40 seconds for each frame, and the most demanding experiments involved a lance and bones. Frankly, they were a little too horrific to be included here, even for virtual bodies, but if you wish to have a look, make sure to click the paper in the video description. But wait, are you seeing what I am seeing? Those examples took more than 1000 times longer to compute! Goodness! How can that be? Look! As you see here, in these cases, the delta-t-step is extremely tiny, which means that we have to advance the simulation with tiny-tiny time steps that takes much longer to compute. How tiny? Quite! In this case, we have to advance the simulation one millionth of a second at a time. The reason for this is that bones have an extremely high stiffness, which makes this method much less efficient. And of course, you know the drill, two more papers down the line, and this may run interactively on a consumer machine at home. So what\u2019s the verdict? Algorithm design. A+. Exposition, A+. Quality of presentation A double plus. And it\u2019s still Mr. Wolper\u2019s third paper in computer graphics. Unreal. And we, researchers even get paid to create beautiful works like this. I also couldn\u2019t resist creating a slow-motion version of some of these videos, so if this is something that you wish to see, make sure to visit our Instagram page in the video description for more. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fE9BqmJrrW0",
        "paper_link": "https://joshuahwolper.com/anisompm\n\n\u2757Erratum: At 4:17, I should have written \"Anisotropic damage (new method)\". Apologies!",
        "paper_title": "AnisoMPM: Animating Anisotropic Damage Mechanics"
    },
    {
        "video_id": "u4HpryLU-VI",
        "video_title": "From Video Games To Reality\u2026With Just One AI!",
        "position_in_playlist": 455,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers \n\n\ud83d\udcdd The paper \"World-Consistent Video-to-Video Synthesis\" is available here: \nhttps://nvlabs.github.io/wc-vid2vid/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Approximately three years ago, a magical learning-based algorithm appeared that was capable of translating a photorealistic image of a zebra into a horse or the other way around, could transform apples into oranges, and more. Later, it became possible to do this even without the presence of a photorealistic image, where all we needed was a segmentation map. This segmentation map provides labels on what should go where, for instance, this should be the road, here will be trees, traffic signs, other vehicles, and so on. And the output was a hopefully photorealistic video, and you can see here that the results were absolutely jaw-dropping. However, \u2026 look! As time goes by, the backside of the car morphs and warps over time, creating unrealistic results that are inconsistent, even on the short term. In other words, things change around from second to second, and the AI does not appear to remember what it did just a moment ago. This kind of consistency was solved surprisingly well in a followup paper from NVIDIA, in which an AI would look at the footage of a video game, for instance, Pacman, and after it has looked for approximately 120 hours, we could shut down the video game, and the AI would understand the rules so well that it could recreate the game that we could even play with. It had memory and used it well, and therefore, it could enforce a notion of world consistency, or in other words, if we return to a state of the game that we visited before, it will remember to present us with very similar information. So, the question naturally arises, would it be possible to create a photorealistic video from these segmentation maps that is also consistent? And in today\u2019s paper, researchers at NVIDIA proposed a new technique, that requests some additional information, for instance, a depth map that provides a little more information on how far different parts of the image are from the camera. Much like the Pacman paper, this also has memory, and I wonder if it is able to use it as well as that one did. Let\u2019s test it out. This previous work is currently looking at a man with a red shirt, we slowly look away, disregard the warping, and when we go back\u2026hey! Do you see what I see here? The shirt became white. This is not because the person is one of those artists who can change their clothes in less than a second, but because this older technique did not have a consistent internal model of the world. And now, let\u2019s see the new one. Once again, we start with the red shirt, look away, and then\u2026yes! Same red to blue gradient. Excellent! So it appears that this new technique also reuses information from previous frames efficiently, and therefore, it is finally able to create a consistent video, with much less morphing and warping, and even better, we have this advantageous consistency property where if we look at something that we looked at before, we will see very similar information there. But there is more. Additionally, it can also generate scenes from new viewpoints, which we also refer to as neural rendering. And as you see, the two viewpoints show similar objects, so the consistency property holds here too. And now, hold on to your papers, because we do not necessarily have to produce these semantic maps ourselves. We can let the machines do all the work by firing up a video game that we like, request that the different object classes are colored differently, and get this input for free. And then, the technique generated a photorealistic video from the game graphics. Absolutely amazing. Now note that it is not perfect, for instance, it has a different notion of time as the clouds are changing in the background rapidly. And, look! At the end of the sequence, we got back to our starting point, and the first frame that we saw is very similar to the last one. The consistency works here too. Very good. I have no doubt that two more papers down the line, and this will be even better. And for now, we can create consistent, photorealistic videos even if all we have is freely obtained video game data. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=u4HpryLU-VI",
        "paper_link": "https://nvlabs.github.io/wc-vid2vid/",
        "paper_title": "World-Consistent Video-to-Video Synthesis"
    },
    {
        "video_id": "SxIkQt04WCo",
        "video_title": "How Can We Simulate Water Droplets? \ud83c\udf0a",
        "position_in_playlist": 456,
        "description": "\u2764\ufe0f Check out Linode here and get $20 free credit on your account: https://www.linode.com/papers\n\n\ud83c\udfacOur Instagram page with the slow-motion videos is available here:\nhttps://www.instagram.com/twominutepapers/\n\n\ud83d\udcdd The paper \"Codimensional Surface Tension Flow using Moving-Least-SquaresParticles\" is available here: \nhttps://web.stanford.edu/~yxjin/pdf/codim.pdf\nhttps://web.stanford.edu/~yxjin/\n\r\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \r\n- https://www.patreon.com/TwoMinutePapers\r\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\r\n\r\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. A computer graphics paper from approximately 3 years ago was able to simulate the motion of these bubbles, and even these beautiful collision events between them in a matter of milliseconds. This was approximately 300 episodes ago, and in this series, we always say that two more papers down the line, and this will be improved significantly. So now, once again, here goes another one of those Two Minute Papers moments of truth. Now, three years later, let\u2019s see how this field evolved. Let\u2019s fire up this new technique that will now, simulate the evolution of two cube-shaped droplets for us. In reality, mother nature would make sure that the surface area of these droplets is minimized, let\u2019s see\u2026yes, droplets form immediately, so the simulation program understands surface tension, and the collision event is also simulated beautifully. A+. However, this was possible with previous methods, for instance, a paper by the name Surface-Only Liquids could also pull it off, so what\u2019s new here? Well, let\u2019s look under the hood and find out. Oh yes. This is different. You see, normally, if we do this breakdown, we get triangle meshes, this is typically how these surfaces are represented. But I don\u2019t see any meshes here, I see particles! Great, but what does this enable us to do? Look here. If we break down the simulation of this beautiful fluid polygon, we see that there is not only one kind of particle here! There are three kinds! With light blue, we see sheet particles, the yellow ones are filament particles, and if we look inside, with dark blue here, you see volume particles. With these building blocks, and the proposed new simulation method, we can create much more sophisticated surface tension-related phenomena! So let\u2019s do exactly that! For instance, here, you see soap membranes stretching due to wind flows. They get separated, lots of topological changes take place, and the algorithm handles it correctly. In an other example, this soap bubble has been initialized with a hole, and you can see it cascading through the entire surface. Beautiful work! And, after we finish the simulation of these fluid chains, we can look under the hood, and see how the algorithm thinks about this piece of fluid. Once again, with dark blue, we have the particles that represent the inner volume of the water chains, and on the outside, there is a thin layer of sheet particles holding them together. What a clean and beautiful visualization. So, how much do we have to wait to get these results? A bit. Simulating this fluid chain example took roughly 60 seconds per frame. This droplet on a plane example runs approximately ten times faster than that, it needs only 6.5 seconds for each frame. This was one of the cheaper scenes in the paper, and you may be wondering, which one was the most expensive? This water bell took almost two minutes for each frame, and here, when you see this breakdown, from the particle color coding, you know exactly what we are looking at. Since part of this algorithm runs on your processor, and a different part on your graphics card, there is plenty of room for improvements in terms of the computation time for a followup paper. I cannot wait to see these beautiful simulations in real time two more papers down the line. What a time to be alive! I also couldn\u2019t resist creating a slow-motion version of some of these videos, if this is something that you wish to see, make sure to click our Instagram page link in the video description for more. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SxIkQt04WCo",
        "paper_link": "https://web.stanford.edu/~yxjin/pdf/codim.pdf",
        "paper_title": "Codimensional Surface Tension Flow using Moving-Least-SquaresParticles"
    },
    {
        "video_id": "qeZMKgKJLX4",
        "video_title": "This AI Removes Shadows From Your Photos! \ud83c\udf12",
        "position_in_playlist": 457,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their post on how to train distributed models is available here: https://app.wandb.ai/sayakpaul/tensorflow-multi-gpu-dist/reports/Distributed-training-in-tf.keras-with-W%26B--Vmlldzo3NzUyNA\n\n\ud83d\udcdd The paper \"Portrait Shadow Manipulation\" is available here: \nhttps://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait\n\n\ud83d\udcdd Our paper with Activision Blizzard on subsurface scattering is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n\r\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. When we look at the cover page of a magazine, we often see lots of well-made, but also idealized photos of people. Idealized here means that the photographer made them in a studio, where they can add or remove light sources and move them around to bring out the best from their models. But most photos are not made in the studio, they are made out there in the wild where the lighting is what it is, and we can\u2019t control it too much. So, with that, today, our question is what if we could change the lighting after the photo has been made? This work proposes a cool technique to perform exactly that by enabling us to edit the shadows on a portrait photo that we would normally think of deleting. Many of these have to do with the presence of shadows, and you can see here that we can really edit these after the photo has been taken. However, before we start taking a closer look at the editing process, we have to note that there are different kinds of shadows. One, there are shadows cast on us by external objects, let\u2019s call those foreign shadows, and there is self-shadowing, which comes from the model\u2019s own facial features. Let\u2019s call those facial shadows. So why divide them into two classes? Simple, because we typically seek to remove foreign shadows, and edit facial shadows. The removal part can be done with a learning algorithm, provided that we can teach it with a lot of training data. Let\u2019s think about ways to synthesize such a large dataset! Let\u2019s start with the foreign shadows. We need image pairs of test subjects with and without shadows to have the neural network learn about their relations. Since removing shadows is difficult without further interfering with the image, the authors opted to do it the other way around. In other words, they take a clean photo of the subject, that\u2019s the one without the shadows, and then, and add shadows to it algorithmically. Very cool! And, the results are not bad at all, and get this, they even accounted for subsurface scattering, which is the scattering of light under our skin. That makes a great deal of a difference. This is a reference from a paper we wrote with scientists at the University of Zaragoza and the Activision Blizzard company to add this beautiful effect to their games. Here is a shadow edge without subsurface scattering, quite dark, and with subsurface scattering, you see this beautiful glowing effect. Subsurface scattering indeed makes a great deal of difference around hard shadow edges, so huge thumbs up for the authors for including an approximation of that. However, the synthesized photos are still a little suspect. We can still tell that they are synthesized. And that is kind of the point. Our question is \u201ccan the neural network still learn the difference between a clean and a shadowy photo\u201d despite all this? As you see, the problem is not easy - previous methods did not do too well on these examples when you compare them to the reference solution. And let\u2019s see this new method. Wow, I can hardly believe my eyes. Nearly perfect. And it did learn all this on not real, but synthetic images. And believe it or not, this was only the simpler part. Now comes the hard part. Let\u2019s look at how well it performs at editing the facial shadows! We can pretend to edit both the size and the intensity of these light sources. The goal is to have a little more control over the shadows in these photos, but, whatever we do with them, the outputs still have to remain realistic. Here are the before and after results. The facial shadows have been weakened, and depending on our artistic choices, we can also soften the image a great deal. Absolutely amazing. As a result, we now have a two-step algorithm, that first, removes foreign shadows, and is able to soften the remainder of the facial shadows, creating much more usable portrait photos of our friends, and all this after the photo has been made. What a time to be alive! Now, of course, even though this technique convincingly beats previous works, it is still not perfect. The algorithm may fail to remove some highly detailed shadows, you can see how the shadow of the hair remains in the output. In this other output, the hair shadows are handled a little better, there is some dampening, but the symmetric nature of the facial shadows here put the output results in an interesting no man\u2019s land where the opacity of the shadow has been decreased, but the result looks unnatural. I can\u2019t wait to see how this method will be improved two more papers down the line. I will be here to report on it to you, so make sure to subscribe and hit the bell icon to not miss out on that. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=qeZMKgKJLX4",
        "paper_link": "https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait",
        "paper_title": "Portrait Shadow Manipulation"
    },
    {
        "video_id": "MwCgvYtOLS0",
        "video_title": "TecoGAN: Super Resolution Extraordinaire!",
        "position_in_playlist": 458,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their instrumentation of a previous paper is available here: https://app.wandb.ai/authors/alae/reports/Adversarial-Latent-Autoencoders--VmlldzoxNDA2MDY\n\n\ud83d\udcdd The paper \"Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation\" is available here: \nhttps://ge.in.tum.de/publications/2019-tecogan-chu/\n\nThe legendary Wavelet Turbulence paper is available here:\nhttps://www.cs.cornell.edu/~tedkim/WTURB/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#enhance #superresolution",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Let\u2019s talk about video super resolution. The problem statement is simple, in goes a coarse video, the technique analyzes it, guesses what\u2019s missing, and out comes a detailed video. However, of course, reliably solving this problem is anything but simple. When learning-based algorithms were not nearly as good as they are today, this problem was mainly handled by handcrafted techniques, but they had their limits - after all, if we don\u2019t see something too well, how could we tell what\u2019s there? And this is where new learning-based methods, especially this one, come into play. This is a hard enough problem for even a still image, yet this technique is able to do it really well even for videos. Let\u2019s have a look. The eye color for this character is blurry, but we see that it likely has a green-ish, blue-ish color. And if we gave this problem to a human, this human would know that we are talking about the eye of another human, and we know roughly what this should look like in reality. A human would also know that this must a bridge, and finish the picture. What about computers? The key is that if we have a learning algorithm that looks at the coarse and fine version of the same video, it will hopefully learn what it takes to create a detailed video when given a poor one, which is exactly what happened here. As you see, we can give it very little information, and it was able to add a stunning amount of detail to it. Now of course, super resolution is a highly studied field these days, therefore it is a requirement for a good paper to compare to quite a few previous works. Let\u2019s see how it stacks up against those! Here, we are given given a blocky image of this garment, and this is the reference image that was coarsened to create this input. The reference was carefully hidden from the algorithms, and only we have it. Previous works could add some details, but the results were nowhere near as good as the reference. So what about the new method? My goodness, it is very close to the real deal! Previous methods also had trouble resolving the details of this region, where the new method, again, very close to reality. It is truly amazing how much this technique understands the world around us from just this training set of low and high-resolution videos. Now, if you have a closer look at the author list, you see that Nils Thuerey is also there. He is a fluid and smoke person, so I thought there had to be an angle here for smoke simulations. And, yup, there we go. To even have a fighting chance of understanding the importance of this sequence, let\u2019s go back to one of Nils\u2019s earlier works, which is one the best papers ever written, Wavelet Turbulence. That\u2019s a paper from twelve years ago. Now, some of the more seasoned Fellow Scholars among you know that I bring this paper up every chance I get, but especially now that it connects to this work we\u2019re looking at. You see, Wavelet Turbulence was an algorithm that could take a coarse smoke simulation after it has been created, and added fine details to it. In fact, so many fine details that creating the equivalently high resolution simulation would have been near impossible at the time. However, it did not work with images, it required knowledge about the inner workings of the simulator. For instance, it would need to know about velocities and pressures at different points in this simulation. Now, this new method can do something very similar, and all it does is just look at the image itself, and improves it, without even looking into the simulation data! Even though the flaws in the output are quite clear, the fact that it can add fine details to a rapidly moving smoke plume is still an incredible feat! If you look at the comparison against CycleGAN, a technique from just 3 years ago, this is just a few more papers down the line, and you see that this has improved significantly. And the new one is also more careful with temporal coherence, or in other words, there is no flickering arising from solving the adjacent frames in the video differently. Very good. And if we look a few more papers further down the line, we may just get a learning-based algorithm that does so well at this task, that we would be able to rewatch any old footage in super high quality. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MwCgvYtOLS0",
        "paper_link": "https://ge.in.tum.de/publications/2019-tecogan-chu/",
        "paper_title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation"
    },
    {
        "video_id": "iKvlOviWs3E",
        "video_title": "This AI Creates Images Of Nearly Any Animal! \ud83e\udd89",
        "position_in_playlist": 459,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder\" is available here: \nhttps://nvlabs.github.io/COCO-FUNIT/\n\r\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \r\n- https://www.patreon.com/TwoMinutePapers\r\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\r\n\r\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. The research field of image translation with the aid of learning algorithms has been on fire lately. For instance, this earlier technique would look at a large number of animal faces, and could interpolate between them, or in other words, blend one kind of dog into another breed. But that\u2019s not all, because it could even transform dogs into cats, or create these glorious plump cats and cheetahs. The results were absolutely stunning, however, it would only work on the domains it was trained on. In other words, it could only translate to and from species that it took the time to learn about. This new method offers something really amazing: it can handle multiple domains, or multiple breeds, if you will, even ones that it hadn\u2019t seen previously. That sounds flat out impossible, so let\u2019s have a look at some results. This dog will be used as content, therefore the output should have a similar pose, but its breed has to be changed to this one. But there is one little problem. And that problem is that the AI has never seen this breed before. This will be very challenging because we only see the head of the dog used for style. Should the body of the dog also get curly hair? You only know if you know this particular dog breed, or if you are smart and can infer missing information by looking at other kinds of dogs. Let\u2019s see the result. Incredible. The remnants of the leash also remain there in the output results. It also did a nearly impeccable job with this bird, where again, the style image is from a previously unseen breed. Now, this is, of course, a remarkably difficult problem domain, translating into different kinds of animals that you know nothing about apart from a tiny, cropped image, this would be quite a challenge, even for a human. However, this one is not the first technique to attempt to solve it, so let\u2019s see how it stacks up against a previous method. This one is from just a year ago, and you will see in a moment how much this field has progressed since then. For instance, in this output, we get two dogs, which seems to be a mix of the content and the style dog. And, while the new method still seems to have some structural issues, the dog type and the pose is indeed correct. The rest of the results also appear to be significantly better. But what do you think? Did you notice something weird? Let me know in the comments below. And now, let\u2019s transition into image interpolation. This will be a touch more involved than previous interpolation efforts. You see, in this previous paper, we had a source and a target image, and the AI was asked to generate intermediate images between them. Simple enough. In this case, however, we have not two, but three images as an input. There will be a content image, this will provide the high-level features, such as pose, and its style is going to transition from this to this. The goal is that the content image remains intact while transforming one breed or species into another. This particular example is one of my favorites. Such a beautiful transition, and surely not all, but many of the intermediate images could stand on their own. Again, the style images are from unseen species. Not all cases do this well with the intermediate images, however. We start with one eye because the content and this style image have one eye visible, while the target style of the owl has two. How do we solve that? Of course, with nuclear fission. Look! Very amusing. Loving this example, especially how impossible it seems because the owl is looking into the camera with both eyes, while, we see its backside below the head. If it looked to the side, like the input content image, this might be a possibility, but with this contorted body posture, I am not so sure, so I\u2019ll give it a pass on this one. So there you go, transforming one known animal into a different one that the AI has never seen before. And it is already doing a more than formidable job at that. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iKvlOviWs3E",
        "paper_link": "https://nvlabs.github.io/COCO-FUNIT/",
        "paper_title": "COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder"
    },
    {
        "video_id": "-6Xn4nKm-Qw",
        "video_title": "OpenAI\u2019s Image GPT Completes Your Images With Style!",
        "position_in_playlist": 460,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-with-W%26B---GradCAM--Vmlldzo5MTIyNw\n\n\ud83d\udcdd The paper \"Generative Pretraining from Pixels (Image GPT)\" is available here: \nhttps://openai.com/blog/image-gpt/\n\nTweets:\nWebsite layout: https://twitter.com/sharifshameem/status/1283322990625607681\nPlots: https://twitter.com/aquariusacquah/status/1285415144017797126?s=12\nTypesetting math: https://twitter.com/pavtalk/status/1285410751092416513\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#GPT3",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In early 2019, a learning-based technique appeared that could perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. This method was developed by scientists at OpenAI, and they called it GPT-2. A followup paper introduced a more capable version of this technique called GPT-3, and among many incredible examples, it could generate website layouts from a written description. The key idea, in both cases was, that we would provide it an incomplete piece of text, and it would try to finish it. However, no one said that these neural networks have to only deal with text information. And sure enough, in this work, scientists at OpenAI introduced a new version of this method that tries to complete not text, but\u2026images! The problem statement is simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. That is, of course, an immensely difficult task, because these images may depict any part of the world around us. It would have to know a great deal about our world to be able to continue the images, so how well did it do? Let\u2019s have a look! This is undoubtedly a cat. But look! See that white part that is just starting? The interesting part has been cut out of the image. What could that be? A piece of paper? Something else? Now, let\u2019s leave the dirty work to the machine and ask it to finish it! Wow. A piece of paper indeed, according to the AI, and it even has text on it. The text has a heading section and a paragraph below it too. Truly excellent. You know what is even more excellent? Perhaps the best part. It also added the indirect illumination on the fur of the cat, meaning that it sees that a blue room surrounds it and therefore some amount of color bleeds onto the fur of the cat, making it bluer. I am a light transport researcher by trade, so I spend the majority of my life calculating things like this, and I have to say that this looks quite good to me. Absolutely amazing attention to detail. But it had more ideas. What\u2019s this? The face of the cat has been finished, quite well in fact, but the rest, I am not so sure. If you have an idea what this is supposed to be, please let me know in the comments. And here go the rest of the results. All quite good! And, the true, real image that was concealed for the algorithm. This is the reference solution. Let\u2019s see the next one. Oh, my, scientists at OpenAI pulled no punches here, this is also quite nasty. How many stripes should this continue with? Zero? Maybe! In any case, this solution is not unreasonable. I appreciate the fact that it continued the shadows of the humans. Next one. Yes, more stripes, great! But likely a few too many. Here are the remainder of the solutions, and, the true reference image again. Let\u2019s have a look at this water droplet example too. We humans, know that since we see the remnants of some ripples over there too, there must be a splash, but does the AI know? Oh yes, yes it does! Amazing! And the true image. Now, what about these little creatures? The first continuation finishes them correctly, and puts them on a twig. The second one involves a stone. The third is my favorite. Hold on to your papers, and look at this. They stand in the water and we can even see their mirror images. Wow! The fourth is a branch, and finally, the true reference image. This is one of its best works I have seen so far. Here are some more results, and note that these are not cherrypicked, or in other words, there was no selection process for the results, nothing was discarded, these came out from the AI as you see them. There is a link to these and to the paper in the video description, so make sure to have a look and let me know in the comments if you have found something interesting! So what about the size of the neural network for this technique? Well, it contains from 1.5 to about 7 billion parameters. Let\u2019s have a look together and find out what that means. These are the results from the GPT-2 paper, the previous version of the text processor on a challenging reading comprehension test as a function of the number of parameters. As you see, around 1.5 billion parameters, which is roughly similar to GPT-2, it learned a great deal, but its understanding was nowhere near the level of human comprehension. However, as they grew the network, something incredible happened. Non-trivial capabilities started to appear as we approached a hundred billion parameters. Look! It nearly matched the level of humans. And all this was measured on a nasty reading comprehension test. So, this Image GPT has the number of parameters that is closer to GPT-2 than GPT-3, so we can maybe speculate that the next version could be, potentially, another explosion in capabilities. I can\u2019t wait to have a look at that. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-6Xn4nKm-Qw",
        "paper_link": "https://openai.com/blog/image-gpt/",
        "paper_title": "Generative Pretraining from Pixels (Image GPT)"
    },
    {
        "video_id": "MD_k3p4MH-A",
        "video_title": "Can We Simulate Merging Bubbles? \ud83c\udf0a",
        "position_in_playlist": 461,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/ajayuppili/efficientnet/reports/How-Efficient-is-EfficientNet%3F--Vmlldzo4NTk5MQ\n\n\ud83d\udcdd The paper \"Constraint Bubbles and Affine Regions: Reduced Fluid Models for Efficient Immersed Bubbles and Flexible Spatial Coarsening\" is available here:\nhttps://cs.uwaterloo.ca/~rgoldade/reducedfluids/\n\nCheck out Blender here (free):\nhttps://www.blender.org/\n\nIf you wish to play with some fluids, try the FLIP Fluids plugin (paid, with free demo):\nhttps://flipfluids.com/\n\nNote that Blender also contains Mantaflow, its own fluid simulation program and that's also great!\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\r\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\r\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\r\n\r\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\r\nInstagram: https://www.instagram.com/twominutepapers/\r\nTwitter: https://twitter.com/twominutepapers\r\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If we write the laws of fluid motion into a computer program, we can create beautiful water simulations like the one you see here. However, with all the progress in computer graphics research, we can not only simulate the water volume itself, but there are also efficient techniques to add foam, spray, and bubbles to this simulation. The even crazier thing is that this paper from 8 years ago can do all three in one go, and is remarkably simple for what it does. Just look at this heavenly footage, all simulated on a computer by using Blender, a piece of free and open source software and the FLIP Fluids plugin. But all this has been possible for quite a while now, so what happened in the 8 years since this paper has been published? How has this been improved? Well, it\u2019s good to have bubbles in our simulation, however, in real life, bubbles have their individual densities, and can coalesce at a moment\u2019s notice. This technique is able to simulate these events, and you will see that offers much, much more. Now, let\u2019s marvel at three different phenomena in this simulation. First, the bubbles here are less dense than the water, and hence, start to rise, then, look at the interaction with the air! Now, after this, the bubbles that got denser than the water start sinking again. And all this can be done on your computer today! What a beautiful simulation! And now, hold on to your papers, because this method also adds simulating air pressure, which opens up the possibility for an interaction to happen at a distance. Look. First, we start pushing the piston here. The layer of air starts to push the fluid, which weighs on the next air pocket, which gets compressed, and so on. Such a beautiful phenomenon. And let\u2019s not miss the best part! When we pull the piston back, the emerging negative flux starts drawing the liquid back. Look! One more time. Simulating all this efficiently is quite a technical marvel. When reading through the paper, I was very surprised to see that it is able to incorporate this air compression without simulating the air gaps themselves. A simulation without simulation, if you will. Let\u2019s simulate pouring water through the neck of the water cooler with a standard, already existing technique. For some reason, it doesn\u2019t look right, does it? So what\u2019s missing here? We see a vast downward flow of liquid, therefore, there also has to be a vast upward flow of air at the same time, but I don\u2019t see any of that here. Let\u2019s see how the new simulation method handles this\u2026we start the down flow, and yes, huge air bubbles are coming up, creating this beautiful glugging effect! I think I now have a good guess as to what scientists are discussing over the watercooler in Professor Christopher Batty\u2019s research group. So, how long do we have to wait to get these results? You see, the quality of the outputs is nearly the same as the reference simulation, however, it takes less than half the amount of time to produce it! Admittedly, these simulations still take a few hours to compute, but it is absolutely amazing that these beautiful, complex phenomena can be simulated in a reasonable amount of time, and you know the drill, two more papers down the line, and it will be improved significantly. But we don\u2019t necessarily need a bubbly simulation to enjoy the advantages of this method. In this scene, you see a detailed splash, where the one on the right here was simulated with the new method, it also matches the reference solution and it was more than 3 times faster. If you have a look at the paper in the video description, you will see how it simplifies the simulation by finding a way to identify regions of the simulation domain where not a lot is happening and coarsen the simulation there. These are the green regions that you see here and the paper refers to them as affine regions. As you see, the progress in computer graphics and fluid simulation research is absolutely stunning, and these amazing papers just keep coming out year after year. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MD_k3p4MH-A",
        "paper_link": "https://cs.uwaterloo.ca/~rgoldade/reducedfluids/",
        "paper_title": "Constraint Bubbles and Affine Regions: Reduced Fluid Models for Efficient Immersed Bubbles and Flexible Spatial Coarsening"
    },
    {
        "video_id": "5NM_WBI9UBE",
        "video_title": "This AI Creates Human Faces From Your Sketches!",
        "position_in_playlist": 462,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their instrumentation of a previous paper is available here: https://app.wandb.ai/stacey/greenscreen/reports/Two-Shots-to-Green-Screen%3A-Collage-with-Deep-Learning--VmlldzoxMDc4MjY\n\nTheir report on this paper is available here: https://app.wandb.ai/authors/deepfacedrawing/reports/DeepFaceDrawing-An-Overview--VmlldzoyMjgxNzM\n\n\ud83d\udcdd The paper \"DeepFaceDrawing: Deep Generation of Face Images from Sketches\" is available here:\nhttp://geometrylearning.com/DeepFaceDrawing/\nAlternative paper link if it is down: https://arxiv.org/abs/2006.01047\n\nOur earlier video on sketch tutorials is available here:\nhttps://www.youtube.com/watch?v=brs1qCDzRdk\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers \n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In 2017, so more than 300 episodes ago, we talked about an algorithm that took a 3D model of a complex object, and would give us an easy to follow, step by step breakdown on how to draw it. Automated drawing tutorials, if you will! This was a handcrafted algorithm that used graph theory to break these 3D objects into smaller, easier to manage pieces, and since then, learning algorithms have improved so much that we started looking more and more to the opposite direction. And that opposite direction would be giving a crude drawing to the machine, and getting a photorealistic image. Now that sounds like science fiction, until we realize that scientists at NVIDIA already had an amazing algorithm for this around 1.5 years ago. In that work, the input was a labeling which we can draw ourselves, and the output is a hopefully photorealistic landscape image that adheres to these labels. I love how first, only the silhouette of the rock is drawn, so we have this hollow thing on the right that is not very realistic, and then, it is now filled in with the bucket tool, and, there you go. And next thing you know, you have an amazing-looking landscape image. It was capable of much, much more, but what it couldn\u2019t do is synthesize human faces this way. And believe it or not, this is what today\u2019s technique is able to do. Look! In goes our crude sketch as a guide image, and out comes a nearly photorealistic human face that matches it. Interestingly, before we draw the hair itself, it gives us something as a starting point, but if we choose to, we can also change the hair shape and the outputs will follow our drawing really well. But it goes much further than this as it boasts a few additional appealing features. For instance, it not only refines the output as we change our drawing, but since one crude input can be mapped to many-many possible people, these output images can also be further art-directed with these sliders. According to the included user study, journeymen users mainly appreciated the variety they can achieve with this algorithm, if you look here, you can get a taste of that, while professionals were more excited about the controllability aspect of this method. That was showcased with the footage with the sliders. Another really cool thing that it can do is called face copy-paste, where we don\u2019t even need to draw anything, and just take a few aspects of human faces that we would like to combine, and there you go. Absolutely amazing. This work is not without failure cases, however. You have probably noticed, but the AI is not explicitly instructed to match the eye colors, where some asymmetry may arise in the output. I am sure this will be improved just one more paper down the line, and I am really curious where digital artists will take these techniques in the near future. The objective is always to get out of the way, and help the artist spend more time bringing their artistic vision to life, and spend less time on the execution. This is exactly what these techniques can help with. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=5NM_WBI9UBE",
        "paper_link": "http://geometrylearning.com/DeepFaceDrawing/",
        "paper_title": "DeepFaceDrawing: Deep Generation of Face Images from Sketches"
    },
    {
        "video_id": "bfuBQp1JmX8",
        "video_title": "Can We Simulate a Rocket Launch? \ud83d\ude80",
        "position_in_playlist": 463,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Fast and Scalable Turbulent Flow Simulation with Two-Way Coupling\" is available here:\nhttp://faculty.sist.shanghaitech.edu.cn/faculty/liuxp/projects/lbm-solid/index.htm\n\nVishnu Menon\u2019s wind tunnel test video: https://www.youtube.com/watch?v=_q6ozALzkF4\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In this series, we often talk about smoke and fluid simulations, and sometimes, the examples showcase a beautiful smoke plume, but not much else. However, in real production environments, these simulations often involve complex scenes with many objects that interact with each other, and therein lies the problem. Computing these interactions is called coupling, and it is very difficult to get right, but is necessary for many of the scenes you will see throughout this video. This new graphics paper builds on a technique called the Lattice Boltzmann Method and promises a better way to compute this two-way coupling. For instance, in this simulation, two-way coupling is required to compute how this fiery smoke trail propels the rocket upward. So, coupling means interaction between different kinds of objects, but what about the two-way part? What does that mean exactly? Well, first, let\u2019s have a look at one way coupling. As the box moves here, it has an effect on the smoke plume around it. This example also showcases one-way coupling, where the falling plate stirs up the smoke around it. The parts with the higher Reynolds numbers showcase more turbulent flows. Typically, that\u2019s the real good stuff if you ask me. And now, on to two-way coupling. In this case, similarly to the previous ones, the boxes are allowed to move the smoke, but the added two-way coupling part means that now, the smoke is also allowed to blow away the boxes. What\u2019s more, the vortices here on the right were even able to suspend the red box in the air for a few seconds. An excellent demonstration of a beautiful phenomenon. Now let\u2019s look at the previous example with the dropping plate and see what happens. Yes, indeed, as the plate drops, it moves the smoke, and as the smoke moves, it also blows away the boxes. Woo-hoo! Due to improvements in the coupling computation, it also simulates these kinds of vortices much more realistically than previous works. Just look at all this magnificent progress in just 2 years. So, what else can we do with all this? What are some typical scenarios that require accurate two-way coupling? Well, for instance, it can perform an incredible tornado simulation, that you see here, and there is an alternative view where we only see the objects moving about. So, all this looks good, but really, how do we know how accurate this technique is? And now comes my favorite part, and this is when we let reality be our judge and compare the simulation results with real-world experiments. Hold on to your papers while you observe the real experiment here on the left. And now, the algorithmic reproduction of the same scene here. How close are they? Goodness\u2026very, very close. I will stop the footage at different times so we can both evaluate it better. Love it. The technique can also undergo the wind tunnel test, here is the real footage, and here is the simulation. And it is truly remarkable how close this is able to match it, and I was wondering that even though as someone who has been doing fluids for a while now, if someone cropped this part of the image and told me that it is real-world footage, I would have believed it in a second. Absolute insanity. So, how much do we have to wait to compute a simulation like this? Well, great news, it uses your graphics card, which is typically the case for the more rudimentary fluid simulation algorithms out there, but the more elaborate ones typically don\u2019t support it, or at least, not without a fair amount of additional effort. The quickest example was this, as it was simulated in less than 6 seconds, which I find to be mind blowing. A smoke simulation with box movements in a few seconds, I am truly out of words. The rocket launch scene took the longest with 16 hours, while the falling plate example with the strong draft that threw the boxes around was 4.5 hours of computation time. The results depend greatly on delta t, which is the size of time steps, or in others words, in how small increments we can advance time when creating these simulations to make sure we don\u2019t miss any important interactions. You see here that in the rocket example, we have to simulate roughly a hundred thousand steps for every second of video footage. No wonder it takes so long! We have an easier time with this scene where these time steps can be 50 times larger without losing any detail, and hence, it goes much faster. The grid resolution also matters a great deal, which specifies how many spatial points the simulation has to take place in. The higher the resolution the grid, the larger region we can cover, and the more details we can simulate. As most research works, this technique doesn\u2019t come without limitations, however. It is less accurate if we have simulations involving thin rods and shells and typically uses two to three times more memory than a typical simulator program. If these are the only tradeoffs to create all this marvelous footage, sign me up this very second! Overall, this paper is extraordinarily well written and presented, and of course, it has been accepted to the SIGGRAPH conference, one of the most prestigious scientific venues in computer graphics research. Huge congratulations to the authors, and if you wish to see something beautiful today, make sure to have a look at the paper itself in the video description. Truly stunning work. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bfuBQp1JmX8",
        "paper_link": "http://faculty.sist.shanghaitech.edu.cn/faculty/liuxp/projects/lbm-solid/index.htm",
        "paper_title": "Fast and Scalable Turbulent Flow Simulation with Two-Way Coupling"
    },
    {
        "video_id": "GniyQkgGlUA",
        "video_title": "Can An AI Create Original Art? \ud83d\udc68\u200d\ud83c\udfa8",
        "position_in_playlist": 464,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their report on this paper is available here: https://app.wandb.ai/authors/rewrite-gan/reports/An-Overview-Rewriting-a-Deep-Generative-Model--VmlldzoyMzgyNTU\n\n\ud83d\udcdd The paper \"Rewriting a Deep Generative Model\" is available here:\nhttps://rewriting.csail.mit.edu/\n\nRead the instructions carefully and try it here:\nhttps://colab.research.google.com/github/davidbau/rewriting/blob/master/notebooks/rewriting-interface.ipynb\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Approximately 7 months ago, we discussed an AI-based technique called StyleGAN2, which could synthesize images of human faces for us. As a result, none of the faces that you see here are real, all of them were generated by this technique. The quality of the images and the amount of detail therein is truly stunning. We could also exert artistic control over these outputs by combining aspects of multiple faces together. And as the quality of these images improve over time, we think more and more about new questions to ask about them. And one of those questions is, for instance, how original are the outputs of these networks? Can these really make something truly unique? And believe it or not, this paper gives us a fairly good answer to that. One of the key ideas in this work is that in order to change the outputs, we have to change the model itself. Now that sounds a little nebulous, so let\u2019s have a look at an example. First, we choose a rule that we wish to change, in our case, this will be the towers. We can ask the algorithm to show us matches to this concept, and indeed, it highlights the towers on the images we haven\u2019t marked up yet, so it indeed understands what we meant. Then, we highlight the tree as a goal, place it accordingly onto the tower, and a few seconds later, there we go! The model has been reprogrammed such that instead of towers, it would make trees. Something original has emerged here, and, look, not only on one image, but on multiple images at the same time. Now, have a look at these human faces. By the way, none of them are real and were all synthesized by StyleGAN2, the method that you saw at the start of this video. Some of them do not appear to be too happy about the research progress in machine learning, but I am sure that this paper can put a smile on their faces. Let\u2019s select the ones that aren\u2019t too happy, then copy a big smile and paste it onto their faces. See if it works! It does, wow! Let\u2019s flick between the before and after images and see how well the changes are adapted to each of the target faces. Truly excellent work. And now, on to eyebrows. Hold on to your papers while we choose a few of them, and now, I hope you agree that this mustache would make glorious replacement for them. And there we go. Perfect! And note that with this, we are violating Betteridge's law of headlines again in this series, because the answer to our central question is a resounding yes, these neural networks can indeed create truly original works, and what\u2019s more, even entire datasets that haven\u2019t existed before. Now, at the start of the video, we noted that instead of editing images, it edits the neural network\u2019s model instead. If you look here, we have a set of input images created by a generator network. Then, as we highlight concepts, for instance, the watermark text here, we can look for the weights that contain this information, and rewrite the network to accommodate these user requests, in this case, to remove these patterns. Now that they are gone, by selecting humans, we can again, rewrite the network weights to add more of them, and finally, the signature tree trick from earlier can take place. The key here is that if we change one image, then we have a new and original image, but if we change the generator model itself, we can make thousands of new images in one go. Or even a full dataset. Loving the idea. And perhaps the trickiest part of this work is minimizing the effect on other weights while we reprogram the ones we wish to change. Of course, there will always be some collateral damage, but the results, in most cases still seem to remain intact. Make sure to have a look at the paper to see how it\u2019s done exactly. Also, good news, the authors also provided an online notebook where you can try this technique yourself. If you do, make sure to read the instructions carefully and regardless of whether you get successes or failure cases, make sure to post them in the comments section here! In research, both are useful information. So, after the training step has taken place, neural networks can be rewired to make sure they create truly original works, and all this on not one image, but on a mass scale. What  a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=GniyQkgGlUA",
        "paper_link": "https://rewriting.csail.mit.edu/",
        "paper_title": "Rewriting a Deep Generative Model"
    },
    {
        "video_id": "YCur6ir6wmw",
        "video_title": "AI Makes Video Game After Watching Tennis Matches!",
        "position_in_playlist": 465,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Vid2Player: Controllable Video Sprites that Behave and Appear like Professional Tennis Players\" is available here:\nhttps://cs.stanford.edu/~haotianz/research/vid2player/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Approximately a year ago, we talked about an absolutely amazing paper by the name vid2game, in which we could grab a controller and become video game characters. It was among the first, introductory papers to tackle this problem, and in this series, we always say that two more papers down the line, and it will be improved significantly, so let\u2019s see what\u2019s in store, and this time, just one more paper down the line. This new work offers an impressive value proposition, which is to transform a real tennis match into a realistic looking video game that is controllable. This includes synthesizing not only movements, but also what effect the movements have on the ball as well. So, how do we control this? And now, hold on to your papers, because we can specify where the next shot would land with just one click. For instance, we can place this red dot here. And now, just think about the fact that this doesn\u2019t just change where the ball should go, but the trajectory of the ball should be computed using a physical model, and, the kind of shot the tennis players has to perform for the resulting ball trajectory to look believable. This physical model even contains the ball\u2019s spin velocity, and the Magnus effect created by this spin. The entire chain of animations has to be correct, and that\u2019s exactly what happens here. Bravo! With blue, we can also specify the position the player has to await in to hit the ball next. And these virtual characters don\u2019t just look like their real counterparts, they also play like them. You see, the authors analyzed the playstyle of these athletes and built a heatmap that contains information about their usual shot placements for the forehand and backhand shots separately, the average velocities of these shots, and even their favored recovery positions. If you have a closer look at the paper, you will see that they not only include this kind of statistical knowledge into their system, but they really went the extra mile and included common tennis strategies as well. So, how does it work? Let\u2019s look under the hood. First, it looks at broadcast footage, from which, annotated clips are extracted that contain the movement of these players. If you look carefully, you see this red line on the spine of the player, and some more, these are annotations that tell the AI about the pose of the players. It builds a database from these clips and chooses the appropriate piece of footage for the action that is about to happen, which sounds great in theory, but in a moment, you will see that this is not nearly enough to produce a believable animation. For instance, we also need a rendering step, which has to adjust this footage to the appropriate perspective as you see here. But we have to do way more to make this work. Look! Without additional considerations, we get something like this. Not good. So, what happened here? Well, given the fact that the source datasets contain matches that are several hours long, they therefore contain many different lighting conditions. With this, visual glitches are practically guaranteed to happen. To address this, the paper describes a normalization step that can even these changes out. How well does this do its job? Let\u2019s have a look. This is the unnormalized case. This short sequence appears to contain at least 4 of these glitches, all of which are quite apparent. And now, let\u2019s see the new system after the normalization step. Yup. That\u2019s what I am talking about! But these are not the only considerations the authors had to take to produce these amazing results. You see, oftentimes, quite a bit of information is missing from these frames. Our seasoned Fellow Scholars know not to despair, because we can reach out to image inpainting methods to address this. These can fill in missing details in images with sensible information. You can see NVIDIA\u2019s work from two years ago that could do this reliably for a great variety of images. This new work uses a learning-based technique called image to image translation to fill in these details. Of course, the advantages of this new system are visible right away, and so are its limitations. For instance, temporal coherence could be improved, meaning that the tennis rackets can appear or disappear from one frame to another. The sprites are not as detailed as they could be, but, none of this really matters. What matters is that now, what\u2019s been previously impossible is now possible, and two more papers down the line, it is very likely that all of these issues will be ironed out. What  a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=YCur6ir6wmw",
        "paper_link": "https://cs.stanford.edu/~haotianz/research/vid2player/",
        "paper_title": "Vid2Player: Controllable Video Sprites that Behave and Appear like Professional Tennis Players"
    },
    {
        "video_id": "T29O-MhYALw",
        "video_title": "This AI Creates Real Scenes From Your Photos! \ud83d\udcf7",
        "position_in_playlist": 466,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/sweep/nerf/reports/NeRF-%E2%80%93-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis--Vmlldzo3ODIzMA\n\n\ud83d\udcdd The paper \"NeRF in the Wild - Neural Radiance Fields for Unconstrained Photo Collections\" is available here:\nhttps://nerf-w.github.io/\n\nPhotos by Flickr users dbowie78, vasnic64, punch, paradasos, itia4u, jblesa, joshheumann, ojotes, chyauchentravelworl, burkeandhare, photogreuhphie / CC BY\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Approximately 5 months ago, we talked about a technique called Neural Radiance Fields, or NERF in short, that worked on a 5D neural radiance field representation. So what does this mean exactly? What this means is that we have 3 dimensions for location and two for view direction, or in short, the input is where we are in space give it to a neural network to learn it, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. In short, it can learn and reproduce entire real-world scenes from only a few views by using neural networks. And the results were just out of this world. Look. It could deal with many kinds of matte and glossy materials, and even refractions worked quite well. It also understood depth so accurately that we could use it for augmented reality applications where we put a new, virtual object in the scene and it correctly determined whether it is in front of, or behind the real objects in the scene. However, not everything was perfect. In many cases, it had trouble with scenes with variable lighting conditions and lots of occluders. You might ask, is that a problem? Well, imagine a use case of a tourist attraction that a lot of people take photos of, and we then have a collection of photos taken during a different time of the day, and of course, with a lot of people around. But, hey, remember that this means an application where we have exactly these conditions: a wide variety of illumination changes and occluders. This is exactly what NERF was not too good at! Let\u2019s see how it did on such a case. Yes, we see both abrupt changes in the illumination, and the remnants of the folks occluding the Brandenburg Gate as well. And this is where this new technique from scientists at Google Research by the name NERF-W shines. It takes such a photo collection, and tries to reconstruct the whole scene from it, which we can, again, render from new viewpoints. So, how well did the new method do in this case? Let\u2019s see. Wow. Just look at how consistent those results are. So much improvement in just 6 months of time. This is unbelievable. This is how it did in a similar case with the Trevi fountain. Absolutely beautiful. And what is even more beautiful is that since it has variation in the viewpoint information, we can change these viewpoints around as the algorithm learned to reconstruct the scene itself. This is something that the original NERF technique could also do, however, what it couldn\u2019t do, is the same, with illumination. Now, we can also change the lighting conditions together with the viewpoint. This truly showcases a deep understanding of illumination and geometry. That is not trivial at all! For instance, when loading this scene into this neural re-rendering technique from last year, it couldn\u2019t tell whether we see just color variations on the same geometry, or if the geometry itself is changing. And, look, this new method does much better on cases like this. So clean! Now that we have seen the images, let\u2019s see what the numbers say for these scenes. The NRW is the neural re-rendering technique we just saw, and the other one is the NERF paper from this year. The abbreviations show different ways of comparing the output images, the up and down arrows show whether they are subject to maximization or minimization. They are both relatively close, but, when we look at the new method, we see one of the rare cases where it wins decisively regardless of what we are measuring. Incredible. This paper truly is a great leap in just a few months, but of course, not everything is perfect here. This technique may fail to reconstruct regions that are only visible on just a few photos in the input dataset. The training still takes from hours to days, I take this as an interesting detail more than a limitation since this training only has to be done once, and then, using the technique can take place very quickly. But, with that, there you go, a neural algorithm that understands lighting, geometry, can disentangle the two, and reconstruct real-world scenes from just a few photos. It truly feels like we are living in a science fiction world. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=T29O-MhYALw",
        "paper_link": "https://nerf-w.github.io/",
        "paper_title": "NeRF in the Wild - Neural Radiance Fields for Unconstrained Photo Collections"
    },
    {
        "video_id": "JKe53bcyBQY",
        "video_title": "Elon Musk\u2019s Neuralink Puts An AI Into Your Brain! \ud83e\udde0",
        "position_in_playlist": 467,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/jack-morris/david-vs-goliath/reports/Does-model-size-matter%3F-A-comparison-of-BERT-and-DistilBERT--VmlldzoxMDUxNzU\n\n\ud83d\udcdd The paper \"An integrated brain-machine interface platform with thousands of channels\" is available here:\nhttps://www.biorxiv.org/content/10.1101/703801v2\n\nNeuralink is hiring! Apply here: https://jobs.lever.co/neuralink\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#neuralink #elonmusk",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Due to popular request, today we will talk about Neuralink, Elon Musk\u2019s neural engineering company that he created to develop brain-machine interfaces. And your first question likely is, why talk about Neuralink now? There was a recent event, and another one last year as well, why did I not cover that? Well, the launch event from last year indeed promised a great deal. In this series, we often look at research works that are just one year apart, and marvel at the difference scientists have been able to make in that tiny-tiny timeframe. So first, let\u2019s talk about their paper from 2019, which will be incredible, and then, see how far they have come in a year, which, as you will see, is even more incredible. The promise is to be able to read and write information to and from the brain. To accomplish this, as of 2019, they used this robot to insert the electrodes into your brain tissue. You can see the insertion process here. From the close-up image you might think that this is a huge needle, but in fact, this needle is extremely tiny, you can see a penny for scale here. This is the story of how this rat got equipped with a USB port. As this process is almost like inserting microphones into its brain, now, we are able to read the neural signals of this rat. Normally, these are analog signals, which are read and digitized by Neuralink\u2019s implant, and now this brain data is represented as a digital signal. Well, at first, this looks a bit like gibberish. Do we really have to undergo a brain surgery to get a bunch of these squiggly curves? What do these really do for us? Well, now that they are digitized, we can have the Neuralink chip analyze these signals and look for action potentials in them. These are also referred to as \u201cspikes\u201d because of their shape. That sounds a bit better, but still, what does this do for us? Let\u2019s see. Here we have a person with a mouse in their hand. This is an outward movement with the mouse, and then, reaching back. Simple enough. Now, what you see here below is the activity of an example neuron. When nothing is happening, there is some firing, but not much activity, and now, look! When reaching out, this neuron fires a great deal, and suddenly, when reaching back, again, nearly no activity. This means that this neuron is tuned for an outward motion, and this other one is tuned for the returning motion. And all this is now information that we can read in real time, and the more neurons we can read, the more complex motion we can read. Absolutely incredible. However, this is still a little difficult to read, so let\u2019s order them by what kind of motion makes them excited. And there we go! Suddenly, this is a much more organized way to present all this neural activity, and now we can detect what kind of motion the brain is thinking about. This was the reading part, and that\u2019s just the start. What is even cooler is that we can invert this process, read this spiking activity, and just by looking at these, we can reconstruct the motion the human wishes to perform. With this, brain-machine interfaces can be created for people with all kinds of disabilities where the brain can still think about the movements, but the connection to the rest of the body is severed. Now, these people only have to think about moving, and then, the Neuralink device will read it and perform the cursor movement for them. It really feels like we live in a science fiction world. And all this signal processing is now possible automatically and in real time, and all we need for this is this tiny-tiny chip that takes just a few square millimeters. And don\u2019t forget, that is just version one from 2019. Now, onwards to the 2020 event, where, it gets even better. The Neuralink device has been placed into Gertrude, the pig\u2019s brain, and here, you see it in action. We see the raster view here, and luckily, we already know what it means, this lays bare the neural action potentials before our eyes, or in other words, which neuron is spiking and exactly when. Below with blue, you see these activities summed up for our convenience, and this way, you will not only see, but hear it too, as these neurons are tuned for snout boops. In other words, you will see and hear that the more the snout is stimulated, the more neural activity it will show. Let\u2019s listen. And all this is possible today, and in real time. That was one of the highlights of the 2020 progress update event, but it went further. Much further! Look! This is a pig on a treadmill, and here you see the brain signal readings. This signal marked with the circle shows where a joint or limb is about to move, where the other, dimmer colored signal is the chip\u2019s prediction as to what is about to happen. It takes into consideration periodicity, and predicts higher-frequency movement, like these sharp turns really well. The two are almost identical, and that means exactly what you think it means - today, we can not only read and write, but even predict what the pig\u2019s brain is about to do. And that was the part where I fell off the chair when I watched this event live. You can also see the real and predicted world-space positions for these body parts as well. Very close. Now note that there is a vast body of research in brain-machine interfaces, and many of these things were possible in lab conditions, and Neuralink\u2019s quest here is to make them accessible for a wider audience within the next decade. If this project further improves at this rate, it could help many paralyzed people around the world live a longer, and more meaningful life, and the neural enhancement aspect is also not out of question. Just thinking about summoning your Tesla might also summon it, which sounds like science fiction, and based on these results, you see that it may even be one of the simplest tasks for a Neuralink chip in the future. And who knows, one day, maybe, with this device, these videos could be beamed into your brain much quicker, and this series would have to be renamed from Two Minute Papers to Two Second Papers, or maybe even Two Microsecond Papers. They might actually fit into two minutes like the title says, now that would truly be a miracle. Huge thanks to scientists at Neuralink for our discussions about the concepts descriped in this video and ensuring that you get accurate information. This is one of the reasons why our coverage of the 2020 event is way too late compared to many mainstream media outlets, which leads to a great deal less views for us, but it doesn\u2019t matter. We are not maximizing views here, we are maximizing learning. Note that they are also hiring, if you wish to be a part of their vision and work with them, make sure to apply! The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=JKe53bcyBQY",
        "paper_link": "https://www.biorxiv.org/content/10.1101/703801v2",
        "paper_title": "An integrated brain-machine interface platform with thousands of channels"
    },
    {
        "video_id": "UiEaWkf3r9A",
        "video_title": "AI-Based Style Transfer For Video\u2026Now in Real Time!",
        "position_in_playlist": 468,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/stacey/yolo-drive/reports/Bounding-Boxes-for-Object-Detection--Vmlldzo4Nzg4MQ\n\n\ud83d\udcdd The paper \"Interactive Video Stylization Using Few-Shot Patch-Based Training\" is available here:\nhttps://ondrejtexler.github.io/patch-based_training/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Daniel Hasegan, Eric Haddad, Eric Martel, Gordon Child, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Style transfer is an interesting problem in machine learning research where we have two input images, one for content, and one for style, and the output is our content image reimagined with this new style. The cool part is that the content can be a photo straight from our camera, and the style can be a painting, which leads to super fun, and really good looking results. We have seen plenty of papers doing variations of style transfer, but I always wonder, can we can push this concept further? And the answer is, yes! For instance, few people know that style transfer can also be done for video! First, we record a video with our camera, then, take a still image from the video and apply our artistic style to it. Then, our style will be applied to the entirety of the video! The main advantage of this new method, compared to previous ones is that they either take too long, or we have to run an expensive pre-training step. With this new one, we can just start drawing and see the output results right away. But it gets even better. Due to the interactive nature of this new technique, we can even do this live! All we need to do is change our input drawing and it transfers the new style to the video as fast as we can draw! This way, we can refine our input style for as long as we wish, or until we find the perfect way to stylize the video. And there is even more. If this works interactively, then it has to be able to offer an amazing workflow where we can capture a video of ourselves live, and mark it up as we go. Let\u2019s see! Oh wow, just look at that. It is great to see that this new method also retains temporal consistency over a long time frame, which means that even if the marked up keyframe is from a long time ago, it can still be applied to the video and the outputs will show minimal flickering. And note that we can not only play with the colors, but with the geometry too! Look, we can warp the style image, and it will be reflected in the output as well. I bet there is going to be a followup paper on more elaborate shape modifications as well! And, this new work improves upon previous methods in even more areas. For instance, this is a method from just one year ago, and here you see how it struggled with contour-based styles. Here is a keyframe of the input video, and here is the style that we wish to apply to it. Later, this method from last year seems to lose not only the contours, but a lot of visual detail is also gone. So, how did the new method do in this case? Look! It not only retains the contours better, but a lot more of the sharp details remain in the outputs. Amazing. Now note that this technique also comes with some limitations. For instance, there is still some temporal flickering in the outputs, and in some cases, separating the foreground and the background is challenging. But really, such incredible progress in just one year! And I can only imagine what this method will be capable of two more papers down the line. What a time to be alive! Make sure to have a look at the paper in the video description, and you will see many additional details, for instance, how you can just partially fill in some of they keyframes with your style, and still get an excellent result. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UiEaWkf3r9A",
        "paper_link": "https://ondrejtexler.github.io/patch-based_training/",
        "paper_title": "Interactive Video Stylization Using Few-Shot Patch-Based Training"
    },
    {
        "video_id": "Popg7ej4AUU",
        "video_title": "Beautiful Results From 30 Years Of Light Transport Simulation! \u2600\ufe0f",
        "position_in_playlist": 469,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Specular Manifold Sampling for Rendering High-Frequency Caustics and\u00a0Glints\" is available here:\nhttp://rgl.epfl.ch/publications/Zeltner2020Specular\n\nMy rendering course is available here, and is free for everyone: https://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\nWish to see the spheres and the volumetric caustic scene in higher resolution? Check out our paper here - https://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\n\nThe PostDoc call is available here -  https://www.cg.tuwien.ac.at/news/2020-10-02-Lighting-Simulation-Architectural-Design-%E2%80%93-Post-Doc-Position\n\nCheck out the following renderers:\n- Mitsuba: https://www.mitsuba-renderer.org/\n- Blender's Cycles - https://www.blender.org/\n- LuxCore - https://luxcorerender.org/\n\nCredits: The test scenes use textures from CC0 Textures and cgbook-case, and are lit by environment maps courtesy of HDRI Havenand Paul Debevec. Kettle: Blend Swap user PrinterKiller.\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I have been yearning for a light transport paper, and goodness, was I ecstatic when reading this one. And by the end of the video, I hope you will be too. And now, we only have to go through just about 30 years of light transport research. As some of you know, if we immerse ourselves into the art of light transport simulations, we can use our computer to simulate millions and millions of light rays, and calculate how they get absorbed or scattered off of our objects in a virtual scene. Initially, we start out with a really noisy image, and as we add more rays, the image gets clearer and clearer over time. The time it takes for these images to clean up depends on the complexity of the geometry and our material models, but it typically takes a while. This microplanet scene mostly contains vegetation, which are matte objects, these we also refer to as diffuse materials, these typically converge very quickly. As you see, we get meaningful progress on the entirety of the image within the first two minutes of the rendering process. And remember, in light transport simulations, noise is public enemy number one. This used a technique called path tracing. Let\u2019s refer to it as the okay technique. And now, let\u2019s try to use path tracing, this okay technique to render this torus in a glass enclosure. This is the first two minutes of the rendering process, and it does not look anything like the previous scene. The previous one was looking pretty smooth after two minutes, whereas here, you see, this is indeed looking grim. We have lots of these fireflies, which will take us up to a few days of computation time to clean up, even if we have a modern, powerful machine. So, why did this happen? The reason for this is that there are tricky cases for specular light transport that take many-many millions, if not billions of light rays to compute properly. Specular here means mirror-like materials, those can get tricky, and this torus that has been enclosed in there is also not doing too well. So this was path tracing, the okay technique. Now let\u2019s try a better technique called Metropolis Light Transport! This method is the result of a decade of research and is much better in dealing with difficult scenes. This particular variant is a proud Hungarian algorithm by a scientist called Csaba Kelemen and his colleagues at the Technical University of Budapest. For instance, here is a snippet of our earlier paper on a similarly challenging scene. This is how the okay path tracer did, and in the same amount of time, this is what Metropolis Light Transport, the better technique could do! This was a lot more efficient, so let\u2019s see how it does with the torus. Now that\u2019s unexpected! This is indeed a notoriously difficult scene to render, even for Metropolis Light Transport, the better technique. As you see, the reflected light patterns that we also refer to as caustics on the floor are much cleaner, but the torus is still not giving up. Let\u2019s jump another 15 years of light transport research, and use a technique that goes by the name Manifold Exploration. Let\u2019s call this the best technique. Wow. Look at how beautifully it improves the image, it is not only much cleaner, but also converges much more gracefully. It doesn\u2019t go from a noisy image to a slightly less noisy image, but almost immediately gives us a solid baseline, and new, cleaned up light paths appear over time. This technique is from 2012, and it truly is mind boggling how good it is. This technique is so difficult to understand and implement, that to the best of my knowledge, the number of people who can and have implemented it properly is exactly one. And that one person is Wenzel Jakob, one of the best minds in the game, and believe it or not, he wrote this method as a PhD student in light transport research. And today, as a professor at EPFL Switzerland, he and his colleagues set out to create a technique that is as good as manifold exploration, the best technique, but is much simpler. Well, good luck with that, I thought when skimming through the paper. Let\u2019s see how it did. For instance, we have some caustics at the bottom of a pool of water, as expected, lots of firefly noise with the okay path tracer, and now, hold on to your papers, and here is the new technique. Just look at that! It can do so much better in the same amount of time! Let\u2019s also have a look at this scene with lots and lots of specular microgeometry, or in other words, glints. This is also a nightmare to render. With the okay path tracer, we have lots of flickering from one frame to the next, and here you see the result with the new technique. Perfect. So it is indeed possible to take the best technique, manifold exploration, and reimagine it in a way that ordinary humans can also implement. Huge congratulations to the authors on this work, that I think, is a crown achievement in light transport research. And that\u2019s why I was ecstatic when I first read through this incredible paper. Make sure to have a look at the paper and you will see how they borrowed a nice little trick from a recent work in nuclear physics to tackle this problem. The presentation of the paper and the talk video with the details is also brilliant and I urge you to have a look at it in the video description. This whole thing got me so excited I was barely able to fall asleep for several days now. What a time to be alive! Now, while we look through some more results from this paper, if you feel a little stranded at home and are thinking that this light transport thing is pretty cool, and you would like to learn more about it, I held a Master-level course on this topic at the Technical University of Vienna. Since I was always teaching it to a handful of motivated students, I thought that the teachings shouldn\u2019t only be available for the privileged few who can afford a college education, but the teachings should be available for everyone. Free education for everyone, that\u2019s what I want. So, the course is available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. We write a full light simulation program from scratch there, and learn about physics, the world around us, and more. Also, note that my former PhD advisor, Michael Wimmer is looking to hire a postdoctoral researcher in this area, which is an amazing opportunity to push this field forward. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Popg7ej4AUU",
        "paper_link": "http://rgl.epfl.ch/publications/Zeltner2020Specular",
        "paper_title": "Specular Manifold Sampling for Rendering High-Frequency Caustics and\u00a0Glints"
    },
    {
        "video_id": "XrOTgZ14fJg",
        "video_title": "This AI Can Deal With Body Shape Variation!",
        "position_in_playlist": 470,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned instrumentation is available here: https://app.wandb.ai/lavanyashukla/cnndetection/reports/Detecting-CNN-Generated-Images--Vmlldzo2MTU1Mw\n\n\ud83d\udcdd The paper \"Learning Body Shape Variation in Physics-based Characters\" is available here:\nhttp://mrl.snu.ac.kr/publications/ProjectMorphCon/MorphCon.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This glorious paper from about 7 years ago was about teaching digital creatures to walk, the numbers here showcase the process of learning over time, and it is clear that the later generations did much better than the earlier ones. These control algorithms are not only able to teach these creatures to walk, but they are quite robust against perturbations as well, or, more simply put, we can engage in one of the favorite pastimes of a computer graphics researcher, which is, of course, throwing boxes at the character and seeing how well it can take it. This one has done really well! Well, kind of. Now we just noted that this is a computer graphics paper, and an amazing one at that, but it does not yet use these incredible new machine learning techniques that just keep getting better year by year. You see, these agents could learn to inhabit a given body, but I am wondering what would happen if we would suddenly change their bodies on the fly? Could previous methods handle it? Unfortunately, not really. And I think it is fair to say that an intelligent agent should have the ability to adapt when something changes. Therefore, our next question is, how far have we come in these 7 years? Can these new machine learning methods help us create a more general agent that could control not just one body, but a variety of different bodies? Let\u2019s have a look at today\u2019s paper, and with that, let the fun begin. This initial agent was blessed with reasonable body proportions, but of course, we can\u2019t just leave it like that. Yes, much better. And, look, all of these combinations can still walk properly, and all of them use the same one reinforcement learning algorithm. But do not think for a second that this is where the fun ends, no, no, no. Now, hold on to your papers, and now let\u2019s engage in these horrific asymmetric changes. There is no way that the same algorithm could be given this body and still be able to walk. Goodness! Look at that! It is indeed still able to walk. If you have been holding on to your papers, good now squeeze that paper, because after adjusting the height and thickness of this character, it could still not only walk, but even dance. But it goes further. Do you remember the crazy asymmetric experiment for the legs? Let\u2019s do something like that with thickness. And as a result, they can still not only walk, but even perform gymnastic moves. Wo-hoo! Now it\u2019s great that one algorithm can adapt to all of these body shapes, but it would be reasonable to ask, how much do we have to wait for it to adapt? Have a look here. Are you seeing what I am seeing? We can make changes to the body on the fly, and the AI adapts to it immediately. No re-training, or parameter tuning is required. And that is the point where I fell off the chair when I read this paper. What a time to be alive! And now, Scholars, bring in the boxes. Ha-haa! It can also inhabit dogs and fish, and we can also have some fun with them as we grab a controller and control them in real time. The technique is also very efficient as it requires very little memory and computation, not to mention that we only have to train one controller for many body types instead of always retraining after each change to the body. However, of course, this algorithm isn\u2019t perfect, one of its key limitations is that it will not do well if the body shapes we are producing strays too far away from the ones contained in the training set. But let\u2019s leave some space for the next followup paper too. And just one more thing that didn\u2019t quite fit into this story. Every now and then I get these heartwarming messages from you Fellow Scholars noting that you\u2019ve been watching the series for a while and decided to turn your lives around, and go back to study more and improve. Good work Mr. Munad! That is absolutely amazing and reading these messages are a true delight to me. Please keep them coming. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=XrOTgZ14fJg",
        "paper_link": "http://mrl.snu.ac.kr/publications/ProjectMorphCon/MorphCon.html",
        "paper_title": "Learning Body Shape Variation in Physics-based Characters"
    },
    {
        "video_id": "OzHenjHBBds",
        "video_title": "Enhance! Neural Supersampling is Here! \ud83d\udd0e",
        "position_in_playlist": 471,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://www.wandb.com/articles/code-comparer\n\n\ud83d\udcdd The paper \"Neural Supersampling for Real-time Rendering\" is available here:\nhttps://research.fb.com/blog/2020/07/introducing-neural-supersampling-for-real-time-rendering/\nhttps://research.fb.com/publications/neural-supersampling-for-real-time-rendering/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Let\u2019s talk about video super resolution. The problem statement is simple, in goes a coarse video, the technique analyzes it, guesses what\u2019s missing, and out comes a detailed video. You know, the CSI thing. Enhance! However, of course, reliably solving this problem is anything but simple. This previous method is called TecoGAN, and it is able to give us results that are often very close to reality. It is truly amazing how much this technique understands the world around us from just this training set of low and high-resolution videos. However, as amazing as super resolution is, it is not the most reliable way of delivering high-quality images in many real-time applications, for instance, video games. Note that in these applications, we typically have more data at our disposal, but in return, the requirements are also higher: we need high-quality images at least 60 times per second, temporal coherence is a necessity, or in other words, no jarring jumps and flickering is permitted. And hence, one of the techniques often used in these cases is called supersampling. At the risk of simplifying the term, supersampling means that we split every pixel into multiple pixels to compute a more detailed image, and then, display that to the user. Does this work? Yes it does, it works wonderfully, but it requires a lot more memory and computation, therefore it is generally quite expensive. So our question today is, is it possible to use these amazing learning-based algorithms to do it a little smarter? Let\u2019s have a look at some results from a recent paper that uses a neural network to perform supersampling at a more reasonable computational cost. Now, in goes the low-resolution input, and, my goodness, like magic, out comes this wonderful, much more detailed result. And here is the reference, which is the true, higher-resolution image. Of course, the closer the neural supersampling is to this, the better. And as you see, this is indeed really close, and much better than the pixelated inputs. Let\u2019s do one more. Wow. This is so close I feel that we are just a couple papers away from the result being indistinguishable from the real reference image. Now, we noted that this method has access to more information than the previously showcased super resolution method. It looks at not just one frame, but a few previous frames as well, can use an estimation of the motion of each pixel over time, and also gets depth information. These can be typically produced inexpensively with any major game engine. So, how much of this data is required to train this neural network? Hold on to your papers, because only 80 videos were used, and the training took approximately 1.5 days on one TITAN V, which is an expensive, but commercially available graphics card. And no matter, because this step only has to be done once, and depending on how high the resolution of the output should be, with the faster version of the technique, the upsampling step takes from 8 to 18 milliseconds, so this runs easily in real time! Now, of course, this is not the only modern supersampling method, this topic is subject to a great deal of research, so let\u2019s see how it compares to others. Here you see the results with TAAU, the temporal upsampling technique used in Unreal Engine, an industry-standard game engine. And, look! This neural supersampler is significantly better at antialiasing, or in other words, smoothing these jagged edges, and not only that, but it also resolves many more of the intricate details of the image. Temporal coherence has also improved a great deal, as you see, the video output is much smoother for the new method. The paper also contains a ton more comparisons against recent methods, so make sure to have a look! Like many of us, I would love to see a comparison against NVIDIA\u2019s DLSS solution, but I haven\u2019t been able to find a published paper on the later versions of this method. I remain very excited about seeing that too. And for now, with techniques like this, the future of video game visuals and other real-time graphics applications is looking as exciting as it\u2019s ever been. What  a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=OzHenjHBBds",
        "paper_link": "https://research.fb.com/blog/2020/07/introducing-neural-supersampling-for-real-time-rendering/",
        "paper_title": "Neural Supersampling for Real-time Rendering"
    },
    {
        "video_id": "86QU7_SF16Q",
        "video_title": "Remove This! \u2702\ufe0f AI-Based Video Completion is Amazing!",
        "position_in_playlist": 472,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Flow-edge Guided Video Completion\" is available here:\nhttp://chengao.vision/FGVC/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Have you ever had a moment where you took the perfect photo, but upon closer inspection, there was this one annoying thing that ruined the whole picture? Well, why not just take a learning algorithm to erase those cracks in the facade of a building, or a photobombing sheep? Or, to even reimagine ourselves with different eye colors, we can try one of the many research works that are capable of something that we call image inpainting. What you see here is the legendary PatchMatch algorithm at work, which, believe it or not, is a handcrafted technique from more than 10 years ago. Later, scientists at NVIDIA published a more modern inpainter that uses a learning-based algorithm to do this more reliably, and for a greater variety of images. These all work really well, but the common denominator for these techniques is that they all work on inpainting still images. Would this be a possibility for video? Like, removing a moving object or person from a video? Is this possible, or is it science fiction? Let\u2019s see if these learning-based techniques can really do more. And now, hold on to your papers, because this new work can really perform proper inpainting for video. Let\u2019s give it a try by highlighting this human. And pro tip: also highlight the shadowy region for inpainting to make sure that not only the human, but its silhouette also disappears from the footage. And, look! Wow. Let\u2019s look at some other examples. Now that\u2019s really something because video is much more difficult due to the requirement of temporal coherence, which means that it\u2019s not nearly enough if the images are inpainted really well individually, they also have to look good if we weave them together into a video. You will hear and see more about this in a moment. Not only that, but if we highlight a person, this person not only needs to be inpainted, but we also have to track the boundaries of this person throughout the footage and then inpaint a moving region. We get some help with that, which I will also talk about in a moment. Now, as you see here, these all work extremely well, and believe it or not, you have seen nothing yet, because so far, another common denominator in these examples was that we highlighted regions inside the video. But that\u2019s not all. If you have been holding on to your papers so far, now squeeze that paper, because we can also go outside, and expand our video spatially with even more content. This one is very short so I will keep looping it. Are you ready? Let\u2019s go. Wow! My goodness! The information from inside of the video frames is reused to infer what should be around the video frame, and all this in a temporally coherent manner. Now, of course, this is not the first technique to perform this, so let\u2019s see how it compares to the competition by erasing this bear from the video footage. The remnants of the bear are visible with a wide selection of previously published techniques from the last few years. This is true even for these four methods from last year. And, let\u2019s see how this new method did one the same case. Yup, very good, not perfect, we still see some flickering. This is the temporal coherence example, or the lack thereof that I have promised earlier. But now, let\u2019s look at this example with the BMX rider. We see similar performance with the previous techniques, and now, let\u2019s have a look at the new one. Now that\u2019s what I\u2019m talking about! Not a trace left from this person, the only clue that we get in reconstructing what went down here is the camera movement. It truly feels like we are living in a science fiction world. What a time to be alive! Now these were the qualitative results, and now, let\u2019s have a look at the quantitative results. In other words, we saw the videos, now let\u2019s see what the numbers say. We could talk all day about the peak signal to noise ratios or structural similarity or other ways to measure how good these techniques are, but you will see in a moment that it is completely unnecessary. Why is that? Well, you see here that the second best results are underscored and highlighted with blue. As you see, there is plenty of competition, as the blues are all over the place. But there is no competition at all for the first place, because this new method smokes the competition in every category. This was measured on a dataset by the name Densely Annotated Video Segmentation, DAVIS in short, this contains 150 video sequences and it is annotated, which means that many of the objects are highlighted throughout this video, so for the cases in this dataset, we don\u2019t have to deal with the tracking ourselves. I am truly out of ideas as to what I should wish for two more papers down the line. Maybe not only removing the tennis player, but putting myself in there as a proxy? We can already grab a controller and play as if we were real characters in real broadcast footage, so who really knows. Anything is possible. Let me know in the comments what you have in mind for potential applications and what you would be excited to see two more papers down  the line! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=86QU7_SF16Q",
        "paper_link": "http://chengao.vision/FGVC/",
        "paper_title": "Flow-edge Guided Video Completion"
    },
    {
        "video_id": "5ePD83StI6A",
        "video_title": "This Is What Simulating a 100 Million Particles Looks Like!",
        "position_in_playlist": 473,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned instrumentation is available here: https://app.wandb.ai/stacey/sfmlearner/reports/See-3D-from-Video%3A-Depth-Perception-for-Self-Driving-Cars--Vmlldzo2Nzg2Nw\n\nOur Instagram page with the slow-motion footage is available here: https://www.instagram.com/twominutepapers/\n\n\ud83d\udcdd The paper \"A Massively Parallel and Scalable Multi-GPU Material Point Method \" is available here:\nhttps://sites.google.com/view/siggraph2020-multigpu\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If we study the laws of fluid motion and implement them in a computer program, we can create and enjoy these beautiful fluid simulations. And not only that, but today, with the amazing progress in computer graphics research, we can even enrich our physics simulations with anisotropic damage and elasticity. So what does that mean exactly? This means that we can simulate more extreme topological changes in these virtual objects. This leads to better material separation when the damage happens. So it appears that today we can do a great deal, but these techniques are pretty complex, and take quite a while to compute, and they typically run on your processor. That\u2019s a pity, because many powerful consumer computers also have a graphics card, and if we could restate some of these algorithms to be able to run them on those, they would run significantly faster. So, do we have any hope for that? Well, today\u2019s paper promises a new particle data structure that is better suited for number crunching on the graphics card, and is, hence, much faster than its predecessors. As a result, this runs the Material Point Method, the algorithm that is capable of simulating these wondrous things you are seeing here, not only on your graphics card, but the work on one problem can be also be distributed between many-many graphics cards. This means that we get crushing concrete, falling soil, candy bowls, sand armadillos, oh my! You name it. And all this, much faster than before. Now, since these are some devilishly detailed simulations, please do not expect several frames per second kind of performance, we are still in the seconds per frame region, but we are not that far away. For instance, hold on to your papers because this candy bowl example contains nearly 23 million particles, and despite that, it runs in about 4 seconds per frame on a system equipped with four graphics cards. Four seconds per frame! My goodness! If somebody told this to me today without showing me this paper, I would have not believed a word of it. But there is more. You know what, let\u2019s double the number of particles and pull up this dam break scene. What you see here is 48 million particles that runs in 15 seconds per frame. Let\u2019s do even more. These sand armadillos contain a total of 55 million particles, and take about 30 seconds per frame. And in return, look at that beautiful mixture of the two sand materials. And with half a minute per frame, that\u2019s a great deal, I\u2019ll take this any day of the week. And if we wish to simulate crushing this piece of concrete with a hydraulic press, that will take nearly a hundred million particles. Just look at that footage. This is an obscene amount of detail. And the price to be paid for this is nearly four minutes of simulation time for every frame that you see on the screen here. Four minutes you say? Hmm, that\u2019s a little more than expected. Why is that? We had several seconds per frame for the others, not minutes per frame. Well, it is because the particle count matters a great deal, but that\u2019s not the only consideration for such a simulation. For instance, here, you see with delta t something that we call time step size. The smaller this number is, the tinier the time steps with which we can advance the simulation when computing every interaction, and hence, the more steps there are to compute. In simpler words, generally, time step size is also an important factor in the computation time, and the smaller this is, the slower the simulation will be. As you see, we have to simulate 5 times more time steps to make sure that we don\u2019t miss any particle interactions, and hence, this takes much longer. Now, this one appears to be perhaps the simplest simulation of the bunch, isn\u2019t it? No, no, no. Quite the opposite! If you have been holding on to your papers so far, now squeeze that paper, because, and watch carefully. They wiggle. They\u2026 wiggle! Why do they wiggle? These are just around 6 thousand bombs, which is not a lot, however, wait a minute\u2026each bomb is a collection of particles, giving us a total of not a measly 6 thousand, but, a whopping 134 million particle simulation, and hence, we may think that it\u2019s nearly impossible to perform in a reasonable amount of time. The time steps are not that far apart for this one, so we can do it in less than one minute per frame. This was nearly impossible when I started my PhD, and today, less than a minute for one frame. It truly feels like we are living in a science fiction world. What a time to be alive! I also couldn\u2019t resist creating a slow-motion version of some of these videos, so if this is something that you wish to see, make sure to visit our Instagram page in the video description for more. Thanks for watching and for  your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=5ePD83StI6A",
        "paper_link": "https://sites.google.com/view/siggraph2020-multigpu",
        "paper_title": "A Massively Parallel and Scalable Multi-GPU Material Point Method "
    },
    {
        "video_id": "2qMw8sOsNg0",
        "video_title": "What is De-Aging? \ud83e\uddd1",
        "position_in_playlist": 474,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their report for this paper is available here: https://wandb.ai/wandb/in-domain-gan/reports/In-Domain-GAN-Inversion--VmlldzoyODE5Mzk\n\n\ud83d\udcdd The paper \"In-Domain GAN Inversion for Real Image Editing\" is available here:\nhttps://genforce.github.io/idinvert/\n\nCheck out the research group's other works, there is lots of cool stuff there:\nhttps://genforce.github.io/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#deaging",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, we are living the advent of neural network-based image generation algorithms. What you see here is some super high quality results from a technique developed by scientists at NVIDIA called StyleGAN2. Right, all of these were generated by a learning algorithm. And, while generating images of this quality is a great achievement, but if we have an artistic vision, we wonder, can we bend these images to our will? Can we control them? Well, kind of, and one of the methods that enables us to do that is called image interpolation. This means that we have a reference image for style, and a target image, and with this, we can morph one human face into another. This is sufficient for some use cases, however, if we are looking for more elaborate edits, we hit a wall. Now it\u2019s good that we already know what StyleGAN is, because this new work builds on top of that, and shows exceptional image editing and interpolation abilities. Let\u2019s start with the image editing part! With this new work, we can give anyone glasses, and a smile, or even better, transform them into a variant of the Mona Lisa. Beautiful. The authors of the paper call this process semantic diffusion. Now, let\u2019s have a closer look at the expression and pose change possibilities. I really like that we have fine-grained control over these parameters, and what\u2019s even better, we don\u2019t just have a start and endpoint, but all the intermediate images make sense, and can stand on their own. This is great for pose and expression because we can control how big of a smile we are looking for, or even better, we can adjust the age of the test subject with remarkable granularity. Let\u2019s go all out! I like how Mr. Cumberbatch looks nearly the same as a baby, we might have a new mathematical definition for baby face right there, and apparently Mr. DiCaprio scores a bit lower on that. And I would say that both results are quite credible! Very cool! And now, onto image interpolation. What does this new work bring to the table in this area? Previous techniques are also pretty good at morphing\u2026until we take a closer look at them. Let\u2019s continue our journey with three interpolation examples, with increasing difficulty. Let\u2019s see the easy one first. I was looking for morphing example with long hair, you will see why right away. This is how the older method did. Uh-oh. One more time. Do you see what I see? If I stop the process here, you see that this is an intermediate image that doesn\u2019t make sense. The hair over the forehead just suddenly vanishes into the ether. Let\u2019s see how the new method deals with this issue! Wow, much cleaner, and I can stop nearly anywhere and leave the process with a usable image. Easy example, checkmark. Now let\u2019s see an intermediate-level example! Let\u2019s go from an old black and white Einstein photo to a recent picture with colors and stop the process at different points, and\u2026yes, I prefer the picture created with the new technique close to every single time! Do you agree? Let me know in the comments below! Intermediate example, checkmark. And now, onwards to the let\u2019s hardest, nastiest example. This is going to sound impossible, but we are going to transform the Eiffel Tower into the Tower Bridge. Yes, that sounds pretty much impossible. So let\u2019s see how a conventional interpolation technique did here. Well\u2026that\u2019s not good. I would argue that nearly none of the images showcased here would be believable if we stopped the process and took them out. And let\u2019s see the new method. Hmm, that makes sense, we start with one tower, then, two towers grow from the ground, and\u2026look! Wow! The bridge slowly appears between them. That was incredible. While we look at some more results, what really happened here? At the risk of simplifying the contribution of this new paper, we can say that during interpolation, it ensures that we remain within the same domain for the intermediate images. Intuitively, as a result, we get less nonsense in the outputs, and can pull off morphing not only between human faces, but even go from a black and white photo to a colored one, and what\u2019s more, it can even deal with completely different building types. Or, you know, just transform people into Mona Lisa variants. Absolutely amazing. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2qMw8sOsNg0",
        "paper_link": "https://genforce.github.io/idinvert/",
        "paper_title": "In-Domain GAN Inversion for Real Image Editing"
    },
    {
        "video_id": "DxW_kk5LWYQ",
        "video_title": "Beautiful Elastic Simulations, Now Much Faster!",
        "position_in_playlist": 475,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/safijari/dqn-tutorial/reports/Deep-Q-Networks-with-the-Cartpole-Environment--Vmlldzo4MDc2MQ\n\n\ud83d\udcdd The paper \"IQ-MPM: An Interface Quadrature Material Point Method for Non-sticky Strongly Two-Way Coupled Nonlinear Solids and Fluids\" is available here:\nhttps://yzhu.io/publication/mpmcoupling2020siggraph/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. It is time for some fluids. Hm-hmm! As many of you know, in this series, we often talk about fluid simulations, and sometimes, the examples showcase a fluid splash, but not much else. However, in real production environments, these simulations often involve complex scenes with many objects that interact with each other, and therein lies the problem. Computing these interactions is called coupling, and it is very difficult to get right, but is necessary for many of the beautiful scenes you will see throughout this video. Getting this right is of utmost importance if we wish to create a realistic simulation where fluids and solids interact. So first question would be, as many of these techniques build upon The Material Point Method or MPM in short, why not just use that? Well, let\u2019s do exactly that and see how it does on this scene. Let\u2019s drop the liquid ball on the bunny\u2026and. Uh-oh..a lot of it is now stuck to the bunny. This is not supposed to happen. So, what about the improved version of MPM? Yup, still too sticky. And now, let\u2019s have a look at how this new technique handles this situation! I want to see dry, and floppy bunny ears. Yes, now that\u2019s what I\u2019m talking about! Now then. That\u2019s great, but what else can this do? A lot more. For instance, we can engage in the favorite pastimes of the computer graphics researcher, which is, of course, destroying objects in a spectacular manner. This is going to be a very challenging scene. Ouch! And now, let physics take care of the rest. This was a harrowing, but beautiful simulation. And we can try to challenge the algorithm even more. Here, we have three elastic spheres filled with water, and now, watch how they deform as they hit the ground, and how the water gushes out exactly as it should. And now hold on to your papers, because there is a great deal to be seen in this animation, but the most important part remains invisible. Get this - all three spheres use a different hyperelasticity model to demonstrate that this new technique can be plugged into many existing techniques. And, it works so seamlessly that I don\u2019t think anyone would be able to tell the difference. And, it can do even more. For instance, it can also simulate wet sand. Wow! And I say wow not only because of this beautiful result, but there is more behind it. If you are one of our hardcore, long time Fellow Scholars, you may remember that three years ago, we needed an entire paper to pull this off. This algorithm is more general and can simulate this kind of interaction between liquids and granular media as an additional side effect. We can also simulate dropping this creature into a piece of fluid, and as we increase the density of the creature, it sinks in in a realistic manner. While we are lifting frogs and help an elastic bear take a bath, let\u2019s look at why this technique works so well. The key to achieving these amazing results in a reasonable amount of time is that this new method is able to find these interfaces where the fluids and solids meet and handles their interactions in a way that we can advance the time in our simulation in larger steps than previous methods. This leads to not only these amazingly general and realistic simulations, but they also run faster. Furthermore, I am very happy about the fact that now, we can not only simulate these difficult phenomena, but we don\u2019t even have to implement a technique for each of them, but we can take this one, and simulate a wide variety of fluid-solid interactions. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=DxW_kk5LWYQ",
        "paper_link": "https://yzhu.io/publication/mpmcoupling2020siggraph/",
        "paper_title": "IQ-MPM: An Interface Quadrature Material Point Method for Non-sticky Strongly Two-Way Coupled Nonlinear Solids and Fluids"
    },
    {
        "video_id": "F0QwAhUnpr4",
        "video_title": "Finally, Deformation Simulation... in Real Time! \ud83d\ude97",
        "position_in_playlist": 476,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their report about a previous paper is available here: https://app.wandb.ai/stacey/stargan/reports/Cute-Animals-and-Post-Modern-Style-Transfer%3A-Stargan-V2-for-Multi-Domain-Image-Synthesis---VmlldzoxNzcwODQ \n\n\ud83d\udcdd The paper \"Detailed Rigid Body Simulation with Extended Position Based Dynamics\" is available here:\n- Paper: https://matthias-research.github.io/pages/publications/PBDBodies.pdf\n- Talk video: https://www.youtube.com/watch?v=zzy6u1z_l9A&feature=youtu.be\n\nWish to see and hear the sound synthesis paper?\n- Our video: https://www.youtube.com/watch?v=rskdLEl05KI\n- Paper: https://research.cs.cornell.edu/Sound/mc/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, with the power of computer\u00a0 graphics research, we can use our\u00a0\u00a0 computers to run fluid simulations, simulate\u00a0 immersing a selection of objects into jelly,\u00a0\u00a0 or tear meat in a way that much like in\u00a0 reality, it tears along the muscle fibers. If we look at the abstract of this amazing new\u00a0 paper, we see this, quoting: \u201cThis allows us to\u00a0\u00a0 trace high speed motion of objects colliding\u00a0 against curved geometry, to reduce the number\u00a0\u00a0 of constraints, to increase the robustness of\u00a0 the simulation, and to simplify the formulation\u00a0\u00a0 of the solver.\u201d What! This sounds impossible,\u00a0 but at the very least, outrageously good. Let\u2019s\u00a0\u00a0 look at three examples of what it can do and see\u00a0 for ourselves if it can live up to its promise. One. It can simulate a steering\u00a0 mechanism full of joints and contact.\u00a0\u00a0 Yup, an entire servo steering mechanism is\u00a0 simulated with a prescribed mass ratio. Loving\u00a0\u00a0 it. I hereby declare that it passes inspection,\u00a0 and now, we can take off for some off-roading.\u00a0\u00a0 All of the movement is simulated really well,\u00a0 and\u2026 wait a minute. Hold on to your papers!\u00a0\u00a0 Are you seeing what I am seeing? Look. Even the\u00a0 tire deformations are part of the simulation!\u00a0\u00a0 Beautiful. And now, let\u2019s do a stress test,\u00a0 and race through a bunch of obstacles and see\u00a0\u00a0 how well those tires can take it. At the end of\u00a0 the video, I will tell you how much time it takes\u00a0\u00a0 to simulate all this\u2026and note that I had to look\u00a0 three times because I could not believe my eyes. Two. Restitution! Or in other words, we can smash\u00a0 an independent marble into a bunch of others\u00a0\u00a0 and their combined velocity will be correctly\u00a0 computed. We know for a fact that the computations\u00a0\u00a0 are correct, because when I stop the video\u00a0 here, you can see that the marbles themselves\u00a0\u00a0 are smiling. The joys of curved geometry and\u00a0 specular reflections. Of course, this is not true,\u00a0\u00a0 because if we attempt to do the same with\u00a0 a classical earlier technique by the name\u00a0\u00a0 position based dynamics, this would happen.\u00a0 Yes, the velocities become erroneously large\u00a0\u00a0 and the marbles jump off of the wire. And\u00a0 they still appear to be very happy about it.\u00a0\u00a0 Of course, with the new technique, the\u00a0 simulation is much more stable and realistic. Talking about stability. Is it stable\u00a0 only in a small-scale simulation,\u00a0\u00a0 or can it take a huge scene with lots\u00a0 of interactions? Would it still work?\u00a0\u00a0 Let\u2019s run a stress test and find out.\u00a0 Ha-haa! This animation can run all day long\u00a0\u00a0 and not one thing appears to\u00a0 behave incorrectly. Loving this. Three. It can also simulate these beautiful\u00a0 high-frequency rolls that we often experience\u00a0\u00a0 when we drop a coin on a table. This kind of\u00a0 interaction is very challenging to simulate\u00a0\u00a0 correctly because of the high-frequency nature of\u00a0 the motion and the curved geometry that interacts\u00a0\u00a0 with the table. I would love to see a technique\u00a0 that algorithmically generates the sound for this.\u00a0\u00a0 I could almost hear its sound\u00a0 in my head! Believe it or not,\u00a0\u00a0 this should be possible and is subject to\u00a0 some research attention in computer graphics.\u00a0\u00a0 The animation was given, but the sounds\u00a0 were algorithmically generated. Listen!\u00a0\u00a0 Let me know in the comments if you are one\u00a0 of our OG Fellow Scholars who were there\u00a0\u00a0 when this episode was published\u00a0 hundreds of videos ago. So, how much do we have to wait to simulate\u00a0 all of these crazy physical interactions?\u00a0\u00a0 We mentioned that the tires are stiff and take a\u00a0 great deal of computation to simulate properly.\u00a0\u00a0 So, as always\u2026 all nighters, right? Nope! Look at\u00a0 that! Holy mother of papers! The car example takes\u00a0\u00a0 only 18 milliseconds to compute per frame,\u00a0 which means 55 frames per second. Goodness!\u00a0\u00a0 Not only do we not need an all-nighter, we\u00a0 don\u2019t even need to leave for a coffee break!\u00a0\u00a0 And the rolling marbles took even less, and,\u00a0 wo-hoo! The high-frequency coin example needs\u00a0\u00a0 only one third of a millisecond, which means that\u00a0 we can generate more than 3000 frames with it per\u00a0\u00a0 second. We not only don\u2019t need an all nighter or\u00a0 a coffee break, we don\u2019t even need to wait at all! Now, at the start of the video, I noted that the\u00a0 claim in the abstract sounds almost outrageous.\u00a0\u00a0 It is because it promises to be able to do more\u00a0 than previous techniques, simplify the simulation\u00a0\u00a0 algorithm itself, make it more robust, and do all\u00a0 this while being blazing fast. If someone told me\u00a0\u00a0 that there is a work that does all this at the\u00a0 same time, I would say that give me that paper\u00a0\u00a0 immediately because I do not believe a word of\u00a0 it. And yet, it really lives up to its promise. Typically, as a research field matures, we see new\u00a0 techniques that can do more than previous methods,\u00a0\u00a0 but the price to be paid for it is in the form\u00a0 of complexity. The algorithms get more and more\u00a0\u00a0 involved over time, and with that, they often\u00a0 get slower and less robust. The engineers in the\u00a0\u00a0 industry have to decide how much complexity they\u00a0 are willing to shoulder to be able to simulate all\u00a0\u00a0 of these beautiful interactions. Don\u2019t forget,\u00a0 these code bases have to be maintained and\u00a0\u00a0 improved for many-many years so choosing a simple\u00a0 base algorithm is of utmost importance. But here,\u00a0\u00a0 none of these factors need to be considered,\u00a0 because there is nearly no tradeoff here:\u00a0\u00a0 it is simpler, more robust,\u00a0 and better at the same time.\u00a0\u00a0 It really feels like we are living in a\u00a0 science fiction world. What a time to be alive! Huge congratulations to scientists at NVIDIA\u00a0 and the university of Copenhagen for this.\u00a0\u00a0 Don\u2019t forget, they could have kept the results\u00a0 for themselves, but they chose to share the\u00a0\u00a0 details of this algorithm with everyone, free\u00a0 of charge. Thank you so much for doing this. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=F0QwAhUnpr4",
        "paper_link": "https://matthias-research.github.io/pages/publications/PBDBodies.pdf",
        "paper_title": "Detailed Rigid Body Simulation with Extended Position Based Dynamics"
    },
    {
        "video_id": "ooZ9rUYOFI4",
        "video_title": "Simulating Dragons Under Cloth Sheets! \ud83d\udc32",
        "position_in_playlist": 477,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/stacey/droughtwatch/reports/Drought-Watch-Benchmark-Progress--Vmlldzo3ODQ3OQ\n\n\ud83d\udcdd The paper \"Local Optimization for Robust Signed Distance Field Collision\" is available here:\nhttps://mmacklin.com/sdfcontact.pdf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Sunil Kim, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, we are going to see a lot of physics\u00a0 simulations with many-many collisions. In\u00a0\u00a0 particular, you will see a lot of beautiful\u00a0 footage which contains contact between thin\u00a0\u00a0 shells and rigid bodies. In this simulation\u00a0 program, at least one of these objects will\u00a0\u00a0 always be represented as a signed distance field.\u00a0 This representation is useful because it helps us\u00a0\u00a0 rapidly compute whether something\u00a0 is inside or outside of this object. However, a collision here takes two objects, of\u00a0 course, so the other object will be represented\u00a0\u00a0 as a triangle mesh, which is perhaps the\u00a0 most common way of storing object geometry in\u00a0\u00a0 computer graphics. However, with this, we have\u00a0 a problem. Signed distance fields work great,\u00a0\u00a0 triangle meshes also work great for a number\u00a0 of applications, however, computing where they\u00a0\u00a0 overlap when they collide is still slow and\u00a0 difficult. If that does not sound bad enough,\u00a0\u00a0 it gets even worse than that. How so? Let\u2019s\u00a0 have a look together. Experiment number one.\u00a0\u00a0 Let\u2019s try to intersect this cone with this\u00a0 rubbery sheet using a traditional technique,\u00a0\u00a0 and there is only one rule - no poking through\u00a0 is allowed. Well, guess what just happened. This\u00a0\u00a0 earlier technique is called point sampling,\u00a0 and we either have to check too many points\u00a0\u00a0 in the two geometries against each other,\u00a0 which takes way too long and still fails, or,\u00a0\u00a0 we skimp on some of them, but then, this happens.\u00a0 Important contact points go missing. Not good. Let\u2019s see how this new method does with\u00a0 this case. Now that\u2019s what I am talking\u00a0\u00a0 about! No poking through anywhere to\u00a0 be seen. And, let\u2019s have another look.\u00a0\u00a0 Wait a second. Are you seeing what I am seeing?\u00a0 Look at this part\u2026after the first collision,\u00a0\u00a0 the contact points are moving ever so slightly,\u00a0 many-many times, and the new method is not missing\u00a0\u00a0 any of them. Then, things get a little out of\u00a0 hand, and it still works perfectly. Amazing!\u00a0\u00a0 I can only imagine how many of these\u00a0 interactions the previous point sampling\u00a0\u00a0 technique would miss, but we won\u2019t know\u00a0 because it has already failed long ago. Let\u2019s do another one. Experiment number two.\u00a0 Dragon versus cloth sheet. This is the previous\u00a0\u00a0 point sampling method. We now see that it can\u00a0 find some of the interactions, but many others\u00a0\u00a0 go missing, and due to the anomalies, we can\u2019t\u00a0 continue the animation by pulling the cloth sheet\u00a0\u00a0 off of the dragon because it is stuck. Let\u2019s see\u00a0 how the new method fared in this case. Oh yeah!\u00a0\u00a0 Nothing pokes through, and therefore, now, we can\u00a0 continue the animation by doing this. Excellent! Experiment number three! Rope curtain. Point\u00a0 sampling. Oh no! This is a disaster. And now,\u00a0\u00a0 hold on to your papers, and marvel at the new\u00a0 proposed method. Just look at how beautifully\u00a0\u00a0 we can pull the rope curtain through this fine\u00a0 geometry. Loving it. So, of course, if you are\u00a0\u00a0 a seasoned fellow scholar, you probably want\u00a0 to know, how much computation do we have to do\u00a0\u00a0 with the old and new methods. How much longer do\u00a0 I have to wait for the new, improved technique? Let\u2019s have a look together! Each rigid shell here\u00a0 is a mesh that uses a 129 thousand triangles, and\u00a0\u00a0 the old point sampling method took 15 milliseconds\u00a0 to compute the collisions, and this time, it has\u00a0\u00a0 done reasonably well. What about the new one?\u00a0 How much more computation do we have to perform\u00a0\u00a0 to make sure that our simulations are more robust?\u00a0 Please stop the video and make a guess. I\u2019ll wait.\u00a0\u00a0 Alright, let\u2019s see\u2026and, the new one does it in\u00a0 half a millisecond. Half a millisecond. It is\u00a0\u00a0 not slower at all, quite the opposite - thirty\u00a0 times faster. My goodness! Huge congratulations\u00a0\u00a0 on yet another masterpiece from scientists\u00a0 at NVIDIA and the University of Copenhagen. While we look at some more results, in case\u00a0 you are wondering, the authors used NVIDIA\u2019s\u00a0\u00a0 Omniverse platform to create these amazing\u00a0 rendered worlds. And now, with this new method,\u00a0\u00a0 we can infuse our physics simulation programs with\u00a0 a robust, and blazing fast collision detector, and\u00a0\u00a0 I truly can\u2019t wait to see where talented artists\u00a0 will take these tools. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ooZ9rUYOFI4",
        "paper_link": "https://mmacklin.com/sdfcontact.pdf",
        "paper_title": "Local Optimization for Robust Signed Distance Field Collision"
    },
    {
        "video_id": "Jy_VZQnZqGk",
        "video_title": "This AI Creates A 3D Model of You! \ud83d\udeb6\u200d\u2640\ufe0f",
        "position_in_playlist": 478,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/stacey/deep-drive/reports/Image-Masks-for-Semantic-Segmentation--Vmlldzo4MTUwMw\n\n\ud83d\udcdd The paper \"PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization\" is available here:\nhttps://shunsukesaito.github.io/PIFuHD/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, a variety of techniques exist that can take an image that contains humans, and perform pose estimation on it. This gives us these interesting skeletons that show us the current posture of the subjects shown in these images. Having this skeleton opens up the possibility for many cool applications, for instance, it\u2019s great for fall detection and generally many kinds of activity recognition, analyzing athletic performance and much, much more. But that would require that we can do it for not only still images, but animations. Can we? Yes, we already can, this is a piece of footage from a previous episode that does exactly that. But what if we wish for more? Let\u2019s think bigger, for instance, can we reconstruct not only the pose of the model, but the entire 3D geometry of the model itself? You know, including the body shape, face, clothes, and more. That sounds like science fiction, right? Or with today\u2019s powerful learning algorithms, maybe it is finally a possibility, who really knows? Let\u2019s have a look together and evaluate it with three, increasingly more difficult experiments. Let\u2019s start with experiment number one, still images. Nice! I think if I knew these people, I might have a shot at recognizing them solely from the 3D reconstruction. And not only that, but I also see some detail in the clothes, a suit can be recognized, and jeans have wrinkles. This new method uses a different geometry representation that enables higher-resolution outputs, and it immediately shows. Checkmark. It is clearly working quite well on still images. And now, hold on to your papers for experiment number two, because it can not only deal with still images of the front side only, but it can also reconstruct the backside of the person. Look! My goodness, but hold on for a second\u2026that part of the data is completely unobserved. We haven\u2019t seen the backside\u2026so, how is that even possible? Well, we have to shift our thinking a little. An intelligent person would be able to infer some of these details, for instance, we know that this is a suit, or that these are boots, and we know roughly what the backside of these objects should look like. This new method leans on an earlier technique by the name image to image translation to estimate this data. And it truly works like magic! If you take a closer look, you see that we have less detail in the backside than in the front, but the fact that we can do this is truly a miracle. But we can go even further. I know it is not reasonable to ask, but what about video reconstruction? Let\u2019s have a look. Don\u2019t expect miracles, at least not yet, there is obviously still quite a bit of flickering left, but the preliminary results are quite encouraging, and I am fairly certain that two more papers down the line, and these video results will be nearly as good as the ones were for the still images. The key idea here is that the new method performs these reconstructions in a way that is consistent, or in other words, if there is a small change in the input model, there will also be a small change in the output model. This is the property that opens up the possibility to extend this method to videos! So, how does it compare to previous methods? All of these competing techniques are quite recent as they are from 2019. They appear to be missing a lot of detail, and I don\u2019t think we would have a chance of recognizing the target subject from the reconstructions. And now, just a year and a half later, look at that incredible progress! It truly feels like we are living in a science fiction world. What  time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Jy_VZQnZqGk",
        "paper_link": "https://shunsukesaito.github.io/PIFuHD/",
        "paper_title": "PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization"
    },
    {
        "video_id": "lLa9DUiJICk",
        "video_title": "Making Talking Memes With Voice DeepFakes!",
        "position_in_playlist": 479,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Wav2Lip: Accurately Lip-syncing Videos In The Wild\" is available here:\n- Paper: https://arxiv.org/abs/2008.10010\n- Try it out! - https://github.com/Rudrabha/Wav2Lip\n\nMore results are available on our Instagram page! -  https://www.instagram.com/twominutepapers/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, hold on to your papers, because we have an emergency situation. Everybody can make deepfakes now by recording a voice sample, such as this one, and the lips of the target subject will move as if they themselves were saying this. Not bad huh? And now to see what else this new method can do, first, let\u2019s watch this short clip of a speech, and make sure to pay attention to the fact that the louder voice is the English translator, and if you pay attention, you can hear the chancellor\u2019s original voice in the background too. So what is the problem here? Strictly speaking, there is no problem here, this is just the way the speech was recorded, however, what if we could recreate this video in a way that the chancellor\u2019s lips would be synced not to her own voice, but to the voice of the English interpreter? This would give an impression as if the speech was given in English, and the video content would follow what we hear. Now that sounds like something straight out of a science fiction movie, perhaps even with today\u2019s advanced machine learning techniques, but let\u2019s see if it\u2019s possible. This is a state of the art technique from last year that attempts to perform this. Hmm\u2026there are extraneous lip movements which are the remnants of the original video, so much so that she seems to be giving two speeches at the same time. Not too convincing. So, is this not possible to pull off? Well, now, hold on to your papers, let\u2019s see how this new paper does at the same problem. Wow, now that\u2019s significantly better! The remnants of the previous speech are still there, but the footage is much, much more convincing. What\u2019s even better is that the previous technique was published just one year ago by the same research group. Such a great leap in just one year, my goodness! So apparently, this is possible. But I would like to see another example, just to make sure. Checkmark. So far, this is an amazing leap, but believe it or not, this is just one of the easier applications of the new model, so let\u2019s see what else it can do! For instance, many of us are sitting at home, yearning for some learning materials, but the vast majority of these were recorded in only one language. What if we could redub famous lectures into many other languages? Look at that! Any lecture could be available in any language and look as if they were originally recorded in these foreign languages as long as someone says the words. Which can also be kind of automated through speech synthesis these days. So, it clearly works well on real characters\u2026but. Are you thinking what I am thinking? Three, what about lip syncing animated characters? Imagine if a line has to be changed in a Disney movie, can we synthesize new video footage without calling in the animators for a yet another all-nighter? Let\u2019s give it a try! Indeed we can! Loving it! Let\u2019s do one more. Four, of course, we have a lot of these meme gifs on the internet. What about redubbing those with an arbitrary line of our choice? Yup, that is indeed also possible. Well done! And imagine that this is such a leap just one more work down the line from the 2019 paper, I can only imagine what results we will see one more paper down the line. It not only does what it does better, but it can also be applied to a multitude of problems. What a time to be alive! When we look under the hood, we see that the two key components that enable this wizardry are here and here. So what does this mean exactly? It means that we jointly improve the quality of the lip syncing, and the visual quality of the video. These two modules curate the results offered by the main generator neural network, and reject solutions that don\u2019t have enough detail or don\u2019t match the speech that we hear, and thereby they steer it towards much higher-quality solutions. If we continue this training process for 29 hours for the lip-sync discriminator, we get these incredible results. Now, let\u2019s have a quick look at the user study, and, humans appear to almost never prefer the older method compared to this one. I tend to agree. If you consider these forgeries to be deepfakes, then\u2026 there you go! Useful deepfakes that can potentially help people around the world stranded at home to study and improve themselves. Imagine what good this could do! Well done! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lLa9DUiJICk",
        "paper_link": "https://arxiv.org/abs/2008.10010",
        "paper_title": "Wav2Lip: Accurately Lip-syncing Videos In The Wild"
    },
    {
        "video_id": "tWu0AWdaTTs",
        "video_title": "This AI Makes Puzzle Solving Look Easy! \ud83e\udde9",
        "position_in_playlist": 480,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://app.wandb.ai/lavanyashukla/visualize-sklearn/reports/Visualize-Sklearn-Model-Performance--Vmlldzo0ODIzNg\n\n\ud83d\udcdd The paper \"C-Space Tunnel Discovery for Puzzle Path Planning\" is available here:\nhttps://xinyazhang.gitlab.io/puzzletunneldiscovery/\nhttps://github.com/xinyazhang/PuzzleTunnelDiscovery\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to immerse ourselves into a wonderful art of rigid body disentanglement, or in simpler words, we are going to solve these puzzles! To be more exact, we\u2019ll sit and enjoy our time while a learning-based algorithm is going to miraculously solve some really challenging puzzles. Yes, this is going to be a spicy paper. Now, these puzzles are roughly ordered by difficulty, where the easier ones are on the left, and as we traverse to the right, things get more and more difficult. And get this, this new technique is able to algorithmically solve all of them. If you are like me and you don\u2019t believe a word of what\u2019s been said, let\u2019s see if it lives up to its promise by solving three of them, in increasing difficulty. Let\u2019s start with an easy one. Example number one. Here, the algorithm recognizes that we need to pull the circular part of the red piece through the blue one and apply the appropriate rotations to make sure that we don\u2019t get stuck during this process. While we finish this sequence, please note that this video contains spoilers for some well-known puzzles. If you wish to experience them yourself, pause this video, and, I guess, buy and try them. This one was good enough to warm ourselves up, so let\u2019s hop on to the next one. Example number two, the duet puzzle. Well that was quite a bit of a jump in difficulty because we seem stuck right at the start. Hmm\u2026 this seems flat out impossible\u2026until the algorithm recognizes that there are these small notches in the puzzle, and if we rotate the red piece correctly, we may go from one cell to the next one. Great, so now it\u2019s not impossible anymore, it is more like a maze that we have to climb through. But the challenges are still not over. Where is the endpoint? How do we finish this puzzle? There has to be a notch on the side\u2026yes, there is this one or this one, so we ultimately have to end up in one of these places\u2026and, there we go. Bravo! Experiment number three. My favorite, the enigma. Hmm..this is exactly the opposite of the previous one. It looks so easy! Just get the tiny opening of the red piece onto this part of the blue one, and we are done. Uh-oh. The curved part is in the way. Something that seemed so easy now suddenly seems absolutely impossible! But fortunately, this learning algorithm does not get discouraged, and does not know what impossible is, and it finds this tricky series of rotations to go around the entirety of the blue piece, and then, finish the puzzle. Glorious. What a rollercoaster! A hallmark of an expertly designed puzzle. And to experience more of these puzzles, make sure to have a look paper and its website with a super fun interactive app. If you do, you will also learn really cool new things\u2026for instance, if you get back home in the middle of the night after a long day, and two of your keys are stuck together, you will know exactly how to handle it. And all this can be done through the power of machine learning and computer graphics research. What a time to be alive! So how does this wizardry work exactly? The key techniques here are tunnel discovery and path planning. First, a neural network looks at the puzzle and identifies where the gaps and notches are, and specifies the starting position and the goal position that we need to achieve to finish the puzzle. Then, a set of collision-free key configurations are identified, after which, the blooming step can commence. So what does that do? Well, the goal is to be able to go through these narrow tunnels that represent tricky steps in the puzzles that typically require some unintuitive rotations. These are typically the most challenging parts of the puzzles, and the blooming step starts from these narrow tunnels and helps us reach the bigger bubbles of the puzzle. But as you see, not all roads connect, or at least, not easily. The forest connect step tries to connect these roads through collision-free paths, and now, finally, all we have to do is find the shortest path from the start to the endpoint to solve the puzzle. And also, according to my internal numbering system, this is Two Minute Papers episode number 478. And to every single one of you Fellow Scholars who are watching this, thank you so much to all of you for being with us for so long on this incredible journey. Man, I love my job, and I jump out of bed full of energy and joy knowing that I get to read research papers and flip out together with many of you Fellow Scholars on a regular basis. Thank you. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=tWu0AWdaTTs",
        "paper_link": "https://xinyazhang.gitlab.io/puzzletunneldiscovery/",
        "paper_title": "C-Space Tunnel Discovery for Puzzle Path Planning"
    },
    {
        "video_id": "atzPvW95ahQ",
        "video_title": "Is Videoconferencing With Smart Glasses Possible? \ud83d\udc53",
        "position_in_playlist": 481,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/egocentric-video-conferencing/reports/Overview-Egocentric-Videoconferencing--VmlldzozMTY1NTA\n\n\ud83d\udcdd The paper \"Egocentric Videoconferencing\" is available here:\nhttp://gvv.mpi-inf.mpg.de/projects/EgoChat/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-820390/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to have a look at the\u00a0 state of egocentric videoconferencing. Now\u00a0\u00a0 this doesn\u2019t mean that only we get to speak during\u00a0 a meeting, it means that we are wearing a camera,\u00a0\u00a0 which looks like this, and the goal is to use\u00a0 a learning algorithm to synthesize this frontal\u00a0\u00a0 view of us. Now note that what you see here is\u00a0 the recorded reference footage, this is reality,\u00a0\u00a0 and this would need to be somehow synthesized\u00a0 by the algorithm. If we could pull that off,\u00a0\u00a0 we could add a low-cost egocentric camera to smart\u00a0 glasses, and it could pretend to see us from the\u00a0\u00a0 front, which would be amazing for hands-free\u00a0 videoconferencing. That would be insanity. But wait a second. How is this even possible?\u00a0\u00a0 For us to even have a fighting chance, there\u00a0 are four major problems to overcome here. One, this camera lens is very close to\u00a0 us, which means that it doesn\u2019t see the\u00a0\u00a0 entirety of the face. That sounds extremely\u00a0 challenging. And if that wasn\u2019t bad enough,\u00a0\u00a0 two, we also have tons of distortion\u00a0 in the images, or in other words,\u00a0\u00a0 things don\u2019t look like they look in reality,\u00a0 we would have to account for that too. Three,\u00a0\u00a0 it would also have to take into account\u00a0 our current expression, gaze, blinking,\u00a0\u00a0 and more. Oh boy. And finally, four, the output\u00a0 needs not be photorealistic, even better,\u00a0\u00a0 videorealistic. Remember, we don\u2019t just need one\u00a0 image, but a continuously moving video output. So the problem is, once\u00a0 again, input, egocentric view,\u00a0\u00a0 output, synthesized frontal view. This is\u00a0 the reference footage, reality if you will,\u00a0\u00a0 and now, let\u2019s see how this learning-based\u00a0 algorithm is able to reconstruct it.\u00a0\u00a0 Um\u2026hello? Is this a mistake? They look\u00a0 identical, as if they were just copied here.\u00a0\u00a0 No, you will see in a moment that it\u2019s not a\u00a0 mistake, this means that the AI is giving us a\u00a0\u00a0 nearly perfect reconstruction of the remainder\u00a0 of the human face. That is absolutely amazing. Now, it is still not perfect, there are some\u00a0 differences. So how do we get a good feel\u00a0\u00a0 of where the inaccuracies are? The answer is a\u00a0 difference image, look. Regions with warmer colors\u00a0\u00a0 indicate where the reconstruction is inaccurate\u00a0 compared to the real reference footage.\u00a0\u00a0 For instance, with an earlier method by the name\u00a0 pix2pix, the hair and the beard are doing fine,\u00a0\u00a0 while we have quite a bit of reconstruction\u00a0 error on the remainder of the face.\u00a0\u00a0 So, did the new method do better than\u00a0 this? Let\u2019s have a look together. Oh yeah!\u00a0\u00a0 It does much better across the entirety of\u00a0 the face. It still has some trouble with the\u00a0\u00a0 cable and the glasses, but otherwise,\u00a0 this is a clean, clean image. Bravo! Now, we talked about the challenge of\u00a0 reconstructing expressions correctly.\u00a0\u00a0 To be able to read the other person is of\u00a0 utmost importance during a video conference.\u00a0\u00a0 So how good is it at gestures? Well, let\u2019s put it through an intense\u00a0 stress test! Well, this is as intense as\u00a0\u00a0 it gets without having access to Jim\u00a0 Carrey as a test subject I suppose,\u00a0\u00a0 and I bet there was a lot of fun to be had in the\u00a0 lab on this day. And the results are outstanding,\u00a0\u00a0 especially if we compare it again\u00a0 to the pix2pix technique from 2017. I love this idea, because if we can overcome\u00a0 the huge shortcomings of the egocentric camera,\u00a0\u00a0 in return, we get an excellent view of subtle\u00a0 facial expressions and can deal with the tiniest\u00a0\u00a0 eye movements, twitches, tongue movements,\u00a0 and more. And it really shows in the results. Now please note that this technique needs to be\u00a0 trained on each of these test subjects. About\u00a0\u00a0 four minutes of video footage is fine and this\u00a0 calibration process only needs to be done once.\u00a0\u00a0 So, once again, the technique knows\u00a0 these people and had seen them before. But in return, it can do even more. If all of this\u00a0 is synthesized, we have a lot of control over this\u00a0\u00a0 data and the AI understands what much of this data\u00a0 means. So with all that extra knowledge, what else\u00a0\u00a0 can we do with this footage? For instance, we can\u00a0 not just reconstruct, but create arbitrary head\u00a0\u00a0 movement. We can guess what the real head movement\u00a0 is because we have a view of the background,\u00a0\u00a0 we can simply remove it, or from the movement of\u00a0 the background, we can infer what kind of head\u00a0\u00a0 movement is taking place. And what\u2019s even better,\u00a0 we can not only get control over the head movement\u00a0\u00a0 and change it, but even remove the movement\u00a0 from the footage altogether. And, we can also\u00a0\u00a0 remove the glasses and pretend to have dressed\u00a0 properly for an occasion. How cool is that? Now make no mistake, the paper contains a\u00a0 ton of comparisons against a variety of other\u00a0\u00a0 works as well, here are some, but make sure to\u00a0 check them all out in the video description. Now, of course, even this new method isn\u2019t\u00a0 perfect, for instance, it does not work all\u00a0\u00a0 that well in low-light situations, but of course,\u00a0 let\u2019s leave something to improve for the next\u00a0\u00a0 paper down the line. And hopefully, in the near\u00a0 future, we will be able to seamlessly get in\u00a0\u00a0 contact with our loved ones through smart glasses\u00a0 and egocentric cameras. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=atzPvW95ahQ",
        "paper_link": "http://gvv.mpi-inf.mpg.de/projects/EgoChat/",
        "paper_title": "Egocentric Videoconferencing"
    },
    {
        "video_id": "vfJz7WlRNk4",
        "video_title": "Near-Perfect Virtual Hands For Virtual Reality! \ud83d\udc50",
        "position_in_playlist": 482,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\ud83d\udcdd The paper \"MEgATrack: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality\" is available here:\nhttps://research.fb.com/publications/megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-4949333/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. The promise of virtual reality, VR is indeed\u00a0 truly incredible. If one day it comes to fruition,\u00a0\u00a0 doctors could be trained to perform surgery in a\u00a0 virtual environment, we could train better pilots\u00a0\u00a0 with better flight simulators, expose astronauts\u00a0 to virtual zero-gravity simulations, you name it. An important part of doing many of these is\u00a0 simulating walking in a virtual environment.\u00a0\u00a0 You see, we can be located in a small room,\u00a0 put on a VR headset and enter a wonderful,\u00a0\u00a0 expansive virtual world. However, as we\u00a0 start walking, we immediately experience\u00a0\u00a0 a big problem. What is that problem?\u00a0 Well, we bump into things. As a remedy,\u00a0\u00a0 we could make our virtual world smaller,\u00a0 but that would defeat the purpose.\u00a0\u00a0 This earlier technique addresses this\u00a0 walking problem spectacularly by redirection. So, what is this redirection thing exactly?\u00a0 Redirection is a simple concept that changes\u00a0\u00a0 our movement in the virtual world so it deviates\u00a0 from our real path in the room in a way that both\u00a0\u00a0 lets us explore the virtual world, and not bump\u00a0 into walls and objects in reality in the meantime.\u00a0\u00a0 Here you can see how the blue and orange lines\u00a0 deviate, which means that the algorithm is at\u00a0\u00a0 work. With this, we can wander about in\u00a0 a huge and majestic virtual landscape or\u00a0\u00a0 a cramped bar, even when being confined\u00a0 to a small physical room. Loving the idea. But there is more to interacting with\u00a0 virtual worlds than walking, for instance,\u00a0\u00a0 look at this tech demo that requires more\u00a0 precise hand movements. How do we perform these?\u00a0\u00a0 Well, the key is here. Controllers!\u00a0 Clearly, they work, but can we get\u00a0\u00a0 rid of them? Can we just opt for a more\u00a0 natural solution and use our hands instead? Well, hold on to your papers, because this\u00a0 new work uses a learning-based algorithm\u00a0\u00a0 to teach a head-mounted camera to tell the\u00a0 orientation of our hands at all times. Of course,\u00a0\u00a0 the quality of the execution matters a great\u00a0 deal, so we have to ensure at least three things. One is that the hand tracking\u00a0 happens with minimal latency,\u00a0\u00a0 which means that we see our actions\u00a0 immediately, with minimal delay. Two, we need low jitter, which means that\u00a0 the keypoints of the reconstructed hand\u00a0\u00a0 should not change too much from frame to frame.\u00a0 This happens a great deal with previous methods,\u00a0\u00a0 and what about the new one? Oh\u00a0 yes, much smoother. Checkmark! Note that the new method also remembers\u00a0 the history of the hand movement,\u00a0\u00a0 and therefore can deal with difficult occlusion\u00a0 situations. For instance, look at the pinky here!\u00a0\u00a0 A previous technique would not know what\u2019s going\u00a0 on with it, but, this new one knows exactly what\u00a0\u00a0 is going on because it has information\u00a0 on what the hand was doing a moment ago. And three, this needs to work in all kinds\u00a0 of lighting conditions. Let\u2019s see if it can\u00a0\u00a0 reconstruct a range of mythical creatures in\u00a0 poor lighting conditions. Yes, these ducks\u00a0\u00a0 are reconstructed just as well as the mighty\u00a0 pokemonster, and, these scissors too. Bravo! So, what can we do with this? A great deal. For\u00a0 instance, we can type on a virtual keyboard,\u00a0\u00a0 or implement all kinds of virtual user\u00a0 interfaces that we can interact with.\u00a0\u00a0 We can also organize imaginary boxes, and\u00a0 of course, we can\u2019t leave out the the Two\u00a0\u00a0 Minute Papers favorite, going into a\u00a0 physics simulation and playing with it. But of course, not everything\u00a0 is perfect here, however. Look.\u00a0\u00a0 Hand-hand interactions don\u2019t work so well, so\u00a0 folks who prefer virtual reality applications\u00a0\u00a0 that include washing our hands should look\u00a0 elsewhere. But of course, one step at a time. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=vfJz7WlRNk4",
        "paper_link": "https://research.fb.com/publications/megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality/",
        "paper_title": "MEgATrack: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality"
    },
    {
        "video_id": "knIzDj1Ocoo",
        "video_title": "This Blind Robot Learned To Climb Any Terrain! \ud83e\udd16",
        "position_in_playlist": 483,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Debug-Compare-Reproduce-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly\n\n\ud83d\udcdd The paper \"Learning Quadrupedal Locomotion over Challenging Terrain \" is available here:\nhttps://leggedrobotics.github.io/rl-blindloco/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#robots",
        "transcript": "the fellow scholars this is too many papers with Dr Connors on anything what you see here is one of my favorite kind of paper where we train a robot in a software simulation and then we try to bring it into the real world and see if it can still navigate there the simulation is the textbook in the real world is the exam if you will and it's a really hard one so let's see if this one passes or fails now the real world is messy and full of obstacles so to even have a fighting chance in training a robot to deal with that the simulation better has those properties too so let's add some heels steps and stairs and let's see if it can overcome these  well not at first as you see the agent is very clumsy and can barely walk through a simple terrain  but as time passes it grows to be a little more confident and with that the terrain also progressively becomes more and more difficult in order to maximize learning that is a great life lesson right there now well look at the store in experiments you can start holding onto your papers because this robot knows absolutely nothing about the outside world it has no traditional cameras no radar no lidar no depth sensors no no no none of that only proprioceptive sensors are allowed which means that the only thing that the robot senses is its own internal state and that's it whoa for instance it knows about its orientation and twist of the base unit a little joint information like positions and velocities and really not that much more all of which is pro price active information yep that's it and very awful it is doing quite well in the simulation however reality is never quite the same as the simulations so I wonder if what was learned here can be used there let's see  wow look at how well it traverses through this rocky mountain  stream  and not even this night may slowly descend gives it too much trouble  it works even if it cannot get the proper foothold and it is sleeping all the time and it also learned to engage in this adorable jumpy behavior when stock investigation and it learned all this by itself absolute witchcraft when looking at this table we now understand that he still has a reasonable speeds through moss and why it is lower in vegetation then in mind really cool  if you have been holding onto your papers so far now squeeze that paper because if all that wasn't hard enough let's add an additional ten kilogram or twenty two pound payload and see if it can shoulder it and let's be honest this should not happen  wow look at that it can not only shoulder it but it also adjusts its movement today changes to its own internal physics to accomplish this it uses a concept that is called learning by cheating or the teacher student architecture when the agent that learned in the simulator will be the teacher  now after we escaped the simulator into the real world we cannot lean on the simulator anymore to try to see what works so then what do we do  well here the teacher takes the place of the simulator it distills its knowledge down to train the student further if both of them do their jobs really well the students who have successfully passed the exam with flying colors as you see it is exactly the case here  this is an outrageously good paper what a time to be alive this episode has been supported by weights and biases indisposed they show you how to use their reports to explain how your model works show plots of how model versions improved discuss bogs and demonstrate progress towards milestones if you work with learning algorithms on a regular basis make sure to check out weights and biases their system is designed to help you organize your experiments and it is so good it could shave off weeks or even months of work from your projects and is completely free for all individuals academics and open source project this really is as good as it gets and it is hardly a surprise that the are now used by over two hundred companies and research institutions make sure to visit them through W. N. B. dot com slash papers or just click the link in the video description and you can get a free demo today I thank the weights and biases for the long standing support and for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=knIzDj1Ocoo",
        "paper_link": "https://leggedrobotics.github.io/rl-blindloco/",
        "paper_title": "Learning Quadrupedal Locomotion over Challenging Terrain "
    },
    {
        "video_id": "fPrxiRceAac",
        "video_title": "These Are Pixels Made of Wood! \ud83c\udf32\ud83e\udde9",
        "position_in_playlist": 484,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\ud83d\udcdd The paper \"Computational Parquetry: Fabricated Style Transfer with Wood Pixels\" is available here:\nhttps://light.informatik.uni-bonn.de/computational-parquetry-fabricated-style-transfer-with-wood-pixels/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Everybody loves style transfer.\u00a0 This is a task typically done\u00a0\u00a0 with neural networks where we have\u00a0 two images, one for content, and one\u00a0\u00a0 for style, and the output is the content image\u00a0 reimagined with this new style. The cool thing\u00a0\u00a0 is that the style can be a different photo,\u00a0 a famous painting, or even, wooden patterns. Feast your eyes on these majestic images of\u00a0 this cat reimagined with wooden parquetry\u00a0\u00a0 with these previous methods. And now, look at\u00a0 the result of this new technique, that looks\u00a0\u00a0 way nicer. Everything is in order here, except one\u00a0 thing. And now, hold on to your papers, because\u00a0\u00a0 this is not style transfer. Not at all. This is\u00a0 not a synthetic photo made by a neural network,\u00a0\u00a0 this is a reproduction of this cat image\u00a0 by cutting wood slabs into tiny pieces\u00a0\u00a0 and putting them together carefully. This\u00a0 is computational parquetry. And here,\u00a0\u00a0 the key requirement is that if we look from afar,\u00a0 it looks like the target image, but if we zoom in,\u00a0\u00a0 it gets abundantly clear that the puzzle\u00a0 pieces here are indeed made of real wood. And that is an excellent intuition for this\u00a0 work. It is kind of like image stylization,\u00a0\u00a0 but done in the real world. Now that is extremely challenging. Why is that?\u00a0 Well, first, there are lots of different kinds\u00a0\u00a0 of wood types. Second, if this piece was not a\u00a0 physical object but an image, this job would not\u00a0\u00a0 be that hard because we could add to it, clone it,\u00a0 and do all kinds of pixel magic to it. However,\u00a0\u00a0 these are real, physical pieces of wood,\u00a0 so we can do exactly none of that. The only\u00a0\u00a0 thing we can do is take away from it,\u00a0 and we have limitations even on that,\u00a0\u00a0 because we have to design it in a way that\u00a0 a CNC device should be able to cut these\u00a0\u00a0 pieces. And third, you will see that\u00a0 initially, nothing seems to work well.\u00a0\u00a0 However, this technique does this with flying\u00a0 colors, so I wonder, how does this really work? First, we can take a photo of the wood panels that\u00a0 we have our disposal, decide how and where to cut,\u00a0\u00a0 give these instructions to the CNC\u00a0 machine to perform the cutting,\u00a0\u00a0 and now, we have to assemble them in a way that\u00a0 it resembles the target image. Well, still, that\u2019s\u00a0\u00a0 easier said than done. For instance, imagine\u00a0 that we have this target image, and we have\u00a0\u00a0 these wood panels. This doesn\u2019t look anything like\u00a0 that, so how could we possibly approximate it? If we try to match the colors of the two, we\u00a0 get something that is too much in the middle,\u00a0\u00a0 and the colors don\u2019t resemble any\u00a0 of the original inputs. Not good.\u00a0\u00a0 Instead, the authors opted to transform both\u00a0 of them to grayscale, and match not the colors,\u00a0\u00a0 but the intensities of the colors instead. This\u00a0 seems a little more usable\u2026until we realize\u00a0\u00a0 that we still don\u2019t know what\u00a0 pieces to use and where. Look. Here, on the left, you see how the image\u00a0 is being reproduced with the wood pieces,\u00a0\u00a0 but we have to mind the fact that as\u00a0 soon as we cut out one piece of wood,\u00a0\u00a0 it is not available anymore, so it has to be\u00a0 subtracted from our wood panel repository here.\u00a0\u00a0 As our resources are constrained, depending on\u00a0 what order we put the pieces together, we may\u00a0\u00a0 get a completely different result. But look. There\u00a0 is still a problem\u2026the left part of the suit gets\u00a0\u00a0 a lot of detail, while the right part, not so\u00a0 much. I cannot judge which solution is better,\u00a0\u00a0 less or more detail, but it needs to be\u00a0 a little more consistent over the image.\u00a0\u00a0 Now you see that whatever we do, nothing\u00a0 seems to work well in the general case. Now, we could get a much better solution\u00a0 if we would run the algorithm with\u00a0\u00a0 every possible starting point in the image, and\u00a0 with every possible ordering of the wood pieces,\u00a0\u00a0 but that would take longer\u00a0 than our lifetime to finish,\u00a0\u00a0 so what do we do? Well, the authors have two\u00a0 really cool heuristics to address this problem.\u00a0\u00a0 First, we can start from the middle, that usually\u00a0 gives us a reasonably good solution, since the\u00a0\u00a0 object of interest is often in the middle of the\u00a0 image and the good pieces are still available for\u00a0\u00a0 it. Or, even better, if that does not work\u00a0 too well, we can look for salient regions,\u00a0\u00a0 these are the places where there is a lot going\u00a0 on, and try to fill them in first. As you see,\u00a0\u00a0 both of these tricks seem to work quite well\u00a0 most of the time. Finally, something that works. And if you have been holding on to your papers,\u00a0 now squeeze that paper, because this technique\u00a0\u00a0 not only works, but provides us a great deal of\u00a0 artistic control over the results. Look at that!\u00a0\u00a0 And that\u2019s not all, we can even\u00a0 control the resolution of the output,\u00a0\u00a0 or, we can create a hand-drawn geometry\u00a0 ourselves. I love how the authors took\u00a0\u00a0 a really challenging problem,\u00a0 where nothing really worked well,\u00a0\u00a0 and still, they didn\u2019t stop until they\u00a0 absolutely nailed the solution. Congratulations! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=fPrxiRceAac",
        "paper_link": "https://light.informatik.uni-bonn.de/computational-parquetry-fabricated-style-transfer-with-wood-pixels/",
        "paper_title": "Computational Parquetry: Fabricated Style Transfer with Wood Pixels"
    },
    {
        "video_id": "K940MNp7V8M",
        "video_title": "Simulating Honey And Hot Showers For Bunnies! \ud83c\udf6f\ud83d\udc30",
        "position_in_playlist": 485,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Visualize-Debug-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly#System-4\n\n\ud83d\udcdd The paper \"An Adaptive Variational Finite Difference Framework for Efficient Symmetric Octree Viscosity\" is available here:\nhttps://cs.uwaterloo.ca/~rgoldade/adaptiveviscosity/\n\nHoudini video: https://www.sidefx.com/products/whats-new/18_5_vfx/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1958464/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#honeysim",
        "transcript": "the fellow scholars this is too many papers with doctor carries on anything here feast your eyes upon the simulation for my previous episode that showcases high viscosity material that is honey the fact that honey is viscous means that it is a material that is highly resistant against defamation in simpler words if we can simulate viscosity well we can engage in the favorite pastimes of the computer graphics researcher or in other words take some of these letters throw them around watch them slowly lose their previous shapes and then of course destroy them in a spectacular manner I love making simulations like this and when I do I want to see a lot of detail which unfortunately means that I also have to run the simulations for a long time so when I saw this new technique promises to compute dis about four times faster it's really grab my attention he is a visual demonstration of such a speed difference  Seoul four times you say that sounds fantastic but what's the catch here this kind of speed up usually comes with cutting corners so let's test the might of this new method through three examples of increasing difficulty experiment number one he is the regular simulation and here is the new technique so let's see the quality differences  one more time well I don't see any do you  four times faster with no degradation in quality  so far so good so now let's give it a harder example experiment number two varying viscosities and temperatures in other words let's give this money the hot shower death was beautiful and that question is again how close is this to the slow reference simulation  wow this is really close I have to look carefully to even have a fighting chance in finding a difference check mark and I also wonder can a deal with extremely detailed simulations and now hold on to your papers for experiment number three dropping a viscous bunny on thin wires and just look at the remnants of the poor bunny stock in there loving it now for almost every episode of this paper I get comments saying Connally this is all great but when do I get to use this when does this make it to the real world these questions are completely justified and the answer is right about now you can use this right now this paper was published in twenty nineteen and now it appears to be already part of Houdini one of the industry standard programs for visual effects and physics simulations tech transfer in just one year wow huge congratulations to Ryan go date and his colleagues for these incredible paper and huge respect to the folks at Houdini who keep outdoing themselves with these amazing updates  this episode has been supported by weights and biases in this post they show you how to monitor and optimize your GPU consumption during model training in real time with one line of cold weights and biases provides tools to track your experimentalism at prestigious labs such as open AI research get top and more and the best part is that weights and biases it's free for all individuals economics and open source project it really is as good as it gets make sure to visit them through W. N. B. dot com slash papers or just click the link in the video description and you can get the free demo today I thank the weights and biases for their longstanding support and for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=K940MNp7V8M",
        "paper_link": "https://cs.uwaterloo.ca/~rgoldade/adaptiveviscosity/",
        "paper_title": "An Adaptive Variational Finite Difference Framework for Efficient Symmetric Octree Viscosity"
    },
    {
        "video_id": "BjkgyKEQbSM",
        "video_title": "What Is 3D Photography? \ud83c\udf91",
        "position_in_playlist": 486,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/authors/One-Shot-3D-Photography/reports/Paper-Summary-One-Shot-3D-Photography--VmlldzozNjE2MjQ\n\n\ud83d\udcdd The paper \"One Shot 3D Photography\" is available here:\nhttps://facebookresearch.github.io/one_shot_3d_photography/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#3dphotos",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This is a standard color photo made with a smartphone. Hence, it contains only a 2D representation of the world, and when we look at it, our brain is able to reconstruct the 3D information from it. And I wonder, would it be possible for an AI to do the same, and go all the way and create a 3D version of this photo that we can rotate around? Well, this new learning-based method promises exactly that, and if that is at all possible, even more. These are big words, so let\u2019s have a look if it can indeed live up to its promise. So, first, we take a photograph, and we\u2019ll find out together in a moment what kind of phone is needed for this. Probably an amazing one, right? For now, this will be the input, and now, let\u2019s see the 3D photo as an output. Let\u2019s rotate this around. And\u2026wow. This is amazing. And you know what is even more amazing? Since pretty much every smartphone is equipped with a gyroscope, these photos can be rotated around in harmony with the rotation of our phones, and wait a second\u2026is this some sort of misunderstanding, or do I see correctly that we can even look behind a human if we wanted to. That content was not even part of the original photo! How does this work? More on that in a moment. Also, just imagine putting on a pair of VR glasses, and looking at a plain 2D photo and get an experience as if we were really there. It truly feels like we are living in a science fiction world. If we grab our trusty smartphone and use these images, we can create a timeline full of these 3D photos and marvel at how beautifully we can scroll such a timeline here. And now, we have piled up quite a few questions here. How is this wizardry possible? What kind of phone do we need for this? Do we need a depth sensor? Maybe even LiDAR? Let\u2019s look under the hood and find out together. This is the input. One colored photograph, that is expected, and let\u2019s continue\u2026goodness! Now this is unexpected\u2026 the algorithm creates a depth map by itself. This depth map tells the algorithm how far different parts of the image are from the camera. Just look at how crisp the outlines are. My goodness, so good. Then, with this depth information, it now has an understanding of what is where in this image, and creates these layers. Which is, unfortunately, not much help because, as you remember, we don\u2019t have any information on what is behind the person. No matter, because we can use a technique that implements image inpainting to fill in these regions with sensible data. And now, with this, we can start exploring these 3D photos. So\u2026if it created this depth map from the color information, this means that we don\u2019t even need a depth sensor for this. Just a simple, color photograph. But wait a minute\u2026this means that we can plug in any photo from any phone or camera that we or someone else took, at any time\u2026and I mean at any time, right? Just imagine taking a black and white photo of a historic event, colorizing it with a previous learning-based method, and passing this color image to this new method, and then, this happens. My goodness. So, all this looks and sounds great, but how long do we have to wait for such a 3D photo be generated? Does my phone battery get completely drained by the time all this computation is done? What is your guess? Please stop the video and leave a comment with your guess. I\u2019ll wait. Alright so is this a battery killer? Let\u2019s see. The depth estimation step takes\u2026whoa, a quarter of a second, inpainting, half a second, and after a little housekeeping, we find out that this is not a battery killer at all, because the whole process is done in approximately one second. Holy mother of papers. I am very excited to see this technique out there in  the wild as soon as possible. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=BjkgyKEQbSM",
        "paper_link": "https://facebookresearch.github.io/one_shot_3d_photography/",
        "paper_title": "One Shot 3D Photography"
    },
    {
        "video_id": "JmVQJg-glYA",
        "video_title": "Painting the Mona Lisa...With Triangles! \ud83d\udcd0",
        "position_in_playlist": 487,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Differentiable Vector Graphics Rasterization for Editing and Learning\" is available here:\n- https://people.csail.mit.edu/tzumao/diffvg/\n- https://people.csail.mit.edu/tzumao/diffvg/supplementary_webpage/ \n\nThe mentioned Mona Lisa genetic algorithm is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/mona_lisa_parallel_genetic_algorithm/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#vectorgraphics",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. What you see here is a bunch of vector images. Vector images are not like most images that you see on the internet, those are raster images. Those are like photos, and are made of pixels, while vector images are not made of pixels, they are made of shapes. These vector images have lots of advantages, for instance, they have really small file sizes, can be zoomed into as much as we desire and things don\u2019t get pixelated. And hence, vector images are really well suited for logos, maps, user interface icons, and more. Now, if we wish to, we can convert vector images into raster images, so the shapes will become pixels, this is easy, but here is the problem. If we do it once, there is no going back. Or at least, not easily. This method promises to make this conversion a two-way street, so we can take a raster image, a photo if you will, and work with it as if it were a vector image. Now what does that mean? Oh boy, a lot of goodies. For instance, we can perform sculpting, or in other words, manipulating shapes without touching any pixels. We can work with the shapes here instead. Much easier. Or, my favorite, perform painterly rendering. Now what you see here is not the new algorithm performing this. This is a genetic algorithm I wrote a few years ago that takes a target image, which is the Mona Lisa here, takes a bunch of randomly colored triangles and starts reorganizing them to get as close to the target image as possible. The source code and a video explaining how it works is available in the video description. Now, let\u2019s see how this new method performs on a similar task. Oh yeah, it can start with a large number of different shapes, and just look at how beautifully these shapes evolve and start converging to the target image. Loving it. But that\u2019s not all. It also has a nice solution to an old, but challenging problem in computer graphics that is referred to as seam carving. If you ask me, I like to call it image squishing. Why? Well, look here. This gives us an easy way of intelligently squishing an image into different aspect ratios. So good. So can we measure how well it does what it does? How does it compare to, for instance, Adobe\u2019s state of the art method when vectorizing a photo? Well, it can not only do more, but it also does it better. The new method is significantly closer to the target image here, no question about it. And now comes the best part: it not only provides higher-quality results than the previous methods, but it only takes approximately a second to perform all this. Wow. So there you go, finally, with this new technique, we can edit pixels as if they weren\u2019t pixels at all. It feels like we a living in a science fiction world. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=JmVQJg-glYA",
        "paper_link": "https://people.csail.mit.edu/tzumao/diffvg/",
        "paper_title": "Differentiable Vector Graphics Rasterization for Editing and Learning"
    },
    {
        "video_id": "tiO43nJKGJY",
        "video_title": "Is Simulating Jelly And Bunnies Possible? \ud83d\udc30",
        "position_in_playlist": 488,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Debug-Compare-Reproduce-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly\n\n\ud83d\udcdd The paper \"Monolith: A Monolithic Pressure-Viscosity-Contact Solver for Strong Two-Way Rigid-Rigid Rigid-Fluid Coupling\" is available here:\nhttps://tetsuya-takahashi.github.io/Monolith/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This new paper fixes many common problems when it\u00a0 comes to two-way coupling in fluid simulations.\u00a0\u00a0 And of course, the first question is, what\u00a0 is two-way coupling? It means that here,\u00a0\u00a0 the boxes are allowed to move the smoke, and\u00a0 the added two-way coupling part means that now,\u00a0\u00a0 the smoke is also allowed to blow away the boxes.\u00a0\u00a0 What\u2019s more, the vortices here on the right\u00a0 are even able to suspend the red box in\u00a0\u00a0 the air for a few seconds. An excellent\u00a0 demonstration of a beautiful phenomenon. However, simulating this effect\u00a0 properly for water simulations\u00a0\u00a0 and for gooey materials is a huge challenge, so\u00a0 let\u2019s see how traditional methods deal with them! Experiment number one! Water bunnies. Do you\u00a0 see what I am seeing here? Did you see the magic\u00a0\u00a0 trick? Let\u2019s look again. Observe how much water we\u00a0 are starting with. A full bunny worth of water\u2026and\u00a0\u00a0 then, by the end, we have maybe a quarter of\u00a0 a bunny left. Oh, yes. We have a substantial\u00a0\u00a0 amount of numerical dissipation in the\u00a0 simulator that leads to volume loss. Can this be solved somehow? Well, let\u2019s see\u00a0 how this new work deals with this. Starting\u00a0\u00a0 with one bunny\u2026and ending it with one bunny.\u00a0 Nice! Just look at the difference of the volume\u00a0\u00a0 of water left with the new method compared to\u00a0 the previous one. Night and day difference. And this was not even the worst volume loss\u00a0 I\u2019ve seen\u2026make sure to hold on to your papers,\u00a0\u00a0 and check out this one. Experiment number two,\u00a0 gooey dragons and bowls. When using a traditional\u00a0\u00a0 technique, whoa, this guy is GONE. And when we try\u00a0 a different method, it does\u2026 this. My goodness.\u00a0\u00a0 So let\u2019s see if the new method can deal\u00a0 with this case. Oh, yes, yes it can! And now, onwards to experiment number\u00a0 three. If you think that research is\u00a0\u00a0 about throwing things at the wall and seeing\u00a0 what sticks, in the case of this scene\u2026you\u00a0\u00a0 are not wrong. So what should happen\u00a0 here, given these materials? Well,\u00a0\u00a0 the bunny should stick to the goo,\u00a0 and not fall too quickly\u2026hm\u2026none\u00a0\u00a0 of which happens here. The previous method does\u00a0 not simulate viscosity properly, and hence,\u00a0\u00a0 this artificial melting phenomenon emerges.\u00a0 I wonder if the new method can do this too?\u00a0\u00a0 And, yes! They stick together and the goo\u00a0 correctly slows down the fall of the bunny. So how does this magic work? Normally,\u00a0 in these simulations, we have to compute\u00a0\u00a0 pressure, viscosity, and frictional contact\u00a0 separately, which are three different tasks.\u00a0\u00a0 The technique described in this paper is\u00a0 called Monolith because it has a monolithic\u00a0\u00a0 pressure-viscosity-contact solver. Yes, this means\u00a0 that it does all three of these tasks in one go,\u00a0\u00a0 which is a mathematically a tiny bit more\u00a0 involved, but it gives us a proper simulator\u00a0\u00a0 where water and goo can interact with solids.\u00a0 No volume loss, no artificial melting, no crazy\u00a0\u00a0 jumpy behavior. And here comes the punchline\u2026I was\u00a0 thinking that alright, a more accurate simulator,\u00a0\u00a0 that is always welcome, but what is the price\u00a0 of this accuracy? How much longer do I have to\u00a0\u00a0 wait? If you have been holding on to your papers,\u00a0 now squeeze that paper, because this technique is\u00a0\u00a0 not slower, but up to 10 times faster than\u00a0 previous methods, and that\u2019s where I fell\u00a0\u00a0 off the chair when reading this paper. And with\u00a0 this, I hope that we will be able to marvel at\u00a0\u00a0 even more delightful two way-coupled simulations\u00a0 in the near future! What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=tiO43nJKGJY",
        "paper_link": "https://tetsuya-takahashi.github.io/Monolith/",
        "paper_title": "Monolith: A Monolithic Pressure-Viscosity-Contact Solver for Strong Two-Way Rigid-Rigid Rigid-Fluid Coupling"
    },
    {
        "video_id": "IDMiMKWucaI",
        "video_title": "NERFIES: The Selfies of The Future! \ud83e\udd33",
        "position_in_playlist": 489,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/stacey/xray/reports/X-Ray-Illumination--Vmlldzo4MzA5MQ\n\n\ud83d\udcdd The paper \"Deformable Neural Radiance Fields\" is available here:\nhttps://nerfies.github.io/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#nerfies #selfies",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to get a taste of how insanely quick progress is in machine learning research. In March of 2020, a paper appeared that goes by the name Neural Radiance Fields, NERF in short. With this technique, we could take a bunch of input photos, get a neural network to learn them, and then, synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. And here, we are talking not only digital environments, but also, real scenes as well! Just to make sure, once again, it can learn and reproduce entire real-world scenes from only a few views by using neural networks. However, of course, NERF had its limitations. For instance, in many cases, it had trouble with scenes with variable lighting conditions and lots of occluders. And to my delight, only 5 months later, in August of 2020, a followup paper appeared by the name NERF in the Wild, or NERF-W in short. Its speciality was tourist attractions that a lot of people take photos of, and we then have a collection of photos taken during a different time of the day, and of course, with a lot of people around. And, lots of people, of course means, lots of occlusions. NERF-W improved the original algorithm to excel more in cases like this. And we are still not done yet, because get this, only three months later, on 2020 November 25th, another followup paper appeared by the name Deformable Neural Radiance Fields. D-NERF. The goal here is to take a selfie video, and turn it into a portrait that we can rotate around freely. This is something that the authors call a nerfie. If we take the original NERF technique to perform this, we see that it does not do well at all with moving things. And here is where the deformable part of the name comes into play. And now, hold on to your papers and marvel at the results of the new D-NERF technique. A clean reconstruction. We indeed get a nice portrait that we can rotate around freely and all of the previous NERF artifacts are gone. It performs well even on difficult cases with beards, all kinds of hairstyles, and more. And now, hold on to your papers, because glasses work too, and not only that, but it even computes the proper reflection and refraction off of the lens. And this is just the start of a deluge of new features. For instance, we can even zoom out and capture the whole body of the test subject. Furthermore, it is not limited to people, it also works on dogs too, although in this case, we will have to settle with a lower the resolution output. It can pull off the iconic dolly zoom effect really well. And, amusingly, we can even perform a nerfception, which is recording ourselves as we record ourselves. I hope that now you have a good feel of the pace of progress in machine learning research, which is absolutely incredible. So much progress in just 9 months of research. My goodness. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=IDMiMKWucaI",
        "paper_link": "https://nerfies.github.io/",
        "paper_title": "Deformable Neural Radiance Fields"
    },
    {
        "video_id": "9XM5-CJzrU0",
        "video_title": "Light Fields - Videos From The Future! \ud83d\udcf8",
        "position_in_playlist": 490,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Immersive Light Field Video with a Layered Mesh Representation\" is available here:\nhttps://augmentedperception.github.io/deepviewvideo/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#lightfields",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Whenever we take a photo, we capture a\u00a0 piece of reality from one viewpoint. Or,\u00a0\u00a0 if we have multiple cameras on our smartphone, a\u00a0 few viewpoints at most. In an earlier episode, we\u00a0\u00a0 explored how to upgrade these to 3D photos, where\u00a0 we could we could kind of look behind the person.\u00a0\u00a0 I am saying kind of, because what we see\u00a0 here is not reality, this is statistical data\u00a0\u00a0 that is filled in by an algorithm to match\u00a0 its surroundings, which we refer to as image\u00a0\u00a0 inpainting. So strictly speaking, it is likely\u00a0 information, but not necessarily true information,\u00a0\u00a0 and also, we can recognize the synthetic parts\u00a0 of the image as they are significantly blurrier.\u00a0\u00a0 So the question naturally arises in the mind\u00a0 of the curious Scholar, how about actually\u00a0\u00a0 looking behind the person. Is that somehow\u00a0 possible or is that still science fiction? Well, hold on to your papers,\u00a0 because this technique shows us\u00a0\u00a0 the images of the future by sticking a\u00a0 bunch of cameras onto a spherical shell,\u00a0\u00a0 and when we capture a video, it will see\u2026something\u00a0 like this. And the goal is to untangle this mess\u00a0\u00a0 and we\u2019re not done yet, we also need to\u00a0 reconstruct the geometry of the scene\u00a0\u00a0 as if the video was captured from many different\u00a0 viewpoints at the same time. Absolutely amazing.\u00a0\u00a0 And yes, this means that we can change\u00a0 our viewpoint while the video is running. Since it is doing the reconstruction in layers,\u00a0 we know how far each object is in these scenes,\u00a0\u00a0 enabling us to rotate these sparks and\u00a0 flames and look at them in 3D. Yum. Now, I am a light transport researcher by\u00a0 trade, so I hope you can tell that I am very\u00a0\u00a0 happy about these beautiful volumetric\u00a0 effects, but I would also love to know\u00a0\u00a0 how it deals with reflective surfaces. Let\u2019s see\u00a0 together\u2026look at the reflections in the sand here,\u00a0\u00a0 and I\u2019ll add a lot of camera movement and\u00a0 wow\u2026this thing works. It really works,\u00a0\u00a0 and it does not break a sweat even if\u00a0 we try a more reflective surface\u2026or\u00a0\u00a0 an even more reflective surface! This\u00a0 is as reflective as it gets I\u2019m afraid,\u00a0\u00a0 and we still get a consistent and\u00a0 crisp image in the mirror. Bravo! Alright, let\u2019s get a little more\u00a0 greedy, what about seeing through\u00a0\u00a0 thin fences\u2026that is quite a challenge.\u00a0 And\u2026look at the tail wags there. This is still\u00a0\u00a0 a touch blurrier here and there,\u00a0 but overall, very impressive. So what do we do with a video like this?\u00a0 Well, we can use our mouse to look around\u00a0\u00a0 within the photo in our web browser,\u00a0 you can try this yourself right now\u00a0\u00a0 by clicking on the paper in the video description.\u00a0 Make sure to follow the instructions if you do.\u00a0\u00a0 Or we can make the viewing experience even more\u00a0 immersive with a head-mounted display, where,\u00a0\u00a0 of course, the image will follow wherever\u00a0 we turn our head. Both of these truly feel\u00a0\u00a0 like entering a photograph and getting\u00a0 a feel of the room therein. Loving it. Now, since there is a lot of information in these\u00a0 Light Field Videos, it also needs a powerful\u00a0\u00a0 internet connection to relay them. Even when using\u00a0 H.265, a powerful video compression standard,\u00a0\u00a0 we are talking in the order of hundreds of\u00a0 megabits. It is like streaming several videos in\u00a0\u00a0 4k resolution at the same time. Compression helps,\u00a0 however, we also have to make sure that we don\u2019t\u00a0\u00a0 compress too much, so that compression artifacts\u00a0 don\u2019t eat the content behind thin geometry,\u00a0\u00a0 or at least, not too much. I bet this will\u00a0 be an interesting topic for a followup paper,\u00a0\u00a0 so make sure to subscribe and hit the\u00a0 bell icon to not miss it when it appears.\u00a0\u00a0 And for now, more practical light field photos and\u00a0 videos will be available that allow us to almost\u00a0\u00a0 feel like we are really in the room with the\u00a0 subjects of the videos. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9XM5-CJzrU0",
        "paper_link": "https://augmentedperception.github.io/deepviewvideo/",
        "paper_title": "Immersive Light Field Video with a Layered Mesh Representation"
    },
    {
        "video_id": "C7D5EzkhT6A",
        "video_title": "OpenAI DALL\u00b7E: Fighter Jet For The Mind! \u2708\ufe0f",
        "position_in_playlist": 491,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The blog post on \"DALL\u00b7E: Creating Images from Text\" is available here:\nhttps://openai.com/blog/dall-e/\n\nTweet sources:\n- Code completion: https://twitter.com/gdm3000/status/1151469462614368256\n- Website layout: https://twitter.com/sharifshameem/status/1283322990625607681\n- Population data: https://twitter.com/pavtalk/status/1285410751092416513\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credits: https://pixabay.com/images/id-3202725/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\n\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#openai #dalle",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In early 2019, a learning-based technique appeared that could perform common natural language processing operations, for instance, answering questions, completing text, reading comprehension, summarization, and more. This method was developed by scientists at OpenAI, and they called it GPT-2. The key idea for GPT-2 was that all of these problems could be formulated as different variants of text completion problems, where all we need to do is provide it an incomplete piece of text, and it would try to finish it. Then in June 2020 came GPT-3 that supercharged this idea, and among many incredible examples, it could generate website layouts from a written description. However, no one said that these neural networks can only deal with text information. And sure enough, a few months later, scientists at OpenAI thought that if we can complete text sentences, why not try to complete images too? They called this project image GPT and the problem statement was simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. It could identify that the cat here likely holds a piece of paper and finish the picture accordingly, and even understood that if we have a droplet here and we see just a portion of the ripples, then this means a splash must be filled in. And now, right in January 2021, just 7 months after the release of GPT-3, here is their new mind-blowing technique that explores the connection between text and images. But finishing images already kind of works, so what new thing can it do? In just a few moments, you will see that the more appropriate question would be \u201cwhat can\u2019t it do\u201d? For now, well, it creates images from our written text captions, and you will see in a moment how monumental of a challenge that is. The name of this technique is a mix of Salvador Dal\u00ed and Pixar\u2019s Wall-e. So please meet Dall-e. And now, let\u2019s see it through an example. For neural network-based learning methods, it is easy to recognize that this text says OpenAI, and what a storefront it. Images of both of these exist in abundance. Understanding that is simple. However, generating a storefront that says OpenAI is quite a challenge. It is really possible that it can do that? Well, let\u2019s try it. Look, it works! Wow! Now, of course, if you look here, you immediately see that it is by no means perfect, but let\u2019s marvel at the fact that we can get all kinds of 2D and 3D texts, look at the storefronts from different orientations, and it can deal with all of these cases reasonably well. And of course, it is not limited to storefronts, we can request license plates, bags of chips, neon signs, and more. It can really do all that. So, what else? Well, get this, it can also kind of invent new things. So let\u2019s put our entrepreneurial hat on and try to invent something here. For instance, let\u2019s try to create a triangular clock. Or pentagonal. Or, you know, just make it a hexagon. It really doesn\u2019t matter because we can ask for absolutely anything and get a bunch of prototypes in a matter of seconds. Now, let\u2019s make it white, and\u2026look! Now we have a happy, happy K\u00e1roly. Why is that? It is because I am a light transport researcher by trade, so the first thing when I look at when seeing these generated images is how physically plausible they are. For instance, look at this white clock here on the blue table. And it did not only put it on the table, but it also made sure to generate appropriate glossy reflections that matches the color of the clock. It can do this too! Loving it. Apparently, it understands geometry, shapes and materials. I wonder what else does it understand? Well, get this, for instance, it even understands styles and rendering techniques. Being a graphics person, I am so happy to see that it learned the concept of low polygon count rendering, isometric views, clay objects, and we can even add an X-ray view to the Owl. Kind of. And now, if all that wasn\u2019t enough, hold on to your papers, because we can also commission artistic illustrations for free, and not only that, but even have fine-grained control over these artistic illustrations. I also learned that if manatees wore suits, they would wear them like this, and after a long and strenuous day walking their dogs, they can go for yet another round\u2026 in pajamas. But it does not stop there, it can not only generate paintings of nearly anything, but we can even choose the artistic style and the time of day as well. The night images are a little on the nose as most of them have the moon in the background, but I\u2019ll be more than happy to take these. And the best part is that you can try this yourself right now through the link in the video description. In general, not all results are perfect, but it\u2019s hard to even fathom all the things this will enable us to do in the near future when we can get our hands on these pre-trained models. This may be the first technique where the results are not limited by the algorithm, but, by our own imagination. Now this is a quote that I said about GPT-3, and notice that the exact same thing can be said about Dall-e. Quote: \u201cThe main point is that working with GPT-3 is a really peculiar process where we know that a vast body of knowledge lies within, but it only emerges if we can bring it out with properly written prompts. It almost feels like a new kind of programming that is open to everyone, even people without any programming or technical knowledge. If a computer is a bicycle for the mind, then GPT-3 is a fighter jet. Absolutely incredible.\" I think this kind of programming is going to be more and more common in the future. Now note that these are some amazing preliminary results, but the full paper is not available yet. So this was not two minutes, and it was not about a paper. Welcome to Two Minute Papers! Jokes aside, I cannot wait for the paper to appear, and I\u2019ll be here to have a closer look whenever it happens. Make sure to subscribe and hit the bell icon to not miss it when the big day comes. And until then, let me know in the comments what crazy concoctions you came up with! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=C7D5EzkhT6A"
    },
    {
        "video_id": "nJ86LCA0Asw",
        "video_title": "Building A Liquid Labyrinth! \ud83c\udf0a",
        "position_in_playlist": 492,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Surface-Only Ferrofluids\" is available here:\nhttp://computationalsciences.org/publications/huang-2020-ferrofluids.html\n\nYou can follow this research group on Twitter too:\nhttps://twitter.com/csgKAUST\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#ferrofluid",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. You\u2019re in for a real treat today, because today, once again, we\u2019re not going to simulate just plain regular fluids. No! We\u2019re going to simulate ferrofluids! These are fluids that have magnetic properties and respond to an external magnetic field and get this, they are even able even climb things. Look at this footage from a previous paper. Here is a legendary real experiment where with magnetism, we can make a ferrofluid climb up on this steel helix. Look at that. And now, the simulation. Look at how closely it matches the real footage. Marvelous, especially that it is hard to overstate how challenging it is to create an accurate simulation like this. And the paper got even better. This footage could even be used as proper teaching material. Look. On this axis, you can see how the fluid disturbances get more pronounced as a response to a stronger magnetic field. And in this direction, you see how the effect of surface tension smooths out these shapes. What a visualization! The information density here is just out of this world, while it is still so easy to read at a glance. And it is also absolutely beautiful. This paper was a true masterpiece. The first author of this work was Libo Huang and it was advised by Prof. Dominik Michels who has a strong physics background. And here is the punchline: Libo Huang is a PhD student and this was his first paper. Let me say it again: this was Libo Huang\u2019s first paper and it is a masterpiece. Wow! And it gets better, because this new paper is called Surface-Only Ferrofluids\u2026and YES!!! It is from the same authors. So this paper is supposed to be better, but the previous technique set a really high bar. How the heck do you beat that?! What more could we possibly ask for? Well, this new method showcases a surface-only formulation, and a key observation here that for a class of ferrofluids, we don\u2019t have to compute how the magnetic forces act on the entirety of the 3D fluid domain, we only have to compute them on the surface of the model. So what does this give us? One of my favorite experiments! In this case, we squeeze the fluid between two glass planes and start cranking up the magnetic field strength perpendicular to these planes. Of course, we expect that it starts flowing sideways, but not at all how we would expect it. Wow. Look as how these beautiful fluid labyrinths start slowly forming. And we can simulate all this on our home computers today. We are truly living in a science fiction world. Now if you find yourself missing the climbing experiment from the previous paper, don\u2019t despair, this can still do that, look. First, we can control the movement of the fluid by turning on the upper magnet, then, slowly turn it off while turning on the lower magnet to give rise to this beautiful climbing phenomenon. And that\u2019s not all, fortunately, this work is also ample in amazing visualizations, for instance, this one shows how the ferrofluid changes if we crank up the strength of our magnets, and how changing the surface tension determines the distance between the spikes and the overall smoothness of the fluid. What a time to be alive! One of the limitations of this technique is that it does not deal with viscosity well, so if we are looking to create a crazy goo simulation like this one, but, with ferrofluids, we will need something else for that. Perhaps that something will be the next  paper down the line. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=nJ86LCA0Asw",
        "paper_link": "http://computationalsciences.org/publications/huang-2020-ferrofluids.html",
        "paper_title": "Surface-Only Ferrofluids"
    },
    {
        "video_id": "2wrOHdvAiNc",
        "video_title": "All Duckies Shall Pass! \ud83d\udc23",
        "position_in_playlist": 493,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Interlinked SPH Pressure Solvers for Strong Fluid-Rigid Coupling\" is available here:\nhttps://cg.informatik.uni-freiburg.de/publications/2019_TOG_strongCoupling.pdf\n\n\ud83d\udcf8 Our Instagram page is available here:\nhttps://www.instagram.com/twominutepapers/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In this paper, you will not only see an amazing technique for two-way coupled fluid-solid simulations, but you will also see some of the most creative demonstrations of this new method I\u2019ve seen in a while. But first things first, what is this two way coupling thing? Two-way coupling in a fluid simulation means being able to process contact. You see, the armadillos can collide with the fluids, but the fluids are also allowed to move the armadillos. And this new work can compute this kind of contact. And by this I mean lots and lots of contact. But one of the important lessons of this paper is that we don\u2019t necessarily need a scene this crazy to be dominated by two-way coupling. Have a look at this experiment with these adorable duckies, and below, the propellers are starting up, please don\u2019t be one of those graphics papers. Okay..okay, good. We dodged this one. So the propellers are not here to dismember things, they are here to spin up to 160 rpm, and since they are two-way coupled, they pump water from one tank to the other, raising the water levels, allowing the duckies to pass. An excellent demonstration of a proper algorithm that can compute two-way coupling really well. And simulating this scene is much, much more challenging than we might think. Why is that? Note that the speed of the propellers is quite high, which is a huge challenge to previous methods. If we wish to complete the simulation in a reasonable amount of time, it simulates the interaction incorrectly and no ducks can pass. The new technique can simulate this correctly, and not only that, but it is also 4.5 times faster than the previous method. Also, check out this elegant demonstration of two-way coupling. We start slowly unscrewing this bolt\u2026and\u2026nothing too crazy going on here. However, look! We have tiny cutouts in the bolt, allowing the water to start gushing out. The pipe was made transparent so we can track the water levels slowly decreasing, and finally, when the bolt falls out, we get some more two-way coupling action with the water. Once more, such a beautiful demonstration of a difficult to simulate phenomenon. Loving it. A traditional technique cannot simulate this properly, unless we add a lot of extra computation. At which point, it is still unstable\u2026 ouch! And with even more extra computation, we can finally do this, but hold on to your papers, because the new proposed technique can do it about 10 times faster. It also supports contact against rich geometry as well. Look, we have a great deal going on here. You are seeing up to 38 million fluid particles interacting with these walls given with lots of rich geometry, and there will be interaction with mud, and elastic trees as well. This can really do them all. And did you notice that throughout this video, we saw a lot of delta t-s. What are those? Delta t is something that we call time step size. The smaller this number is, the tinier the time steps with which we can advance the simulation when computing every interaction, and hence, the more steps there are to compute. In simpler words, generally, time step size is an important factor in the computation time, and the smaller this is, the slower, but more accurate the simulation will be. This is why we needed to reduce the time steps by more than 30 times to get a stable simulation here with the previous method. And this paper proposes a technique that can get away with time steps that are typically from 10 times to a 100 times larger than previous methods. And it is still stable. That is an incredible achievement. So what does that mean in a practical case? Well, hold on to your papers, because this means that it is up to 58 times faster than previous methods. 58 times! Whoa. With a previous method, I would need to run something for nearly two months, and the new method would be able to compute the same within a day. Witchcraft, I\u2019m telling you. What a time to be alive! Also, as usual, I couldn\u2019t resist creating a slow-motion version of some of these videos, so if this is something that you wish to see, make sure to visit our Instagram page in the video description  for more. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2wrOHdvAiNc",
        "paper_link": "https://cg.informatik.uni-freiburg.de/publications/2019_TOG_strongCoupling.pdf",
        "paper_title": "Interlinked SPH Pressure Solvers for Strong Fluid-Rigid Coupling"
    },
    {
        "video_id": "Aq93TSau8GE",
        "video_title": "This AI Learned To Create Dynamic Photos! \ud83c\udf01",
        "position_in_playlist": 494,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their report on this paper is available here: https://wandb.ai/wandb/xfields/reports/-Overview-X-Fields-Implicit-Neural-View-Light-and-Time-Image-Interpolation--Vmlldzo0MTY0MzM\n\n\ud83d\udcdd The paper \"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation\" is available here:\nhttp://xfields.mpi-inf.mpg.de/\n\n\ud83d\udcdd Our paper on neural rendering (and more!) is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\udcdd Our earlier paper with high-resolution images for the caustics is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-820011/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\n\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Approximately 5 months ago, we talked about a technique called Neural Radiance Fields, or NERF in short, where the input is the location of the camera and an image of what the camera sees, we take a few of those, give them to a neural network to learn them, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. In short, we take a few samples, and the neural network learns what should be there between the samples. In comes a non-continuous data, a bunch of photos, and out goes a continuous video where the AI fills in the data between these samples. With this, we can change the view direction, but only that! This concept can also be used for other variables. For instance, this work is able to change the lighting, but only the lighting. By the way, this is from long ago, from around Two Minute Papers episode number 13, so our seasoned Fellow Scholars know that this was almost 500 episodes ago. Or, the third potential variable is time. With this AI-based physics simulator, we can advance the time, and the algorithm would try to guess how a piece of fluid would evolve over time. This was amazing, but as you might have guessed, we can advance time, but only the time. And this was just a couple of examples from a slew of works that are capable of doing one, or at most, two of these. These are all amazing techniques, but they offer separate features. One can change the view, but nothing else, one for the illumination, but nothing else, and one for time, but nothing else. With the advent of neural network-based learning algorithms, I wonder if it is possible to create an algorithm that does all three? Or is this just science fiction? Well, hold on to your papers, because with this new work that goes by the name X-Fields, we can indeed change the time\u2026and the view direction...and the lighting separately. Or, even better, do all three at the same time. Wo-hoo! Look at how we can play with the time back and forth and set the fluid levels as we desire, that is the time part, and we can also play with the other two parameters as well at the same time. But still, the results that we see here can range from absolutely amazing, to trivial, depending on just one factor. And that factor is, how much training data was available for the algorithm. Neural networks typically require loads of training data to learn a new concept. For instance, if we wish to teach to a neural network what a cat is, we have to show it thousands and thousands of images of cats. So, how much training data is needed for this? And now, hold on to your papers, and\u2026whoa\u2026look at these 5 dots here. Do you know what this means? It means that all the AI saw was five images, that is five samples from the scene with different light positions, and it could fill in all the missing details with such accuracy that we can create this smooth and creamy transition. It almost feels like we have made at least a 100 photographs of the scene. And all this from 5 input photos. Absolutely amazing. Now, here is my other favorite example. I am a light transport simulation researcher by trade, so by definition, I love caustics. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, and concentrate it to a relatively small area. I hope that you are not surprised when I say that it is the favorite phenomenon of most light transport researchers. And, just look at how beautifully it deals with it. You could take any of these intermediate, AI-generated images and sell them as real ones and I doubt anyone would notice. So, it does three things that previous techniques could do one by one, but really, how does its quality compare to these previous methods? Let\u2019s see how it does on thin geometry, which is a notoriously difficult case for these methods. Here is a previous one. Look. The thick part is reconstructed correctly, however, look at the missing top of the grass blade. Yup, that\u2019s gone. A different previous technique by the name Local Light Field Fusion not only missed the top as well, but also introduced halo-like artifacts to the scene. And, as you see with this footage, the new method solves all of these problems really well, and is quite close to the true reference footage that we kept hidden from the AI. Perhaps the best part is that it also has an online demo that you can try right now, so make sure to click the link in the video description to have a look. Of course, not even this technique is perfect, there are cases where it might confuse the foreground with the background, and we are still not out of the water when it comes to thin geometry. Also, an extension that I would love to see is changing material properties. Here, you see some results from our earlier paper on neural rendering where we can change the material properties of this test object, and get a near-perfect photorealistic image of it in about 5 milliseconds per image. I would love to see it combined with a technique like this one, and while it looks super challenging, it is easily possible that we will have something like that within 2 years. The link to our neural rendering paper and its source code is also available in the video description. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Aq93TSau8GE",
        "paper_link": "http://xfields.mpi-inf.mpg.de/",
        "paper_title": "X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation"
    },
    {
        "video_id": "7O7W-_FKRMQ",
        "video_title": "Episode 500 - 8 Years Of Progress In Cloth Simulations! \ud83d\udc55",
        "position_in_playlist": 495,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/stacey/yolo-drive/reports/Bounding-Boxes-for-Object-Detection--Vmlldzo4Nzg4MQ\n\n\ud83d\udcdd The paper \"Robust Eulerian-On-Lagrangian Rods\" is available here:\nhttp://mslab.es/projects/RobustEOLRods/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers episode number 500 with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. And on this glorious day, we are going to simulate the kinematics of yarn and cloth on our computers. We will transition into today\u2019s paper in a moment, but for context, here is a wonderful work to show you what we were able to do in 2012 and we will see how far we have come since. This previous work was about creating these highly detailed cloth geometries for digital characters. Here you see one of its coolest results where it shows how the simulated forces pull the entire piece of garment together. We start out with dreaming up a piece of cloth geometry, and this simulator gradually transforms it into a real-world version of that by subjecting it to real physical forces. This is a step that we call yarn-level relaxation. A few years ago, when I worked at Disney Research, I attended to the talk of the Oscar award winning researcher Steve Marschner who presented this paper. And when I saw these results, shockwaves went through my body. It was one of my truly formative \u201chold on to your papers\u201d moments that I\u2019ll never forget. Now note that to produce these results, one had to wait for hours and hours to compute all these interactions. So, this paper was published in 2012, and now, nearly 9 years have passed, so I wonder how far have come some since? Well, let\u2019s see together! Today, with this new technique, we can conjure up similar animations where pieces of garments tighten. Beautiful. Now, let\u2019s look under the hood of the simulator, and\u2026well, well, well. Do you see what I see here? Red dots. So, why did I get so excited about a couple red dots? Let\u2019s find out together! These red dots solve a fundamental problem when simulating the movement of these yarns. The issue is that in the mathematical description of this problem, there is a stiffness term that does not behave well when two of these points slide too close to each other. Interestingly, our simulated material gets infinitely stiff in these points. This is incorrect behavior and it makes the simulation unstable. Not good. So what do we to to alleviate this? Now, we can use this new technique that detects these cases and addresses them by introducing these additional red nodes. These are used as a stand-in until things stabilize. Look, we wait until these two points slide off of each other. And now, the distances are large enough so that the mathematical framework can regain its validity and compute the stiffness term correctly, and look, the red dot disappears, and the simulation can continue without breaking. So if we go back to another piece of under the hood footage, we now understand why these red dots come and go. They come when two nodes get too close to each other, and they disappear as they pass each other, keeping the simulation intact. And with this method, we can simulate this beautiful phenomenon when we throw a piece of garment on the sphere, and all kinds of stretching and sliding takes place. Marvelous. So what else can this do? Oh boy, it can even simulate multiple cloth layers, look at the pocket and the stitching patterns here. Beautiful. We can also put a neck tag on this shirt and start stretching and shearing it into oblivion. Pay special attention to the difference in how the shirt and the neck tag reacts to the same forces. We can also stack three tablecloths on top of each other and see how they would behave if we would not simulate friction. And now, the same footage with friction. Much more realistic. And if we look under the hood, you see that the algorithm is doing a ton of work with these red nodes. Look, the table notes that they had to insert tens of thousands of these nodes to keep the simulation intact. Goodness! So, how long do we have to wait for a simulation like this? The 2012 paper took several hours, what about this one? Well, this says we need a few seconds per timestep, and typically, several timesteps correspond to one frame, so where does this put us? Well, it puts us in the domain of not hours per every frame of animation here, but to minutes, and sometimes even seconds per frame. And not only that, but this simulator is also more robust as it can deal with these unpleasant cases where these points get too close to each other. So, I think this was a great testament to the amazing rate of progress in computer graphics research. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7O7W-_FKRMQ",
        "paper_link": "http://mslab.es/projects/RobustEOLRods/",
        "paper_title": "Robust Eulerian-On-Lagrangian Rods"
    },
    {
        "video_id": "mb6WJ34xQXg",
        "video_title": "This Neural Network Makes Virtual Humans Dance! \ud83d\udd7a",
        "position_in_playlist": 496,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/in-between/reports/-Overview-Robust-Motion-In-betweening---Vmlldzo0MzkzMzA\n\n\ud83d\udcdd The paper \"Robust Motion In-betweening\" is available here:\n- https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2020/07/09155337/RobustMotionInbetweening.pdf\n- https://montreal.ubisoft.com/en/automatic-in-betweening-for-faster-animation-authoring/\n\nDataset: https://github.com/XefPatterson/Ubisoft-LaForge-Animation-Dataset\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background image credit: https://pixabay.com/images/id-2122473/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Most people think that if we have a piece of camera footage that is a little choppy, then there is nothing we can do with it, we better throw it away. Is that true? No, not at all! Earlier, we discussed two potential techniques to remedy this common problem. The problem statement is simple, in goes a choppy video, something happens, and then, out comes a smooth and creamy video. This process is often referred to as frame interpolation, or frame inbetweening. And of course, it\u2019s easier said than done. If it works well, it really looks like magic - much like in the science fiction movies. So what are the potential somethings that we can use to make this happen? One, optical flow. This is an originally handcrafted method that tries to predict the motion that takes place between these frames. This can kind of produce new information and I use this in these videos on a regular basis, but the output footage also has to be carefully inspected for unwanted artifacts. Which are a relatively common occurrence. Two, we can also try to give a bunch of training data to a neural network, and teach it to perform this frame inbetweening. And if we do, the results are magnificent. We can do so much with this! But wait a second\u2026if we can do this for video frames\u2026 here is a crazy idea - how about a similar kind of inbetweening\u2026for animating humanoids? That would really be something else and it would save us so much time and work! Let\u2019s see what this new method can do in this area! The value proposition of this technique is as simple as it gets: we set up a bunch of keyframes, these are the transparent figures, and the neural network creates realistic motion that transitions from one stage to the next one. Look, it really seems to be able to do it all, it can perform twists and turns, brisk walks and runs, and you will see in a minute, even dance moves. Hmm\u2026this inbetweening for animating humanoid motion idea may not be so crazy after all! What\u2019s more, this could be super useful for artists working in the industry, who can not only do all this, but they can also set up movement variations by moving the keyframes around spatially. Or we can even set up temporal variations to create different timings for the movement. Excellent. Of course, it cannot do everything, if we set up the intermediate stages in a way that uncommon motions would be required to fill in, we might end up with one of these failure cases. And all these results depend on how much training data we have with the kinds of motions we need to fill in. Let\u2019s have a look at a more detailed example! This smooth chap has been given lots of training data with dancing moves, and\u2026look! And when we pull out these dance moves from his training data, he becomes a drunkard. So, talking about training data. How much motion capture footage was given to this algorithm? It used the Ubisoft La Forge Animation Dataset. This contains 5 subjects, 77 sequences, about 4.5 hours of footage in total. Wow, that is not that much. For instance, it only has 8 movement sequences for dancing. That is not that much at all. And we\u2019ve already seen that the model can dance. That is some serious data efficiency, especially given that it can even climb through obstacles. So much knowledge has been extracted from so little data. It truly feels like we are living in a science fiction world. What a time to be alive! So, when we write a paper like this, how do we compare the results to previous techniques? How can we decide which technique is better? Well, the level 1 solution is a user study. We call some folks in, show them the footage, ask which one they liked best - the previous method, or this one? That would work, but of course, it is quite laborious, but fortunately, there is a level 2 solution. And this level 2 solution is called the Normalized Power Spectrum Similarity, NPSS in short. This is a number that we can produce with a computer, no humans are required, and it measures how believable these motions are. And the key of NPSS is that it correlates with human judgement, or in other words, if this says that a technique is better, then it is likely that humans would also come to the same conclusion. So let\u2019s see. Here are the previous methods, NPSS is subject to minimization, in other words, the lower, the better. And, let\u2019s see the new method\u2026oh yes, it indeed outpaces the competition. So, there is no wonder that this incredible paper was accepted to the SIGGRAPH ASIA conference. What does that mean exactly? If research were the olympics, a SIGGRAPH or SIGGRAPH ASIA paper would be the gold medal. And, this was Mr. Felix Harvey\u2019s first few papers. Huge congratulations! And as an additional goodie, it can create an animation of me when I lost my papers, and this is me when I found them. Do you have some more ideas on how we could put such an amazing technique to use? Let me know in the comments below. Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=mb6WJ34xQXg",
        "paper_link": "https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2020/07/09155337/RobustMotionInbetweening.pdf",
        "paper_title": "Robust Motion In-betweening"
    },
    {
        "video_id": "JSNE_PIG1UQ",
        "video_title": "7 Years of Progress In Snow Simulation! \u2744\ufe0f",
        "position_in_playlist": 497,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"An Implicit Compressible SPH Solver for Snow Simulation\" is available here:\nhttps://cg.informatik.uni-freiburg.de/publications/2020_SIGGRAPH_snow.pdf\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-2522720/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Let\u2019s talk about snow simulations! Being able to simulate snow on our computers is not new, it\u2019s been possible for a few years now, for instance, this legendary Disney paper from 2013 was capable of performing that. So why do researchers write new papers on this? Well, because the 2013 paper had its limitations. Let\u2019s talk about them while looking at some really sweet footage from a new paper. Limitation number one for previous works is that their snow simulations were sinfully expensive. Most of them took not seconds, and sometimes not even minutes, but half an hour per frame. Yes, that means all nighter simulations. They were also not as good in fracturing interactions, simulating powder snow and avalanches were also out of reach. Until now. You see, simulating snow is quite challenging. It clumps up, deforms, breaks, and hardens under compression. And even my favorite, phase change from fluid to snow! These are all really challenging to simulate properly, and in a moment, you will see that this new method is capable of even more beyond that. Let\u2019s start with friction. First, we turn on the snow machine. And then, engage the wipers! That looks wonderful. And now, may I request seeing some tire marks? There you go! This looks really good, so how about taking a closer look at this phenomenon? Vb here is the boundary friction coefficient and it is a parameter that can be chosen freely by us. So let\u2019s see what that looks like! If we initialize this to a low value, we\u2019ll get very little friction, and if we crank this up, look, things get a great deal more sticky. The big clump of snow also breaks apart in a spectacular manner, also showcasing compression and fracturing beyond the boundary friction effect we just looked at. Oh my! This is beautiful. Okay, now, this is a computer graphics paper. If you are a seasoned Fellow Scholar, you know that this means\u2026it means that it is time to put some virtual bunnies into the oven. This is a great example for rule number one for watching physics simulations, which is that we discuss the physics part, and not the visuals. So why are these bunnies blue? Well, let\u2019s chuck\u2019em in the oven and find out. Aha! They are color coded for temperature. Look, they start from -100C, that\u2019s the blue, and we see the colors change as they approach zero degrees celsius. At this point they don\u2019t yet start melting, but they are already falling apart, so it was indeed a good design decision to show the temperatures, because it tells us exactly what is going on here. Without it, we would be expecting melting. Well, can we see that melting in action too? You bet. Now, hold on to your papers, and bring forth the soft-ice-o-mat! This machine can not only create an exquisite dessert for computer graphics researchers, but also showcases the individual contributions of this new technique one by one. Look! There is the melting, yes! Add a little frosting, and there you go. Bon appetit! Now, as we feared, many of these larger-scale simulations require computing the physics for millions of particles. So how long does that take? When we need millions of particles, we typically have to wait a few minutes per frame, but if we have a smaller scene, we can get away with these computations in a few seconds per frame! Goodness, we went from hours per frame to seconds per frame in just one paper. Outstanding work. And also, wait a second\u2026if we are talking millions of particles, I wonder how much memory it takes to keep track of them? Let\u2019s see. Whoa\u2026this is very appealing. I was expecting a few gigabytes, yet it only asks for a fraction of it\u2026a couple hundred megabytes. So, with this hefty value proposition, it is no wonder that this paper has been accepted to the SIGGRAPH conference. This is the olympic gold medal of computer graphics research, if you will. Huge congratulations to the authors of  this paper\u2026 this  was quite an experience. What a  time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=JSNE_PIG1UQ",
        "paper_link": "https://cg.informatik.uni-freiburg.de/publications/2020_SIGGRAPH_snow.pdf",
        "paper_title": "An Implicit Compressible SPH Solver for Snow Simulation"
    },
    {
        "video_id": "MbZ0ld1ShFo",
        "video_title": "Perfect Virtual Hands - But At A Cost! \ud83d\udc50",
        "position_in_playlist": 498,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Constraining Dense Hand Surface Tracking with Elasticity\" is available here:\nhttps://research.fb.com/publications/constraining-dense-hand-surface-tracking-with-elasticity/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#vr",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. The promise of virtual reality, VR is indeed truly incredible. If one day it comes to fruition, doctors could be trained to perform surgery in a virtual environment, we could train better pilots with better flight simulators, expose astronauts to virtual zero-gravity simulations, you name it. This previous work uses a learning-based algorithm to teach a head-mounted camera to tell the orientation of our hands at all times. Okay, so what can we do with this? A great deal. For instance, we can type on a virtual keyboard, or implement all kinds of virtual user interfaces that we can interact with. We can also organize imaginary boxes, and of course, we can\u2019t leave out the the Two Minute Papers favorite, going into a physics simulation and playing with it with our own hands. But of course, not everything is perfect here, however. Look. Hand-hand interactions don\u2019t work so well, so folks who prefer virtual reality applications that include washing our hands should look elsewhere. And in this series, we often say \u201cone more paper down the line, and it will be significantly better\u201d. So now, here is the moment of truth, let\u2019s see that one more paper down the line. Let\u2019s go in guns blazing and give it examples with challenging hand-hand interactions, deformations, lots of self-contact and self-occlusion. Take a look at this footage. This seems like a nightmare for any hand reconstruction algorithm. Who the heck can solve this? And, look, interestingly, they also recorded the hand model with gloves on. How curious! And now, hold on to your papers because these are not gloves\u2026no, no, no. What you see here is the reconstruction of the hand model by a new algorithm. Look. It can deal with all of these rapid hand motions. And what\u2019s more, it also works on this challenging hand massage scene. Look at all those beautiful details! It not only fits like a glove here too, but I see creases, folds, and deformations too - this reconstruction is truly out of this world. To be able to do this, the algorithm has to output triangle meshes that typically contain over a hundred thousand faces. Please remember this as we will talk about it later. And now, let\u2019s see how it does all this magic, because there is plenty of magic under the hood. Let\u2019s look at the five ingredients that are paramount to getting an output of this quality. Ingredient number one is the physics term. Without it, we can\u2019t even dream of tracking self-occlusion and contact properly. Two, since there are plenty of deformations going on in the input footage, the deformation term accounts for that. It makes a huge difference in the reconstruction of the thumb here. And if you think \u201cwow, that is horrific\u201d, then you\u2019ll need to hold on to your papers for the next one, which is\u2026three\u2026the geometric consistency term. This one is not for the faint of the heart. You have been warned. Are you ready? Let\u2019s go. Yikes! A piece of advice, if you decide to implement this technique, make sure to include this geometric consistency term so no one has to see this footage ever again. Thank you. With the worst already behind us, let\u2019s proceed to ingredient number four, the photo-consistency term. This ensures that fingernail tips don\u2019t end up sliding into the finger. And five, the collision term fixes problems like this to make sure that the fingers don\u2019t penetrate each other. And this is an excellent paper, so in the evaluation section, these terms are also tested in isolation, and the authors tell us exactly how much each of these ingredients contribute to the solution. Now, these five ingredients are not cheap in terms of computation time, and remember, we also mentioned that many of these meshes have several hundred thousand faces. This means that this technique takes a very long time to compute all this. It is not real time, not even close, for instance, reconstructing the mesh for the hand massage scene takes more than 10 minutes per frame. This means hours, or even days of computation to accomplish this. Now the question naturally arises - is that a problem? No, not in the slightest. This is a zero to one paper, which means that it takes a problem that was previously impossible, and now, it makes it possible. That is absolutely amazing. And as always, research is a process, and this is an important stepping stone in this process. I bet that two more good papers down the line, and we will be getting these gloves interactively. I am so happy about this solution, as it could finally give us new ways interact with each other in virtual spaces, add more realism to digital characters, help us better understand human-human interactions, and it may also enable new applications in physical rehabilitation. And these reconstructions indeed fit like a glove. What a time  to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=MbZ0ld1ShFo",
        "paper_link": "https://research.fb.com/publications/constraining-dense-hand-surface-tracking-with-elasticity/",
        "paper_title": "Constraining Dense Hand Surface Tracking with Elasticity"
    },
    {
        "video_id": "o7dqGcLDf0A",
        "video_title": "These Neural Networks Have Superpowers! \ud83d\udcaa",
        "position_in_playlist": 499,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/ayush-thakur/taming-transformer/reports/-Overview-Taming-Transformers-for-High-Resolution-Image-Synthesis---Vmlldzo0NjEyMTY\n\n\ud83d\udcdd The paper \"Taming Transformers for High-Resolution Image Synthesis\" is available here:\nhttps://compvis.github.io/taming-transformers/\n\nTweet links:\nWebsite layout: https://twitter.com/sharifshameem/status/1283322990625607681\nPlots: https://twitter.com/aquariusacquah/status/1285415144017797126?s=12\nTypesetting math: https://twitter.com/sh_reya/status/1284746918959239168\nPopulation data: https://twitter.com/pavtalk/status/1285410751092416513\nLegalese: https://twitter.com/f_j_j_/status/1283848393832333313\nNutrition labels: https://twitter.com/lawderpaul/status/1284972517749338112\nUser interface design: https://twitter.com/jsngr/status/1284511080715362304\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to support the series, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I got so excited by the amazing results of this\u00a0 paper. I will try my best to explain why, and by\u00a0\u00a0 the end of this video, there will be a comparison\u00a0 that blew me away and I hope you will appreciate\u00a0\u00a0 it too. With the rise of neural network-based\u00a0 learning algorithms, we are living the advent\u00a0\u00a0 of image generation techniques. What you see\u00a0 here is a set of breathtaking results created\u00a0\u00a0 with a technique called StyleGAN2. This can\u00a0 generate images of humans, cars, cats, and more. As you see, the progress in machine\u00a0 learning-based image generation is just stunning. And don\u2019t worry for a second about the progress\u00a0 in text processing, because that is also\u00a0\u00a0 similarly amazing these days. A few months ago,\u00a0 OpenAI published their GPT-3 model that they\u00a0\u00a0 unleashed to read the internet, and learn not just\u00a0 our language, but much, much more. For instance,\u00a0\u00a0 the internet also contains a lot of computer code,\u00a0 so it learned to generate website layouts from a\u00a0\u00a0 written description. But that\u2019s not all, not even\u00a0 close, to the joy of technical PhD students around\u00a0\u00a0 the world, it can properly typeset mathematical\u00a0 equations from a plain English description as\u00a0\u00a0 well. And get this, it can also translate\u00a0 a complex legal text into plain language,\u00a0\u00a0 or, the other way around. And it does many\u00a0 of these things nearly as well as humans. So what was the key to this\u00a0 work? One of the keys of GPT-3\u00a0\u00a0 was that it uses a neural network architecture\u00a0 that is called the transformer network. These\u00a0\u00a0 really took the world by storm in the last few\u00a0 years, so our first question is, why transformers?\u00a0\u00a0 One, transformer networks can typically learn\u00a0 on stupendously large datasets, like the whole\u00a0\u00a0 internet, and extract a lot of information\u00a0 from it. That is a very good thing. And two,\u00a0\u00a0 transformers are attention-based neural networks,\u00a0 which means that they are good at learning and\u00a0\u00a0 generating long sequences of data. Okay, but\u00a0 how do we benefit from this? Well, when we ask\u00a0\u00a0 OpenAI\u2019s GPT-3 to continue our sentences, it\u00a0 is able to look back at what we have written\u00a0\u00a0 previously. And it looks at not just a couple\u00a0 of characters, no-no, it looks at up to several\u00a0\u00a0 pages of writing backwards to make sure that\u00a0 it continues what we write the best way it can. This sounds amazing. But what is the lesson here?\u00a0 Just use transformers for everything and off we\u00a0\u00a0 go? Well, not quite. They are indeed good at a lot\u00a0 of things when it comes to text processing tasks,\u00a0\u00a0 but they don\u2019t excel at generating high-resolution\u00a0 images at all. Can this be improved somehow?\u00a0\u00a0 Well, this is what this new\u00a0 technique does, and much, much more. So let\u2019s dive in and see what it can do! First,\u00a0\u00a0 we can give it an incomplete\u00a0 image and ask it to finish it. Not bad\u2026 but! OpenAI\u2019s Image-GPT could do that\u00a0 too, so what else can it do? Oh boy, a lot more!\u00a0\u00a0 And by the way, we will compare the results of\u00a0 this technique against Image-GPT at the end of\u00a0\u00a0 this video, make sure not to miss that, I almost\u00a0 fell off the chair, you will see in a moment why. Two, it can do one of my favorites, depth\u00a0 to image generation. We give it a depth map,\u00a0\u00a0 which is very easy to produce, and it creates\u00a0 a photorealistic image that corresponds to it,\u00a0\u00a0 which is very hard. We do the easy\u00a0 part, the AI does the hard part. Great!\u00a0\u00a0 And with this, we not only get a selection of\u00a0 these images, but since we have their depth maps,\u00a0\u00a0 we can also rotate them around\u00a0 as if they were 3D objects. Nice! Three, we can also give it a map of\u00a0 labels, which is, again, very easy to do,\u00a0\u00a0 we just say here goes the sea, put\u00a0 some mountains here, and the sky here,\u00a0\u00a0 and it will create a beautiful landscape\u00a0 image that corresponds to that.\u00a0\u00a0 I can\u2019t wait to see what these amazing artists all\u00a0 over the world will be able to get out of these\u00a0\u00a0 techniques, and these results are already\u00a0 breathtaking\u2026but research is a process,\u00a0\u00a0 and just imagine how good they will become\u00a0 two more papers down the line. My goodness! Four, it can also perform super resolution.\u00a0 This is the CSI thing where in goes a blurry\u00a0\u00a0 image, and out comes a finer, more\u00a0 detailed version of it. Witchcraft. And finally, five, we can give it a pose, and\u00a0 it generates humans that take these poses. Now, the important thing here\u00a0 is that it can supercharge\u00a0\u00a0 transformer networks to do these things\u00a0 at the same time, with just one technique. So how does it compare to OpenAI\u2019s Image\u00a0 completion technique? Well, remember,\u00a0\u00a0 that technique was beyond amazing, and set a\u00a0 really high bar. So let\u2019s have a look together!\u00a0\u00a0 They were both given the upper half of this image,\u00a0 and had to fill in the lower half. Remember,\u00a0\u00a0 as we just learned transformers are not\u00a0 great at high-resolution image synthesis.\u00a0\u00a0 So here, for OpenAI Image-GPT we expect heavily\u00a0 pixelated images\u2026.and\u2026oh yes, that\u2019s right.\u00a0\u00a0 So now, hold on to your papers, and let\u2019s see\u00a0 how much more detailed the new technique is.\u00a0\u00a0 Holy mother of papers! Do you see what I see\u00a0 here? Image-GPT came out just a few months ago,\u00a0\u00a0 and there is already this kind of progress. So\u00a0 there we go, just imagine what we will be able\u00a0\u00a0 to do with these supercharged transformers\u00a0 just two more papers down the line.\u00a0\u00a0 Wow. And that\u2019s where I almost fell off the\u00a0 chair when reading this paper. Hope you held\u00a0\u00a0 on to yours. It truly feels like we are living in\u00a0 a science fiction world. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=o7dqGcLDf0A",
        "paper_link": "https://compvis.github.io/taming-transformers/",
        "paper_title": "Taming Transformers for High-Resolution Image Synthesis"
    },
    {
        "video_id": "ZZ-kORb8grA",
        "video_title": "This AI Learn To Climb Crazy Terrains! \ud83e\udd16",
        "position_in_playlist": 500,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper ALLSTEPS: Curriculum-driven Learning of Stepping Stone skills\"\" is available here:\n- https://www.cs.ubc.ca/~van/papers/2020-allsteps/index.html\n- https://github.com/belinghy/SteppingStone\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord. If you drop by, make sure to write a short introduction if you feel like it! https://discordapp.com/invite/hbcTJu2\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1696507/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In 2017, scientists at OpenAI published a paper where virtual humans learned to tackle each other in a sumo competition of sorts, and found out how to rock a stable stance to block others from tackling them. This was a super interesting work because it involved self-play, or in other words, copies of the same AI were playing against each other, and the question was, how do we pair them with each other to maximize their learning. They found something really remarkable when they asked the algorithm to defeat an older version of itself. If it can reliably pull that off, it will lead to a rapid and predictable learning process. This kind of curriculum-driven learning can supercharge many different kinds of AIs. For instance, this robot from a later paper is essentially blind as it only has proprioceptive sensors, which means that the only thing that the robot senses is its own internal state and that\u2019s it. No cameras, depth sensors, no LIDAR, nothing. And at first, it behaves as we would expect it\u2026look, when we start out, the agent is very clumsy and can barely walk through a simple terrain\u2026 but as time passes, it grows to be a little more confident, and with that, the terrain also becomes more difficult over time in order to maximize learning. That is a great life lesson right there. So, how potent is this kind of curriculum in teaching the AI? Well, it learned a great deal in the simulation, and as scientists deployed it into the real world, just look at how well it traversed through this rocky mountain, stream, and not even this nightmarish snowy descent gave it too much trouble. This new technique proposes a similar curriculum-based approach where we would teach all kinds of virtual lifeforms to navigate on stepping stones. The examples include a virtual human, a bipedal robot called cassie, and\u2026this sphere with toothpick legs too. The authors call it \u201cmonster\u201d, so, you know what, monster it is. So, the fundamental question here is, how do we organize the stepping stones in this virtual environment to deliver the best teaching to this AI? We can freely choose the heights and orientations of the upcoming steps, and\u2026of course, it is easier said than done. If the curriculum is too easy, no meaningful learning will take place, and if gets too difficult too quickly, well\u2026then\u2026in the better case, this happens\u2026and in the worst case, whoops! This work proposes an adaptive curriculum that constantly measures how these agents perform, and creates challenges that progressively get harder, but in a way that they can be solved by the agents. It can even deal with cases where the AI already knows how to climb up and down, and even deal with longer steps. But that does not mean that we are done, because if we don\u2019t build these spirals right, this happens. But, after learning 12 to 24 hours with this adaptive curriculum learning method, they become able to even run, deal with huge step height variations, high step tilt variations, and let\u2019s see if they can pass the hardest exam\u2026look at this mess, my goodness, lots of variation in every parameter. And\u2026Yes! It works! And the key point is that the system is general enough that it can teach different body types to do the same. If there is one thing that you take home from this video, it shouldn\u2019t be that it takes from 12 to 24 hours. It should be that the system is general. Normally, if we have a new body type, we need to write a new control algorithm, but in this case, whatever the body type is, we can use the same algorithm to teach it. Absolutely amazing. What a time to be alive! However, I know what you\u2019re thinking. Why teach them to navigate just stepping stones? This is such a narrow application of locomotion, so why this task? Great question, and the answer is that the generality of this technique we just talked about also means that the stepping stone navigation truly was just a stepping stone, and here it is - we can deploy these agents to a continuous terrain and expect them to lean on their stepping stone chops to navigate well here too. Another great triumph for curriculum-based AI training environments. So what do you think? What would you use this technique for? Let me know in the comments, or if you wish to discuss similar topics with other Fellow Scholars in a warm and welcoming environment, make sure to join our Discord channel. The link is available in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZZ-kORb8grA",
        "paper_link": "https://www.cs.ubc.ca/~van/papers/2020-allsteps/index.html",
        "paper_title": ""
    },
    {
        "video_id": "2wcw_O_19XQ",
        "video_title": "This is What Abraham Lincoln May Have Looked Like! \ud83c\udfa9",
        "position_in_playlist": 501,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/instacolorization/reports/Overview-Instance-Aware-Image-Colorization---VmlldzoyOTk3MDI\n\n\ud83d\udcdd The paper \"Time-Travel Rephotography\" is available here:\nhttps://time-travel-rephotography.github.io/\n\n\ud83d\udcdd Our \"Separable Subsurface Scattering\" paper with Activision Blizzard is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to travel in time. With the ascendancy of neural-network based learning techniques, this previous method enables us to take and old old black and white movie that suffers from a lot of problems like missing data, flickering, and more, give it to a neural network, and have it restore it for us. And here, you can not only see how much better this restored version is, but, it did take it one step further. It also performed colorization! Essentially, here, we could produce 6 colorized reference images, and the neural network uses them as art direction and propagates all this information to the remainder of the frames. So this work did restoration and colorization at the same time. This was absolutely amazing, and now comes something even better, today, we have a new piece of work that performs not only restoration and colorization, but super-resolution as well! What this means is that we can take an antique photo, which suffers from a lot of issues. Look, these old films exaggerate wrinkles a great deal, they even even darken the lips and do funny things with red colors. For instance, subsurface scattering is also missing, this is light penetrating our skin and bouncing inside before coming out again, and the lack of this effect is why the skin looks a little plasticky here. Luckily, we can simulate all these phenomena on our computers. I am a light transport researcher by trade, and this is from our earlier paper with the Activision Blizzard game development company, this is the same phenomenon, a simulation without subsurface scattering, and this one is with simulating this effect. Beautiful. You can find a link to this paper in the video description. So with all these problems with the antique photos, our question is, what did Lincoln really look like? Well, let\u2019s try an earlier framework for restoration, colorization and super resolution\u2026and. Well, unfortunately, most of our issues still remain. Lots of exaggerated wrinkles, plasticky look, lots of detail is missing. Can we do better? Well, hold on to your papers, and observe the output with the new technique. Wow. The restoration indeed took place properly, brought the wrinkles down to a much more realistic level, skin looks like skin because of subsurface scattering, and the super resolution part is responsible for a lot of new detail everywhere, but especially around the lips. Outstanding. It truly feels like this photo has been rephotographed with a modern camera. And with that, please meet Time-Travel Rephotography. And the curious thing is that all this sounds flat out impossible. Why is that? Since we don\u2019t have old and new image pairs of Lincoln and many other historic figures, the question naturally arises in the mind of the curious Fellow Scholar - how do we train a neural network to perform this? And the answer is that we need to use their siblings. Now this doesn\u2019t mean that Lincoln had a long lost sibling that we don\u2019t know about. What this means is that as the input image is fed through our neural network, we can generate a photorealistic image of someone, and this someone kind of resembles the target subject, and has all the details filled in. Then, in the next step, we can start morphing the sibling until is starts resembling the test subject. With this previously existing StyleGAN2 technique, morphing is now easy to do, but restoration is hard, so essentially, with this, we can skip the difficult restoration part, and just do the easier morphing instead. Trading a difficult problem for an easier one. Absolutely brilliant idea. And if you have been holding on to your papers so far, now, squeeze that paper, because it can do even more. Age progression! Look. If we have only a few a few target photos of Thomas Edison throughout his life, these will be our yardsticks, and the algorithm is able to generate his aging process between these yardstick images. And the best part is that these images have different lighting, pose, and none of this is an issue for the technique. It just doesn\u2019t care and it still works beautifully. Wow. So we saw earlier that there are other methods that attempt to do this too, at least the colorization part. Yes, we have colorization and other techniques in abundance. So how does this compare to them? It appears to outpace all of them really convincingly. The numbers from the user study and the algorithmically generated scores also favor the new technique. This is a huge leap forward. Do you have some other applications in mind for this new technique? Let me know in the comments what you would do with this or how you would like to see it improved. Now, of course, not even this technique is perfect. Blurry and noisy regions can still appear here and there. And note that StyleGAN2, the basis for this algorithm came out just a little more than a year ago. And it is amazing that we are witnessing such incredible progress in so little time. My goodness. And just imagine what the next paper down the line will bring! What a  time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2wcw_O_19XQ",
        "paper_link": "https://time-travel-rephotography.github.io/\n\n\ud83d\udcdd Our \"Separable Subsurface Scattering\" paper with Activision Blizzard is available here:",
        "paper_title": "Time-Travel Rephotography"
    },
    {
        "video_id": "-Ny-p-CHNyM",
        "video_title": "Finally, Instant Monsters! \ud83d\udc09",
        "position_in_playlist": 502,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU\n\n\ud83d\udcdd The paper \"Monster Mash: A Single-View Approach to Casual 3D Modeling and Animation\" is available here:\nhttps://dcgi.fel.cvut.cz/home/sykorad/monster_mash\n\nWeb demo - make sure to click \"Help\" and read the instructions:\nhttp://monstermash.zone/#\n\nMore on Flow by Mih\u00e1ly Cs\u00edkszentmih\u00e1lyi - it is immensely important to master this!\nhttps://www.youtube.com/watch?v=8h6IMYRoCZw\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Lau, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If we wish to create an adorable virtual monster\u00a0 and animate it, we first have to engage in 3D\u00a0\u00a0 modeling. Then, if we wish to make it move,\u00a0 and make this movement believable, we have\u00a0\u00a0 to specify where the bones and joints are located\u00a0 within the model. This process is called rigging.\u00a0\u00a0 As you see, it is quite laborious, and requires\u00a0 expertise in this domain to pull this off. And now, imagine this process with no 3D\u00a0 modeling, and no rigging. This is, of course,\u00a0\u00a0 impossible\u2026right? Well, now, you know what\u2019s\u00a0 coming, so hold on to your papers, because\u00a0\u00a0 here is a newer technique that indeed performs\u00a0 the impossible. All we need to do is grab a\u00a0\u00a0 pencil and create a rough sketch of our character,\u00a0 then, it will take a big breath, and inflate it\u00a0\u00a0 into a 3D model. This process was nearly 7 times\u00a0 faster than the classical workflow, but what\u00a0\u00a0 matters even more, this new workflow requires\u00a0 zero expertise in 3D modeling and rigging.\u00a0\u00a0 With this technique, absolutely\u00a0 anybody can become an artist. So, we noted that these models can also be\u00a0 animated. Is that so? Yes, that\u2019s right,\u00a0\u00a0 we can indeed animate these models by using\u00a0 these red control points. And even better,\u00a0\u00a0 we get to specify where these points go. That\u2019s\u00a0 a good thing because we can make sure that a\u00a0\u00a0 prescribed part can move around, opening up\u00a0 the possibility of creating and animating\u00a0\u00a0 a wide range of characters. And I would say\u00a0 all this can be done in a matter of minutes,\u00a0\u00a0 but even better, sometimes\u00a0 even within a minute. Whoa. This new technique does a lot\u00a0 of legwork that previous methods\u00a0\u00a0 were not able to pull off so well. For instance,\u00a0 it takes a little information about which part\u00a0\u00a0 is in front or behind the model. Then, it\u00a0 stitches all of these strokes together and\u00a0\u00a0 inflates our drawing into a 3D model, and it\u00a0 does this better than previous methods, look.\u00a0\u00a0 Well, okay, the new one looks a bit better where\u00a0 the body parts connect here, and that\u2019s it?\u00a0\u00a0 Wait a second\u2026a ha! Somebody\u00a0 didn\u2019t do their job correctly.\u00a0\u00a0 And we went from this work to this, in just\u00a0 two years. This progress is absolute insanity. Now let\u2019s have a look at a full workflow from\u00a0 start to end. First, we draw the strokes,\u00a0\u00a0 note that we can specify that one arm and leg is\u00a0 in front of the body, and the other is behind,\u00a0\u00a0 and, bam the 3D model is now done! Wow, that was\u00a0 quick. And now, add the little red control points\u00a0\u00a0 for animation, and let the fun begin! Mister, your\u00a0 paper has been officially accepted! Move the feet,\u00a0\u00a0 pin the hands, rock the body. Wait. Not only\u00a0 that, but this paper was accepted to the\u00a0\u00a0 SIGGRAPH ASIA conference, which is equivalent\u00a0 to winning an olympic gold medal in computer\u00a0\u00a0 graphics research if you will, so add a little\u00a0 neck movement too. Oh yeah! Now we\u2019re talking! With this technique, the possibilities really feel\u00a0 endless. We can animate humanoids, monsters, other\u00a0\u00a0 adorable creatures, or can even make scientific\u00a0 illustrations come to life without any modeling\u00a0\u00a0 and rigging expertise. Do you remember this\u00a0 earlier video where we could paint on a piece of\u00a0\u00a0 3D geometry, and transfer its properties onto a 3D\u00a0 model? This method can be combined with that too.\u00a0\u00a0 Yum! And in case you are wondering how quick this\u00a0 combination is - my goodness. Very, very quick. Now, this technique is also not perfect,\u00a0 one of the limitations of this single-view\u00a0\u00a0 drawing workflow is that we have only limited\u00a0 control over the proportions in depth. Texturing\u00a0\u00a0 occluded regions is also not that easy. The\u00a0 authors proposed possible solutions to these\u00a0\u00a0 limitations in the paper, so make sure to have a\u00a0 look in the video description, and it appears to\u00a0\u00a0 me that with a little polishing, this may be\u00a0 ready to go for artistic projects right now.\u00a0\u00a0 If you have a closer look, you will also\u00a0 see that this work also cites the flow paper\u00a0\u00a0 from Mihaly Csikszentmihalyi.\u00a0 Extra style points for that. And with that said, when can we use this?\u00a0\u00a0 And that\u2019s the best part - right now! The authors\u00a0 really put their papers where their mouth is,\u00a0\u00a0 or in other words, the source code for\u00a0 this project is available, also, there is,\u00a0\u00a0 an online demo. Wo-hoo! The link is available\u00a0 in the video description, make sure to read the\u00a0\u00a0 instructions before you start. So there you\u00a0 go, instant 3D models with animation without\u00a0\u00a0 requiring 3D modeling and rigging expertise. What\u00a0 do you think? Let me know in the comments below! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=-Ny-p-CHNyM",
        "paper_link": "https://dcgi.fel.cvut.cz/home/sykorad/monster_mash\n\nWeb demo - make sure to click \"Help\" and read the instructions:\nhttp://monstermash.zone/#",
        "paper_title": "Monster Mash: A Single-View Approach to Casual 3D Modeling and Animation"
    },
    {
        "video_id": "9kllWAX9tHw",
        "video_title": "Differentiable Material Synthesis Is Amazing! \u2600\ufe0f",
        "position_in_playlist": 503,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"MATch: Differentiable Material Graphs for Procedural Material Capture\" is available here:\nhttp://match.csail.mit.edu/\n\n\ud83d\udcdd Our Photorealistic Material Editing paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-editing/\n\n\u2600\ufe0f The free course on writing light simulations is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic, Jace O'Brien, Javier Bustamante, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image: https://pixabay.com/images/id-4238615/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I am a light transport simulation researcher\u00a0 by trade, and I am very happy today, because we\u00a0\u00a0 have an absolutely amazing light transport paper\u00a0 we\u2019re going to enjoy today. As many of you know,\u00a0\u00a0 we write these programs that you can run on your\u00a0 computer to simulate millions and millions of\u00a0\u00a0 light rays, and calculate how they get absorbed or\u00a0 scattered off of our objects in a virtual scene.\u00a0\u00a0 Initially, we start out with a really noisy image,\u00a0 and as we add more rays, the image gets clearer\u00a0\u00a0 and clearer over time. We can also simulate\u00a0 sophisticated material models in these programs.\u00a0\u00a0 A modern way of doing that is\u00a0 through using these material nodes. With these, we can conjure up a ton of\u00a0 different material models and change\u00a0\u00a0 their physical properties to our liking. As you\u00a0 see, they are very expressive indeed, however,\u00a0\u00a0 the more nodes we use, the less clear it becomes\u00a0 how they interact with each other. And as you see,\u00a0\u00a0 every time we change something, we have to wait\u00a0 until a new image is rendered. That is very time\u00a0\u00a0 consuming, and more importantly, we have to have\u00a0 some material editing expertise to use this. This concept is very powerful, for instance, I\u00a0 think if you watch the Perceptilabs sponsorship\u00a0\u00a0 spot at the end of this video, you will be very\u00a0 surprised to see that they also use node groups,\u00a0\u00a0 but with theirs, you don\u2019t build material\u00a0 models, you can build machine learning models. What would be really cool if we could just give\u00a0 the machine a photo and it would figure out how\u00a0\u00a0 to set these nodes up so it looks exactly like\u00a0 the material in the photo. So, is that possible,\u00a0\u00a0 or is that science fiction? Well, have a look our\u00a0 paper, called Photorealistic Material Editing.\u00a0\u00a0 With this technique, we can easily create these\u00a0 beautiful material models in a matter of seconds,\u00a0\u00a0 even if we don\u2019t know a thing\u00a0 about light transport simulations.\u00a0\u00a0 It does something that is similar to\u00a0 what many call differentiable rendering. Here is the workflow, we give it a bunch\u00a0 of images like these, which were created on\u00a0\u00a0 this particular test scene, and it guesses what\u00a0 parameters to use to get these material models.\u00a0\u00a0 Now, of course, this doesn\u2019t make any\u00a0 sense whatsoever, because we have produced\u00a0\u00a0 these images ourselves, so we know exactly what\u00a0 parameters to use to produce this. In other words,\u00a0\u00a0 this thing seems useless. And now comes the magic\u00a0 part, because we don\u2019t use these images. No-no! We load them into Photoshop, and edit them to our\u00a0 liking, and just pretend that these images were\u00a0\u00a0 created with the light simulation program. This\u00a0 means that we can create a lot of quickly, really\u00a0\u00a0 poorly executed edits. For instance, the stitched\u00a0 specular highlight in the first example isn\u2019t very\u00a0\u00a0 well done, and neither is the background of the\u00a0 gold target image in the middle. However, the\u00a0\u00a0 key observation is that we built a mathematical\u00a0 framework, which makes this pretending really\u00a0\u00a0 work! Look, in the next step, our method proceeds\u00a0 to find a photorealistic material description\u00a0\u00a0 that, when rendered, resembles this target\u00a0 image, and works well even in the presence\u00a0\u00a0 of these poorly executed edits. So these\u00a0 materials are completely made up in Photoshop,\u00a0\u00a0 and it turns out, we can create photorealistic\u00a0 materials through these node graphs that look\u00a0\u00a0 almost exactly the same. Quite remarkable.\u00a0 The whole process executes in 20 seconds. If you are one of the more\u00a0 curious Fellow Scholars out there,\u00a0\u00a0 this paper and its source code are\u00a0 available in the video description. Now, this differentiable thing has a lot\u00a0 of steam. For instance, there are more\u00a0\u00a0 works on differentiable rendering. In this\u00a0 other work, we can take a photo of a scene,\u00a0\u00a0 and a learning-based method turns the\u00a0 knobs until it finds a digital object\u00a0\u00a0 that matches its geometry and material\u00a0 properties. This was a stunning piece of work,\u00a0\u00a0 from Wenzel Jakob and his group, of course, who\u00a0 else. They are some of the best in the business. And we don\u2019t even need to be in the area of light\u00a0 transport simulations to enjoy the benefits of\u00a0\u00a0 differentiable formulations, for instance, this\u00a0 is differentiable physics. So what is that? Imagine that we have this billiard game, where\u00a0 we would like to hit the white ball with just\u00a0\u00a0 the right amount of force and from the right\u00a0 direction, such that the blue ball ends up close\u00a0\u00a0 to the black spot. Well, this example shows that\u00a0 this is unlikely to happen by chance, and we have\u00a0\u00a0 to engage in a fair amount of trial and error\u00a0 to make this happen. What this differentiable\u00a0\u00a0 programming system does for us is that we can\u00a0 specify an end state, which is the blue ball\u00a0\u00a0 on the black dot, and it is able to compute the\u00a0 required forces and angles to make this happen.\u00a0\u00a0 Very close. So after you look here,\u00a0\u00a0 maybe you can now guess what\u2019s next\u00a0 for this differentiable technique\u2026it\u00a0\u00a0 starts out with a piece of simulated ink\u00a0 with a checkerboard pattern, and it exerts\u00a0\u00a0 just the appropriate forces so that it forms\u00a0 exactly the Yin-Yang symbol shortly after. And now that we understand what\u00a0 differentiable techniques are capable of,\u00a0\u00a0 we are ready to proceed to today\u2019s paper. This is\u00a0 a proper, fully differentiable material capture\u00a0\u00a0 technique for real photographs. All this needs\u00a0 is one flash photograph of a real-world material.\u00a0\u00a0 We have those around us in abundance, and\u00a0 similarly to our previous method, it sets up\u00a0\u00a0 the material nodes for it. That is a good thing,\u00a0 because I don\u2019t know about you, but I do not want\u00a0\u00a0 to touch this mess at all. Luckily, we don\u2019t have\u00a0 to, look! The left is the target photo, and the\u00a0\u00a0 right is the initial guess of the algorithm,\u00a0 that is not bad, but also not very close.\u00a0\u00a0 And now, hold on to your papers and just look at\u00a0 how it proceeds to refine this material until it\u00a0\u00a0 closely matches the target. And with that, we have\u00a0 a digital representation of these materials. We\u00a0\u00a0 can now easily build a library of these materials\u00a0 and assign them to the objects in our scene.\u00a0\u00a0 And then, we run the light simulation\u00a0 program, and here we go. Beautiful. At this point, if we feel adventurous, we can\u00a0 adjust small things in the material graphs to\u00a0\u00a0 create a digital material that is more in line\u00a0 with our artistic vision. That is great, because\u00a0\u00a0 it is must easier to adjust an already existing\u00a0 material model than creating one from scratch. So what are the key differences\u00a0 between our work from last year,\u00a0\u00a0 and this new paper? Our work made a rough initial\u00a0 guess and optimized the parameters afterwards,\u00a0\u00a0 it was also chock full of neural networks,\u00a0 it also created materials from a sample,\u00a0\u00a0 but that sample was not a photograph, but\u00a0 a photoshopped image. That is really cool,\u00a0\u00a0 however, this new method takes an almost\u00a0 arbitrary photo, many of these we can take\u00a0\u00a0 ourselves or even get them from the internet,\u00a0 therefore this new method is more general. It also supports 131 different\u00a0 material node types, which is insanity.\u00a0\u00a0 Huge congratulations to the\u00a0 authors, if I would be an artist,\u00a0\u00a0 I would want to work with this right\u00a0 about now. What a time to be alive! So there you go, this was quite a ride, and I\u00a0 hope you enjoyed it just half as much as I did.\u00a0\u00a0 And if you enjoyed it at least as much as I did,\u00a0 and you feel a little stranded at home and are\u00a0\u00a0 thinking that this light transport thing is pretty\u00a0 cool, and you would like to learn more about it,\u00a0\u00a0 I held a Master-level course on this topic\u00a0 at the Technical University of Vienna.\u00a0\u00a0 Since I was always teaching it to a handful of\u00a0 motivated students, I thought that the teachings\u00a0\u00a0 shouldn\u2019t only be available for the privileged\u00a0 few who can afford a college education, but\u00a0\u00a0 the teachings should be available for everyone.\u00a0 Free education for everyone, that\u2019s what I want.\u00a0\u00a0 So, the course is available free of charge\u00a0 for everyone, no strings attached, so make\u00a0\u00a0 sure to click the link in the video description\u00a0 to get started. We write a full light simulation\u00a0\u00a0 program from scratch there, and learn about\u00a0 physics, the world around us, and more. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9kllWAX9tHw",
        "paper_link": "http://match.csail.mit.edu/",
        "paper_title": "MATch: Differentiable Material Graphs for Procedural Material Capture"
    },
    {
        "video_id": "I04zRq6UlIg",
        "video_title": "This Magnetic Simulation Took Nearly A Month! \ud83e\uddf2",
        "position_in_playlist": 504,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"A Level-Set Method for Magnetic Substance Simulation\" is available here:\nhttps://binwangbfa.github.io/publication/sig20_ferrofluid/SIG20_FerroFluid.pdf\nhttps://starryuniv.cn/\nhttp://vcl.pku.edu.cn/publication/2020/magnetism\nhttps://starryuniv.cn/publication/a-level-set-method-for-magnetic-substance-simulation/\nSome links may be  down, trying to add several of them to make sure you find one that works!\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Have a look at these beautiful ferrofluid simulations from a previous paper. These are fluids that have magnetic properties and thus respond to an external magnetic field, and you are seeing correctly, they are able to even climb things. And the best part is that simulator was so accurate that we could run it side-by-side with real-life footage, and see that they run very similarly. Excellent. Now, running these simulations took a considerable amount of time. To address this, a followup paper appeared that showcased a surface-only formulation. What does that mean? Well, a key observation here was that for a class of ferrofluids, we don\u2019t have to compute how the magnetic forces act on the entirety of the 3D fluid domain, we only have to compute them on the surface of the model. So what does this get us? Well, these amazing fluid labyrinths, and all of these ferrofluid simulations, but\u2026 faster! So, remember, the first work did something new, but took a very long time, and the second work improved it to make it faster and more practical. Please remember this for later in this video. And now let\u2019s fast forward to today\u2019s paper, this new work can also simulate ferrofluids, and not only that, but also supports a broader range of magnetic phenomena, including rigid and deformable magnetic bodies and two-way coupling too! Oh my! That is sensational, but first, what do these terms mean exactly? Let\u2019s perform four experiments, and after you watch them, I promise that you will understand all about them. Let\u2019s look at the rigid bodies first in experiment number one. Iron box versus magnet. We are starting out slow, and now, we are waiting for the attraction to kick in, and there we go. Wonderful. Experiment number two, deformable magnetic bodies. In other words, magnetic lotus versus a moving magnet. This one is absolutely beautiful, look at how the the petals here are modeled as thin elastic sheets that dance around in the presence of a moving magnet. And if you think this is dancing, stay tuned, there will be an example with even better dance moves in a moment. And experiment number three, two-way coupling. We noted this coupling thing earlier, so, what does that mean? What coupling means is that here, the water can have an effect on the magnet, and the two-way part means that in return, the magnet can also have an effect on the water as well. This is excellent, because we don\u2019t have to think about the limitations of the simulation, we can just drop nearly anything into our simulation domain, be it a fluid, solid, magnetic or not, and we can expect that their interactions are going to be modeled properly. Outstanding. And, I promised some more dancing, so here goes, experiment number four, the dancing ferrofluid. I love how informative the compass is here, it is a simple object that tells us how an external magnetic field evolves over time. I love this elegant solution. Normally, we have to visualize the magnetic induction lines so we can better see why the tentacles of a magnetic octopus move, or why two ferrofluid droplets repel or attract each other. In this case, the authors opted for a much more concise and elegant solution, and I also liked that the compass is not just a 2D overlay, but a properly shaded 3D object with specular reflections as well. Excellent attention to detail. This is really my kind of paper. Now, these simulations were not run on any kind of supercomputer or a network of computers, this runs on the processor of your consumer machine at home. However, simulating even the simpler scenes takes hours. For more complex scenes, even days. And that\u2019s not all, the ferrofluid with the Yin-Yang symbol took nearly a month to compute. So, is that a problem? No, no, of course not. Not in the slightest. Thanks to this paper, general magnetic simulations that were previous impossible are now possible, and don\u2019t forget, research is a process. As you saw in the example at the start of this video with the surface-only ferrofluid formulation, it may become much faster just one more paper down the line. I wanted to show you the first two papers in this video to demonstrate how quickly that can happen. And two more papers down the line, oh my, then, the sky is the limit. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=I04zRq6UlIg",
        "paper_link": "https://binwangbfa.github.io/publication/sig20_ferrofluid/SIG20_FerroFluid.pdf",
        "paper_title": "A Level-Set Method for Magnetic Substance Simulation"
    },
    {
        "video_id": "lxzGraohijU",
        "video_title": "5 Crazy Simulations That Were Previously Impossible! \u26d3",
        "position_in_playlist": 505,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-With-W-B-CAM-and-GradCAM--Vmlldzo5MTIyNw\n\n\ud83d\udcdd The paper \"Incremental Potential Contact: Intersection- and Inversion-free Large Deformation Dynamics\" is available here:\nhttps://ipc-sim.github.io/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If we study the laws of physics and program them into a computer, we can create a beautiful simulation that showcases the process of baking, and if we so desire, when we are done, we can even tear a loaf of bread apart. And with this previous method, we can also smash oreos, candy crabs, pumpkins, and much, much more. This jelly fracture scene is my long-time favorite. And this new work asks a brazen question only a proper computer graphics researcher could ask - can we write an even more extreme simulation? I don\u2019t think so, but apparently, this paper promises a technique that supports more extreme compression and deformation, and when they say that, they really mean it. Let\u2019s see what this can do through five super fun experiments. Experiment number one. Squishing. As you see, this paper aligns well with the favorite pastimes of a computer graphics researcher, which is, of course, destroying virtual objects in a spectacular fashion. First, we force these soft elastic virtual objects through a thin obstacle tube. Things get quite squishy here\u2026ouch! And, when they come out on the other side, their geometries can also separate properly. And watch how beautifully they regain their original shapes afterwards. Experiment number two. The tendril test. We grab a squishy ball, and throw it at the wall, and here comes the cool part, this panel was made of glass, so we also get to view the whole interaction through it, and this way, we can see all the squishing happening. Look - the tendrils are super detailed and every single one remains intact and intersection-free despite the intense compression. Outstanding. Experiment number three. The twisting test. We take a piece of mat, and keep twisting and twisting, and\u2026 still going. Note that the algorithm has to compute up to half a million contact events every time it advances the time a tiny bit, and still, no self intersections, no anomalies. This is crazy. Some of our more seasoned Fellow Scholars will immediately ask - okay, great, but how real is all this? Is this just good enough to fool the untrained eye, or does it really simulate what would happen in reality? Well, hold on to your papers, because here comes my favorite part in these simulation papers and this is when we let reality be our judge, and try to reproduce real-world footage with a simulation. Experiment number four, the high-speed impact test. Here is the real footage of a foam practice ball fired at a plate. And now, at the point of impact, this part of the ball has stopped, but the other side is still flying with a high velocity. So what will be the result? A ton of compression. So what does the simulator say about this? My goodness. Just look at that. This is really accurate. Loving it. This sounds all great, but do we really need this technique? The answer shall be given by experiment number five, ghosts and chains. What could that mean? Here, you see Houdini\u2019s Vellum, the industry standard simulator for cloth, soft-body and a number of other kinds of simulations. It is an absolutely amazing tool, but, wait a second. Look. Artificial ghost forces appear, even on a simple test case with 35 chain links. And I wonder if the new method can deal with these 35 chain links? The answer is a resounding yes. No ghost forces. And not only that, but it can deal with even longer chains, let\u2019s try a 100 links. Oh, yeah! Now we\u2019re talking! And now, only one question remains. How much do we have to wait for all this? All this new technique asks for is a few seconds per frame for the simpler scenes, and in the order of minutes per frame for the more crazy tests out there. Praise the papers! That is a fantastic deal. And, what is even more fantastic, all this is performed on your processor, so of course, if someone can implement it in a way that it runs on the graphics card, the next paper down the line will be much, much faster. What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lxzGraohijU",
        "paper_link": "https://ipc-sim.github.io/",
        "paper_title": "Incremental Potential Contact: Intersection- and Inversion-free Large Deformation Dynamics"
    },
    {
        "video_id": "B8RMUSmIGCI",
        "video_title": "3 New Things An AI Can Do With Your Photos!",
        "position_in_playlist": 506,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/mathisfederico/wandb_features/reports/Visualizing-Confusion-Matrices-With-W-B--VmlldzoxMzE5ODk\n\n\ud83d\udcdd The paper \"GANSpace: Discovering Interpretable GAN Controls\" is available here:\nhttps://github.com/harskish/ganspace\n\n\ud83d\udcdd Our material synthesis paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\udcdd The font manifold paper is available here:\nhttp://vecg.cs.ucl.ac.uk/Projects/projects_fonts/projects_fonts.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background image credit: https://pixabay.com/images/id-5330343/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Here you see people that don\u2019t exist. How can that be? Well, they don\u2019t exist because these images were created with a neural network-based learning method by the name StyleGAN2, which can not only create eye-poppingly detailed looking images, but it can also fuse these people together, or generate cars, churches, horses, and of course, cats. The even cooler thing is that many of these techniques allow us to exert artistic control over these images. So how does that happen? How do we control a neural network? It happens through exploring latent spaces. And what is that? A latent space is a made-up place where we are trying to organize data in a way that similar things are close to each other. What you see here is a 2D latent space for generating different fonts. It is hard to explain why these fonts are similar, but most of us would agree that they indeed share some common properties. The cool thing here is that we can explore this latent space with our cursor, and generate all kinds of new fonts. You can try this work in your browser, the link is available in the video description. And, luckily, we can build a latent space, not only for fonts, but, for nearly anything. I am a light transport researcher by trade, so in this earlier paper, we were interested in generating somewhat hundreds of variants of a material model to populate this scene. In this latent space, we can concoct all of these really cool digital material models. A link to this work is also available in the video description. So let\u2019s recap - one of the cool things we can do with latent spaces is generate new images that are somewhat similar. But there is a problem. As we go into nearly any direction, not just one thing, but many things about the image change. For instance, as we explore the space of fonts here, not just the width of the font changes, everything changes. Or if we explore materials here, not just the shininess or the colors of the material change, everything changes. This is great to explore if we can do it in real time. If I change this parameter, not just the car shape changes, the foreground changes, the background changes\u2026again, everything changes! So, these are nice and intuitive controls, but not interpretable controls. Can we get that somehow? The answer is, yes, not everything must change, this previous technique is based on StyleGAN2 and is called StyleFlow, and it can take an input photo of a test subject, and edit a number of meaningful parameters. Age, expression, lighting, pose, you name it. For instance, it could also grew Elon musk a majestic beard. And that\u2019s not all, because Elon Musk is not the only person who got a beard. Look, this is me here, after I got locked up for dropping my papers. And I spent so long in there, that I grew a beard. Or I mean, this neural network gave me one. And since the punishment for dropping your papers is not short\u2026in fact, it is quite long\u2026this happened. Ouch. I hereby promise to never drop my papers, ever again. You will also have to hold on to yours too, so stay alert. So, apparently interpretable controls already exist. And I wonder, how far can we push this concept? Beard or no beard is great, but what about cars, what about paintings? Well, this new technique found a way to navigate these latent spaces and introduces 3 amazing new examples of interpretable controls that I haven\u2019t seen anywhere else yet. One, it can change the car geometry. We can change the sportiness of a car, and even ask the design to be more or less boxy. Note that there is some additional damage here, but we can counteract that by changing the foreground to our taste, for instance, add some grass in there. Two, it can repaint paintings. We can change the roughness of the brush strokes, simplify the style  or even rotate the model. This way, we can create or adjust a painting without having to even touch a paintbrush. Three, facial expressions. First, when I started reading this paper, I was a little suspicious. I have seen these controls before so I looked at it like this, but as I saw how well it did, I went more\u2026 like this. And this paper can do way more, for instance, it can add lipstick, change the shape of the mouth or the eyes, and do all this with very little collateral damage to the remainder of the image. Loving it. It can also find and blur the background similarly to those amazing portrait mode photos that newer smartphones can do. And, of course, it can also do the usual suspects. Adjusting the age, hairstyle, or growing a beard. So with that, there we go, now, with the power of neural network-based learning methods, we can create new car designs, can repaint paintings without ever touching a paintbrush, and give someone a shave. It truly feels like we are living in a science fiction world. What  a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=B8RMUSmIGCI",
        "paper_link": "https://github.com/harskish/ganspace",
        "paper_title": "GANSpace: Discovering Interpretable GAN Controls"
    },
    {
        "video_id": "4etSuEQOzDw",
        "video_title": "All Hail The Adaptive Staggered Grid! \ud83c\udf10\ud83e\udd2f",
        "position_in_playlist": 507,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"An adaptive staggered-tilted grid for incompressible flow simulation\" is available here:\nhttps://cs.nyu.edu/~sw4429/files/sa20-fluid.pdf\nhttps://dl.acm.org/doi/abs/10.1145/3414685.3417837\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to concoct some absolutely\u00a0 insane fluid and smoke simulations. A common\u00a0\u00a0 property of these simulation programs is that\u00a0 they subdivide the simulation domain into a grid,\u00a0\u00a0 and they compute important quantities like\u00a0 velocity and pressure in these gridpoints. Normally, a regular grid looks something\u00a0 like this, but this crazy new technique\u00a0\u00a0 throws away the idea of using this as a grid, and\u00a0 uses this instead. This is called the Adaptive\u00a0\u00a0 Staggered-Tilted grid, an AST grid in\u00a0 short. So what does that really mean? The tilted part means that cells can be rotated\u00a0 by 45 degrees like this. And interestingly,\u00a0\u00a0 they typically appear only where needed, I\u2019ll\u00a0 show you in a moment. The adaptive part means\u00a0\u00a0 that the size of the grid cells is\u00a0 not fixed, and can be all over the\u00a0\u00a0 place. And even better, this concept can\u00a0 be easily generalized to 3D grids as well.\u00a0\u00a0 Now, when I first read this paper, two things\u00a0 came to my mind, one, that is an insane idea,\u00a0\u00a0 I kinda like it, and two, it cannot possibly\u00a0 work! It turns out, only one of these is true. And I was also wondering, why? Why do all this?\u00a0 And the answer is because this way, we get\u00a0\u00a0 better fluid and smoke simulations. Oh yeah! Let\u2019s\u00a0 demonstrate it through four beautiful experiments! Experiment number one. K\u00e1rm\u00e1n vortex\u00a0 streets. We noted that the tilted grid points\u00a0\u00a0 appear only where they are needed, and these are\u00a0 places where there is great deal of vorticity.\u00a0\u00a0 Let\u2019s test that. This phenomenon showcases\u00a0 repeated vortex patterns and the algorithm is hard\u00a0\u00a0 at work here. How do we know? Well, of course, we\u00a0 don\u2019t know that\u2026yet! So let\u2019s look under the hood\u00a0\u00a0 together and see what is going on! Oh wow, look at\u00a0 that! The algorithm knows where the vorticity is,\u00a0\u00a0 and as a result, these tilted cells are\u00a0 flowing through the simulation beautifully. Experiment number two. Smoke plumes and\u00a0 porous nets. This technique refines the\u00a0\u00a0 grids with these tilted cells in the areas\u00a0 where there is a great deal of turbulence,\u00a0\u00a0 and, wait a second. What is this? The net is\u00a0 also covered with tilted cells. Why is that?\u00a0\u00a0 The reason for this is that the tilted cells not\u00a0 only cover turbulent regions, but other regions\u00a0\u00a0 of interest as well. In this case, it enables us\u00a0 to capture this narrow flow around the obstacle.\u00a0\u00a0 Without this new AST grid, some of these\u00a0 smoke plumes wouldn\u2019t make it through the net. Experiment number three. The boat ride. Note that\u00a0 the surface of the pool is completely covered with\u00a0\u00a0 the new tilted cells, making sure that the wake\u00a0 of the boat is as detailed as it can possibly\u00a0\u00a0 be. But in the meantime, the algorithm is not\u00a0 wasteful, look, the volume itself is free of them. And now, hold on to your papers\u00a0 for experiment number four.\u00a0\u00a0 Thin water sheets. You can see the final\u00a0 simulation here, and if we look under the hood,\u00a0\u00a0 my goodness, just look at how much work this\u00a0 algorithm is doing. And what is even better,\u00a0\u00a0 it only does so where it is really needed it\u00a0 doesn\u2019t do any extra work in these regions. I am so far, very impressed with this technique.\u00a0 We saw that it does a ton of work for us,\u00a0\u00a0 and increases the detail in our simulations, and\u00a0 helps things flow through where they should really\u00a0\u00a0 flow through. Now, with that said, there is only\u00a0 one question left. What does this cost us? How\u00a0\u00a0 much more expensive is this kind of AST grid\u00a0 simulation than a regular grid? +100 percent\u00a0\u00a0 computation time? +50 percent? How much is it\u00a0 worth to you? Please stop the video and leave a\u00a0\u00a0 comment with your guess. I\u2019ll wait. Thank you! The\u00a0 answer is none of those. It costs almost nothing,\u00a0\u00a0 and adds typically an additional 1% of computation\u00a0 time. And in return for that almost nothing,\u00a0\u00a0 we get all of these beautiful fluid and\u00a0 smoke simulations. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4etSuEQOzDw",
        "paper_link": "https://cs.nyu.edu/~sw4429/files/sa20-fluid.pdf",
        "paper_title": "An adaptive staggered-tilted grid for incompressible flow simulation"
    },
    {
        "video_id": "dVa1xRaHTA0",
        "video_title": "NVIDIA\u2019s AI Puts Video Calls On Steroids! \ud83d\udcaa",
        "position_in_playlist": 508,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/ayush-thakur/face-vid2vid/reports/Overview-of-One-Shot-Free-View-Neural-Talking-Head-Synthesis-for-Video-Conferencing--Vmlldzo1MzU4ODc\n\n\ud83d\udcdd The paper \"One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing\" is available here:\nhttps://nvlabs.github.io/face-vid2vid/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-820315/\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This paper is really something else. Scientists at NVIDIA just came up with an absolutely insane idea for video conferencing. Their idea is not to do what everyone else is doing, which is, transmitting our video to the person on the other end. No, of course not, that would be too easy! What they do in this work, is take only the first image from the video, and they throw away the entire video afterwards! But before that, it stores a tiny bit of information from it, which is, how our head is moving over time, and how our expressions change. That is an absolutely outrageous idea\u2026 and of course, we like those around here, so, does this work? Well, let\u2019s have a look. This is the input video, note that this is not transmitted, only the first image and some additional information, and the rest of this video is discarded. And hold on to your papers, because this is the output of the algorithm compared to the input video. No, this is not some kind of misunderstanding, nobody has copy-pasted the results there. This is a near-perfect reconstruction of the input, except that the amount of information we need to transmit through the network is significantly less than with previous compression techniques. How much less? Well, you know what\u2019s coming, so let\u2019s try it out! Here is the output of the new technique, and here is the comparison against H.264, a powerful and commonly used video compression standard. Well, to our disappointment, the two seem close, the new technique appears better, especially around the glasses, but the rest is similar. And if you have been holding on to your papers so far, now, squeeze that paper, because this is not a reasonable comparison. And that is because the previous method was allowed to transmit 6 to 12 times more information. Look, as we further decrease the data allowance of the previous method, it still can transmit more than twice as much information, and at this point, there is no contest. This bitrate would be unusable for any kind of videoconferencing, while the new method uses less than half as much information, and still transmits a sharp and perfectly fine video. Overall, the authors report that their new method is ten times more efficient. That is unreal. This is an excellent video reconstruction technique, that much is clear. And if it only did that, it would be a great paper. But this is not a great paper, this is an absolutely amazing paper, so it does even more. Much, much more! For instance, it can also rotate our head and make a frontal video, can also fix potential framing issues by translating our head, and transferring all of our gestures to a new model. And, it is also evaluated well, so all of these new features are tested in isolation. Look at these two previous methods trying to frontalize the input video. One would think that it\u2019s not even possible to perform properly given how much these techniques are struggling with the task\u2026until we look at the new method. My goodness. There is some jumpiness in the neck movement in the output video here, and some warping issues here, but otherwise, very impressive results. Now if you have been holding on to your papers so far, now, squeeze that paper, because these previous methods are not some ancient papers that were published a long time ago. Not at all! Both of them were published within the same year as the new paper. How amazing is that. Wow. I really liked this page from the paper, which showcases both the images and the mathematical measurements against previous methods side by side. There are many ways to measure how close two videos are to each other, the up and down arrows tell us whether the given quality metric is subject to minimization or maximization, for instance, pixelwise errors are typically minimized, so lesser is better, but we are to maximize the the peak signal to noise ratio. And the cool thing is that none of this matters too much as soon as we insert the new technique, which really outpaces all of these. And we are still not done yet! So we said that the technique takes the first image, reads the evolution of expressions and the head pose from the input video, and then, it discards the entirety of the video, save for the first image. The cool thing about this was that we could pretend to rotate the head pose information, and the result is that the head appears rotated in the output image. That was great. But what if we take the source image from someone, and take this data, the driving keypoint sequence from someone else? Well, what we get is, motion transfer. Look! We only need one image of the target person, and we can transfer all of our gestures to them, in a way that is significantly better than most previous methods. Now, of course, not even this technique is perfect, it still struggles a great deal in the presence of occluder objects, but still, just the fact that this is possible feels like something straight out of a science fiction movie. What  a  time  to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=dVa1xRaHTA0",
        "paper_link": "https://nvlabs.github.io/face-vid2vid/",
        "paper_title": "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing"
    },
    {
        "video_id": "bnm7skt2aYE",
        "video_title": "An AI That Makes Dog Photos - But How? \ud83d\udc36",
        "position_in_playlist": 509,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/ayush-thakur/ada/reports/Train-Generative-Adversarial-Network-With-Limited-Data--Vmlldzo1NDYyMjA\n\n\ud83d\udcdd The paper \"Training Generative Adversarial Networks with Limited Data\" is available here:\nPaper: https://arxiv.org/abs/2006.06676\nPytorch implementation: https://github.com/NVlabs/stylegan2-ada-pytorch\n\n\ud83d\udcdd My thesis with the quote is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-learning-and-synthesis/\n\nUnofficial StyleGAN2-ADA trained on corgis (+ colab notebook):\nhttps://github.com/seawee1/Did-Somebody-Say-Corgi\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#nvidia #stylegan2",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, we are going to explore a paper that improves on the incredible StyleGAN2. What is that? StyleGAN2 is a neural network-based learning algorithm that is not only capable of creating these eye-poppingly detailed images of human beings that don\u2019t even exist, but, it also improved on its previous version in a number of different ways. For instance, with the original StyleGAN method, we could exert some artistic control over these images, however, look, you see how this part of the teeth and eyes are pinned to a particular location and the algorithm just refuses to let it go, sometimes to the detriment of its surroundings. The improved StyleGAN2 method addressed this problem, you can see the results here. Teeth and eyes are now allowed to float around freely, and perhaps this is the only place on the internet where we can say that and be happy about it. It could also mix two images together, and it could do it not only for human faces, but for cars, buildings, horses, and more. And get this, this paper was published in December 2019, and since then, it has been used in a number of absolutely incredible applications and followup works. Let\u2019s look at three of them. One, for instance, the first question I usually hear when I talk about an amazing paper like this is \u201cokay, great, but when do I get to use this\u201d? And the answer is, right now, because it is implemented in Photoshop in a feature that is called Neural Filters. Two, artistic control over these images has improved so much that now, we can pin down a few intuitive parameters and change them with minimal changes to other parts of the image. For instance, it could grow Elon Musk a majestic beard\u2026and, Elon Musk was not the only person who got an algorithmic beard, I hope you know what\u2019s coming\u2026yes, I got one too! Let me know in the comments whose beard you liked better! Three, a nice followup paper that could take a photo of Abraham Lincoln and other historic figures, and could restore their images as if we were time travelers and took these photos with a more modern camera. The best part here was that it leveraged the superb morphing capabilities of StyleGAN2 and took an image of their siblings, a person who has somewhat similar proportions to the target subject, and morph them into a modern image of this historic figure. This was brilliant, because restoring images is hard, but with StyleGAN2, morphing is now easy, so the authors decided to trade a difficult problem for an easier one. And the results speak for themselves. We cannot know for sure if this is what these historic figures really looked like, but for now, it makes one heck of a thought experiment. And now, let\u2019s marvel together at these beautiful results with the new method, that goes by the name, StyleGAN2-ADA. While we look through these results, all of which were generated with the new method, here are three things that it does better. One, if often works just as well as StyleGAN2 but requires ten times fewer images for training. This means that now, it can create these beautiful images, and this can be done by training a set of neural networks from less than 10 thousand images at a time. Whoa. That is not much at all. Two, it creates better quality results. The baseline here is original StyleGAN2, the numbers are subject to minimization and are a measure of the quality of these images. As you see from the bolded numbers, the new method not only beats the baseline method substantially, but it does it across the board. That is a rare sight indeed. And three, we can train this method faster, it generates these images faster, and in the meantime, also consumes less memory, which is usually in short supply on our graphics cards. Now, we noted that the new version of the method is called StyleGAN2-ADA. What is ADA? This part means Adaptive Discriminator Augmentation. What does that mean exactly? This means that the new method endeavors to squeeze as much information out of these training datasets as it can. Data augmentation is not new, it has been done for many years now, and essentially this means that we rotate, colorize, or even corrupt these images during the training process. The key here is that with this, we are artificially increasing the number of training samples the neural network sees. The difference here is that they used a greater set of augmentations, and the adaptive part means that these augmentations are tailored more to the dataset at hand. And now comes the best part, hold on to your papers, and let\u2019s look at the timeline here. StyleGAN2 appeared in December 2019, and StyleGAN2-ADA, this method came out just half a year later. Such immense progress, in just 6 months of time. The pace of progress in machine learning research is absolutely stunning these days. Imagine what we will be able to do with these techniques just a couple more years down the line. What a time to be alive! But this paper also teaches a very important lesson to us that I would like to show you. Have a look at this table that shows the energy expenditure for this project for transparency, but it also tells us the number of experiments that were required to finish such an amazing paper. And that is more than 3300 experiments. 255 of which were wasted due to technical problems. In the foreword of my PhD thesis, I wrote the following: \u201cResearch is the study of failure. More precisely, research is the study of obtaining new knowledge through failure. A bad researcher fails 100% of the time, while a good one fails only 99% of the time. Hence, what you see written here (and in most papers) is only 1% of the work that has been done. I would like to thank Fel\u00edcia, my wife, for providing motivation, shielding me from distractions, and bringing sunshine to my life to endure through many of these failures.\u201d This paper is a great testament to show how difficult the life of a researcher is. How many people give up their dreams, after being rejected once, or maybe two times? Ten times? Most people give up after 10 tries. And just imagine having a 1000 failed experiments and still not even being close to publishing a paper yet. And, with a little more effort, this amazing work came out of it. Failing doesn\u2019t mean losing. Not in the slightest. Huge congratulations to the authors for their endurance, and for this amazing work, and I think this also shows that these Weights and Biases experiment tracking tools are invaluable, because it is next to impossible to remember what went wrong with each of them, and what should  be fixed. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=bnm7skt2aYE",
        "paper_link": "https://arxiv.org/abs/2006.06676",
        "paper_title": "Training Generative Adversarial Networks with Limited Data"
    },
    {
        "video_id": "jjfDO2pWpys",
        "video_title": "DeepMind\u2019s AI Watches YouTube and Learns To Play! \u25b6\ufe0f\ud83e\udd16",
        "position_in_playlist": 510,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/latentspace/published-work/The-Science-of-Debugging-with-W-B-Reports--Vmlldzo4OTI3Ng\n\n\ud83d\udcdd The paper \"Playing hard exploration games by watching YouTube\" is available here:\nPaper: https://papers.nips.cc/paper/7557-playing-hard-exploration-games-by-watching-youtube.pdf\nGameplay videos: https://www.youtube.com/playlist?list=PLZuOGGtntKlaOoq_8wk5aKgE_u_Qcpqhu\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Between 2013 and 2015, DeepMind worked on\u00a0 an incredible learning algorithm by the name\u00a0\u00a0 Deep Reinforcement Learning. This technique looked\u00a0 at the pixels of the game, was given a controller\u00a0\u00a0 and played much like a human would\u2026 with the\u00a0 exception that it learned to play some Atari games\u00a0\u00a0 on a superhuman level. I have tried to train it\u00a0 a few years ago and would like to invite you for\u00a0\u00a0 a marvelous journey to see what happened. When\u00a0 it starts learning to play an old game, Atari\u00a0\u00a0 breakout, at first, the algorithm loses all of\u00a0 its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing\u00a0 the game, roughly matching the skill level of an\u00a0\u00a0 adept player. But here's the catch, if we wait for\u00a0 longer, we get something absolutely spectacular.\u00a0\u00a0 Over time, it learns to play like a pro, and\u00a0 finds out that the best way to win the game\u00a0\u00a0 is digging a tunnel through the\u00a0 bricks and hit them from behind. This technique is combination of a neural network\u00a0 that processes the visual data that we see on\u00a0\u00a0 the screen, and a reinforcement learner that\u00a0 comes up with the gameplay-related decisions.\u00a0\u00a0 This is an amazing algorithm, a\u00a0 true breakthrough in AI research. A key point in this work was that the\u00a0 problem formulation here enabled us\u00a0\u00a0 to measure our progress easily: we\u00a0 hit one brick, we get some points,\u00a0\u00a0 so do a lot of that. Lose a few lives, the game\u00a0 ends, don\u2019t do that! Easy enough. But there are\u00a0\u00a0 other, exploration-based games like Montezuma\u2019s\u00a0 revenge or Pitfall that it was not good at. And\u00a0\u00a0 man, these games are a nightmare for any AI,\u00a0 because there is no score, or at the very least,\u00a0\u00a0 it\u2019s hard to define how well we are doing. Because\u00a0 there are no scores, it is hard to motivate the\u00a0\u00a0 AI to do anything at all other than just wander\u00a0 around aimlessly. If no one tells us if we are\u00a0\u00a0 doing well or not, which way do we go? Explore\u00a0 this place or go to the next one? How do we solve\u00a0\u00a0 all this? And with that, let\u2019s discuss\u00a0 the state of play in AIs playing difficult\u00a0\u00a0 exploration-based computer games. And I think\u00a0 you will love to see how far we have some since. First, there is a previous line of work that\u00a0 infused these agents with a very human-like\u00a0\u00a0 property\u2026 curiosity. That agent was able to do\u00a0 much, much better at these games\u2026and then got\u00a0\u00a0 addicted to the TV. But that\u2019s a different story.\u00a0 Note that this TV problem has been remedied since. And this new method attempts\u00a0 to solve hard exploration games\u00a0\u00a0 by watching Youtube videos\u00a0 of humans playing the game,\u00a0\u00a0 and learning from that, as you see, it just rips\u00a0 through these levels in Montezuma\u2019s revenge and\u00a0\u00a0 other games too. So, I wonder how does all this\u00a0 magic happen? How did this agent learn to explore? Well, it has three things going\u00a0 for it that really makes this work. One, the Skeptical Scholar would say, that all is\u00a0 takes is just copy-pasting what it saw from the\u00a0\u00a0 human player! Also, imitation learning is not new,\u00a0 which is a point that we will address in a moment,\u00a0\u00a0 so, why bother with this? Now, hold on\u00a0 to your papers, and observe as it seems\u00a0\u00a0 noticeably less efficient than the human\u00a0 teacher was. Until we realize that this\u00a0\u00a0 is not the human player, and this is\u00a0 not the AI\u2026but the other way around!\u00a0\u00a0 Look, it was so observant and took away so much\u00a0 from the human demonstrations that in the end,\u00a0\u00a0 it became even more efficient than its\u00a0 human teacher. Whoa! Absolutely amazing. And while we are here, I would like\u00a0 to dissect this copy-paste argument.\u00a0\u00a0 You see, it has an understanding of the game,\u00a0 and does not just copy the human demonstrator.\u00a0\u00a0 But even if it just copied what it saw, it would\u00a0 not be so easy because the AI only sees images,\u00a0\u00a0 and it has to translate how the images change in\u00a0 response to us pressing buttons on the controller.\u00a0\u00a0 We might also encounter the same\u00a0 level, but at a different time,\u00a0\u00a0 and we have to understand how to vanquish\u00a0 an opponent and how to perform that. Two, nobody hooked the agent\u00a0 into the game information,\u00a0\u00a0 which is huge. This means that it doesn\u2019t know\u00a0 what buttons are pressed on the controller,\u00a0\u00a0 no internal numbers or the game state are given\u00a0 to it, and most importantly, it is also not given\u00a0\u00a0 the score of the game. We discussed how difficult\u00a0 this makes everything. Unfortunately, this means\u00a0\u00a0 that there is no easy way out - it really has to\u00a0 understand what it sees and mine out the relevant\u00a0\u00a0 information from each of these videos. And as you\u00a0 see, it does that with flying colors. Loving it. And three, it can handle the domain gap. Previous\u00a0 imitation learning methods did not deal with that\u00a0\u00a0 too well. So what does that mean? Let\u2019s look at\u00a0 this latent space together and find out. This\u00a0\u00a0 is what a latent space looks like if we just\u00a0 embed the pixels that we see in the videos.\u00a0\u00a0 Don\u2019t worry, I\u2019ll tell you in a moment what that\u00a0 is. Here, the clusters are nicely clumped up\u00a0\u00a0 away from each other, so that\u2019s probably good,\u00a0 right? Well, in this problem, not so much!\u00a0\u00a0 A latent space means a place where\u00a0 similar kinds of data are meant to\u00a0\u00a0 be close to each other. These are snippets of the\u00a0 demonstration videos that the clusters relate to.\u00a0\u00a0 Let\u2019s test that together. Do you\u00a0 think these images are similar? Yes?\u00a0\u00a0 Most of us humans would say that these are quite\u00a0 similar, in fact, they are nearly the same. So,\u00a0\u00a0 is this a good latent space embedding? No,\u00a0 not in the slightest. This data is similar,\u00a0\u00a0 therefore, these should be close to each other,\u00a0 but this previous technique did not recognize that\u00a0\u00a0 because these images have slightly different\u00a0 colors, aspect ratios, this has a text overlay,\u00a0\u00a0 but we all understand that despite all that, we\u00a0 are looking at the same game through different\u00a0\u00a0 windows. So, does the new technique recognize\u00a0 that? Oh yes, beautiful. Praise the papers!\u00a0\u00a0 Similar game states are now close to each\u00a0 other, we can align them properly and therefore,\u00a0\u00a0 we can learn more easily from them. This is\u00a0 one of the reasons why it can play so well. So there you go, these new AI agents can look\u00a0 at how we perform complex exploration games,\u00a0\u00a0 and learn so well from us, that in the end, they\u00a0 do even better than we do. And now, to get them to\u00a0\u00a0 write some amazing papers for us\u2026or, you know, Two\u00a0 Minute Papers episodes. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=jjfDO2pWpys",
        "paper_link": "https://papers.nips.cc/paper/7557-playing-hard-exploration-games-by-watching-youtube.pdf",
        "paper_title": "Playing hard exploration games by watching YouTube"
    },
    {
        "video_id": "RUDWn_obddI",
        "video_title": "OpenAI Outperforms Some Humans In Article Summarization! \ud83d\udcdc",
        "position_in_playlist": 511,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/openai/published-work/Learning-Dexterity-End-to-End--VmlldzoxMTUyMDQ\n\n\ud83d\udcdd The paper \"Learning to Summarize with Human Feedback\" is available here:\nhttps://openai.com/blog/learning-to-summarize-with-human-feedback/\n\nReddit links to the showcased posts:\n1. https://www.reddit.com/r/AskAcademia/comments/lf7uk4/submitting_a_paper_independent_of_my_post_doc/\n2. https://www.reddit.com/r/AskAcademia/comments/l988py/british_or_american_phd/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Alex Paden, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bruno Miku\u0161, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Joshua Goller, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background image credit: https://pixabay.com/images/id-1989152/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This paper will not have the visual fireworks\u00a0 that you see in many of our videos. Oftentimes,\u00a0\u00a0 you get ice cream for the eyes, but today,\u00a0 you\u2019ll get an ice cream for the mind. And,\u00a0\u00a0 when I read this new paper,\u00a0 I almost fell off the chair,\u00a0\u00a0 and I think this work teaches us important\u00a0 lessons and I hope you will appreciate them too. So, with that, let\u2019s talk about AIs and dealing\u00a0 with text! This research field is improving at an\u00a0\u00a0 incredible pace. For instance, four years ago,\u00a0 in 2017, scientists at OpenAI embarked on an AI\u00a0\u00a0 project where they wanted to show a neural network\u00a0 a bunch of Amazon product reviews and wanted to\u00a0\u00a0 teach it to be able to generate new ones, or\u00a0 continue a review when given one. Upon closer\u00a0\u00a0 inspection, they noticed that the neural network\u00a0 has built up a knowledge of not only language,\u00a0\u00a0 but also learned that it needs to create a\u00a0 state-of-the-art sentiment detector as well.\u00a0\u00a0 This means that the AI recognized that in order to\u00a0 be able to continue a review, it needs to be able\u00a0\u00a0 to understand English, and efficiently detect\u00a0 whether the review seems positive or negative. This new work is about text summarization, and it\u00a0 really is something else. If you read reddit, the\u00a0\u00a0 popular online discussion website, and encounter\u00a0 a longer post, you may also find a short summary,\u00a0\u00a0 a TLDR of the same post, written by a fellow\u00a0 human. This is good for not only the other readers\u00a0\u00a0 who are in a hurry, but, it is less obvious\u00a0 is that it is also good for something else. And now, hold on to your papers, because\u00a0 these summaries also provide fertile\u00a0\u00a0 grounds for a learning algorithm to read a\u00a0 piece of long text, and its short summary,\u00a0\u00a0 and learn how the two relate to each other.\u00a0 This means that it can be used as training\u00a0\u00a0 data and can be fed to a learning algorithm.\u00a0 Yum! And the point is that if we give enough\u00a0\u00a0 of these pairs to these learning algorithms,\u00a0 they will learn to summarize other reddit posts. So, let\u2019s see how well it performs. First,\u00a0 this method learned on about a hundred thousand\u00a0\u00a0 well-curated reddit posts, and was also tested\u00a0 on other posts that it hadn\u2019t seen before. It\u00a0\u00a0 was asked to summarize this post from relationship\u00a0 advice subreddit, and let\u2019s see how well it did. If you feel like reading the text, you\u00a0 can pause the video here, or if you feel\u00a0\u00a0 like embracing the TLDR spirit, just carry\u00a0 on, and look at these two summarizations.\u00a0\u00a0 One of these is written by a human, and the other\u00a0 one by this new summarization technique. Do you\u00a0\u00a0 know which is which? Please stop the video and let\u00a0 me know in the comments below. Thank you! So this,\u00a0\u00a0 was written by a human and this by the new\u00a0 AI. And while, of course, this is subjective,\u00a0\u00a0 I would say that the AI-written one feels at\u00a0 the very least as good as the human summary,\u00a0\u00a0 and I can\u2019t wait to have a look at the\u00a0 more principled evaluation in the paper.\u00a0\u00a0 Let\u2019s see\u2026the higher we go here, the higher the\u00a0 probability of a human favoring the AI-written\u00a0\u00a0 summary to a human-written one. And we have\u00a0 smaller AI models on the left, bigger ones to\u00a0\u00a0 the right. This is the 50% reference line, below\u00a0 it, people tend to favor the human\u2019s version,\u00a0\u00a0 and if it can get above the 50% line, the\u00a0 AI does a better job than human-written\u00a0\u00a0 TLDRs in the dataset. Here are two proposed\u00a0 models, this one significantly underperforms,\u00a0\u00a0 this other one is a better match. However,\u00a0 whoa! Look at that! The authors also proposed a\u00a0\u00a0 human feedback model that, even for the smallest\u00a0 model, handily outperforms human-written TLDRs,\u00a0\u00a0 and as we grow the AI model, it gets even\u00a0 better than that. Now that\u2019s incredible,\u00a0\u00a0 and this is when I almost fell off\u00a0 the chair when reading this paper. But! We\u2019re not done yet, not even close. Don\u2019t\u00a0 forget, this AI was trained on reddit, and was\u00a0\u00a0 also tested on reddit. So our next question is,\u00a0 of course, can it do anything else? How general\u00a0\u00a0 is the knowledge that it gained? What if we\u00a0 give it a full news article from somewhere else,\u00a0\u00a0 outside of reddit? Let\u2019s see how it performs.\u00a0 Hmm\u2026of course, this is also subjective, but I\u00a0\u00a0 would say both are quite good. The human-written\u00a0 summary provides a little more information,\u00a0\u00a0 while the AI-written one captures the essence of\u00a0 the article and does it very concisely. Great job. So, let\u2019s see the same graph for summarizing\u00a0 these articles outside reddit. I don\u2019t expect\u00a0\u00a0 the AI to perform as well as with the reddit\u00a0 posts as it is outside the comfort zone,\u00a0\u00a0 but\u2026my goodness, this still performs nearly\u00a0 as well as humans. That means that it indeed\u00a0\u00a0 derived general knowledge from a really narrow\u00a0 training set, which is absolutely amazing. Now,\u00a0\u00a0 ironically, you see this Lead-3 technique\u00a0 dominating both humans and the AI. What could that\u00a0\u00a0 be? Some unpublished, superintelligent technique?\u00a0 Well, I will have to disappoint, this is not a\u00a0\u00a0 super sophisticated technique, but a dead simple\u00a0 one. So simple that it is just taking the first\u00a0\u00a0 three sentences of the article, which humans seem\u00a0 to prefer a great deal. But note, that this simple\u00a0\u00a0 Lead-3 technique only works for a narrow domain,\u00a0 while the AI has learned the English language,\u00a0\u00a0 probably knows about sentiment, and a lot\u00a0 of other things that can be used elsewhere. And now, the two most impressive\u00a0 things from the paper, in my opinion:\u00a0 One, this is not a neural network, but a\u00a0 reinforcement learning algorithm that learns\u00a0\u00a0 from human feedback. A similar technique has been\u00a0 used by DeepMind and other research labs to play\u00a0\u00a0 video games or control drones and it is really\u00a0 cool to see them excel in text summarization too. Two, it learned from humans, but derived so much\u00a0 knowledge from these scores, that over time,\u00a0\u00a0 it outperformed its own teacher. And the\u00a0 teacher here is not humans in general,\u00a0\u00a0 but people who write TLDRs along their posts\u00a0 on reddit. That truly feels like something\u00a0\u00a0 straight out of a science fiction\u00a0 movie. What a time to be alive! Now, of course, not even this technique is\u00a0 perfect, this human vs AI preference thing\u00a0\u00a0 is just one way of measuring the quality of the\u00a0 summary, there are more sophisticated methods\u00a0\u00a0 that involve coverage, coherence, accuracy, and\u00a0 more. In some of these measurements, the AI does\u00a0\u00a0 not perform as well. But just imagine what this\u00a0 will be able to do two more papers down the line. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=RUDWn_obddI",
        "paper_link": "https://openai.com/blog/learning-to-summarize-with-human-feedback/",
        "paper_title": "Learning to Summarize with Human Feedback"
    },
    {
        "video_id": "4CYI6dt1ZNY",
        "video_title": "This AI Learned To Stop Time! \u23f1",
        "position_in_playlist": 512,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes\" is available here:\nhttp://www.cs.cornell.edu/~zl548/NSFF/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-918686/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we get to be a paper historian, and witness the amazing progress in machine learning research together, and learn what is new in the world of NERFs. But, first, what is a NERF? In March of 2020, a paper appeared describing an incredible technique by the name, Neural Radiance Fields, or NERF in short. This work enables us to take a bunch of input photos and their locations, learn it, and synthesize new, previously unseen views of not just the materials in the scene, but the entire scene itself. And here, we are talking not only digital environments, but also, real scenes as well! Now that\u2019s quite a value proposition, especially given that it also supported refractive and reflective surfaces as well, these are both quite a challenge. However, of course, NERF had its limitations. For instance, in many cases, it had trouble with scenes with variable lighting conditions and lots of occluders. And to my delight, only 5 months later, in August of 2020, a followup paper appeared by the name NERF in the Wild, or NERF-W in short. Its speciality was tourist attractions that a lot of people take photos of, and we then have a collection of photos taken during a different time of the day, and of course, with a lot of people around. And, lots of people, of course means, lots of occlusions. NERF-W improved the original algorithm to excel more in cases like this. A few months later, on 2020 November 25th, another followup paper appeared by the name Deformable Neural Radiance Fields. D-NERF. The goal here was to take a selfie video, and turn it into a portrait that we can rotate around freely. This is something that the authors call a nerfie. If we take the original NERF technique to perform this, we see that it does not do well at all with moving things and that's where this new deformable variant really shines. And today\u2019s paper not only has some nice video results embedded in the front page, and it offers a new take on this problem and offers quote \u201cSpace-Time View Synthesis of Dynamic Scenes\u201d. Whoa, that is amazing. But what does that mean? What does this paper really do? The space-time view synthesis means that we can record a video of someone doing something. Since we are recording movement in time, and there is also movement in space, or in other words, the camera is moving. Both time and space are moving. And what this can do is one, freeze one of these variables, in other words, pretend as if the camera didn\u2019t move. Or, two, pretend as if time didn\u2019t move. Or, three, generate new views of the scene while movement takes place. My favorite is that we can pretend to zoom in and even better, zoom out even if the recorded video looked like this, or, we can also make a really choppy family memory smoother and much more enjoyable. So how does this compare to previous methods? There are plenty of NERF variants around, is this really any good? Let\u2019s find out together! This is the original NERF, we already know about this, and, we are not surprised in the slightest that it\u2019s not so great on dynamic scenes with a lot of movement. However, what I am surprised by is that all of these previous techniques are from 2020, and all of them struggle with these cases. These comparisons are not against some ancient technology from 1985. No-no, all of them are from the same year. For instance, this previous work is called Consistent Video Depth Estimation, and it is from August 2020. We showcased it in this series, and marveled at all of these amazing augmented reality applications that it offered. The snowing example here was one of my favorites. And today\u2019s paper appeared just three months later, in November 2020. And the authors still took the time and effort to compare against this work from just three months ago. That is fantastic. As you see, this previous method kind of works on this dog, but the lack of information in some regions is quite apparent. This is still maybe usable, but as soon as we transition into a more dynamic example, what do we get? Well, pandemonium. This is true for all previous methods. I cannot imagine that the new method from just a few months later could deal with this difficult case\u2026and. Look at that. So much better! It is still not perfect, you see that the we have lost some detail, but witnessing this kind of progress in just a few months is truly a sight to behold. It really consistently outperforms all of these techniques from the same year. What a time to be alive! If you, like me, find yourself yearning for more quantitative comparisons, the numbers also show that the two variants of the new proposed technique indeed outpace the competition. And it can even do one more thing. Previous video stabilization techniques were good at taking a shaky input video and creating a smoother output, however, these results often came at the cost a great deal of cropping. Not this new work, look at how good it is at stabilization, and it does not have to crop all this data. Praise  the papers! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=4CYI6dt1ZNY",
        "paper_link": "http://www.cs.cornell.edu/~zl548/NSFF/",
        "paper_title": "Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes"
    },
    {
        "video_id": "6SJ19OgHi4w",
        "video_title": "Soap Bubble Simulations Are Now Possible! \ud83e\uddfc",
        "position_in_playlist": 513,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"A Model for Soap Film Dynamics with Evolving Thickness\" is available here:\nhttps://sadashigeishida.bitbucket.io/soapfilm_with_thickness/index.html\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-3594979/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today I will try to show\u00a0 you the incredible progress\u00a0\u00a0 in computer graphics research through the\u00a0 lens of bubbles in computer simulations. Yes, bubbles indeed. Approximately a year ago,\u00a0 we covered a technique which could be used\u00a0\u00a0 to add bubbles to an already existing fluid\u00a0 simulation. This paper appeared in 2012 and\u00a0\u00a0 described a super simple method that helped us\u00a0 compute where bubbles appear and disappear over\u00a0\u00a0 time. The best part of this was that this could\u00a0 be added after the simulation has been finalized,\u00a0\u00a0 which is an insane value proposition. If we\u00a0 find ourselves yearning for some bubbles,\u00a0\u00a0 we just add it afterwards, if we don\u2019t like the\u00a0 results, we can take them out with one click. Now, simulations are not only about sights,\u00a0\u00a0 what about sounds? In 2016, this paper did\u00a0 something that previously seemed impossible:\u00a0\u00a0 it took this kind of simulation data,\u00a0 and made sure that now, we can not only\u00a0\u00a0 add bubbles to a plain water simulation,\u00a0 but also simulate how they would sound. On the geometry side, a followup paper appeared\u00a0\u00a0 just a year later that could simulate a handful\u00a0 of bubbles colliding, sticking together. Then, three years later, in 2020, Christopher\u00a0 Batty\u2019s group also proposed a method\u00a0\u00a0 that was capable of simulating merging and\u00a0 coalescing behavior on larger-scale simulations. So, what about today\u2019s paper? Are we going\u00a0 even larger with hundreds of thousands,\u00a0\u00a0 or maybe even millions of bubbles? No,\u00a0 we are going to take just one bubble\u2026\u00a0\u00a0 or at most a handful, and have a real close look\u00a0 at a method that is capable of simulating these\u00a0\u00a0 beautiful evolving rainbow patterns. The\u00a0 key to this work is that it is modeling\u00a0\u00a0 how the thickness of the surfaces changes\u00a0 over time. That makes all the difference. Let\u2019s look under the hood and observe how much\u00a0 of an effect the evolving layer thickness has on\u00a0\u00a0 the outputs. The red color coding represents\u00a0 thinner, and the blue shows us the thicker\u00a0\u00a0 regions. This shows us that some regions in these\u00a0 bubbles are more than twice as thick as others.\u00a0\u00a0 And there are also more extreme cases, there is\u00a0 a six-time difference between this and this part.\u00a0\u00a0 You can see how the difference in\u00a0 thickness leads to waves of light\u00a0\u00a0 interfering with the bubble\u00a0 and creating these beautiful\u00a0\u00a0 rainbow patterns. You can\u2019t get this without\u00a0 a proper simulator like this one. Loving it. This variation in thicknesses is responsible\u00a0 for a selection of premium-quality effects\u00a0\u00a0 in a simulation beyond surface vortices,\u00a0 interference patterns can also be simulated, and,\u00a0\u00a0 deformation-dependent rupturing of soap films.\u00a0\u00a0 This incredible technique can\u00a0 simulate all of these phenomena. And now, our big question is, okay,\u00a0\u00a0 it simulates all these, but how well does it do\u00a0 that? It is good enough to fool the human eye,\u00a0\u00a0 but how does it compare to the\u00a0 strictest adversary of all\u2026 reality! I hope you know what is coming. Oh yeah!\u00a0 Hold on to your papers, because now\u00a0\u00a0 we will let reality be our judge and\u00a0 compare the simulated results to that.\u00a0\u00a0 That is one of the biggest challenges\u00a0 in any kind of simulation research,\u00a0\u00a0 so, let\u2019s see. This is a piece of real footage of\u00a0 a curved soap film surface, where these rainbow\u00a0\u00a0 patterns get convected by an external force field.\u00a0 Beautiful. And now, let\u2019s see the simulation.\u00a0\u00a0 Wow, this has to be really close. Let\u2019s\u00a0 see them side by side and decide together.\u00a0\u00a0 Whoa. The match in the swirly region here is just\u00a0 exceptional. Now, note that even if the algorithm\u00a0\u00a0 is a 100% correct, this experiment cannot be a\u00a0 perfect match because not only the physics of\u00a0\u00a0 the soap film has to be simulated correctly, but\u00a0 the forces that move the rainbow patterns as well.\u00a0\u00a0 We don\u2019t have this information from the\u00a0 real-world footage, so the authors had to try to\u00a0\u00a0 reproduce these forces, which is not part of the\u00a0 algorithm, but a property of the environment. So,\u00a0\u00a0 I would say that this footage is as close as\u00a0 one can possibly get. My goodness, well done! So, how much do we have to pay for\u00a0 this in terms of computation time?\u00a0\u00a0 If you ask me, I would pay at the very least\u00a0 double for this. And if you have been holding on\u00a0\u00a0 to your papers so far, now, squeeze that paper,\u00a0 because now comes the best part, because in the\u00a0\u00a0 cheaper cases, only 4 to 7% extra\u00a0 computation, which is outrageous.\u00a0\u00a0 There is this more complex case, with the large\u00a0 deforming sphere. In this case, the new technique\u00a0\u00a0 indeed makes a huge difference. So, how much\u00a0 extra computation do we have to pay for this?\u00a0\u00a0 Only 31%. 31% extra computation\u00a0 for this? That is a fantastic deal,\u00a0\u00a0 you can sign me up right away. As you see, the\u00a0 pace of progress in computer graphics research\u00a0\u00a0 is absolutely incredible, and these simulations\u00a0 are just getting better and better by the day.\u00a0\u00a0 Imagine what we will be able to do just two more\u00a0 papers down the line! What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6SJ19OgHi4w",
        "paper_link": "https://sadashigeishida.bitbucket.io/soapfilm_with_thickness/index.html",
        "paper_title": "A Model for Soap Film Dynamics with Evolving Thickness"
    },
    {
        "video_id": "v5pOsQEOsyA",
        "video_title": "Finally, Video Stabilization That Works! \ud83e\udd33",
        "position_in_playlist": 514,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"FuSta - Hybrid Neural Fusion for Full-frame Video Stabilization\" is available here:\n- Paper https://alex04072000.github.io/FuSta/\n- Code: https://github.com/alex04072000/FuSta \n- Colab: https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH?usp=sharing\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-2375579/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#stabilization #selfies",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to talk\u00a0 about video stabilization.\u00a0\u00a0 A typical application of this is when we\u00a0 record family memories, and other cool events,\u00a0\u00a0 and sometimes, the footage gets so shaky\u00a0 that we barely know what is going on. In these cases, video stabilization\u00a0 techniques can come to the rescue,\u00a0\u00a0 which means that in goes a shaky video,\u00a0 and out comes a smooth video. Well,\u00a0\u00a0 that is easier said than done. Despite\u00a0 many years of progress, there is a great\u00a0\u00a0 selection of previous methods that can do that,\u00a0 however, they suffer from one of two issues. Issue number one is cropping. This\u00a0 means that we get usable results,\u00a0\u00a0 but we have to pay a great price for it, which is,\u00a0 cropping away a great deal of the video content.\u00a0\u00a0 Issue number two is when we get the\u00a0 entirety of the video, no cropping,\u00a0\u00a0 however, the price to be paid for this is that we\u00a0 get lots of issues that we call visual artifacts. Unfortunately, today, when we\u00a0 stabilize, we have to choose our poison.\u00a0\u00a0 It\u2019s either cropping, or artifacts. Which one\u00a0 would you choose? That is difficult to decide,\u00a0\u00a0 of course, because none of these two tradeoffs are\u00a0 great. So our question today is, can we do better?\u00a0\u00a0 Well, the Law of Papers says that of course, just\u00a0 one or two more papers down the line, this will be\u00a0\u00a0 way better. So, let\u2019s see\u2026this is\u00a0 what this new method is capable of.\u00a0\u00a0 Hold on to your papers, and notice that this will\u00a0 indeed be a full-size video, so we already know\u00a0\u00a0 that probably there will be artifacts. But\u2026wait\u00a0 a second! No artifacts. Whoa. How can that be? What does this new method do that previous\u00a0 techniques didn\u2019t? These magical results are a\u00a0\u00a0 combination of several things: one, the new method\u00a0 can estimate the motion of these objects better,\u00a0\u00a0 two, it removes blurred images from the videos\u00a0 and three, collects data from neighboring video\u00a0\u00a0 frames more effectively. This leads to a greater\u00a0 understanding of the video it is looking at. Now, of course, not even this technique is\u00a0 perfect - rapid camera motion may lead to warping,\u00a0\u00a0 and if you look carefully, you may find some\u00a0 artifacts, usually around the sides of the screen. So far, we looked at previous methods, and the\u00a0 new method. It seems better. That\u2019s great. But\u00a0\u00a0 how do we measure which one is better? Do we\u00a0 just look? An even harder question would be,\u00a0\u00a0 if the new method is indeed better,\u00a0 okay, but by how much better is it? Let\u2019s try to answer all of these questions. We\u00a0 can evaluate these techniques against each other\u00a0\u00a0 in three different ways. One, we can look at the\u00a0 footage ourselves. We have already done that,\u00a0\u00a0 and we had to tightly hold on to our\u00a0 papers, it has done quite well in this test.\u00a0\u00a0 Test number two is a quantitative test. In other\u00a0 words, we can mathematically define how much\u00a0\u00a0 distortion there is in an output video, how smooth\u00a0 it is, and more, and compare the output videos\u00a0\u00a0 based on these metrics. In many cases, these\u00a0 previous techniques are quite close to each other,\u00a0\u00a0 and now, let\u2019s unveil the new method.\u00a0 Whoa. It scored best or second best\u00a0\u00a0 on 6 out of 8 tests. This is truly remarkable,\u00a0 especially given that some of these competitors\u00a0\u00a0 are from less than a year ago. That is nimble\u00a0 progress in machine learning research. Loving it. And the third way to test which technique is\u00a0 better, and by how much is by conducting a user\u00a0\u00a0 study. The authors have done that too! In this,\u00a0 46 humans were called in, were shown the shaky\u00a0\u00a0 input video, the result of a previous method, and\u00a0 the new method, and were asked three questions.\u00a0\u00a0 Which video preserves the most content, which has\u00a0 fewer imperfections, and which is more stable. And the results were stunning - despite\u00a0 looking at many different competing techniques,\u00a0\u00a0 the participants found the new method to be\u00a0 better at the very least 60% of the time,\u00a0\u00a0 on all three questions. In some cases, even\u00a0 90% of the time or higher. Praise the papers! Now, there is only one question left. If it\u00a0 is so much better than previous techniques,\u00a0\u00a0 how much longer does it take to run? With one exception, these previous methods take\u00a0\u00a0 from half a second to about 7.5 seconds per frame,\u00a0 and this new one asks for 9.5 seconds per frame.\u00a0\u00a0 And in return, it creates these absolutely amazing\u00a0 results. So, from this glorious day on, fewer,\u00a0\u00a0 or maybe no important memories will be lost\u00a0 due to camera shaking. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=v5pOsQEOsyA",
        "paper_link": "https://alex04072000.github.io/FuSta/",
        "paper_title": "FuSta - Hybrid Neural Fusion for Full-frame Video Stabilization"
    },
    {
        "video_id": "HNJPasJUGqs",
        "video_title": "Do Neural Networks Think Like Our Brain? OpenAI Answers! \ud83e\udde0",
        "position_in_playlist": 515,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/gudgud96/big-sleep-test/reports/Image-Generation-Based-on-Abstract-Concepts-using-CLIP-BigGAN--Vmlldzo1MjA2MTE\n\n\ud83d\udcdd The paper \"Multimodal Neurons in Artificial Neural Networks\" is available here:\nhttps://openai.com/blog/multimodal-neurons/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-2900362/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to cover many important questions in life. For instance, who is this? This is Halle Berry. Now, I show you this, who is it? It is, of course, also Halle Berry. And if I show you this piece of text, who does it refer to? Again, Halle Berry. So why are these questions interesting? Well, an earlier paper found out from brain readings that we indeed have person neurons in our brain. These are neurons specialized to recognize a particular human being. That is quite interesting. And what is even more interesting is that not that we have person neurons, but that these neurons are multimodal. What does that mean? This means that we understand the essence of what makes Halle Berry, regardless of whether it is a photo, a drawing, or anything else. I see, alright. Well then, our first question today is, do neural networks also have multimodal neurons? Not necessarily. The human brain is an inspiration for an artificial neural network that we can simulate on our computers, but do they work like the brain? Well, if we study their inner workings, our likely answer will be no, not in the slightest. But still, no one can stop us from a little experimentation, so let\u2019s try this for a common neural network architecture. This neuron responds to human faces and says that this is indeed a human face. So far so good. Now, if we provide it with a drawing of a human face, it won\u2019t recognize it to be a face. Well, so much for this multimodal idea, this one is surely not a brain in a jar. But wait, we don\u2019t give up so easily around here. This is not the only neural network architecture that exists, let\u2019s grab a different one. This one is called OpenAI\u2019s CLIP, and it is remarkably good at generalizing concepts. Let\u2019s see how it can deal with the same problem. Yes, this neuron responds to spiders, and Spiderman. That\u2019s the easy part. Now, please hold on to your papers, because now comes the hard part. Drawings and comics of spiders and Spiderman. Yes, it responds to that too! Wonderful. Now comes the final boss, which is spider-related writings. And.. it responds to that too. Now, of course, this doesn\u2019t mean that this neural network would be a brain in a jar, but it is a tiny bit closer to our thinking than previous architectures. And now comes the best part, this insight opens up the possibility for three amazing experiments. Experiment number one. Essence. So it appears to understand the essence of a concept or a person. That is absolutely amazing, so I wonder if we can turn this problem around and ask what it thinks about different concepts? It would be the equivalent to saying give me all things spiders and Spiderman. Let\u2019s do that with Lady Gaga. It says this is the essence of Lady Gaga. We get the smug smile, very good. And it says that the essence of Jesus Christ is this, and it also includes the crown of thorns. So far, flying colors. Now, we will see some images of feelings, some of you might find some of them disturbing. I think the vast majority of you humans will be just fine looking at them, but I wanted to let you know just in case. So, what is the essence of someone being shocked? Well, this. I can attest to that, this is basically me when reading this paper. My eyes were popping out just like this. Sleepiness. Yes, that is a coffee person before coffee alright. The happiness, crying and seriousness neurons also embody these feelings really well. Experiment number two, adversarial attacks. We know that OpenAI\u2019s clip responds to photos and drawings of the same thing, so let\u2019s try some nasty attacks involving combining the two. When we give it these images, it can classify them with ease. This is an Apple, this is a laptop, a mug, and so on. Nothing too crazy going on here. However, now, let\u2019s prepare a nasty adversarial attack. Previously, sophisticated techniques were developed to fool a neural network by adding some nearly imperceptible noise to an image. Let\u2019s have a look at how this works. First, we present a previous neural network with an image of a bus, and it will successfully tell us that yes, this is indeed a bus. Of course it does. Now, we show it not an image of a bus, but a bus plus some carefully crafted noise that is barely perceptible, that forces the neural network to misclassify it as an ostrich. I will stress that this is not any kind of noise, but the kind of noise that exploits biases in the neural network, which is, by no means trivial to craft. So now, I hope you are expecting a sophisticated adversarial attack against the wonderful CLIP neural network. Yes, that will do. Or will it? Let\u2019s see together! Yes, indeed, I don\u2019t know if you knew, but this is not an apple, this is a pizza. And so is this one. The neural network fell for these ones, but it was able to resist this sophisticated attack in the case of the coffee mug and the phone. Perhaps the pizza labels had too small a footprint in an image, so let\u2019s try an even more sophisticated version of this attack. Now, you may think that this is a chihuahua, but that is completely wrong, because this, is a pizza indeed. Not a chihuahua in sight anywhere in this image. No-no! So what did we learn here? Well, interestingly, this CLIP neural network is more general than previous techniques, however, its superpowers come at a price. And that price is that it can be exploited easily with simple systematic attacks. That is a great lesson indeed. Experiment number three, understanding feelings. Now this will be one heck of an experiment. We will try to answer the age-old question, which is, how would you describe feelings to a machine? Well, it\u2019s hard to explain such a concept, but all humans understand what being bored means. However, luckily, these neural networks have neurons, and they can use those to explain to us what they think about different concepts. An interesting idea here is that feelings could sort of emerge as a combination of other, more elementary neurons that are already understood. If this sounds a little nebulous, let\u2019s go with that example, what does the machine think it means that someone is bored? Well, it says that bored, is relaxed + grumpy. This isn\u2019t quite the way I think about it, but not bad at all, little machine. I like the way you think. Let\u2019s try one more. How do we explain to a machine what a surprise means? Well, it says surprise is celebration + shock. Nice. What about madness? Let\u2019s see. Evil + serious + a hint of mental illness. And when talking about combinations, there are two more examples that I really liked. If we are looking for text that embodies evil, we get something like this. And now, give me an evil building. Oh yes, I think that works really well, but there are internet forums where we have black-belt experts specializing in this very topic, so, if you are one of them, please let me know in the comments what you think. The paper also contains a ton more information, for instance, there is an experiment with the Stroop effect. This explores whether the neural network reacts to the meaning of the text, or the color of the text? I will only tease this because I would really like you to read the paper, which is available below in the video description. So there we go, neural networks are by no means brains in a jar, they are very much computer programs, however, CLIP has some similarities, and we also found that there is a cost to that. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HNJPasJUGqs",
        "paper_link": "https://openai.com/blog/multimodal-neurons/",
        "paper_title": "Multimodal Neurons in Artificial Neural Networks"
    },
    {
        "video_id": "adHjNqh5iGY",
        "video_title": "Can You Put All This In a Photo? \ud83e\udd33",
        "position_in_playlist": 516,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/NSFF/reports/Overview-Neural-Scene-Flow-Fields-NSFF-for-Space-Time-View-Synthesis-of-Dynamic-Scenes--Vmlldzo1NzA1ODI\n\n\ud83d\udcdd The paper \"OmniPhotos: Casual 360\u00b0 VR Photography\" is available here:\nhttps://richardt.name/publications/omniphotos/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Virtual Reality, VR in short, is maturing at a rapid pace and its promise is truly incredible. If one day it comes to fruition, doctors could be trained to perform surgery in a virtual environment, we could train pilots with better flight simulators, teach astronauts to deal with zero-gravity simulations, you name it. And you will see that with today\u2019s paper, we are able to visit nearly any place from afar. Now, to be able to do anything in a virtual world, we have to put on a head-mounted display that can tell the orientation of our head, and often, hands at all times. So, what can we do with this? Oh boy, a great deal. For instance, we can type on a virtual keyboard, or implement all kinds of virtual user interfaces that we can interact with. We can also organize imaginary boxes, and of course, we can\u2019t leave out the Two Minute Papers favorite, going into a physics simulation and playing with it with our own hands. In this previous work, hand-hand interactions did not work too well, which was addressed one more paper down the line, which absolutely nailed the solution. This followup work would look at our hands in challenging hand-hand interactions, and could deal with deformations, lots of self-contact and self-occlusion. Take a look at this footage. And, look, interestingly, they also recorded the real hand model with gloves on. We might think, what a curious design decision! What could that be for? Well, what you see here is not a pair of gloves, what you see here is the reconstruction of the hand model by this followup paper. Absolute witchcraft. Now, as you see, we can manipulate objects, or even wash our hands in virtual reality. This is all great when we play in a computer game, because the entire world around us was previously modeled, so we can look and go anywhere, anytime. But what about operating in the real world? What if we wish to look at a historic landmark from afar? Well, in this case, someone needs to capture a 360-degree photo of it. A regular photo will not cut it, because we can\u2019t turn our head around and look behind things. And this, is what today\u2019s paper will be about. This new paper is called Omniphotos, and it helps us produce this 360 view synthesis, and when we put on that head-mounted display, we can really get a good feel of a remote place, a group photo, or an important family festivity. So, clearly, the value proposition is excellent, but we have two questions. One, what do we have to do for it? Flailing. Yes. We need to be flailing. You see, we need to attach a consumer 360-camera to a selfie stick, and start flailing for about 10 seconds. Like this. This is a crazy idea, because now, we created a ton of raw data, roughly what you see here. So this, is a deluge of information, and the algorithm needs to crystallize all this mess into a proper 360 photograph. What is even more difficult here is that this flailing will almost never create a perfect circle trajectory, so the algorithm first has to estimate the exact camera positions and view directions. And hold on to your papers, because the entirety of this work is handcrafted, no machine learning is in sight, and the result is quite general technique, or in other words, it works on a wide variety of real-world scenes, you see a good selection of those here. Excellent. Our second question is, this is, of course, not the first method published in this area, so how does it relate to previous techniques? Is it really better? Well, let\u2019s see for ourselves! Previous methods either suffered from not allowing too much motion, or, the ones that give us more freedom to move around, did it by introducing quite a bit of warping into the outputs. And now, let\u2019s see if the new method improves upon that. Oh yeah, a great deal! Look, we have the advantages of both methods, we can move around freely, and additionally there is much less warping than here. Now, of course, not even this new technique is perfect, if you look behind the building, you see that the warping hasn\u2019t been completely eliminated, but it is a big step ahead of the previous paper. While we look at some more side by side comparisons. One more bonus question: what about memory consumption? Well, it eats over a gigabyte of memory. That is typically not too much of a problem for desktop computers, but we might need a little optimization if we wish to do these computations on a mobile device. And now comes the best part. You can browse these Omniphotos online through the link in the video description, and even the source code, and a Windows-based demo is available that works with and without a VR headset. Try it out and let me know in the comments how it went! So, with that, we can create these beautiful Omniphotos cheaply and efficiently, and navigate the real world as if it were a computer game. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=adHjNqh5iGY",
        "paper_link": "https://richardt.name/publications/omniphotos/",
        "paper_title": "OmniPhotos: Casual 360\u00b0 VR Photography"
    },
    {
        "video_id": "t7nO7MPcOGo",
        "video_title": "This AI Makes Beautiful Videos From Your Images! \ud83c\udf0a",
        "position_in_playlist": 517,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/authors/image-captioning/reports/Generate-Meaningful-Captions-for-Images-with-Attention-Models--VmlldzoxNzg0ODA\n\n\ud83d\udcdd The paper \"Animating Pictures with Eulerian Motion Fields\" is available here:\nhttps://eulerian.cs.washington.edu/\n\nGPT-3 website layout tweet:\nhttps://twitter.com/sharifshameem/status/1283322990625607681\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1761027/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In June 2020, OpenAI published an incredible\u00a0 AI-based technique by the name Image-GPT. The problem here was simple to understand, but\u00a0 nearly impossible to actually do: so here goes,\u00a0\u00a0 we give it an incomplete image, and we ask\u00a0 the AI to fill in the missing pixels. That is,\u00a0\u00a0 of course, an immensely difficult task, because\u00a0 these images may depict any part of the world\u00a0\u00a0 around us. It would have to know a great deal\u00a0 about our world to be able to continue the images,\u00a0\u00a0 so how well did it do? Let\u2019s have a look! This is undoubtedly a cat. But look! See that\u00a0 white part that is just starting? The interesting\u00a0\u00a0 part has been sneakily cut out of the image. What\u00a0 could that be? A piece of paper? Something else?\u00a0\u00a0 Now, let\u2019s leave the dirty work to the machine and\u00a0 ask it to finish it! Oh yeah, that makes sense. Now, even better, let\u2019s have a look at this\u00a0 water droplet example too. We humans, know that\u00a0\u00a0 since we see the remnants of ripples over there\u00a0 too, there must be a splash, but the question\u00a0\u00a0 is - does the AI know that? Oh yes, yes it\u00a0 does! Amazing! And the true image for reference. But wait a second. If Image GPT could understand\u00a0 that this is a splash, and finish the image like\u00a0\u00a0 this, then, here is an absolutely insane idea. If\u00a0 a machine can understand that this is a splash,\u00a0\u00a0 could it, maybe, not only finish the\u00a0 photo, but make a video out of it? Yes,\u00a0\u00a0 that is indeed an absolutely insane idea, we\u00a0 like those around here. So, what do you think,\u00a0\u00a0 is this a reasonable question,\u00a0 or is this still science fiction? Well, let\u2019s have a look at what this new\u00a0 learning-based method does when looking at\u00a0\u00a0 such an image. It would do something\u00a0 very similar to what we would do,\u00a0\u00a0 look at the image, estimate the direction of\u00a0 the motion, recognize that these ripples should\u00a0\u00a0 probably travel outwards, and based on the fact\u00a0 that we\u2019ve seen many splashes in our lives, if\u00a0\u00a0 we had the artistic skill, we could surely fill in\u00a0 something similar. So, can the machine do it too? And now, hold on to your papers, because\u00a0 this technique does exactly that.\u00a0\u00a0 Whoa! Please meet Eulerian Motion Synthesis.\u00a0 And it not only works amazingly well,\u00a0\u00a0 but look at the output video. It even\u00a0 loops perfectly. Yum yum yum, I love it! And it works mostly on fluids and smoke.\u00a0 I like that! I like that a lot, because\u00a0\u00a0 fluids and smoke have difficult, but predictable\u00a0 motion. That is an excellent combination for us,\u00a0\u00a0 especially given that you see plenty\u00a0 of those simulations on this channel,\u00a0\u00a0 so if you are a long-time Fellow Scholar,\u00a0 you already have a keen eye for them. Here are a few example images, paired\u00a0 with the synthesized motion fields,\u00a0\u00a0 these define the trajectory of\u00a0 each pixel, or in other words,\u00a0\u00a0 regions that the AI thinks should be animated\u00a0 and how it thinks should be animated. Now, it gets better, I have found three things\u00a0\u00a0 that I did not expect to work, but was\u00a0 pleasantly surprised that they did.\u00a0 One, reflections, kind of work. Two, fire. Kind of works.\u00a0 And now, if you have been holding on to your\u00a0 papers so far, now, squeeze that paper, because\u00a0\u00a0 here comes the best one, three, my beard works\u00a0 too. Yes, you heard it right. Now, first things\u00a0\u00a0 first, this is not any kind of beard, this is an\u00a0 algorithmic beard that was made by an AI, and now,\u00a0\u00a0 it is animated as if it were a piece of\u00a0 fluid using a different AI. Of course,\u00a0\u00a0 this is not supposed to be a correct result,\u00a0 just a happy accident, but in any case, this\u00a0\u00a0 sounds like something straight out of a science\u00a0 fiction movie. I also like how this has a nice\u00a0\u00a0 Obi-Wan Kenobi quality to it. Loving it. Thank you\u00a0 very much to my friend Oliver Wang and the authors\u00a0\u00a0 for being so kind and generating these results\u00a0 only for us. That is a huge honor, thank you. This previous work is from 2019 and creates\u00a0 high-quality motion, but, has a limited\u00a0\u00a0 understanding of the scene itself. And of course,\u00a0 let\u2019s see how the new method fares in these cases.\u00a0\u00a0 Oh yeah, this is a huge leap forward.\u00a0\u00a0 And what I like even better here is that new\u00a0 research techniques often provide different\u00a0\u00a0 tradeoffs than previous methods, but are rarely\u00a0 strictly better than them. In other words,\u00a0\u00a0 competing techniques usually do some things better\u00a0 and some things worse than their predecessors\u2026but\u00a0\u00a0 not this. Look, this is so much better across\u00a0 the board. That is such a rare sight. Amazing. Now, of course, not even this technique is\u00a0 perfect. For example, this part of the image\u00a0\u00a0 should have been animated, but remains stationary.\u00a0 Also, even though it did well with reflections,\u00a0\u00a0 refraction is a tougher nut to crack. Finally,\u00a0 thin geometry also still remains a challenge. But this was one paper that made the impossible\u00a0 possible, and just think about what we will\u00a0\u00a0 be able to do, two more papers down the\u00a0 line. My goodness. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=t7nO7MPcOGo",
        "paper_link": "https://eulerian.cs.washington.edu/",
        "paper_title": "Animating Pictures with Eulerian Motion Fields"
    },
    {
        "video_id": "rzIiuGrOZAo",
        "video_title": "9 Years of Progress In Cloth Simulation! \ud83e\uddf6",
        "position_in_playlist": 518,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/carlolepelaars/numerai_tutorial/reports/Build-the-World-s-Open-Hedge-Fund-by-Modeling-the-Stock-Market--VmlldzoxODU0NTQ\n\n\ud83d\udcdd The paper \"Homogenized Yarn-Level Cloth\" is available here:\nhttp://visualcomputing.ist.ac.at/publications/2020/HYLC/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This day is the perfect day to simulate the kinematics of yarn and cloth on our computers. As you just saw, this is not the usual intro that we use in every episode, so, what could that be? Well, this is a simulation specifically made for us, using a technique from today\u2019s paper. And it has a super high stitching density, which makes it all the better, by the end of this video, you will know exactly what that means and why that matters. But first, for context I would like to show you what researchers were able to do in 2012 and we will see together how far we have come since. This previous work was about creating these highly detailed cloth geometries for digital characters. Here you see one of its coolest results where it shows how the simulated forces pull the entire piece of garment together. We start out by dreaming up a piece of cloth geometry, and this simulator gradually transforms it into a real-world version of that by subjecting it to real physical forces. This is a step that we call yarn-level relaxation. So, this paper was published in 2012, and now, nearly 9 years have passed, so I wonder how far have we come some since? Well, we can still simulate knitted and woven materials through similar programs that we call direct yarn-level simulations. Here\u2019s one. I think we can all agree that these are absolutely beautiful, so what\u2019s the catch? The catch is that this is not free, there is a price we have to pay for these results. Look. Whoa, these really take forever. We are talking several hours, or for this one, almost even an entire day to simulate just one piece of garment. And it gets worse, look, this one takes more than two full days to compute. Imagine how long we would have to wait for complex scenes in a feature-length movie with several characters. Now, of course, this problem is very challenging, and to solve it, we have to perform a direct yarn-level simulation. This means that every single strand of yarn is treated as an elastic rod, and we have to compute how they react to external forces, bending deformations, and more. That takes a great deal of computation. So, our question today is, can we do this in a more reasonable amount of time? Well, the first Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. Let\u2019s see if the law holds up here! This new paper promises to retain many important characteristics of the full simulation, but takes much less time to compute. That is amazing. Things stretch, bend, and even curl up similarly, but the simulation time is cut down a great deal. How much less? Well, this one is five times faster, that\u2019s great. This one, twenty times, oh my! But it gets better! Now, hold on to your papers, and look! This one is almost 60 times faster than the full yarn-level simulation. My goodness! However, of course, it\u2019s not perfect, in return, pulling effects on individual yarns is neglected, so we lose the look of this amazing holey geometry. I\u2019d love to get that back. Now, these examples were using materials with a relatively small stitching density. Now, the previous, full yarn-level simulation method scales with the number of yarn segments we add to the garment. So, what does all this mean? This means that higher stitching density gives us more yarn strands, and the more yarn strands there are, the longer it takes to simulate them. In these cases, you can see the knitting patterns, so there aren\u2019t that many yarns, and even with that, it still took multiple days to compute a simulation with one piece of garment. So, I hope you know what\u2019s coming. It can also simulate super high stitching densities efficiently. What does that mean? It means that is can also simulate materials like the satin example here. This is not bad by any means, but similar simulations can be done with much simpler simulators, so our question is, why does this matter? Well, let\u2019s look at the backside here, and marvel at this beautiful scene showcasing the second best curl of the day. Loving it. And now I hope you are wondering what the best curl of the day is. Yes, here goes, this is the one that was showcased in our intro, which is Two Minute Papers curling up. You can only see the footage here. Beautiful. Huge congratulations and a big thank you to Georg Sperl, the first author of this paper, who simulated this Two Minute Papers scene only for us, and with a super high stitching density. That is quite an honor. Thank you so much! As I promised, we now understand exactly why the stitching density makes it all the better. And if you have been watching this series for a while, I am sure you have seen computer graphics researchers destroy armadillos in the most spectacular manner. Make sure to leave a comment if this is the case. That is super fun, and I thought there must be a version of that for cloth simulations. And of course there is. Now, please please meet the Yarnmadillo. The naming game is very strong here. And just one more thing. Georg Sperl, the first author of this work was a student of mine in 2014 at the Technical University of Vienna where he finished a practical project in light simulations programs, and, he did excellent work there. Of course he did. And I will note that I was not part of this project in any way, I am just super happy to see him come so far since then, he is now nearing the completion of his PhD, and this cloth simulation paper of his was accepted to the SIGGRAPH conference. That is as good  as it gets - well done Georg! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rzIiuGrOZAo",
        "paper_link": "http://visualcomputing.ist.ac.at/publications/2020/HYLC/",
        "paper_title": "Homogenized Yarn-Level Cloth"
    },
    {
        "video_id": "_4fL4jnC8xQ",
        "video_title": "Is Simulating Wet Papers Possible? \ud83d\udcc3\ud83d\udca7",
        "position_in_playlist": 519,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/authors/RayTune-dcgan/reports/Ray-Tune-Distributed-Hyperparameter-Optimization-at-Scale--VmlldzoyMDEwNDY\n\n\ud83d\udcdd The paper \"A moving least square reproducing kernel particle method for unified multiphase continuum simulation\" is available here:\nhttps://cg.cs.tsinghua.edu.cn/papers/SIGASIA-2020-fluid.pdf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Yes, you see it correctly, this is a paper on paper. The paper-paper if you will. And today, you will witness some amazing works in the domain of computer graphics and physics simulations. There is so much progress in this area. For instance, we can simulate honey coiling, baking and melting, bouncy jelly and many related phenomena. And none of these techniques use any machine learning, these are all good old-fashined handcrafted algorithms. And using these, we can even simulate stretching and compression, to the point that muscle movement simulations are possible. When attaching the muscles to bones, as we move the character, the muscles move and contract accurately. What\u2019s more, this work can even perform muscle growth simulations. So, are we done here? Did these ingenious computer graphics researchers max out physics simulations, where there is nothing else to do? Oh no, of course not! Look, this footage is from an earlier graphics paper that simulates viscosity and melting fluids, and what I would like you to look at here is not what it does, but what it doesn\u2019t do. It starts melting these Armadillos beautifully, however, what there is something that it doesn\u2019t do, which is, mixing. The materials start separate, and remain separate. Can we improve upon that somehow? Well, this new paper promises that and so much more that it truly makes my head spin. For instance, it can simulate hyperelastic, elastoplastic, viscous, fracturing and multiphase coupling behaviors, and most importantly, all of these can be simulated within the same framework. Not one paper for each behavior, one paper that can do all of these. That is absolutely insane. What does all that mean? Well, I say, let\u2019s see them all right now through 5 super fun experiments. Experiment number one. Wet papers. As you see, this technique handles the ball of water. Okay, we\u2019ve seen that before. And what else? Well, it handles the paper too, okay, that\u2019s getting better, but, hold on to your papers, and look, it also handles the water\u2019s interaction with the paper. Now we\u2019re talking! And careful with holding on to that paper, because if you do it correctly, this might happen. As you see, the arguments contained within this paper really hold water. Experiment number two, fracturing. As you know, most computer graphics papers on physics simulation contain creative solutions to destroying Armadillos in the most spectacular fashion. This work, is, of course, no different. Yum. Experiment number three. Dissolution. Here, we take a glass of water, add some starch powder, it starts piling up, and then, slowly starts to dissolve. And note that the water itself also becomes stickier during the process. Number four. Dipping. We first take a piece of biscuit, and dip it into the water. Note that the coupling works correctly here, in other words, the water now moves, but what is even better is that the biscuit started absorbing some of that water. And now, when we rip it apart, oh yes. Excellent! And as a light transport researcher by trade, I love watching the shape of the biscuits distorted here due to the refraction of the water. This is a beautiful demonstration of that phenomenon. And, number five. The dog! What kind of dog you ask? Well, this virtual dog gets a big splash of water, starts shaking it off, and manages to get rid of most of it. But only most of it. And it can do all of these, using one algorithm. Not one per each of these beautiful phenomena, one technique can perform all of these. That is absolutely amazing. But it does not stop there, it can also simulate snow, and it not only does it well, but it does that swiftly. How swiftly? It simulated this a bit faster than one frame per second. The starch powder experiment was about one minute per frame, and the slowest example was the dog shaking off the ball of water. The main reason for this is that it required near a quarter million particles of water and for hair, and when the algorithm computes these interactions between them, it can only advance the time in very small increments. It has to do this a hundred thousand times for each second of footage that you see here. Based on how much computation there is to do, that is really, really fast. And, don\u2019t forget that the First Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. And even now, the generality of this system is truly something to behold. Congratulations to the authors on this amazing paper. What a time to be alive! So, if you wish to read a beautifully written paper today that does not dissolve in your hands, I highly  recommend this one. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_4fL4jnC8xQ",
        "paper_link": "https://cg.cs.tsinghua.edu.cn/papers/SIGASIA-2020-fluid.pdf",
        "paper_title": "A moving least square reproducing kernel particle method for unified multiphase continuum simulation"
    },
    {
        "video_id": "gfMyGad1Gmc",
        "video_title": "5 Fiber-Like Tools That Can Now Be 3D-Printed!",
        "position_in_playlist": 520,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI\n\n\ud83d\udcdd The paper \"Freely orientable microstructures for designing deformable 3D prints\" and the Shadertoy implementation are available here:\n- https://hal.inria.fr/hal-02524371\n- https://www.shadertoy.com/view/WtjfzW\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I would like to show you some results from the area of 3D printing, a topic which is, I think, a little overlooked, and show you that works this field are improving at an incredible pace. Now, a common theme among research papers in this area is that they typically allow us to design objects and materials by thinking about how they should look. Let\u2019s see if this is really true by applying the Second Law of Papers, which says whatever you are thinking about, there is already a Two Minute Papers episode on that. Let\u2019s see if it applies here! For instance, just prescribing a shape for 3D printing is old-old news. Here is a previous technique that is able to print auxetic materials. These are materials that we can start stretching, and if we do, instead of thinning, they get fatter. We can also 3D print filigree patterns with ease. These are detailed, thin patterns typically found in jewelry, fabrics and ornaments, and as you may imagine, crafting such motifs on objects would be incredibly laborious to do by hand. We can also prescribe an image, and 3D print an object that will cast a caustic pattern that shows exactly that image. And printing textured 3D objects in a number of different ways is also possible. This is called hydrographic printing and is one of the most flamboyant ways of doing that. So what happens here? Well, we place a film in water, use a chemical activator spray on it, and shove the object in the water, and\u2026oh yes, there we go! Note that these were all showcased in previous episodes of this series. So, in 3D printing, we typically design things by how they should look. Of course, how else would we be designing? Well, the authors of this crazy paper don\u2019t care about looks at all. Well, what else would they care about if not the looks? Get this, they care about how these objects deform. Yes, with this work, we can design deformations, and the algorithm will find out what the orientation of the fibers should be to create a prescribed effect. Okay, but what does this really mean? This means that we can now 3D print really cool, fiber-like microstructures deform well from one direction. In other words, they can be smashed easily and flatten a great deal during that process. I bet there was a ton of fun to be had at the lab on this day. However, research is not only fun and joy, look, if we turn this object around, ouch. This side is very rigid, and resists deformations well, so there was probably a lot of injuries in the lab that day too. So, clearly, this is really cool. But of course, our question is, what is all this good for? Is this just an interesting experiment, or is this thing really useful? Well, let\u2019s see what this paper has to offer in 5 amazing examples. Example number one. Pliers. The jaws and the hand grips are supposed to be very rigid, checkmark, however, there needs to be a joint between them to allow us to operate it. This joint needs to be deformable, and not any kind of deformable, but exactly the right kind of deformable to make sure it opens and closes properly. Loving this one. 3D printing pliers from fiber-like structures. How cool is that? Example number two. Structured plates. This shows that not all sides have to have the same properties. We can also print a material which has rigid and flexible parts on the same side, a few inches apart, thus introducing interesting directional bending characteristics. For instance, this one shows a strong collapsing behavior, and can grip our finger at the same time. Example number three. Bendy plates. We can even design structures where one side absorbs deformations, while the other one transfers it forward, bending the whole structure. Example number four. Seat-like structures. The seat surface is designed to deform a little more to create a comfy sensation, but the rest of the seat has to be rigid to not collapse and last a long time. And finally, example number five. Knee-like structures. These freely collapse in this direction to allow movement. However, they resist forces from any other direction. And these are really just some rudimentary examples of what this method can do, but the structures showcased here could be used in soft robotics, soft mechanisms, prosthetics, and even more areas. The main challenge of this work is creating an algorithm that can deal with these breaking patterns, which make for an object that is nearly impossible to manufacture. However, this method can not only eliminate these, but it can design structures that can be manufactured on low-end 3D printers and it also uses inexpensive materials to accomplish that. And hold on to your papers, because this work showcases a handcrafted technique to perform all this. Not a learning algorithm in sight. And there are two more things that I really liked in this paper. One is that these proposed structures collapse way better than this previous method. And, not only the source code of this project is available, but it is available for you to try on one of the best websites on the entirety of the internet. Shadertoy. So good! So, I hope you now agree that the field of 3D printing research is improving at an incredible pace, and I also hope that you had some fun learning about it. What a time  to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=gfMyGad1Gmc",
        "paper_link": "https://hal.inria.fr/hal-02524371",
        "paper_title": "Freely orientable microstructures for designing deformable 3D prints"
    },
    {
        "video_id": "2jwVDRKKDME",
        "video_title": "Burning Down Virtual Trees... In Real Time! \ud83c\udf32\ud83d\udd25",
        "position_in_playlist": 521,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://www.wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/authors/adv-dl/reports/An-Introduction-to-Adversarial-Examples-in-Deep-Learning--VmlldzoyMTQwODM\n\n\ud83d\udcdd The paper \"Interactive Wood Combustion for Botanical Tree Models\" is available here:\nhttps://repository.kaust.edu.sa/bitstream/10754/626814/1/a197-pirk.pdf\nhttps://github.com/art049/InteractiveWoodCombustion\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev #physics",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to burn some virtual trees down. This is a fantastic computer graphics paper from four years ago. I ask you to hold on to your papers immediately, and do not get surprised if it spontaneously lights on fire. Yes, this work is about simulating wood combustion, and it is one of my favorite kinds of papers that takes an extremely narrow task, and absolutely nails it. Everything we can possibly ask for from such a simulation is there. Each leaf has its own individual mass and area, they burn individually, transfer heat to their surroundings, and finally, branches bend, and, look can eventually even break in this process. If we look under the hood, we see that these trees are defined as a system of connected particles embedded within a physics simulator. These particles have their own properties, for instance, you see the temperature changes here at different regions of the tree as the fire gradually consumes it. Now if you have been holding on to your papers, now, squeeze that paper and look. What do you think - is this fire movement pre-programmed? It doesn\u2019t seem like it. This seems more like some real-time mouse movement, which is great news indeed, and, yes, that means that this simulation, and all the interactions we can do with it runs in real time. Here is a list of the many quantities it can simulate, oh my goodness! There is so much yummy physics here I don\u2019t even know where to start. Let\u2019s pick the water content here and see how changing it would look. This is a tree with a lower water content, it catches fire rather easily, and now, let\u2019s pour some rain on it. Then, afterwards, look, it becomes much more difficult to light on fire and emits huge plumes of dense, dense smoke. Beautiful. And, we can even play with these parameters in real time. We can also have a ton of fun by choosing non-physical parameters for the breaking coefficient, which, of course, can lead to the tree suddenly falling apart in a non-physical way. The cool thing here is that we can either set these parameters to physically plausible values and get a really realistic simulation, or, we can choose to bend reality in directions that are in line with our artistic vision. How cool is that? I could play with this all day. So, as an experienced Scholar, you ask, okay, this looks great, but how good are these simulations, really? Are they just good enough to fool the untrained eye, or are they indeed close to reality? I hope you know what\u2019s coming. Because what is coming is my favorite part in all simulation research, and that is when we let reality be our judge and compare the simulation to that. This is a piece of real footage of a piece of burning wood, and this is the simulation. Well, we see that the resolution of the fire simulation was a little limited, it was four years ago after all, however, it runs very similarly to the real life footage. Bravo! And all this was done in 2017. What a time to be alive! But we are not even close to be done yet, this paper teaches us one more important lesson. After publishing such an incredible work, it was accepted to SIGGRAPH ASIA 2017. That is one of the most prestigious conferences in this research field, getting a paper accepted here is equivalent to winning the olympic gold medal of computer graphics research. So with that, we would expect that the authors now revel in eternal glory. Right? Well, let\u2019s see. What? Is this serious? The original video was seen by less than a thousand people online. How can that be? And the paper was referred to only ten times by other works in these four years. Now, you see, that is not so bad in computer graphics at all, it is an order, maybe even orders of magnitude smaller field than machine learning, but I think this is an excellent demonstration of why I started this series. And it is because I get so excited by these incredible human achievements, and I feel that they deserve a little more love than they are given, and of course, these are so amazing, everybody has to know about them. Happy to have you Fellow Scholars watching this and celebrating these papers with me for more than 500 episodes now. Thank you so much, it is a true honor to have such an amazing and receptive audience. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2jwVDRKKDME",
        "paper_link": "https://repository.kaust.edu.sa/bitstream/10754/626814/1/a197-pirk.pdf",
        "paper_title": "Interactive Wood Combustion for Botanical Tree Models"
    },
    {
        "video_id": "iXqLTJFTUGc",
        "video_title": "AI Makes Near-Perfect DeepFakes in 40 Seconds! \ud83d\udc68",
        "position_in_playlist": 522,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Iterative Text-based Editing of Talking-heads Using Neural Retargeting\" is available here:\nhttps://davidyao.me/projects/text2vid/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background image credit: \u00c9cole polytechnique - J.Barande - https://www.flickr.com/photos/117994717@N06/36055906023\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#deepfake",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Imagine that you are a film critic and you are recording a video review of a movie, but unfortunately you are not the best kind of movie critic, and you record it before watching the movie. But here is the problem - you don\u2019t really know if it\u2019s going to be any good. So you record this. So far so good. Nothing too crazy going on here. However, you go in, watch the movie, and it turns out to be amazing. So, what do we do if we don\u2019t have time to re-record the video? Well, we grab this AI, type in the new text, and it will give us this. Whoa! What just happened? What kind of black magic is this? Well, let\u2019s look behind the person, on the blackboard you see some delicious partial derivatives, and I am starting to think that this person is not a movie critic. And of course, of course he isn\u2019t, because this is Yoshua Bengio, legendary machine learning researcher. And this was an introduction video where he says this. And, what happened is that it has been repurposed by this new DeepFake generator AI, where we can type in anything we wish, and out comes a near-perfect result. It synthesizes both the video and audio content for us. But we are not quite done yet. Something is missing. If the movie gets an A+, the gestures of the subject also have reflect that this is a favorable review. So, what do we do? Maybe add a smile there. Is that possible? Oh yes, there we go! Amazing. Let\u2019s have a closer look at one more example where we see how easily we can drop in new text with this editor. Now, this is not the first method performing this task - previous techniques typically required hours and hours of video from a target subject. So how much training data does this require to perform all this? Well, let\u2019s have a look together! Look, this is not the same footage copy-pasted three times, this is a synthesized video output if we have 10 minutes of video data from the test subject. This looks nearly as good, has fewer sharp details, but, in return, this only requires 2.5 minutes. And here comes the best part. If you look here, you may be able to see the difference. And if you have been holding on to your papers so far, now, squeeze that paper because synthesizing this only required 30 seconds of video footage of the target subject. My goodness. But we are not nearly done yet, it can do more! For instance, it can tone up or down the intensity of gestures to match the tone of what is being said. Look. So how does this wizardry happen? Well, this new technique improves two things really well, one is that it can search for phonemes and other units better. Here is an example, we crossed out the word \u201cspider\u201d and wish to use the word \u201cfox\u201d instead, and it tries to assemble this word from previous occurrences of individual sounds. For instance, the \u201cox\u201d part is available when the test subject utters the word \u201cbox\u201d. And two, it can then stitch them together better than previous methods. And surely this means that since it needs less data, the synthesis must take a great deal longer, right? No, not at all, the synthesis part only takes 40 seconds. And even if it couldn\u2019t this so quickly, the performance control aspect where we can tone the gestures up or down, or add a smile would still be an amazing selling point in and of itself. But no, it does all of these things quickly, and with high quality at the same time. Wow. I now invite you to look at the results carefully, and give them a hard time. Did you find anything out of ordinary? Did you find this believable? Let me know in the comments below. The authors of the paper also conducted a user study with a 110 participants, who were asked to look at 25 videos and say which one they felt was real. The results showed that the new method outperforms previous techniques, even if they have access to 12 times more training data. Which is absolutely absolutely amazing, but what is even better, the longer the video clips were, the better this method fared. What a time to be alive! Now, of course, beyond the many amazing use-cases of DeepFakes in reviving deceased actors, creating beautiful visual art, redubbing movies, and more, we have to be vigilant about the fact that they can also be used for nefarious purposes. The goal of this video is to let you, and the public know that these DeepFakes can now be created quickly and inexpensively, and they don\u2019t require a trained scientist anymore. If this can be done, it is of utmost importance that we all know about it! And beyond that, whenever they invite me, I inform key political and military decision makers about the existence and details of these techniques to make sure that they also know about these and using that knowledge, they can make better decisions for us. You can see me doing that here. Note that these talks and consultations all happen free of charge, and if they keep inviting me, I\u2019ll keep showing up to help with this in the future as a service  to the public. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iXqLTJFTUGc",
        "paper_link": "https://davidyao.me/projects/text2vid/",
        "paper_title": "Iterative Text-based Editing of Talking-heads Using Neural Retargeting"
    },
    {
        "video_id": "yc1WpkthV3g",
        "video_title": "This AI Made Me Look Like Obi-Wan Kenobi! \ud83e\uddd4",
        "position_in_playlist": 523,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\udcdd The paper \"StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\" is available here:\n- https://arxiv.org/abs/2103.17249\n- https://github.com/orpatashnik/StyleCLIP\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today\u2019s paper is about creating synthetic human faces, and not only that, but it can also make me look like Obi-Wan Kenobi. You will see the rest of this footage in a few minutes. Now, of course, this is not the first paper to generate artificial human faces. For instance, in December of 2019 a technique by the name StyleGAN2 was published. This is a neural network-based learning algorithm that is capable of synthesizing these eye-poppingly detailed images of human beings that don\u2019t even exist. This work answered some questions, and as any good paper, raised many more good ones. For instance, generating images of virtual humans is fine, but what if the results are not exactly what we are looking for? Can we have some artistic control over the outputs? How do we even tell the AI what we are looking for? Well, we are in luck because StyleGAN2 offers somewhat rudimentary control over the outputs where we can give it input images of two people, and fuse them together. Now, that is absolutely amazing, but I wonder if we can ask for a little more? Can we get even more granular control over these images? What if we could just type in what we are looking for, and somehow the AI would understand and execute our wishes? Is that possible, or, is that science fiction? Well, hold on to your papers, and let\u2019s see. This new technique works as follows. We type what aspect of the input image we wish to change, and what the change should be. Wow, really cool! And we can even play with these sliders to adjust the magnitude of the changes as well. This means that we can give someone a new hairstyle, add or remove makeup, or give them some wrinkles for good measure. Now, the original StyleGAN2 method worked on not only humans, but on a multitude of different classes too. And the new technique also inherits this property, look. We can even design new car shapes, make them a little sportier, or make our adorable cat even cuter. For some definition of cuter, of course. We can even make their hair longer  or change their colors, and the results are of super high quality. Absolutely stunning. While we are enjoying some more results here, make sure to have a look at the paper in the video description, and if you do, you will find that we really just scratched the surface here, for instance, it can even add clouds to the background of an image, or redesign the architecture of buildings, and much, much more. There are also comparisons against previous methods in there, showcasing the improvements of the new method. And now, let\u2019s experiment a little on me. Look, this is me here, after I got locked up for dropping my papers. And I spent so long in there, that I grew a beard. Or I mean, a previous learning-based AI called StyleFlow gave me one. And since the dropping your papers is a serious crime, the sentence is long\u2026quite long. Ouch. I hereby promise to never drop my papers, ever again. So now, let\u2019s try to move forward with this image, and give it to this new algorithm for some additional work. This is the original. And by original I mean image with the added algorithmic beard from a previous AI. And this is the embedded version of the image. This image looks a little different. Why is that? It is because StyleGAN2 runs an embedding operation on the photo before starting its work. This is its own internal understanding of my image if you will. This is great information, and is something that we can only experience if we have hands-on experience with the algorithm. And now, let\u2019s use this new technique to apply some more magic to this image. This is where the goodness happens. And, oh my, it does not disappoint! You see, it can gradually transform me into Obi-Wan Kenobi. An elegant algorithm, for a more civilized age. But that\u2019s not all. It can also create a ginger K\u00e1roly, hippie K\u00e1roly, K\u00e1roly who found a shiny new paper on fluid simulations, and K\u00e1roly who read said paper outside for quite a while, and was perhaps disappointed. And now, hold on to your papers and please welcome Dr. Karolina Zsolnai-Feh\u00e9r. And one more, I apologize in advance\u2026rockstar K\u00e1roly, with a mohawk. How cool is that? I would like to send a huge thank you to the authors for taking time out of their workday to create these images only for us, you can really only see these here on Two Minute Papers. And as you see, the pace of progress in machine learning research is absolutely stunning, and with this, the limit of our artistic workflow is going to be not our mechanical skills, but only our imagination. What  a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=yc1WpkthV3g",
        "paper_link": "https://arxiv.org/abs/2103.17249",
        "paper_title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery"
    },
    {
        "video_id": "3IFLVOaFAus",
        "video_title": "Can An AI Perform A Cartwheel? \ud83e\udd38\u200d\u2642\ufe0f",
        "position_in_playlist": 524,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Learning and Exploring Motor Skills with Spacetime Bounds\" is available here:\nhttps://milkpku.github.io/project/spacetime.html\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we will see how this AI-based technique can help our virtual characters not only learn new movements, but they can even perform them with style. Now, here you see a piece of reference motion - this is what we would like our virtual character to learn. The task is to then, enter a physics simulation where try to find the correct joint angles and movements to perform that. Of course, this is already a challenge because even a small difference in joint positions can make a great deal of difference in the output. Then, the second, more difficult task is to do this with style. No two people perform cartwheels exactly the same way, so would it be possible to have our virtual characters imbued with style, so that they, much like people, would have their own kinds of movement? Is that possible somehow? Well, let\u2019s have a look at the simulated characters. Nice, so this chap surely learned to at the very least reproduce the reference motion, but let\u2019s stop the footage here and there and look for differences. Oh yes, this is indeed different. This virtual character indeed has its own style, but at the same time, it is still faithful to the original reference motion. This is a magnificent solution to a very difficult task. And the authors made it look deceptively easy, but you will see in a moment that this is really challenging. So how does all this magic happen? How do we imbue these virtual characters with style? Well, let\u2019s define style as a creative deviation from the reference motion, so it can be different, but not too different, or else, this happens. So, what are we seeing here? Here, with green, you see the algorithm\u2019s estimation of the center of mass for this character. And our goal would be to reproduce that as faithfully as possible. That would be the copying machine solution. But, here comes the key for style. And that key is using spacetime bounds. This means that the center of mass of the character can deviate from the original, but only as long as it remains strictly within these boundaries. And that is where the style emerges! If we wish to add a little style to the equation, we can set relatively loose spacetime bounds around it, leaving room for the AI to explore. If we wish to strictly reproduce the reference motion, we can set the bounds to be really tight, instead. This is a great technique to learn running, jumping, rolling behaviors, it can even perform a stylish cartwheel and backflips. Oh yeah. Loving it. These spacetime bounds also help us retarget the motion to different virtual body types. And furthermore, it also helps us salvage really bad quality reference motions and make something useful out of them. So, are we done here? Is that all? No, not in the slightest! Now, hold on to your papers because here comes the best part. With these novel spacetime bounds, we can specify additional stylistic choices to the character moves. For instance, we can encourage the character to use more energy for a more intense dancing sequence, or, we can make it sleepier by asking it to decrease its energy use. And I wonder if we can put bounds on the energy use, can we do more, for instance, do the same with, for instance, body volume use. Oh yeah! This really opens up new kinds of motions that I haven\u2019t seen virtual characters perform yet. For instance, this chap was encouraged to use its entire body volume for a walk, and thus, looks like someone who is clearly looking for trouble. And this poor thing just finished their paper for a conference deadline and is barely alive. We can even mix multiple motions together. For instance, what could be a combination of a regular running sequence, and a bent walk? Well, this. And if we have a standard running sequence, and a happy walk, we can fuse them into a happy running sequence. How cool is that? So, with this technique, we can finally not only teach virtual characters to perform nearly any kind of reference motion, but we can even ask them to do these with style. What an incredible idea. Loving it! Now, before we go. I would like to show you a short message that we got that melted my heart. This I got from Nathan, who has been inspired by these incredible works and he decided to turn his life around, and go back to study more. I love my job, and reading messages like this is one of the absolute best parts of it. Congratulations Nathan, thank you so much, and good luck! If you feel that you have a similar story with this video series, make sure to let us know in  the comments! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=3IFLVOaFAus",
        "paper_link": "https://milkpku.github.io/project/spacetime.html",
        "paper_title": "Learning and Exploring Motor Skills with Spacetime Bounds"
    },
    {
        "video_id": "9RzCZZBjlxM",
        "video_title": "AI \u201cArtist\u201d Creates Near-Perfect Toonifications! \ud83d\udc69\u200d\ud83c\udfa8",
        "position_in_playlist": 525,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Debug-Compare-Reproduce-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly\n\n\ud83d\udcdd The paper \"ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement\" is available here:\nhttps://yuval-alaluf.github.io/restyle-encoder/\n\n\ud83d\udcdd Our material synthesis paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\udcdd The font manifold paper is available here:\nhttp://vecg.cs.ucl.ac.uk/Projects/projects_fonts/projects_fonts.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to generate human faces, and even better, we will keep them intact. You will see what that means in a moment. This new neural network-based technique can dream up completely new images, and more. However, this is not the first technique to do this, but this\u2026this does them better. Let\u2019s look at 3 amazing features that it offers, and then discuss, how, and why it is better than its predecessors. Hold on to your papers for the first example, which is my favorite, image toonification. Would you like to see what the AI thinks you would look like if you were a Disney character? Well, here you go. And these are not some rudimentary, first-paper-in-the-works kind of results. These are proper toonifications, you could ask for money for some of these, and they are done completely automatically by a learning algorithm. Woah. At the end of the video, you will also witness as I myself get toonified. And what is even cooler is that we can not only produce these still images, but even compute intermediate images between two input photos, and get meaningful results. I\u2019ll stop the process here and there to show you how good these are. I am blown away. Two, it can also perform the usual suspects. For instance, it can make us older or younger, or put a smile on our face too. However, three it works not only on human faces, but cars, animals, and buildings too. So the results are all great, but how does all this wizardry happen? Well, we take an image, embed it into a latent space, and in this space, we can easily apply modifications. Okay\u2026but what is this latent space thing? A latent space is a made-up place where we are trying to organize data in a way that similar things are close to each other. What you see here is a 2D latent space for generating different fonts. It is hard to explain why these fonts are similar, but most of us would agree that they indeed share some common properties. The cool thing here is that we can explore this latent space with our cursor, and generate all kinds of new fonts. You can try this work in your browser, the link is available in the video description. And, luckily, we can build a latent space, not only for fonts, but, for nearly anything. I am a light transport researcher by trade, so in this earlier paper, we were interested in generating hundreds of variants of a material model to populate this scene. In this latent space, we can concoct all of these really cool digital material models. A link to this work is also available in the video description. Now, for the face generation algorithms, this embedding step is typically imperfect, which means that we might lose some information during the process. In the better cases, things may look a little different, and that\u2019s not even the worst case scenario. I\u2019ll show you that in a moment. For the milder case, here is an earlier example from a paper by the name StyleFlow, where the authors embedded me into a latent space and it indeed came out a little different. But not so bad. A later work, StyleClip, was able to make me look like Obi-Wan Kenobi, which is excellent. However, the embedding step was more imperfect. The bearded image was embedded like this. You are probably saying that this looks different, but even this is not so bad. If you want to see a much worse example, look at this. My goodness. Now this is quite different. Now that we saw what it could do, it is time to ask the big question. How much better is it than previous works? Do we have an A-B test for that? And the answer is yes, of course! Let\u2019s embed this gentleman and see how he comes out on the other end. Well, without the improvements of this paper, once again, quite different. The beard is nearly gone. And when we toonify the image, let\u2019s see\u2026yup, that beard is gone for good. So, can this paper get that beard back? Let\u2019s see\u2026oh yes! If we refine the embedding with the new method, we get that glorious beard back. That is one heck of a toon image, congratulations! Loving it. And now, it\u2019s my turn. One of the results was amazing, I really like this one. How about this one? Well, not bad. And I wonder if it can deal with sunglasses? Well, kind of, but not in the way you might think. What do you think? Let me know in the comments below! Note that you can only see these results here on Two Minute Papers, and a big thank you to the authors for taking time off their busy day and doing these experiments for us. And here are a few more tests, let\u2019s see how it fares with these. The inputs are a diverse set of images from different classes, and the initial embeddings are, well, a bit of a disappointment. But that is kind of the point, because this new technique does not let it stop there and iteratively improves them, yes, getting better, and by the end, my goodness. Very close to the input. Don\u2019t forget that the goal here is not to implement a copying machine, the key difference is that we can\u2019t do too much with the input image, but after the embedding step, we can do all this toonification and other kinds magic with it, and the results are only relevant as long as the two images are close. And they are really close. Bravo! So good! So, I hope that now you agree that the pace of progress in machine learning and synthetic image generation is absolutely incredible. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=9RzCZZBjlxM",
        "paper_link": "https://yuval-alaluf.github.io/restyle-encoder/",
        "paper_title": "ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement"
    },
    {
        "video_id": "x2zDrSgrlYQ",
        "video_title": "Beautiful Glitter Simulation\u2026Faster Than Real Time! \u2728",
        "position_in_playlist": 526,
        "description": "\u2764\ufe0f Check out the Gradient Dissent podcast by Weights & Biases: http://wandb.me/gd\u00a0\n\n\ud83d\udcdd The paper \"Procedural Physically based BRDF for Real-Time Rendering of Glints\" is available here:\nhttp://igg.unistra.fr/People/chermain/real_time_glint/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This incredible paper promises procedural and physically-based rendering of glittering surfaces. Whoa. Okay, I am sold\u2026 provided that it does as well as advertised. We shall see about that! Oh, goodness, the results look lovely. And now, while we marvel at these, let\u2019s discuss what these terms really mean. One, physically-based means that it is built on a foundation that is based in physics. That is not surprising, as the name says so. The surprise usually comes when people use the term without careful consideration. You see, light transport researchers take this term very seriously, if you claim that your model is physically-based, you better bring your A-game. We will inspect that. Two, the procedural part means that we ourselves can algorithmically generate many of these materials models ourselves. For instance, this earlier paper was able procedurally generate the geometry of climbing plants and simulate their growth. Or, procedural generation can come to the rescue when we are creating a virtual environment and we need a hundreds of different flowers, thousands of blades of grass, and more. This is an actual example of procedural geometry that was created with the wonderful Terragen program. Three, rendering means a computer program that we run on our machine at home and it creates a beautiful series of images, like these ones. In the case of photorealistic rendering, we typically need to wait from minutes to hours for every single image. So, how fast is this technique? Well, hold on to your papers, because we don\u2019t need to wait from minutes to hours for every image. Instead, we need only 2.5 milliseconds per frame. Absolute witchcraft. The fact that we can do this in real time blows my mind, and, yes! This means that we can test the procedural part in an interactive demo too! Here we can play with a set of parameters, look, the roughness of the surface is not set in stone, we can specify it and see as the material changes in real time. We can also change the density of microfacets, these are tiny imperfections in the surface that really make these materials come alive. And if we make the microfacet density much larger, the material becomes more diffuse. And if we make them really small\u2026ohoho! Loving this. So, as much as I love this, I would also love to know how accurate this is. For reference, here is a result from a previous technique that is really accurate, however, this is one of those methods that takes from minutes to hours for just one image. And here is the other end of the spectrum, this is a different technique that is lower in quality, however, in return, it can produce these in real time. So which one is the new one closer to? The accurate, slow one, or less accurate, quick one What?! Its quality is as good as the accurate one, and it also runs in real time. The best of both worlds. Wow! Now, of course, not even this technique is perfect, the fact that this particular example worked so well is great, but it doesn\u2019t always come out so well. And, I know, I know, you\u2019re asking, can we try this new method. And the answer is a resounding yes, you can try it right now in multiple places! There is a web-demo, and it was even implemented in Shadertoy. So, now we know what it means to render procedural and physically-based rendering of glittering surfaces in real time. Absolutely incredible. What a time to be alive! And if you enjoyed this episode, we may have two more incredible glint rendering papers coming up in the near future. This, and this. Let me know in the comments below if you would like to see them. Write something like, \u201cyes please!\u201d. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=x2zDrSgrlYQ",
        "paper_link": "http://igg.unistra.fr/People/chermain/real_time_glint/",
        "paper_title": "Procedural Physically based BRDF for Real-Time Rendering of Glints"
    },
    {
        "video_id": "eksOgX3vacs",
        "video_title": "Meet Your Virtual AI Stuntman! \ud83d\udcaa\ud83e\udd16",
        "position_in_playlist": 527,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills\" is available here:\nhttps://xbpeng.github.io/projects/DeepMimic/index.html\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail tree image credit: https://pixabay.com/images/id-576847/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to look at a paper from three years ago, and not any kind of paper, but my kind of paper, which is in the intersection of machine learning, computer graphics, and physics simulations. This work zooms in on reproducing reference motions, but, with a twist, and adds lots of amazing additional features. So what does all this mean? You see, we are given this virtual character, a reference motion that we wish to teach it, and here, additionally, we are given a task that needs to be done. So, when the reference motion is specified, we place our AI into a physics simulation where it tries to reproduce these motions. That is a good thing because if it would try to learn to run by itself alone, it would look something like this. And if we ask it to mimic the reference motion, oh yes\u2026much better. Now that we have built up confidence in this technique, let\u2019s think bigger, and perform a backflip. Uh-oh. Well, that didn\u2019t quite work. Why is that? We just established that we can give it a reference motion and it can learn it by itself. Well, this chap failed to learn a backflip because it explored many motions during training, most of which resulted in failure. So it didn\u2019t find a good solution and settled for a mediocre solution instead. A proposed technique by the name Reference State Initialization, RSI in short remedies this issue by letting the agent explore better during the training phase. Got it, so we add this RSI, and now, all is well, right? Let\u2019s see. Ouch. Not so much! It appears to fall on the ground and tries to continue the motion from there. A+ for effort, little AI, but unfortunately that\u2019s not what we are looking for. So what is the issue here? The issue is that the agent has hit the ground, and after that, it still tries to score some additional points by continuing to mimic the reference motion. Again, A+ for effort, but this should not give the agent additional scores. This method we just described is called early termination. Let\u2019s try it! Now, we add the early termination and RSI together, and let\u2019s see if this will do the trick! \u2026And\u2026yes! Finally, with these two additions, it can now perform that sweet sweet backflip, rolls, and much, much more with flying colors. So now, the agent has the basics down, and can even perform explosive, dynamic motions as well. So, it is time. Now hold on to your papers as now comes the coolest part - we can perform different kinds of retargeting as well. What is that? Well, one kind is retargeting the environment. This means that we can teach the AI a landing motion in an idealized case, and then, ask it to perform the same, but now, off of a tall ledge. Or, we can teach it to run, and then drop it into computer game level and see if it performs well. And it really does. Amazing! This part is very important because in any reasonable industry use, these characters have to perform in a variety of environments that are different from the training environment. Two is retargeting not the environment, but the body type. We can have different types of characters learn the same motions. This is pretty nice for the Atlas robot, which has a drastically different weight distribution, and you can also see that the technique is robust against perturbations. Yes, this means one of the favorite pastimes of a computer graphics researcher, which is throwing boxes at virtual characters and seeing how well it can take it. Might as well make sure of the fact that in a simulated world, we make up all the rules! This one is doing really well, \u2026 oh. Note that the Atlas robot is indeed different than the previous model, and these motions can be retargeted to it, however, this is also a humanoid. Can we ask for non-humanoids as well perhaps? Oh yes! This technique supports retargeting to T-Rexes, dragons, lions, you name it. It can even get used to the gravity of different virtual planets that we dream up. Bravo! So the value proposition of this paper is just completely out of this world. Reference State Initialization, Early Termination, retargeting to different body types, environments, oh my! To have digital applications, like computer games use this would already be amazing, and just imagine what we could do if we could deploy these to real-world robots. And don\u2019t forget, these research works just keep on improving every year. The First Law Of Papers says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. Now, fortunately, we can do that right now! Why is that? It is because this paper is from 2018, which means that followup papers already exist. What\u2019s more, we even discussed one that teaches these agents to not only reproduce these reference motions, but to do those with style. And style there meant that the agent is allowed to make creative deviations from the reference motion, thus, developing its own way of doing it. An amazing improvement. And I wonder what researchers will come up with in the near future? If you have some ideas, let me know in the comments below. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=eksOgX3vacs",
        "paper_link": "https://xbpeng.github.io/projects/DeepMimic/index.html",
        "paper_title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills"
    },
    {
        "video_id": "LtyvS7NYonw",
        "video_title": "Beautiful Fluid Simulations...In Just 40 Seconds! \ud83e\udd2f",
        "position_in_playlist": 528,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Visualize-Debug-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly#System-4\n\n\ud83d\udcdd The paper \"Wave Curves: Simulating Lagrangian water waves on dynamically deforming surfaces\" is available here:\nhttp://visualcomputing.ist.ac.at/publications/2020/WaveCurves/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Through the power of computer graphics research works, today, it is possible to simulate honey coiling, water flow with debris, or even get a neural network to look at these simulations and learn how to continue them! Now, if we look under the hood, we see that not all, but many of these simulations contain particles. And our task is to simulate the pressure, velocity, and other physical quantities for these particles and create a surface where we can watch the evolution of their movement. Once again, the simulations are typically based on particles. But not this new technique. Look. It takes a coarse simulation, well, this one is not too exciting. So why are we looking at this? Well, look! Whoa! The new method can add these crispy, high-frequency details to it. And the result is an absolutely beautiful simulation. And it does not use millions and millions of particles to get this done. In fact, it does not use any particles at all! Instead, it uses wave curves. These are curve-shaped wave-packets that can enrich a coarse wave simulation and improve it a great deal to create a really detailed, crisp output. And it gets even better, because these wave curves can be applied as a simple post processing step. What this means is that the workflow what you saw here really works like that. When we have a coarse simulation that is already done, and we are not happy with it, with many other existing methods, it is time to simulate the whole thing again from scratch, but not here. With this one, we can just add all this detail to an already existing simulation. Wow. Loving it. Note that the surface of the fluid is made opaque so that we can get a better view of the waves. Of course, the final simulations that we get for production use are transparent, like the one you see here. Now, another interesting detail is that that the execution time is linear with respect to the curve points. So what does that mean? Well, let\u2019s have a look together. In the first scenario, we get a low-quality underlying simulation, and we add a 100 thousand wave curves. This takes approximately 10 seconds and looks like this. This already greatly enhanced the quality of the results, but we can decide to add more. So, first case, a 100k wave curves, in 10-ish seconds. Now comes the linear part - if we decide that we are yearning for a little more, we can run 200k wave curves, and the execution time will be 20-ish seconds. It looks like this. Better, we\u2019re getting there! And for a 400k wave curves, 40-ish seconds, and for 800k curves, yes, you guessed it right, 80-ish seconds. Double the number of curves, double the execution time. This is what the linear scaling part means. Now, of course, not even this technique is perfect. The post-processing nature of the method means that it can enrich the underlying simulation a great deal, but, it cannot add changes that are too intrusive to it. It can only add small waves relative to the size of the fluid domain. But even with these, the value proposition of this paper is just out of this world. So, from now on, if we have a relatively poor quality fluid simulation that we abandoned years ago, we don\u2019t need to despair. What we need is to harness the power of wave curves. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=LtyvS7NYonw",
        "paper_link": "http://visualcomputing.ist.ac.at/publications/2020/WaveCurves/",
        "paper_title": "Wave Curves: Simulating Lagrangian water waves on dynamically deforming surfaces"
    },
    {
        "video_id": "g7bEUB8aLvM",
        "video_title": "Can We Teach Physics To A Machine? \u269b",
        "position_in_playlist": 529,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\udcdd The paper \"Learning mesh-based simulation with Graph Networks\" is available here:\nhttps://arxiv.org/abs/2010.03409\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Alex Serban, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Haris Husic,  Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Robin Graham, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. If you have been watching this series for\u00a0 a while, you know very well that I love\u00a0\u00a0 learning algorithms and fluid simulations.\u00a0 But do you know what I like even better?\u00a0\u00a0 Learning algorithms applied to fluid simulations,\u00a0 so I couldn\u2019t be happier with today\u2019s paper. We can create wondrous fluid simulations like\u00a0 the ones you see here by studying the laws of\u00a0\u00a0 fluid motion from physics, and writing a\u00a0 computer program that contains these laws.\u00a0\u00a0 However, I just mentioned learning algorithms.\u00a0\u00a0 How do these even come to the picture? If we\u00a0 can write a program that simulates the laws,\u00a0\u00a0 why would we need learning-based algorithms?\u00a0 This doesn\u2019t seem to make any sense. You see, in this task, the neural networks\u00a0 can be also applied to solve something that\u00a0\u00a0 we already know how to solve. However, if we\u00a0 use a neural network to perform this task,\u00a0\u00a0 we have to train it, which is a long and\u00a0 arduous process. I hope to have convinced you\u00a0\u00a0 that this is a bad, bad idea. Why would anyone\u00a0 bother to do that? Does this make any sense? Well, it does make a lot of sense! And the\u00a0 reason for that is that this training step\u00a0\u00a0 only has to be done once, and afterwards,\u00a0 querying the neural network, that is, predicting\u00a0\u00a0 what happens next in the simulation runs almost\u00a0 immediately. This takes way less time than\u00a0\u00a0 calculating all the forces and pressures in the\u00a0 simulation while retaining high quality results. This earlier work from last year\u00a0 absolutely nailed this problem. Look,\u00a0\u00a0 this is a scene with the\u00a0 boxes it has been trained on.\u00a0\u00a0 And now, let\u2019s ask it to try to simulate the\u00a0 evolution of significantly different shapes.\u00a0\u00a0 Wow. It not only does well with\u00a0 these previously unseen shapes,\u00a0\u00a0 but it also handles their\u00a0 interactions really well. But there was more! We could also train it\u00a0 on a tiny domain with only a few particles,\u00a0\u00a0 and then, it was able to learn general\u00a0 concepts that we can reuse to simulate\u00a0\u00a0 a much bigger domain, and also,\u00a0 with more particles. Fantastic! This was a simple, general model that\u00a0 truly is a force to be reckoned with.\u00a0\u00a0 Now, this is a great leap in neural network-based\u00a0 physics simulations, but of course, not everything\u00a0\u00a0 was perfect there. For instance, over longer\u00a0 timeframes, solids became incorrectly deformed. And now, a newer iteration of a similar system\u00a0 just came out from DeepMind\u2019s research lab that\u00a0\u00a0 promises to extend these neural networks\u00a0 for an incredible set of use cases:\u00a0\u00a0 aerodynamics, structural mechanics,\u00a0\u00a0 cloth simulations and more. I am very\u00a0 excited to see how far they have come since! So let\u2019s see how well it does, first, with\u00a0 rollouts, then, with generalization experiments.\u00a0\u00a0 Here is the first rollout experiment, so what\u00a0 does that mean, and what are we seeing here?\u00a0\u00a0 On the left, you see a verified handcrafted\u00a0 algorithm performing the simulation,\u00a0\u00a0 we will accept this as the true data, and\u00a0 on the right, the AI is trying to continue\u00a0\u00a0 the initial simulation. But there is one problem.\u00a0 And that problem is that the AI was only trained\u00a0\u00a0 on short simulations with 400 time steps,\u00a0 that\u2019s only a few seconds! And unfortunately,\u00a0\u00a0 this test will be a hundred times longer. So, it\u00a0 only learned on short simulations, can it manage\u00a0\u00a0 to run a longer one and remain stable? Well, that\u00a0 will be tough\u2026but, so far so good\u2026still running.\u00a0\u00a0 My goodness, this is really something. Still\u00a0 running and it\u2019s very close to the ground truth! Okay, that is fantastic, but that was just a\u00a0 piece of cloth. What about interaction with other\u00a0\u00a0 objects? Well, let\u2019s see. I\u2019ll stop the process\u00a0 here and there so we can inspect the differences.\u00a0\u00a0 Again, flying colors. Loving it. And, apparently, the same can be said for\u00a0 simulations in structural mechanics, and\u00a0\u00a0 incompressible fluid dynamics. Now, that is one\u00a0 more important lesson here - to be able to solve\u00a0\u00a0 such a wide variety of simulation problems, we\u00a0 need a bunch of different hancrafted algorithms\u00a0\u00a0 that took many-many years to develop. But this\u00a0 one neural network can learn and perform them all,\u00a0\u00a0 and it can do it 10 to a 100 times quicker. And now comes the second half, generalization\u00a0 experiments. This means a simulation scenario\u00a0\u00a0 with shapes that the algorithm has never seen\u00a0 before. And let\u2019s see if it obtained general\u00a0\u00a0 knowledge of the underlying laws of physics\u00a0 to be able to pull this off. Oh my! Look at\u00a0\u00a0 that! Even the tiny piece that is hanging off\u00a0 of the flag is simulated nearly perfectly. In this one, they gave it different wind speeds\u00a0 and directions that it hadn\u2019t seen before, and not\u00a0\u00a0 only that, but we are varying these parameters\u00a0 in time, and it doesn\u2019t even break a sweat. And hold on to your papers, because\u00a0 here comes my favorite - it can even\u00a0\u00a0 learn on a small-scale simulation with\u00a0 a simple rectangular flag, and now,\u00a0\u00a0 we throw at it a much more detailed, cylindrical\u00a0 flag with tassels. Surely this will be way beyond\u00a0\u00a0 what any learning algorithm can do today.\u00a0 And\u2026okay, come on\u2026I am truly out of words. Look. So now, this is official. We can ask an AI to\u00a0 perform something that we already know how to do,\u00a0\u00a0 and it will not only be able to reproduce\u00a0 similar simulations, but we can even ask things\u00a0\u00a0 that were previously quite unreasonably outside\u00a0 of what it had seen, and it handles all these\u00a0\u00a0 with flying colors. And it does this much better\u00a0 than previous techniques were able to. And it can\u00a0\u00a0 learn from multiple different algorithms at\u00a0 the same time. Wow. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=g7bEUB8aLvM",
        "paper_link": "https://arxiv.org/abs/2010.03409",
        "paper_title": "Learning mesh-based simulation with Graph Networks"
    },
    {
        "video_id": "22Sojtv4gbg",
        "video_title": "A Video Game That Looks Like Reality! \ud83c\udf34",
        "position_in_playlist": 530,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Enhancing Photorealism Enhancement\" is available here:\nhttps://intel-isl.github.io/PhotorealismEnhancement/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This paper is called Enhancing photorealism enhancement. Hmm! Let\u2019s try to unpack what that exactly means. This means that we take video footage from a game, for instance, GTA 5, which is an action game where the city we can play in was modeled after real places in California. Now, as we are living the advent of neural network-based learning algorithms, we have a ton of training data at our disposal on the internet. For instance, the cityscapes dataset contains images and videos taken in 50 real cities, and it also contains annotations that describe which object is which. And the authors of this paper looked at this, and had an absolutely insane idea. And the idea is let\u2019s learn on the cityscapes dataset what cars, cities and architecture looks like, then take a piece of video footage from the game, and translate it into a real movie. So basically something that is impossible. That is an insane idea, and when I read this paper, I thought that cannot possibly work in any case, but especially not given that the game takes place in California, and the Cityscapes dataset contains mostly footage of German cities. How would a learning algorithm pull that off? There is no way this will work. Now, there are previous techniques that attempted this, here you see a few of them. And\u2026well, the realism is just not there, and there was an even bigger issue. And that is the lack of temporal coherence. This is the flickering that you see where the AI processes these images independently and does not do that consistently. This quickly breaks the immersion and is typically a deal-breaker. And now, hold on to your papers\u2026and let\u2019s have a look together at the new technique. Whoa! This is nothing like the previous ones! It renders the exact same place, the exact same cars, and the badges are still correct and still refer to real-world brands. And that\u2019s not even the best part, look! The carpaint materials are significantly more realistic, something that is really difficult to capture in a real-time rendering engine. Lots of realistic looking specular highlights off of something that feels like the real geometry of the car. Wow. Now, as you see, most of the generated photorealistic images are dimmer, and less saturated than the video game graphics. Why is that? This is because computer game engines often create a more stylized world where the saturation, haze, and bloom effects are often more pronounced. Let\u2019s try to fight this bias where many people consider the more saturated images to be better, and focus our attention to the realism in these image pairs. While we are there, for reference, we can have a look at what the output would be if we didn\u2019t do any of the photorealistic magic, but instead, we just tried to breathe more life into the video game footage by trying to transfer the color schemes from these real-world videos in the training set. So, only color transfer. Let\u2019s see. Yes, that helps\u2026until we compare the results with the photorealistic images synthesized by this new AI. Look. The trees don\u2019t look nearly as realistic as the new method, and after we see the real roads, it\u2019s hard to settle for the synthetic ones from the game. However, no one said that Cityscapes is the only dataset we can use for this method. In fact, if we still find ourselves yearning for that saturated look, we can try to plug in a more stylized dataset, and get\u2026this! This is fantastic, because these images don\u2019t have many of the limitations of computer graphics rendering systems. Why is that? Because, look at the grass here. In the game, it looks like a 2D texture to save resources and be able to render an image quicker. However, the new system can put more real-looking grass in there, which is a fully 3D object where every single blade of grass is considered. The most mind-blowing thing here is that this AI finally has enough generalization capabilities to learn about cities in Germany, and still be able to make convincing photorealistic images for California. The algorithm never saw California, and yet, it can recreate it from video game footage better than I ever imagined would be possible. That is mind blowing. Unreal. And if you have been holding on to your papers so far, now, squeeze that paper. Because here, we have one of those rare cases where we squeeze our papers for not a feature, but for a limitation\u2026of sorts. You see, there are limits to this technique too. For instance since the AI was trained on the beautiful lush hills of Germany and Austria, it hasn\u2019t really seen the dry hills of LA. So, what does it do with them? Look, it redrew the hills the only way it saw hills exist, which is, with trees. Now, we can think of this as a limitation, but also as an opportunity. Just imagine the amazing artistic effects we could achieve by playing this trick to our advantage. Also, we won\u2019t need to create an 80% photorealistic game like this one and push it up to a 100% with the AI. We could draw not 80%, but the bare minimum, maybe only 20% for the video game, a coarse draft, if you will, and let the AI do the heavy lifting! Imagine how much modeling time we could save for artists as well. I love this. What a time to be alive! Now, all of this only makes sense for real-world use if it can run quickly. So can it? How long do we have to wait to get such a photorealistic video? Do we wait from minutes to hours? No! The whole thing runs interactively, which means that it is already usable, we can plug this into the game as a post-processing step. And remember the First Law Of Papers, which says that two more papers down the line, and it will be even better. What improvements do you expect to happen soon? And what would you use this for? Let me know in the comments below! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=22Sojtv4gbg",
        "paper_link": "https://intel-isl.github.io/PhotorealismEnhancement/",
        "paper_title": "Enhancing Photorealism Enhancement"
    },
    {
        "video_id": "vx7H7GrE5KA",
        "video_title": "Can An AI Heal This Image?\ud83d\udc69\u200d\u2695\ufe0f",
        "position_in_playlist": 531,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Debug-Compare-Reproduce-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly\n\n\ud83d\udcdd The paper \"Self-Organising Textures\" is available here:\nhttps://distill.pub/selforg/2021/textures/\n\nGame of Life animation source: https://copy.sh/life/\nGame of Life image source: https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to play with a cellular automaton and fuse it together with a neural network. It will be quite an experience! But of course, first, what are these things anyway? You can imagine a cellular automaton as a small game where we have a bunch of cells, and a set of simple rules that describe when a cell should be full, and when it should be empty. What you see here is a popular example called Game of Life, which simulates a tiny world where each cell represents a little life form. So why is this so interesting? Well, this cellular automaton shows us that even a small set of simple rules can give rise to remarkably complex life forms, such as gliders, spaceships, and even John von Neumann\u2019s universal constructor, or in other words, self-replicating machines. Now, this gets more interesting, a later paper fused this cellular automaton with a neural network. It was tasked to grow, and even better, maintain a prescribed shape. Remember these two words, grow and maintain shape. And the question was, if it can recover from undesirable states, can it perhaps..regenerate when damaged? Well, here, you will see all kinds of damage\u2026and then, this happens. Nice! The best part is that this thing wasn\u2019t even trained to be able to perform this kind of regeneration! The objective for training was that it should be able to perform its task of growing and maintaining shape, and it turns out, some sort of regeneration is included in that. This sounds very promising, and I wonder if we can we apply this concept to something where healing is instrumental? Are there such applications in computer science? If so, what could those be? Oh yes, yes there are! For instance, think about texture synthesis. This is a topic that is subject to a great deal of research in computer graphics, and those folks have this down to a science. So, what are we doing here? Texture synthesis typically means that we need lots of concrete or gravel road, skin, marble, create unique stripes for zebras, for instance, for a computer game or the post-production of a movie, and we really don\u2019t want to draw miles and miles of these textures by hand. Instead, we give it to an algorithm to continue this small sample, where the output should be a bigger version of this pattern, with the same characteristics. So how do we know if we have a good technique at hand? Well, first, it must not be repetitive, checkmark, and it has to be free of seams. This part means that we should not be able to see any lines or artifacts that would quickly give the trick away. Now, get this, this new paper attempts to do the same with neural cellular automatons. What an insane idea! We like those around here, so let\u2019s give it a try! How? Well, first, by trying to expand this simple checkerboard pattern. The algorithm is starting out from random noise, and as it evolves\u2026well, this is a disaster. We are looking for squares, but we have quadrilaterals. They are also misaligned. And they are also inconsistent. But, luckily, we are not done yet. And now, hold on to your papers, and observe how the grid cells communicate with each other to improve the result. First, the misalignment is taken care of, then, the quadrilaterals become squares, and then, the consistency of their placement is improved. And the end result is, look! In this other example, we can not only see these beautiful bubbles grow out of nowhere. But the density of the bubbles remains roughly the same over the process. Look! As two of them get too close to each other, they coalesce, or pop. Damaged bubbles can also regrow. Very cool! Okay, it can do proper texture synthesis, but so can a ton of other handcrafted computer graphics algorithms, so why is this interesting? Why bother with this? Well, first, you may think that the result of this technique is the same as other techniques, but it isn\u2019t. The output is not necessarily just an image, but can be an animation too! Excellent. Here, it was also able to animate the checkerboard pattern, and, even better, it can not only reproduce the weave pattern, but the animation part extends to this too. And now comes the even more interesting part. Let\u2019s ask why does it output an animation and not an image? The answer lies within these weaving patterns. We just need to carefully observe them. Let\u2019s see. Yes, again, we start out from noise, where some woven patterns emerge, but then, it almost looks like a person who started weaving them until it resembles the initial sample. Yes, that is the key! The neural network learned to create not an image, not an animation, but no less than a computer program to accomplish this kind of texture synthesis! How cool is that? So, armed with all that knowledge, do you remember the regenerating iguana project? Let\u2019s try to destroy these textures too and see if it can use these computer programs to recover and get us a seamless texture. First, we delete parts of the texture, then, it fills in the gap with noise, and now, let\u2019s run that program! Wow! Resilient, self-healing texture synthesis. How cool is that? And in every case, it starts out from a solution that is completely wrong, improves it to be just kind of wrong, and after further improvement, there you go. Fantastic! What a time to be alive! And, note that this is a paper in the wonderful Distill journal, which not only means that it is excellent, but also interactive, so you can run many of these experiments yourself right in your web browser. The link is available  in the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=vx7H7GrE5KA",
        "paper_link": "https://distill.pub/selforg/2021/textures/",
        "paper_title": "Self-Organising Textures"
    },
    {
        "video_id": "rSPwOeX46UA",
        "video_title": "This is Grammar For Robots. What? Why? \ud83e\udd16",
        "position_in_playlist": 532,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"RoboGrammar: Graph Grammar for Terrain-Optimized Robot Design \" is available here:\nhttps://people.csail.mit.edu/jiex/papers/robogrammar/index.html\n\nBreakdancing robot paper:\nhttp://moghs.csail.mit.edu/\n\nBuilding grammar paper:\nhttps://www.cg.tuwien.ac.at/research/publications/2015/Ilcik_2015_LAY/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to generate robots with grammars. Wait a second. Grammars? Of all things, what do grammars have to do with robots? Do we need to teach them grammar to speak correctly? No no, of course not! To answer these questions, let\u2019s invoke the Second Law Of Papers, which says that whatever you are thinking about, there is already Two Minute Papers episode on that. Even on grammars. Let\u2019s see if it applies here! In this earlier work, we talked about generating buildings with grammars. So how does that work? Grammars are a set of rules that tell us how to build up a structure, such as a sentence properly from small elements, like nouns, adjectives and so on. My friend, Martin Ilcik loves to build buildings from grammars. For instance, a shape grammar for buildings can describe rules like a wall can contain several windows, below a window goes a window sill, one wall may have at most two doors attached, and so on. A later paper also used a similar concept to generate tangle patterns. So this grammar thing has some power in assembling things after all! So, can we apply this knowledge to build robots! First, the robots in this new paper are built up as a collection of these joint types, links and wheels, which can come in all kinds of sizes and weights. Now, our question is, how do we assemble them in a way so that they can traverse a given terrain effectively? Well, time for some experiments! Look at this robot. It has a lot of character, I must say, and can deal with this terrain pretty well. Now look at this poor thing. Someone in the lab at MIT had a super fun day with this one I am sure. Now, these can sort of do the job, but now, let\u2019s see the power of grammars and search algorithms in creating more optimized robots for a variety of terrains! First, a flat terrain. Let\u2019s see\u2026yes, now we\u2019re talking! This one is traversing at great speed\u2026 and this one works too. I like how it was able to find vastly different robot structures that both perform well here. Now, let\u2019s look at a little harder level, with gapped terrains. Look, oh wow, loving this. The algorithm recognized that a more rigid body is required to efficiently step through the gaps. And now, I wonder what happens if we add some ridges to the levels, so it cannot only step through the gaps, but has to climb? Let\u2019s see\u2026and we get those long long limbs that can indeed climb through the ridges. Excellent! Now, add a staircase, and see who can climb these well! The algorithm says, well, someone with long arms and a somewhat elastic body. Let\u2019s challenge the algorithm some more! Let\u2019s add, for instance, a frozen lake. Who can climb a flat surface that is really slippery? Does the algorithm know? Look, it says, someone who can utilize a low-friction surface by dragging itself through it, or someone with many legs. Loving this. Now, this is way too much fun, so let\u2019s do two more. What about a walled terrain example? What kind of robot would work there? One with a more elastic body, carefully designed to be able curve sharply, enabling rapid direction changes. But it cannot be too long, or else it would bang its head into the wall. This is indeed a carefully crafted specimen for this particular level. Now, of course, real-world situations often involve multiple kinds of terrains, not just one. And of course, the authors of this paper know that very well, and also asked the algorithm to design a specimen that can traverse walled and ridged terrains really well. Make sure to have a look at the paper, which even shows graphs for robot archetypes that work on different terrains. It turns out, one can even make claims about the optimality, which is a strong statement. I did not expect that at all. So, apparently, grammars are amazing at generating many kinds of complex structures, including robots. And note that this paper also has a followup work from the same group where they took it a step further, and made figure skating and breakdancing robots. What a time to be alive! The link is also available in the video description for  that one. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rSPwOeX46UA",
        "paper_link": "https://people.csail.mit.edu/jiex/papers/robogrammar/index.html",
        "paper_title": "RoboGrammar: Graph Grammar for Terrain-Optimized Robot Design "
    },
    {
        "video_id": "SEsYo9L5lOo",
        "video_title": "Google\u2019s New AI Puts Video Calls On Steroids! \ud83d\udcaa",
        "position_in_playlist": 533,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\udcdd The paper \"Total Relighting: Learning to Relight Portraits for Background Replacement\" is available here:\nhttps://augmentedperception.github.io/total_relighting/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. With the increased popularity of online meetings, telepresence applications are on the rise where we can talk to each other from afar. Today, let\u2019s see how these powerful, new neural network-based learning methods can be applied to them. It turns out, they can help us do everyone\u2019s favorite, which is, showing up to a meeting, and changing our background to pretend we are somewhere else. Now that is a deceptively difficult problem. Here, the background has been changed, that is the easier problem, but, look! The lighting of the new environment hasn\u2019t been applied to the subject. And now, hold on to your papers, and check this out. This is the result of the new technique after it recreates the image as if she was really there. I particularly like the fact that the result includes high-quality specular highlights too, or in other words, the environment reflecting off of our skin. However, of course, this is not the first method attempting this. So let\u2019s see how it performs compared to the competition! These techniques are from one and two years ago, and\u2026they don\u2019t perform so well. Not only did they lose a lot of detail all across the image, but, the specular highlights are gone. As a result, the image feels more like a video game character than a real person. Luckily, the authors also have access to the reference information to make our job of comparing the results easier. Roughly speaking, the more the outputs look like this, the better. So now, hold on to your papers, and let\u2019s see how the new method performed. Oh yes! Now we\u2019re talking! Now, of course, not even this is perfect. Clearly, the specularity of clothing was determined incorrectly, and the matting around the thinner parts of the hair could be better, which is notoriously difficult to get right. But, this is such a huge step forward in just one paper. And we are not nearly done - there are two more things that I found to be remarkable about this work. One, is that the whole method was trained on still images, yet, it still works on video too! And we don\u2019t have any apparent temporal coherence issues, or in other words, no flickering arises from the fact that it processes the video as not a video, but a series of separate images. Very cool. Two, if we are in a meeting with someone and we like their background, we can simply borrow it. Look. This technique can take their image, get the background out, estimate its lighting, and give the whole package to us too. I think this will be a game changer. People may start to become more selective with these backgrounds, not just because of how the background looks, but, because how it makes them look. Remember, lighting off of a well chosen background makes a great deal of a difference in our appearance in the real world, and now, with this method in virtual worlds too. And this will likely happen not decades from now, but in the near future. So this new method is clearly capable of some serious magic. But how? What is going on under the hood to achieve this? This method performs two important steps to accomplish this, step number one is matting. This means separating the foreground from the background, and then, if done well, we can now easily cut out the background and also have the subject on a separate layer and proceed to step number two. Which is, relighting. In this step, the goal is to estimate the illumination of the new scene and recolor the subject as if she were really there. This new technique performs both, but most of the contributions lie in this step. To be able to accomplish this, we have to be able to estimate the material properties of the subject. The technique has to know one, where the diffuse parts are, these are the parts that don\u2019t change too much as the lighting changes, and two, where the specular parts are, in other words, shiny regions that reflect back the environment more clearly. Putting it all together, we get really high-quality relighting for ourselves, and given that this was developed by Google, I expect that this will supercharge our meetings quite soon. And just imagine what we will have two more papers down the line. My goodness, what a time  to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SEsYo9L5lOo",
        "paper_link": "https://augmentedperception.github.io/total_relighting/",
        "paper_title": "Total Relighting: Learning to Relight Portraits for Background Replacement"
    },
    {
        "video_id": "2qqDwaZlkE0",
        "video_title": "Glitter Simulation, Now Faster Than Ever! \u2728",
        "position_in_playlist": 534,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/wandb/getting-started/reports/Debug-Compare-Reproduce-Machine-Learning-Models--VmlldzoyNzY5MDk?utm_source=karoly\n\n\ud83d\udcdd The paper \"Slope-Space Integrals for Specular Next Event\u00a0Estimation\" is available here:\nhttps://rgl.epfl.ch/publications/Loubet2020Slope\n\n\u2600\ufe0f Free rendering course:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/\n\n\ud83d\udd2e Paper with the difficult scene: https://users.cg.tuwien.ac.at/zsolnai/gfx/adaptive_metropolis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I am a light transport researcher by trade, and due to popular request, today I am delighted to show you these beautiful results from a research paper. This is a new light simulation technique that can create an image like this. This looks almost exactly like reality. It can also make an image like this. And, have a look at this - this is a virtual object that glitters. Oh my goodness. Absolutely beautiful. Right? Well, believe it or not, this third result is wrong. Now, you see, it is not so bad, but there is a flaw in it somewhere. By the end of this video, you will know exactly where and what is wrong. So, light transport eh? How do these techniques work anyway? We can create such an image by simulating the path of millions and millions of light rays. And initially, this image will look noisy, and as we add more and more rays, this image will slowly clean up over time. If we don\u2019t have a well-optimized program, this can take from hours to days to compute for difficult scenes. For instance, this difficult scene took us several weeks to compute. Okay, so what makes a scene difficult? Typically, caustics and specular light transport. What does that mean? Look! Here we have a caustic pattern that takes many-many millions, if not billions of light rays to compute properly. This can get tricky because these are light paths that we are very unlikely to hit with randomly generated light rays. So, how do we solve this problem? Well, one way of doing it is not trusting random light rays, but systematically finding these caustic light paths and computing them. This fantastic paper does exactly that, so let\u2019s look at one of those classic closeups that are the hallmark of any modern light transport paper. Let\u2019s see. Yes. On this scene, you see beautiful caustic patterns under these glossy metallic objects. Let\u2019s see what a simple, random algorithm can do with this with an allowance of two minutes of rendering time. Well, do you see any caustics here? Do you see these bright points? These are the first signs of the algorithm finding small point samples of the caustic pattern, but that\u2019s about it. It would take at the very least several days for this algorithm to compute the entirety of it. This is what the fully rendered reference image looks like. This is the one that takes forever to compute. Quite different, right? So let\u2019s allocate 2 minutes of our time for the new method and see how well it does. Which one will it be closer to? Can it beat the naive algorithm? Now, hold on to your papers, and let\u2019s see together. What? On this part, it looks almost exactly the same as the reference. This is insanity! A converged caustic region in two minutes! Whoa. The green closeup is also nearly completely done. Now, not everything is sunshine and rainbows, look, the blue closeup is still a bit behind, but it still beats the naive algorithm handily. That is quite something. And yes, it can also render these beautiful underwater caustics as well in as little as 5 minutes. 5 minutes! And I would not be surprised if many people would think this is an actual photograph from the real world. Loving it. Now, what about the glittery origami scene from the start of the video? This one. Was that footage really wrong? Yes it was! Why? Well, look here! These glittery patterns are unstable. The effect especially pronounced around here. This arises from the fact that the technique does not take into consideration the curvature of this object correctly when computing the image. Let\u2019s look at the corrected version, and, oh my goodness. No unnecessary flickering anywhere to be seen, just the beautiful glitter slowly changing as we rotate the object around. I could stare at this all day. Now, note that these kinds of glints are much more practical than most people would think. For instance, it also has a really pronounced effect when rendering a vinyl record and many other materials as well. So, from now on, we can render photorealistic images of difficult scenes with caustics and glitter, not in a matter of days, but in a matter of minutes. What a time to be alive! And when watching all these beautiful results, you are thinking that this light transport thing is pretty cool, and you would like to learn more about it, I held a Master-level course on this topic at the Technical University of Vienna. Since I was always teaching it to a handful of motivated students, I thought that the teachings shouldn\u2019t only be available for the privileged few who can afford a college education, but the teachings should be available for everyone. Free education for everyone, that\u2019s what I want. So, the course is available free of charge for everyone, no strings attached, so make sure to click the link in the video description to get started. We write a full light simulation program from scratch there, and learn about physics, the world around us, and more. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=2qqDwaZlkE0",
        "paper_link": "https://rgl.epfl.ch/publications/Loubet2020Slope",
        "paper_title": "Slope-Space Integrals for Specular Next Event\u00a0Estimation"
    },
    {
        "video_id": "AGCH1GR7pPU",
        "video_title": "Burning Down an Entire Virtual Forest! \ud83c\udf32\ud83d\udd25",
        "position_in_playlist": 535,
        "description": "\u2764\ufe0f Check out the Gradient Dissent podcast by Weights & Biases: http://wandb.me/gd\u00a0\n\n\ud83d\udcdd The paper \"Fire in Paradise: Mesoscale Simulation of Wildfires\" is available here:\nhttp://computationalsciences.org/publications/haedrich-2021-wildfires.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. In a previous episode not so long ago, we burned down a virtual tree. This was possible through an amazing simulation paper from 4 years ago, where each leaf has its own individual mass and area, they burn individually, transfer heat to their surroundings, and finally, branches bend, and, look can eventually even break in this process. How quickly did this run? Of course, in real time. Well, that is quite a paper, so if this was so good, how does anyone improve that? Burn down another virtual tree? No, no, that would be too easy. You know what, instead, let\u2019s set on fire an entire virtual forest. Oh yeah! Here you see a simulation of a devastating fire from a lightning strike in Yosemite national park. The simulations this time around are typically sped up a great deal to be able to give us a better view of how it spreads, so if you see some flickering, that is the reason for that. But wait, is that really that much harder? Why not just put a bunch of trees next to each other and start the simulation? Would that work? The answer is a resounding no. Let\u2019s have a look why, and with that, hold on to your papers because here comes the best part: it also simulates not only the fire, but cloud dynamics as well. Here you see how the wildfire creates lots of hot, and dark smoke closer to the ground, and, wait for it\u2026yes! There we go! Higher up, the condensation of water creates this lighter, cloudy region. Yes, this is key to the simulation, not just because of the aesthetic effects, but this wildfire can indeed create a cloud type that goes by the name flammagenitus. So, is that good or bad news? Well, both! Let\u2019s start with the good news: it often produces rainfall, which helps putting out the fire. Well, that is wonderful news, so then, what is so bad about it? Well, flammagenitus clouds may also trigger a thunderstorm, and thus, create another huge fire. That\u2019s bad news number one. And bad news number two: it also occludes the fire, thereby making it harder to locate and extinguish it. So, got it, add cloud dynamics to the tree fire simulator, and we are done, right? No, not even close. In a forest fire simulation, not just clouds, everything matters. For instance, first we need to take into consideration the wind intensity and direction. This can mean the difference between a manageable or a devastating forest fire. Second, it takes into consideration the density and moisture intensity of different tree types - for instance, you see that the darker trees here are burning down really slowly. Why is that? This is because these trees are denser birches and oak trees. Third, the distribution of the trees also matter. Of course, the more the area is covered by trees, the more degrees of freedom there are for the fire to spread. And, fourth. Fire can not only spread horizontally from tree to tree, but vertically too. Look! When a small tree catches fire, this can happen. So, as we established from the previous paper, one tree catching fire can be simulated in real time. What about an entire forest? Let\u2019s take the simulation with the most number of trees, my goodness, they simulated 120k trees there. And the computation time for one simulation step was\u202695. So, 95 what? 95 milliseconds. Wow! So this thing runs interactively, which means that all of these phenomena can be simulated in close to real time. With that, we can now model how a fire would spread in real forests around the world, test different kinds of fire barriers and their advantages, and we can even simulate how to effectively put out the fire. And don\u2019t forget, we went from simulating one burning tree to a hundred and twenty thousand in just one more paper down the line. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=AGCH1GR7pPU",
        "paper_link": "http://computationalsciences.org/publications/haedrich-2021-wildfires.html",
        "paper_title": "Fire in Paradise: Mesoscale Simulation of Wildfires"
    },
    {
        "video_id": "7WgtK1C4hQg",
        "video_title": "Simulating The Olympics\u2026 On Mars! \ud83c\udf17",
        "position_in_playlist": 536,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Discovering Diverse Athletic Jumping Strategies\" is available here:\nhttps://arpspoof.github.io/project/jump/jump.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, it is possible to teach virtual characters to perform highly dynamic motions, like a cartwheel or backflips. And not only that, but we can teach an AI to perform this differently from other characters, to do it with style if you will. But! Today, we are not looking to be stylish. Today, we are looking to be efficient! In this paper, researchers placed an AI in a physics simulation and asked it to control a virtual character and gave it one task: to jump as high as it can. And when I heard this idea, I was elated, and immediately wondered - did it come up with popular techniques that exist in the real world? Well, let\u2019s see\u2026yes! Woo-hoo! That is indeed a Fosbury flop. This allows the athlete to jump backward over the bar, thus lowering their center of gravity. Even today, this is the prevalent technique in high jump competitions. With this technique, the takeoff takes place relatively late. The only problem is that the AI didn\u2019t clear the bar so far\u2026so, can it? Well, this is a learning-based algorithm, so, with a little more practice, it should improve - yes, great work! If we lower the bar just a tiny bit for this virtual athlete, we can also observe it performing the western roll. With this technique, we take off a little earlier and we don\u2019t jump backward, but sideways. If it had nothing else, this would already be a great paper, but we are not nearly done yet. The best is yet to come! This is a simulation. A virtual world if you will, and here, we make all the rules. The limit is only our imagination. The authors know that very well and you will see that they indeed have a very vivid imagination. For instance, we can also simulate a jump with a weak take-off leg and see that with this condition, the little AI can only clear a bar that is approximately one foot lower than its previous record. What about another virtual athlete with an inflexible spine? It can jump approximately two feet lower. Here is the difference compared to the original. I am enjoying this a great deal, and it\u2019s only getting better. Next, what happens if we are injured and have a cast on the take-off knee? What results can we expect? Something like this. We can jump a little more than two feet lower. What about organizing the olympics on Mars? What would that look like? What would the world record be with the weaker gravity there? Well, hold on to your papers, and look\u2026 yes, we could jump three feet higher than on earth, and then\u2026ouch, well missed the foam matting, but otherwise, very impressive. And if we are already there, why limit the simulation to high jumps? Why not try something else? Again, in a simulation, we can do anything! Previously, the task was to jump over the bar, but we can also recreate the simulation to include instead, jumping through obstacles. To get all of these magical results, the authors propose a step they call \u201cBayesian Diversity Search\u201d. This helps systematically creating a rich selection of novel strategies, and it does this efficiently. The authors also went the extra mile and included a comparison to motion capture footage performed by a real athlete. But note that the AI\u2019s version uses a similar technique and is able to clear a significantly higher bar without ever seeing a high jump move. The method was trained on motion capture footage to get used to humanlike movements, like walking, running, and kicks, but it has never seen any high jump techniques before. Wow. So, if this can invent high jumping techniques that took decades for humans to invent, I wonder what else it could invent? What do you think? Let me know in the comments below! What a time to be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=7WgtK1C4hQg",
        "paper_link": "https://arpspoof.github.io/project/jump/jump.html",
        "paper_title": "Discovering Diverse Athletic Jumping Strategies"
    },
    {
        "video_id": "Lp4k4O_HEeQ",
        "video_title": "One Simulation Paper, Tons of Progress! \ud83d\udc87",
        "position_in_playlist": 537,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Revisiting Integration in the Material Point Method\" is available here:\nhttp://yunfei.work/asflip/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Through the power of computer\u00a0 graphics research, today,\u00a0\u00a0 we can write wondrous programs which can simulate\u00a0 all kinds of interactions in virtual worlds.\u00a0\u00a0 These works are some of my favorites, and\u00a0 looking at the results, one would think that\u00a0\u00a0 these algorithms are so advanced, there is\u00a0 hardly anything new to invent in this area.\u00a0\u00a0 But as amazing as these previous techniques\u00a0 are, they don\u2019t come without limitations. Alright, well, what do those limitations look\u00a0 like? Let\u2019s have a look at this example. The\u00a0\u00a0 water is coming out of the nozzle, and it behaves\u00a0 unnaturally. But that\u2019s only the smaller problem,\u00a0\u00a0 there is an even bigger one. What is that\u00a0 problem? Let\u2019s slow this down, and look carefully.\u00a0\u00a0 Oh! Where did the water go? Yes, this is\u00a0 the classical numerical dissipation problem\u00a0\u00a0 in fluid simulations, where due to an averaging\u00a0 step, particles disappear into thin air.\u00a0\u00a0 And now, hold on to your papers and let\u2019s see if\u00a0 this new method can properly address this problem.\u00a0\u00a0 And\u2026oh yes! Fantastic! So it dissipates less. Great! What else does this\u00a0 do? Let\u2019s have a look through this experiment\u00a0\u00a0 where a wheel gets submerged into sand, that\u2019s\u00a0 good, but\u2026the simulation is a little mellow.\u00a0\u00a0 You see, the sand particles\u00a0 are flowing down like a fluid,\u00a0\u00a0 and the wheel does not really roll up the\u00a0 particles in the air. And the new one.\u00a0\u00a0 So, apparently, it not only\u00a0 helps with numerical dissipation,\u00a0\u00a0 but also with particle separation\u00a0 too. More value. I like it. If this technique can really solve these two\u00a0 phenomena, we don\u2019t even need sandy tires and\u00a0\u00a0 water sprinklers to make it shine, there are\u00a0 so many scenarios where it performs better\u00a0\u00a0 than previous techniques. For instance, when\u00a0 simulating this non-frictional elastic plate\u00a0\u00a0 with previous methods, some of the particles get\u00a0 glued to it. And, did you catch the other issue?\u00a0\u00a0 Yes, the rest of the particles also\u00a0 refuse to slide off of each other.\u00a0\u00a0 And now, let\u2019s see the new method. Oh my! It\u00a0 can simulate these phenomena correctly too! And it does not stop there. It also simulates\u00a0 strand-strand interactions better than previous\u00a0\u00a0 methods. In these cases, sometimes the\u00a0 collision of short strands with boundaries\u00a0\u00a0 was also simulated incorrectly. Look at how all\u00a0 this geometry intersected through the brush.\u00a0\u00a0 And the new method? Yes, of course,\u00a0 it addresses these issues too. So, if it can simulate the movement and\u00a0 intersection of short strands better\u2026does\u00a0\u00a0 that mean that it can also perform higher-quality\u00a0 hair simulations? Oh yes, yes it does! Excellent! So, as you see, the pace of progress\u00a0 in computer graphics research\u00a0\u00a0 is absolutely stunning. Things\u00a0 that were previously impossible\u00a0\u00a0 can become possible in a matter of just\u00a0 one paper. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Lp4k4O_HEeQ",
        "paper_link": "http://yunfei.work/asflip/",
        "paper_title": "Revisiting Integration in the Material Point Method"
    },
    {
        "video_id": "jl0XCslxwB0",
        "video_title": "NVIDIA\u2019s GANCraft AI: Feels Like Magic! \ud83c\udf34 \u2026Also, 1 Million Subs! \ud83e\udd73",
        "position_in_playlist": 538,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Unsupervised 3D Neural Rendering of Minecraft Worlds\" is available here:\nhttps://nvlabs.github.io/GANcraft/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#minecraft #gancraft",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. We just hit a million subscribers! I can\u00a0 hardly believe that so many of you Fellow\u00a0\u00a0 Scholars are enjoying the Papers! Thank you so\u00a0 much for all the love! In a previous episode,\u00a0\u00a0 we explored an absolutely insane idea. The idea\u00a0 was to unleash a learning algorithm on a dataset\u00a0\u00a0 that contains images and videos of cities,\u00a0 then take a piece of video footage from a game,\u00a0\u00a0 and translate it into a real movie. It\u00a0 is an absolute miracle that this works,\u00a0\u00a0 and it not only works, but it works reliably and\u00a0 interactively. And it also works much better than\u00a0\u00a0 its predecessors. Now, we discussed that the\u00a0 input video game footage is pretty detailed. And I was wondering, what if we don\u2019t create\u00a0 the entire game in such detail. What about,\u00a0\u00a0 creating just the bare minimum,\u00a0 a draft of the game if you will\u00a0\u00a0 and let the algorithm do the heavy lifting.\u00a0 Let\u2019s call this world to world translation!\u00a0\u00a0 So, is world to world translation\u00a0 possible, or is this science fiction? Fortunately, scientists at NVIDIA and Cornell\u00a0 University thought of that problem and came\u00a0\u00a0 up with a remarkable solution. But, the first\u00a0 question is - what form should this draft take?\u00a0\u00a0 And they say, it should be a Minecraft world, or\u00a0 in other words, a landscape assembled from little\u00a0\u00a0 blocks. Yes, that is simple enough indeed. So this\u00a0 goes in. And now, let\u2019s see what comes out. Oh my! It created water, it understands the concept of\u00a0 an island, and it created a beautiful landscape,\u00a0\u00a0 also, with vegetation. Insanity. It even seems to have some concept\u00a0\u00a0 of reflections, although they will need\u00a0 some extra work to get perfectly right. But, what about artistic control? Do we get\u00a0 this one solution, or can we give more detailed\u00a0\u00a0 instructions to the technique? Yes we can! Look at\u00a0 that. Since the training data contains desert and\u00a0\u00a0 snowy landscapes too, is also supports them as\u00a0 outputs. Whoa, this is getting wild. I like it. And it even supports interpolation, which\u00a0 means that we can create one landscape\u00a0\u00a0 and ask the AI to create a\u00a0 blend between different styles.\u00a0\u00a0 We just look at the output animations, and pick\u00a0 the one that we like best. Absolutely amazing. What I also really liked is that\u00a0 it also supports rendering fog.\u00a0\u00a0 But this is not some trivial fog technique,\u00a0 no-no, look how beautifully it occludes the trees.\u00a0\u00a0 If we look under the hood, oh my! I am a\u00a0 light transport researcher by trade, and boy,\u00a0\u00a0 am I happy to see the authors having done\u00a0 their homework. Look, we are not playing\u00a0\u00a0 games here, the technique contains bona-fide\u00a0 volumetric light transmission calculations. Now, this is not the first technique to perform\u00a0 this kind of world to world translation.\u00a0\u00a0 What about the competition? As you see, there\u00a0 are many prior techniques here, but there is\u00a0\u00a0 one key issue that almost all of them share.\u00a0 So, what is that? Oh yes, much like with the\u00a0\u00a0 other video game papers, the issue is the lack of\u00a0 temporal coherence, which means that the previous\u00a0\u00a0 techniques don\u2019t remember they it did a few images\u00a0 earlier, and may create a drastically different\u00a0\u00a0 series of images. And the result is this kind of\u00a0 flickering that is often a deal-breaker regardless\u00a0\u00a0 of how good the technique is otherwise. Look,\u00a0 the new method does this significantly better. This could help level generation for computer\u00a0 games, creating all kinds simulations, and\u00a0\u00a0 if it improves some more, these could maybe even\u00a0 become backdrops to be used in animated movies.\u00a0\u00a0 Now, of course, this is still not perfect,\u00a0 some of the outputs are still blocky. But, with this method, creating virtual worlds\u00a0 has never been easier. I cannot believe that we\u00a0\u00a0 can have a learning-based algorithm where the\u00a0 input is one draft world, and it transforms it\u00a0\u00a0 to a much more detailed and beautiful one. Yes,\u00a0 it has its limitations, but just imagine what\u00a0\u00a0 we will be able to do two more papers down the\u00a0 line. Especially given that the quality of the\u00a0\u00a0 results can be algorithmically measured, which is\u00a0 a godsend for comparing this to future methods.\u00a0\u00a0 And for now, huge congratulations to NVIDIA\u00a0 and Cornell University for this amazing paper. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=jl0XCslxwB0",
        "paper_link": "https://nvlabs.github.io/GANcraft/",
        "paper_title": "Unsupervised 3D Neural Rendering of Minecraft Worlds"
    },
    {
        "video_id": "Nz-X3cCeXVE",
        "video_title": "This AI Helps Testing The Games Of The Future! \ud83e\udd16",
        "position_in_playlist": 539,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://colab.research.google.com/drive/1gKixa6hNUB8qrn1CfHirOfTEQm0qLCSS\n\n\ud83d\udcdd The paper \"Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents\" is available here:\nhttps://www.ea.com/seed/news/cog2021-curiosity-driven-rl-agents\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Have you ever got stuck in a video game? Or found a glitch that would prevent you from finishing it? As many of you know, most well-known computer games undergo a ton of playtesting, an important step that is supposed to unveil these issues. So how is it possible that all these bugs and glitches still make it the final product? Why did the creators not find these issues? Well, you see, playtesting is often done by humans. That sounds like a good thing, and it often is. But, here comes the problem - whenever we change something in the game, our changes may also have unintended consequences somewhere else away from where we applied them. New oversights may appear elsewhere, for instance, moving a platform may make the level more playable, however, also, this might happen. The player may now be able to enter a part of the level that shouldn\u2019t be accessible, or, be more likely to encounter a collision bug and get stuck. Unfortunately, all this means that it\u2019s not enough to just test what we have changed, but we have to retest the whole level, or maybe the whole game itself. For every single change, no matter how small. That not only takes a ton of time and effort, but is often flat out impractical. So what is the solution? Apparently, a proper solution would require asking tons of curious humans to test the game. But wait a second. We already have curious learning-based algorithms. Can we use them for playtesting? That sounds amazing! Well, yes, until we try it. You see, here is an automated agent, but a naive one trying to explore the level. Unfortunately, it seems to have missed half the map! Well, that\u2019s not the rigorous testing we are looking for, is it? Let\u2019s see what this new AI offers. Can it do any better? Oh my, now we\u2019re talking! The new technique was able to explore not less than 50%, but a whopping 95% of the map. Excellent. But we are experienced Fellow Scholars over here, so of course, we have some questions. So, apparently this one has great coverage, so it can cruise around, great, but our question is, can these AI agents really find game-breaking issues? Well, look, it just found a bug where it could climb to the top of the platform without having to use the elevator. It can also build a graph that describes which parts of the level are accessible and through what path. Look! This visualization tells us about the earlier issue where one could climb the wall through an unintentional issue, and, after the level designer supposedly fixed it by adjusting the steepness of the wall, let\u2019s see the new path. Yes, now it could only get up there by using the elevator. That is the intended way to traverse the level. Excellent! And it gets better, it can even tell us the trajectories that enabled it to leave the map so we know exactly what issues we need to fix without having to look through hours and hours of video footage. And, whenever we applied the fixes, we can easily unleash another bunch of these AIs to search every nook and cranny, and try these crazy strategies, even ones that don\u2019t make any sense, but appear to work well. So, how long does this take? Well, the new method can explore half the map in approximately an hour or two, can explore 90% of the map in about 28 hours, and if we give it a couple more days, it goes up to about 95%. That is quite a bit, so we don\u2019t get immediate feedback as soon as we change something, since this method is geared towards curiosity and not efficiency. Note that this is just the first crack at the problem, and I would not be surprised if just one more paper down the line, this would take about an hour, and two more papers down the line, it might even be done in a matter of minutes. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=Nz-X3cCeXVE",
        "paper_link": "https://www.ea.com/seed/news/cog2021-curiosity-driven-rl-agents",
        "paper_title": "Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents"
    },
    {
        "video_id": "BpApq2EPDXE",
        "video_title": "This Magical AI Makes Your Photos Move! \ud83e\udd33",
        "position_in_playlist": 540,
        "description": "\u2764\ufe0f Check out the Gradient Dissent podcast by Weights & Biases: http://wandb.me/gd\u00a0\n\n\ud83d\udcdd The paper \"Endless Loops: Detecting and Animating Periodic Patterns in Still Images\" and the app are available here:\nhttps://pub.res.lightricks.com/endless-loops/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit - Pascal Wiemers: https://pixabay.com/images/id-792193/\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Look at this video of a moving escalator.\u00a0 Nothing too crazy going on, only the escalator\u00a0\u00a0 is moving. And I am wondering, would it be\u00a0 possible to not record a video for this,\u00a0\u00a0 just an image, and have one of these amazing\u00a0 new learning-based algorithms animate it? Well, that is easier said than done. Look, this\u00a0 is what was possible with a research work from\u00a0\u00a0 2 years ago, but the results are\u2026well, what you\u00a0 see here. So how about a method from one year ago?\u00a0\u00a0 This is the result. A great deal of improvement\u00a0 - the water is not animated in this region,\u00a0\u00a0 and is generally all around the place, and we\u00a0 still have a lot of artifacts around the fence.\u00a0\u00a0 And now, hold on to your papers, and\u00a0 let\u2019s see this new method\u2026and\u2026whoa!\u00a0\u00a0 Look at that! What an improvement! Apparently, we can now give this one a still\u00a0\u00a0 image, and for the things that\u00a0 should move, it makes them move.\u00a0\u00a0 It is still not perfect by any means, but\u00a0 this is so much progress in just two years. And there\u2019s more! Get this, for the things\u00a0 that shouldn\u2019t move, it even imagines\u00a0\u00a0 how they should move. It works\u00a0 on this building really well.\u00a0\u00a0 But it also imagines how my tie would move around.\u00a0 Or my beard, which is not mine, by the way,\u00a0\u00a0 but was made by a different AI, or the windows.\u00a0 Thank you very much for the authors of the paper\u00a0\u00a0 for generating these results only for us! And\u00a0 this can lead to really cool artistic effects,\u00a0\u00a0 for instance, this moving brickwall, or\u00a0 animating the stairs here. Loving it. So, how does this work exactly? Does\u00a0 it know what regions to animate?\u00a0\u00a0 No it doesn\u2019t, and it shouldn\u2019t. We can\u00a0 specify that ourselves by using a brush\u00a0\u00a0 to highlight the region that we wish to see\u00a0 animated, and we also have a little more\u00a0\u00a0 artistic control over the results by prescribing\u00a0 a direction in which things should go. And it appears to work on a really wide\u00a0 variety of images, which is only one\u00a0\u00a0 of its most appealing features. Here\u00a0 are some of my favorite results,\u00a0\u00a0 I particularly love the one with the\u00a0 apple rotation here. Very impressive. Now, let\u2019s compare it to the previous method from\u00a0 just one year ago, and let\u2019s see what the numbers\u00a0\u00a0 say. Well, they say that the previous\u00a0 one performs better on fluid elements\u00a0\u00a0 than the new one. My experience is that it\u00a0 indeed works better on specialized cases,\u00a0\u00a0 like this fire texture, but on many water\u00a0 images, they perform roughly equivalently.\u00a0\u00a0 Both are doing quite well. So,\u00a0 is the new one really better? Well, here comes the interesting part. When\u00a0 presented with a diverse set of images,\u00a0\u00a0 look. There is no contest here - the previous\u00a0 one creates no results, incorrect results,\u00a0\u00a0 or if it does something, the new technique\u00a0 almost always comes out way better. Not only that, but let\u2019s see what the\u00a0 execution time looks like for the new method.\u00a0\u00a0 How much do we have to wait\u00a0 for these results? The one from\u00a0\u00a0 last year took 20 seconds per image and\u00a0 required a big honking graphics card,\u00a0\u00a0 while the new one only needs your smartphone,\u00a0 and runs in\u2026 what? Just one second. Loving it. So, what images would you try with this\u00a0 one? Let me know in the comments. Well,\u00a0\u00a0 in fact, you don\u2019t need to just think about\u00a0 what you would try, because you can try this\u00a0\u00a0 yourself. It has a mobile app, the link\u00a0 is available in the video description,\u00a0\u00a0 make sure to let me know in the comments\u00a0 below if you had some success with it! Here comes the even more interesting part.\u00a0 The previous method was using a learning-based\u00a0\u00a0 algorithm, while this one is a bona-fide,\u00a0 almost completely handcrafted technique.\u00a0\u00a0 Partly because training neural networks\u00a0 requires a great deal of training data,\u00a0\u00a0 and there are very few, if any training examples\u00a0 for moving buildings and these other surreal\u00a0\u00a0 phenomena. Ingenious. Huge congratulations\u00a0 to the authors for pulling this off! Now, of course, not even this technique is\u00a0 perfect, there are still cases where it does\u00a0\u00a0 not create appealing results, however,\u00a0 since it only takes a second to compute,\u00a0\u00a0 we can easily retry with a different pixel mask\u00a0 or direction and see if it does better. And just\u00a0\u00a0 imagine what we will be able to do two more\u00a0 papers down the line. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=BpApq2EPDXE",
        "paper_link": "https://pub.res.lightricks.com/endless-loops/",
        "paper_title": "Endless Loops: Detecting and Animating Periodic Patterns in Still Images"
    },
    {
        "video_id": "VqeNSZqiBzc",
        "video_title": "A Simulation That Looks Like Reality! \ud83e\udd2f",
        "position_in_playlist": 541,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Solid-Fluid Interaction with Surface-Tension-Dominant Contact\" is available here:\nhttps://lwruan.com/publication/waterstrider/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nThumbnail background design: http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I am stunned by this new graphics paper that promises to simulate three-way coupling and enables beautiful surface tension simulations like this, and this, and more. Yes, none of this is real footage, these are all simulated on a computer. I have seen quite a few simulations, and I am still baffled by this. How is this even possible? Also, three-way coupling, eh? That is quite peculiar, to the point that the term doesn\u2019t even sound real. Let\u2019s find out why together. So, what does that mean exactly? Well, first, let\u2019s have a look at one way coupling. As the box moves here, it has an effect on the smoke plume around it. This example also showcases one-way coupling, where the falling plate stirs up the smoke around it. And now, on to two-way coupling. In this case, similarly to the previous ones, the boxes are allowed to move the smoke, but the added two-way coupling part means that now, the smoke is also allowed to blow away the boxes. What\u2019s more, the vortices here on the right were even able to suspend the red box in the air for a few seconds. An excellent demonstration of a beautiful phenomenon. So, coupling means interaction between different kinds of objects. And two-way coupling seems like the real deal. Here, it is also required to compute how this fiery smoke trail propels the rocket upward. But wait, we just mentioned that the new method performs three-way coupling. Two-way was solid-fluid interactions, and it seemed absolutely amazing, so what is the third element then? And why do we even need that? Well, depending on what object is in contact with the liquid, gravity, buoyancy, and surface tension forces need additional considerations. To be able to do this, now look carefully! Yes, there is the third element, it simulates this thin liquid membrane too, which is in interaction with the solid and the fluid at the same time. And with that, please meet three-way coupling! So, what can it do? It can simulate this paperclip floating on water. That is quite remarkable because the density of the paperclip is 8 times as much as the water itself, and yet, it still sits on top of the water. But how is that possible? Especially given that gravity wants to constantly pull down a solid object. Well, it has two formidable opponents, two forces that try to counteract it, one is buoyancy, which is an upward force, and two, the capillary force, which is a consequence of the formation of a thin membrane. If these two friends are as strong as gravity, the object will float. But, this balance is very delicate, for instance, in the case of milk and cherries, this happens. And, during that time, the simulator creates a beautiful bent liquid surface that is truly a sight to behold. Once again, all of this footage is simulated on a computer. The fact that this new work can simulate these three physical systems and their interactions is a true miracle. Absolutely incredible. Now, if you have been holding on to your papers so far, squeeze that paper, because we will now do my favorite thing in any simulation paper, and that is when we let reality be our judge, and compare the simulated results to real life footage. This is a photograph. And now comes the simulation. Whoa. I have to say, if no one told me which is which, I might not be able to tell. And I am delighted to no end by this fact, so much so that I had to ask the authors to double-check if this really is a simulation and they managed to reproduce the illumination of these scenes so perfectly. Yes they did! Fantastic attention to detail. Very impressive. So, how long do we have to wait for all this? For a 2 dimensional scene, it pretty much runs interactively, that is great news. And, we are firmly in the seconds per frame region for the 3D scenes, but look, the Boat and Leaves scene runs in less than two seconds per time step. That is absolutely amazing. Not real time, because one frame contains several time steps, but why would it be real time? That this is the kind of paper that makes something previously impossible possible, and it even does that swiftly. I would wager, we are just one, or at most, two more papers away from getting this in real time. This is unbelievable progress in just one paper. And all handcrafted, no learning algorithms anywhere to be seen. Huge congratulations to the authors. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=VqeNSZqiBzc",
        "paper_link": "https://lwruan.com/publication/waterstrider/",
        "paper_title": "Solid-Fluid Interaction with Surface-Tension-Dominant Contact"
    },
    {
        "video_id": "1F-WnarzkX8",
        "video_title": "Neural Materials Are Amazing! \ud83d\udd2e",
        "position_in_playlist": 542,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/stacey/xray/reports/X-Ray-Illumination--Vmlldzo4MzA5MQ\n\n\ud83d\udcdd The paper \"NeuMIP: Multi-Resolution Neural Materials\" is available here:\nhttps://cseweb.ucsd.edu/~viscomp/projects/NeuMIP/\n\n\ud83d\udcdd  Our latent space technique:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\udcdd  Our \u201cPhotoshop\u201d technique:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-editing/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. This image is the result of a light simulation program, created by research scientists. It looks absolutely beautiful, but the light simulation algorithm is only part of the recipe here. To create something like this, we also need a good artist who can produce high-quality geometry, lighting, and, of course, good, life-like material models. For instance, without the materials part, we would see something like this. Not very exciting, right? Previously, we introduced a technique that learns our preferences and helps filling these scenes with materials. This work can also generate variants of the same materials as well. In a later technique, we could even take a sample image, completely destroy it in photoshop, and our neural networks would find a photorealistic material that matches these crazy ideas. Links to both of these works are available in the video description. And, to improve these digital materials, this new paper introduces something that the authors call a multi-resolution neural material representation. What is that? Well, it is something that is able to put amazingly complex material models in our light transport programs, and not only that, but\u2026oh my. Look at that! We can even zoom in so far that we see the snagged threads. That is the magic of the multi-resolution part of the technique. The neural part means that the technique looks at lots of measured material reflectance data, this is what describes a real-world material, and compresses this description down into a representation that is manageable. Okay\u2026why? Well, look. Here is a reference material. You see, these are absolutely beautiful, no doubt, but are often prohibitively expensive to store directly. This new method introduces these neural materials to approximate the real world materials, but in a way that is super cheap to compute and store. So, our first question is, how do these neural materials compare to these real, reference materials? What do you think? How much worse a quality do we have to expect to be able to use these in our rendering systems? Well, you tell me, because you are already looking at the new technique right now. I quickly switched from the reference to the result with new method already. How cool is that? Look. This was the expensive reference material, and this is fast neural material counterpart for it. So, how hard is this to pull off? Well, let\u2019s look at some more results side by side. Here is the reference. And here are two techniques from one and two years ago that try to approximate it. And you see that if we zoom in real close, these fine details are gone. Do we have to live with that? Or, maybe, can the new method do better? Hold on to your papers, and let\u2019s see. Wow! While it is not a 100% perfect, there is absolutely no contest compared to the previous methods. It outperforms them handily in every single case of these complex materials I came across. And when I say complex materials, I really mean it. Look at how beautifully it captures not only the texture of this piece of embroidery, but, when we move the light source around, oh wow! Look at the area here around the vertical black stripe and how its specular reflections change with the lighting. And note that none of these are real images, all of them come from a computer program. This is truly something else. Loving it. So, if it really works so well, where is the catch? Does it work only on cloth-like materials? No-no, not in the slightest! It also works really well on rocks, insulation foam, even turtle shells and a variety of other materials. The paper contains a ton more examples than we can showcase here, so make sure to have a look in the video description. I guess this means that it requires a huge and expensive neural network to pull off, right? Well, let\u2019s have a look. Whoa, now that\u2019s something. It does not require a deep and heavy-duty neural network, just 4 layers are enough. And this, by today\u2019s standard, is a lightweight network that can take these expensive reference materials and compress them down in a matter of milliseconds. And they almost look the same. Materials into our computer simulations straight from reality? Yes please! So, from now on, we will get cheaper and better material models for animation movies, computer games, and visualization applications! Sign me up right now! What a time to  be alive! Thanks  for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=1F-WnarzkX8",
        "paper_link": "https://cseweb.ucsd.edu/~viscomp/projects/NeuMIP/",
        "paper_title": "NeuMIP: Multi-Resolution Neural Materials"
    },
    {
        "video_id": "0zaGYLPj4Kk",
        "video_title": "NVIDIA\u2019s Face Generator AI: This Is The Next Level! \ud83d\udc69\u200d\ud83d\udd2c",
        "position_in_playlist": 543,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\udcdd The paper \"Alias-Free GAN\" is available here:\nhttps://nvlabs.github.io/alias-free-gan/\n\n\ud83d\udcdd Our material synthesis paper is available here: https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#nvidia #stylegan3",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today, we will see how a small change to an already existing learning-based technique can result in a huge difference in its results. This is StyleGAN2, is a technique that appeared in December of 2019. It is a neural network-based learning algorithm that is capable of synthesizing these eye-poppingly detailed images of human beings that don\u2019t even exist. This is all synthetic. It also supports a cool feature where we can give it a photo, then, it embeds this image into a latent space, and in this space, we can easily apply modifications to it. Okay\u2026but what is this latent space thing? A latent space is a made-up place where we are trying to organize data in a way that similar things are close to each other. In our earlier work, we were looking to generate hundreds of variants of a material model to populate this scene. In this latent space, we can concoct all of these really cool digital material models. A link to this work is available in the video description. StyleGAN uses walks in a similar latent space to create these human faces and animate them. So, let\u2019s see that. When we take a walk in the internal latent space of this technique, we can generate animations. Let\u2019s see how StyleGAN2 does this. It is a true miracle that a computer can create images like this. However, wait a second. Look closely\u2026Did you notice it? Something is not right here. Don\u2019t despair if not, it is hard to pin down what the exact problem is, but it is easy to see that there is some sort of flickering going on. So, what is the issue? Well, the issue is that there are landmarks, for instance, the beard, which don\u2019t really, or just barely move, and essentially, the face is being generated under it with these constraints. The authors refer to this problem as texture sticking. The AI suffers from a sticky beard if you will. Imagine saying that 20 years ago to someone, you would end up in a madhouse. Now, this new paper from scientists at NVIDIA promises a tiny but important architectural change. And we will see if this issue, which seems like quite a limitation, can be solved with it, or not. And now, hold on to your papers, and let\u2019s see the new method. Holy Mother of Papers. Do you see what I see here? The sticky beards are a thing of the past, and facial landmarks are allowed to fly about freely. And not only that, but the results are much smoother and more consistent, to the point that it can not only generate photorealistic images of virtual humans. Come on, that is so 2020. This generates photorealistic videos of virtual humans! So, I wonder, did the new technique also inherit the generality of StyleGAN2? Let\u2019s see. We know that it works on real humans, and now, paintings and art pieces, yes, excellent, and of course, cats, and other animals as well. The small change that creates these beautiful results is what we call an equivariant filter design, essentially this ensures that finer details move together in the inner thinking of the neural network. This is an excellent lesson on how a small and carefully designed architectural change can have a huge effect on the results. If we look under the hood, we see that the inner representation of the new method is completely different from its predecessor. You see, the features are indeed allowed to fly about, and the new method even seems to have invented a coordinate system of sorts to be able to move these things around. What an incredible idea. These learning algorithms are getting better and better with every published paper. Now, good news! It is only marginally more expensive to train and run than StyleGAN2, and less good news is that training these huge neural networks still requires a great deal of computation. The silver lining is that if it has been trained once, it can be run inexpensively for as long as we wish. So, images of virtual humans might soon become a thing of the past, because from now on, we can generate photorealistic videos of them. Absolutely amazing. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=0zaGYLPj4Kk",
        "paper_link": "https://nvlabs.github.io/alias-free-gan/",
        "paper_title": "Alias-Free GAN"
    },
    {
        "video_id": "_8ExhGic_Co",
        "video_title": "DeepMind\u2019s Robot Inserts A USB Stick! \ud83e\udd16",
        "position_in_playlist": 544,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/stacey/yolo-drive/reports/Bounding-Boxes-for-Object-Detection--Vmlldzo4Nzg4MQ\n\n\ud83d\udcdd The paper \"Scaling data-driven robotics with reward sketching and batch reinforcement learning\" is available here:\nhttps://sites.google.com/view/data-driven-robotics/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "this fellow scholars this is too many favors with Dr Connors on anything yet two years ago in twenty nineteen scientists at OpenAI published an outstanding paper that showcased her robot arm that could dexterous Lee many Pilate Rubik's cube  and how good were the results well nothing short of spectacular it could not only manipulate and solve the cube Bayat they could even have string the hand in many different ways and it would still be able to do well  and I am telling you scientists at OpenAI got very creative in tormenting these little hand they added a rubber gloves  tonight multiple fingers together  through a blanket on it and pushed it around with a plush giraffe and a pen  it still worked but probably had nightmares about that day in the lab robot nightmares if you will and now let's see what is going on over DeepMind side and have a look at this work on manipulating objects in this example the human starts operating this contraption and after that step is done we leave it alone and our first question is how do we tell this robot arm what it should do it doesn't yet know what the task is that we can tell it by this reward sketching step  essentially this works like a video game not in the classical setting not the other way around here we are not playing the video game but we are the video game and the robot is the character that plays the video game in which we can provided feedback and tell it when it is doing well or not  then from these rewards it can learn what the task is so %HESITATION that sounds great but what do we get for all this work four things one we can instructed to learn to lift the formals this includes a piece of cloth rope a softball and more too much like opening as robot arm it is robust against perturbations in other words it can recover from my evil machinations look we can get creative here too for instance we can reorganize the objects on the table manage an already got object out of his grip there or simply just undo the stacks that are already done  these diligent little AI is not fazed it just keeps on trying and trying and eventually it succeeds a great life lesson right there  and three it generalizes to no object types well and does not get confused by different object collars and geometries  and now hold on to your papers for number four because here comes one of the most frustrating tasks for any intelligent being something that not many humans can perform inserting a USB key correctly on the first try can you do that  well does this count as a first try I don't know for sure but dear fellow scholars the singularity is officially getting very very close especially given that we can even move the machine around a little and it was still find the correct port if only the first machine could pronounce my name correctly Dan we would conclude that we have reached the singularity but wait who noted that first human starts controlling the R. how does the fully trained AI compare to these humans work and here comes the best part it learn to perform these tasks faster look by the six second mark the humans started grabbing the green block but by this time the A. I. he is already me there with it and by nine seconds it is done while the human is still at work  excellent  and we get an eighty percent success rate on this task with only eight hours of training that is within a single working day one of the key ideas here and the part that I like the best is that we can essentially reprogram the API with the reward sketching step and I wonder what else this could be used for if we did that do you have some ideas let me know in the comments below this episode has been supported by weights and biases in this post they show you how to use that tool to draw bounding boxes for object detection and even more importantly how to debug them weights and biases provides tools to track your experiments in your deep learning projects their system is designed to save you a ton of time and money and it is actively used in projects at prestigious labs such as open AI research guitar and more and the best part is that weights and biases it's free for all individuals economics and open source project it really is as good as it gets make sure to visit them through W. N. B. dot com slash papers or just click the link in the video description and you can get the free demo today I thank the weights and biases for their longstanding support and for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=_8ExhGic_Co",
        "paper_link": "https://sites.google.com/view/data-driven-robotics/",
        "paper_title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning"
    },
    {
        "video_id": "8qeCjeJTnvI",
        "video_title": "Is This Simulation Wrong? \ud83d\udc55",
        "position_in_playlist": 545,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Fast Linking Numbers for Topology Verification of Loopy Structures \" is available here:\nhttps://graphics.stanford.edu/papers/fastlinkingnumbers/\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "NA",
        "transcription_mode": "NA",
        "source_link": "https://www.youtube.com/watch?v=8qeCjeJTnvI",
        "paper_link": "https://graphics.stanford.edu/papers/fastlinkingnumbers/",
        "paper_title": "Fast Linking Numbers for Topology Verification of Loopy Structures "
    },
    {
        "video_id": "-4M-xoE6iH0",
        "video_title": "This AI Synthesizes Dessert Photos...And More! \ud83c\udf70",
        "position_in_playlist": 546,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\udcdd The paper \"NeX: Real-time View Synthesis with Neural Basis Expansion \" is available here:\nhttps://nex-mpi.github.io/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background image credit: https://pixabay.com/images/id-5712284/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is two minute paper said Dr cattle is only for him today we are going to synthesize beautiful and photorealistic shiny objects this amazing new technique is another variant which means that it is a learning based algorithm that tries to reproduce Hayward scenes from only a few views in goes a few photos of the scene  and it has to be able to synthesize new photorealistic images in between these photos  this is view center says in short  as you see here it can be done quite well with the previous method  so our question number one is why bother publishing a new research paper on this and question number two there are plenty of fuel synthesis papers sloshing around so why choose this paper well first when we tested the original nerf method I noted that theme structures are still quite a challenge look we have some issues here  okay that is a good opening for a potential follow up paper and in a moment we'll see how the new method handles them can you do any better in the green close up you see that these structures are blurry and lack of definition and %HESITATION it gets even worse the red example shows that these structures are completely missing at places and given that we have zoomed in quite far and the disparities technique is from just one year ago I wonder how much of an improvement can we expect in just one year  let's have a look  wow that is outstanding both problems got solved just like that  but it does more way more this new method also boasts being able to measure reflectance coefficients for every single pixel  that sounds great but what does that mean what this should mean is that the reflections and specular highlights in its outputs are supposed to be much better these are you dependent effects so they are quite easy to find what you need to look for these things that change when we move the camera around let's test it remember the input is just inspires bunch of photos here they are handed the A. I. it's supposed to fill in the missing data and produce a smooth video where hopefully high quality specular highlights these are especially difficult to get a ride because they changed a great deal when we move the camera just a little  yet look the A. I can still deal with them really well  so %HESITATION yes this looks good not we are experienced fellow scholars over here so we'll put this method to the test let's try a challenging task for entering inflections by using this piece of ancient technology called CD not the best for data storage these days back fantastic for testing rendering algorithms so I would like to see two things one is the environment reflected in the silvery regions did we get it yes check mark end to I would like to see the rainbow changing and demanding as we move the camera around let's see %HESITATION look at how beautifully it has done it I love it check mark the specular objects are not just a CD thing as you see here they are absolutely everywhere around us therefore it is important to get its rights for view synthesis and I must say these results are very close to getting good enough where we can put on a VR headset and venture into what is essentially a bunch of photos not even a video and to make it feel like we are really in the room absolutely amazing  when I read the paper I thought well %HESITATION that's great but we probably need to wait forever to get results like this and now hold on to your papers because this technique is not only much better in terms of quality no no it is more than a thousand times faster thousand times in one year I try to hold on to my papers but I have to admit that I have field and now they are flying about but that's not all not even close look it can also the couple of the camera movements from these view dependent specular effect  so what does that mean it is like a thought experiment where we keep the camera stationary and led the shiny things change as if we moved around the paper also contains source code and a web demo that you can try yourself right now it reveals that we still have some more to go until the true real images can be produced but my goodness this is so much progress in just one paper  absolutely mind blowing now this technique also has its limitations beyond not being as good as the real photos for instance the reproduction of refracted thin structures  as you see not so good  not just think about it the fact that we need to make up these crazy scenes to be able to give you trouble is a true testament to how good this technique is and all this improvement in just one year what a time to be alive this video has been supported by weights and biases check out the recent offering fully connected the place where they bring machine learning practitioners together to share and discuss their ideas learn from industry leaders and even collaborate on projects together you see I get messages from your fellow scholars telling me that you have been inspired by the series but don't really know where to start and here it is fully connected is a great way to learn about the fundamentals how to reproduce experiments get your papers accepted to a conference and more make sure to visit them through W. N. B. A. dot M. E. slash papers or just click the link in the video description I thank the weights and biases for the longstanding support and for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=-4M-xoE6iH0",
        "paper_link": "https://nex-mpi.github.io/",
        "paper_title": "NeX: Real-time View Synthesis with Neural Basis Expansion "
    },
    {
        "video_id": "G00A1Fyr5ZQ",
        "video_title": "New AI Research Work Fixes Your Choppy Videos! \ud83c\udfac",
        "position_in_playlist": 547,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Time Lens: Event-based Video Frame Interpolation\" is available here:\nhttp://rpg.ifi.uzh.ch/TimeLens.html\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to take a bad choppy video, and make a beautiful, smooth and creamy footage out of it. With today's camera and graphics technology, we can create videos with 60 frames per second. Those are really smooth, I also make each of these videos using 60 frames per second, however, it almost always happens that I encounter the paper videos that have only from 24 to 30 frames per second. In this case, I put them in my video editor that has a 60 fps timeline, where half or even more of these frames will not provide any new information. That\u2019s neither smooth nor creamy. And it gets worse. Look! As we try to slow down the videos for some nice slow-motion action, this ratio becomes even worse, creating an extremely choppy output video because we have huge gaps between these frames. So, does this mean that there is nothing we can do and have to put up with this choppy footage? No, not at all! Look at this technique from 2019 that we covered in an earlier video. The results truly speak for themselves. In goes a choppy video, and out comes a smooth, and creamy result. So good! But wait, it is not 2019, it is 2021, and we always say that two more papers down the line, and it will be improved significantly. From this example, it seems that we are done here, we don\u2019t need any new papers. Is that so? Well, let\u2019s see what we have only one more paper down the line! Now, look. It promises that it can deal with 10 to 1, or even 20 to 1 ratios, which means that for every single image in the video, it creates 10 or 20 new ones, and supposedly we shouldn\u2019t notice that. Well, those are big words, so I will believe it when I see it. Let\u2019s have a look together! Holy mother of papers! This can really pull this off, and it seems nearly perfect. Wow. It also knocked it out of the park with this one. And all this improvement in just one more paper down the line. The pace of progress in machine learning research is absolutely amazing. But, we are experienced Fellow Scholars over here, so we will immediately ask, is this really better than the previous 2019 paper? Let\u2019s compare them! Can we have side by side comparisons? Of course we can! You know how much I love fluid simulations. Well, these are not simulations, but a real piece of fluid, and in this one, there is no contest. The new one understands the flow so much better, while the previous method sometimes even seems to propagate the waves backwards in time. A big checkmark for the new one. In this case, the previous method assumes linear motion when it shouldn\u2019t, thereby introducing a ton of artifacts. The new one isn\u2019t perfect either, but it performs significantly better. Do not worry for a second, we will talk about linear motion some more in a moment. So how does all this wizardry happen? One of the key contributions of the paper is that it can find out when to use the easy way and the hard way. What are those? The easy way is using already existing information in the video and computing inbetween states for a movement. That is all well and good if we have simple, linear motion in our video. But, look, the easy way fails here. Why is that? It fails because we have a difficult situation where reflections off of this object rapidly change, and it reflects something. We have to know what that something is. So, look, this is not even close to the true image, which means that here, we can\u2019t just reuse the information in the video, this requires introducing new information. Yes, that is the hard way! And this excels when new information has to be synthesized. Let\u2019s see how well it does! My goodness, look at that, it matches the true reference image almost perfectly. And also, look, the face of the human did not require synthesizing a great deal of new information, it did not change over time, so we can easily refer to the previous frame for it, hence, the easy way did better here. Did you notice? That is fantastic, because the two are complementary. Both techniques work well, but they work well elsewhere. They need each other! So, yes, you guessed right, to tie it all together, there is also an attention-based averaging step that helps us decide when to use the easy, and the hard ways. Now, this is a good paper, so it tells us how these individual techniques contribute to the final image. Using only the easy way can give us about 26 decibels, that would not beat the previous methods in this area. However, look! By adding the hard way, we get a premium quality result that is already super competitive, and, if we add the step that helps us decide when to use the easy and hard ways, we get an extra decibel. I will happily take it, thank you very much! And, if we put it all together, oh yes, we get a technique that really outpaces the competition. Excellent. So, in the near future, perhaps we will be able to record a choppy video of a family festivity, and have a chance at making this choppy video enjoyable, or maybe even create slow-motion videos with a regular camera. No slow-motion camera is required. What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=G00A1Fyr5ZQ",
        "paper_link": "http://rpg.ifi.uzh.ch/TimeLens.html",
        "paper_title": "Time Lens: Event-based Video Frame Interpolation"
    },
    {
        "video_id": "eQRZ7FUkwKo",
        "video_title": "Simulating Bursting Soap Bubbles! \ud83e\uddfc",
        "position_in_playlist": 548,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Thin-Film Smoothed Particle Hydrodynamics Fluid\" is available here:\nhttps://arxiv.org/abs/2105.07656\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nThumbnail background image credit: https://pixabay.com/images/id-801835/\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to simulate these absolutely beautiful thin film structures. You see, computer graphics researchers have been writing physics simulation programs for decades now, and the pace of progress in this research area is absolutely stunning. Here are three examples of where we are at the moment. One, this work was able to create a breathtaking honey coiling simulation. I find it absolutely amazing that through the power of computer graphics research, all this was possible four years ago. And the realistic simulation works just kept coming in. This work appeared just one year ago and could simulate not only a piece of viscous fluid, but, also deal with glugging and coalescing bubbles. And three, this particular one is blazing fast. So much so that it can simulate this dam break scene in about 5 frames per second, not seconds per frame, while it can run this water drop scene with about about 7 frames per second. Remember, this simulates quantities like the velocity, pressure, and more for several million particles this quickly. Very impressive. So, are we done here? Is there anything else left to be done in fluid simulation research? Well, hold on to your papers, and check this out. This new paper can simulate thin-film phenomena. What does that mean? Four things. First, here is a beautiful oscillating soap bubble. Yes, its color varies as a function of the evolving film thickness. But that\u2019s not all. Let\u2019s poke it, and then\u2026did you see that? It can even simulate it bursting into tiny, sparkly droplets. Phew. One more time. Loving it. Second, it can simulate one of my favorites, the Rayleigh-Taylor instability. The upper half of the thin film has a larger density, while the lower half carries a larger volume. Essentially, this is the phenomenon when two fluids of different densities meet. And what is the result? Turbulence. First, the interface between the two is well defined, but over time, it slowly disintegrates into this beautiful swirly pattern. Oh yeah\u2026oh yeah! Look! And it just keeps on going and going. Third, ah yes, the catenoid experiment. What is that? This is a surface tension-driven deformation experiment, where the film is trying to shrink as we move the two rims away from each other, forming this catenoid surface. Of course, we won\u2019t stop there, what happens when we keep moving them away? What do you do think? Please stop the video and let me know in the comments below. I\u2019ll wait. A little. Thank you! Now then, the membrane keeps shrinking, until\u2026yes, it finally collapses into a small droplet. The authors also went the extra mile and did the most difficult thing for any physics simulation paper\u2026comparing the results to reality. So, is this just good enough to fool the untrained human eye, or is this the real deal? Well, look at this, this is an actual photograph of the catenoid experiment. And this is the simulation. Dear Fellow Scholars, that is a clean simulation right there. And, fourth, a thin film within a square subjected to a gravitational pull that is changing over time. And the result is more swirly patterns. So how quickly can we perform all this? Disregard the FPS, this is the inverse of the time step size, and is mainly information for fellow researchers. For now, gaze upon the time per frame column, and, my goodness. This is blazing fast too! It takes less than a second per frame for the catenoid experiment, this is one of the cheaper ones. And all this on a laptop! Wow! Now, the most expensive experiment in this paper was the Rayleigh-Taylor instability, this took about 13 seconds per frame. This is not bad at all, we can get a proper simulation of this quality within an hour or so. However, note that the authors used a big honking machine to compute this scene. And remember, this paper is not about optimization, but it is about making the impossible possible. And it is doing all that, swiftly. Huge congratulations to the authors! What  a  time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=eQRZ7FUkwKo",
        "paper_link": "https://arxiv.org/abs/2105.07656",
        "paper_title": "Thin-Film Smoothed Particle Hydrodynamics Fluid"
    },
    {
        "video_id": "iZA9bl-t6J4",
        "video_title": "Virtual Bones Make Everything Better! \ud83d\udcaa",
        "position_in_playlist": 549,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU \n\n\ud83d\udcdd The paper \"Direct Delta Mush Skinning Compression with Continuous Examples\" is available here:\nhttps://binh.graphics/papers/2021s-DDMC/\nhttps://media.contentapi.ea.com/content/dam/ea/seed/presentations/ddm-compression-with-continuous-examples.pdf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see that virtual bones make everything better. This new paper is about setting up bones and joints for our virtual characters to be able to compute deformations. Deformations are at the heart of computer animation, look, all of these sequences require a carefully designed technique that can move these joints around and simulate its effect on the entirety of the body. Things move around, stretch, and bulge. But, there is a problem. What\u2019s the problem? Well, even with state of the art deformation techniques, sometimes, this happens. Did you catch it? There is the problem, look! The hip region unfortunately bulges inwards. Is this specific to this technique? No-no, not in the slightest - pretty much all of the previous techniques showcase that to some effect. This is perhaps our most intense case of this inward bulging. So, let\u2019s have a taste of the new method. How does it deal with a case like this? Perfectly, that\u2019s how. Loving it. Now, hold on to your papers, because it works by creating something that the authors refer to as virtual bones. Let\u2019s look under the hood and locate them. There they are! These red dots showcase these virtual bones. We can set them up as a parameter and the algorithm distributes them automatically. Here, we have a 100 of them, but we can go to 200, or, if we so desire, we can request even 500 of them. So, what difference does this make? With a 100 virtual bones. Let\u2019s see\u2026yes. Here you see that the cooler colors like blue showcase the regions that are deforming accurately, and the warmer colors, for instance, red showcases the problematic regions where the technique did not perform well. The red part means that these deformations can be off by about 2 centimeters, or about three quarters of an inch. I would say that is great news, because even even with only a 100 virtual bones, we get an acceptable animation. However, the technique is still somewhat inaccurate around the knee and the hips. However, if you are one of our really precise Fellow Scholars, and feel that even that tiny mismatch is too much, we can raise the number of virtual bones to 500, and, let\u2019s see\u2026there we go! Still some imperfections around the knees, but, the rest is accurate to a small fraction of an inch. Excellent. The hips and knees seem to be a common theme, look, they show up in this example too. And, as in the previous case, even the 100 virtual bone animation is acceptable, and most of the problems can be remedied by adding 500 of them. Still some issues around the elbows. So far, we have looked at the new solution, and marked the good and bad regions with heatmaps. So now, how about looking at the reference footage and the new technique side by side. Why? Because we\u2019ll find out whether it is just good at fooling the human eye, or does it really match up? Let\u2019s have a look together. This is linear blend skinning, a state of the art method. For now, we can accept this as a reference. Note that setting this up is expensive both in terms of computation, and it also requires a skilled artist to place these helper joints correctly. This looks great. So how does the new method with the virtual bones look under the hood? These correspond to those, so, why do all this? Because the new method can be computed much, much cheaper. So let\u2019s see what the results look like! Mmmm! Yeah! Very close to the reference results. Absolutely amazing. Now, let\u2019s run a torture test that would make any computer graphics researcher blush. Oh my. There are so many characters here animated at the same time. So how long do we have to wait for these accurate simulations? Minutes to hours? Let me know your guess in the comments below. I\u2019ll wait. Thank you! Now, hold on to your papers because all this takes about 5 milliseconds per frame. 5 milliseconds! This seems well over a hundred characters rocking out, and the new technique doesn\u2019t even break a sweat. So, I hope that with this, computer animations are going to become a lot more realistic in the near future. What a time to be alive! Also, make sure to have a look at the paper in the video description. I loved the beautiful mathematics, and the clarity in there. It clearly states the contributions in a bulleted list, which is a more and more common occurrence, that\u2019s good, but look! It even provides an image of these contributions right there, making it even clearer to the reader. Generally, details like this show that the authors went out of their way and spent a great deal of extra time writing a crystal-clear paper. It takes much, much more time than many may imagine, so, I would like to send big thank you to the authors for that. Way  to go! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=iZA9bl-t6J4",
        "paper_link": "https://binh.graphics/papers/2021s-DDMC/",
        "paper_title": "Direct Delta Mush Skinning Compression with Continuous Examples"
    },
    {
        "video_id": "uuzow7TEQ1s",
        "video_title": "DeepMind\u2019s AI Plays Catch\u2026And So Much More! \ud83e\udd16",
        "position_in_playlist": 550,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Open-Ended Learning Leads to Generally Capable Agents\" is available here:\nhttps://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\nhttps://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#deepmind",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see how an AI can win a complex game that it has never seen before. Zero prior training on that game. Yes, really! Now, before that, for context, have a look at this related work from 2019, where scientists at OpenAI built a super fun hide and seek game for their AI agents to play. And, boy, did they do some crazy stuff. Now, these agents learn from previous experiences, and to the surprise of no one, for the first few million rounds, we start out with\u2026pandemonium. Everyone just running around aimlessly. Then, over time, the hiders learned to lock out the seekers by blocking the doors off with these boxes and started winning consistently. I think the coolest part about this is that the map was deliberately designed by the OpenAI scientists in a way that the hiders can only succeed through collaboration. But then, something happened. Did you notice this pointy, doorstop-shaped object? Are you thinking what I am thinking? Well, probably, and not only that, but later, the AI also discovered that it can be pushed near a wall and be used as a ramp, and, tadaa! Got\u2019em! Then, it was up to the hiders again to invent something new. So, did they do that? Can this crazy strategy be defeated? Well, check this out. These resourceful little critters learned that since there is a little time at the start of the game when the seekers are frozen, apparently, during this time, they cannot see them, so why not just sneak out and steal the ramp, and lock it away from them. Absolutely incredible. Look at those happy eyes as they are carrying that ramp. But today is not 2019, it is 2021, so I wonder what scientists at the other amazing AI lab, DeepMind have been up to. Can this paper be topped? Well, believe it or not, they have managed to create something that is perhaps even crazier than this. This new paper proposes that these AI agents look at the screen, just like a human would, and engage in open-ended learning where the tasks are always changing. What does this mean? Well, it means that these agents are not preparing for an exam. They are preparing for life! And hence, hopefully they learn more general concepts, and, as a result, maybe excel at a variety of different tasks. Even better, these scientists at DeepMind claim that their AI agents not only excel at a variety of tasks, but they excel at new ones they have never seen before! Those are big words, so, let\u2019s see the results! The red agent here is the hider, and the blue is the seeker. They both understand their roles, the red agent is running, and the blue is seeking. Look, its viewing direction is shown with this lightsaber-looking line pointing at the red agent. No wonder it is running away! And, look, it manages to get some distance from the seeker, and finds a new, previously unexplored part of the map and hides there. Excellent. And you would think that the Star Wars references end here? No! Not even close. Look, in a more advanced variant of the game, this green seeker lost the two other hiders, and what does he do. Ah yes, of course, grabs his lightsaber, and takes the high ground. Then, it spots the red agent and starts chasing it. All all this without ever having played this game before. That is excellent. In this cooperative game, the agents are asked to get as close to the purple pyramid as they can. Of course, to achieve that, they need to build a ramp. Which they successfully realize. Excellent. But it gets better! Now note that we did not say that the task is to build a ramp. The task is to get as close to the purple pyramid as we can. Does that mean that? \u2026Yes, yes it does. Great job bending the rules, little AI! In this game, the agent is asked to stop the purple ball from touching the red floor. At first, it tries its best to block the rolling of the ball with its body, then, look! It realizes that it is much better to just push it against the wall. And it gets even better, look, it learned that best is to just chuck the ball behind this slab. It is completely right, this needs no further energy expenditure, and the ball never touches the red floor again. Great! And finally, in this King of the Hill game, the goal is to take the white floor and get the other agent out of there. As they are playing this game for the first time, they have no idea where the white floor is. As soon as the blue agent finds it, it stays there\u2026so far so good. But, this is not a cooperative game, we have an opponent here. Look! Boom! A quite potent opponent indeed who can take the blue agent out, and, it understands that it has to camp in there and defend the region. Again. Awesome! So, the goal here is not to be an expert in one game, but to be a journeyman in many games. And these agents are working really well at a variety of games without ever having played them. So, in summary, OpenAI\u2019s agent - expert in a narrower domain. DeepMind\u2019s agent - journeyman in a broader domain. Two different kinds of intelligence. Both doing amazing things. Loving it. What a time to be alive! Scientists at DeepMind have knocked it out of the park with this one. They have also published AlphaFold this year, a huge breakthrough that makes an AI predict protein structures. Now, I saw some of you asking why we didn\u2019t cover it. Is it not an important work? Well, quite the opposite! I am spellbound by it and I think that paper is a great gift to humanity, however. I try my best to to educate myself on this topic, however, I don\u2019t feel that I am qualified to speak about it. Not yet anyway. So, I think it is best to let the experts who know more about this take the stage! This is, of course, bad for views, but no matter, we are not maximizing views here. We are maximizing meaning. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=uuzow7TEQ1s",
        "paper_link": "https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play",
        "paper_title": "Open-Ended Learning Leads to Generally Capable Agents"
    },
    {
        "video_id": "lCBSGOwV-_o",
        "video_title": "This Magical AI Cuts People Out Of Your Videos! \u2702\ufe0f",
        "position_in_playlist": 551,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned report is available here: https://wandb.ai/_scott/omnimatte/reports/Omnimatte-Associating-Objects-and-Their-Effects--Vmlldzo5MDQxNTc\n\n\ud83d\udcdd The paper \"Omnimatte: Associating Objects and Their Effects in Video\" is available here:\nhttps://omnimatte.github.io/\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\nOr join us here: https://www.youtube.com/user/keeroyz/join\n\nThumbnail background image credit: https://pixabay.com/images/id-4762800/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see magical things that\u00a0 open up when we are able to automatically find\u00a0\u00a0 the foreground and the background of a video. Let\u2019s see why that matters! This new technique leans on a previous method\u00a0 to find the boy, and the dog. Let\u2019s call this\u00a0\u00a0 level 1 segmentation. So far so good, but\u00a0 this is not the state of the art. Yet.\u00a0 Now, comes level 2 - it also found the shadow\u00a0 of the boy, and, the shadow of the dog.\u00a0\u00a0 Now we\u2019re talking! But it doesn\u2019t stop there!\u00a0\u00a0 It gets even better. Level three, this\u00a0 is where things get out of hand - look,\u00a0\u00a0 the dog is occluding the boy\u2019s shadow,\u00a0 and it is able to deal with that too. So, if we can identify all of the effects that\u00a0 are attached to the boy and the dog, what can\u00a0\u00a0 we do with all this information? Well, for\u00a0 instance, we can even remove them from the video.\u00a0\u00a0 Nothing to see here. Now, a common problem\u00a0 is that still, the silhouette of the subject\u00a0\u00a0 still remains in the final footage,\u00a0 so let\u2019s take a close look together!\u00a0\u00a0 I don\u2019t see anything at all. Wow. Do\u00a0 you? Let me know in the comments below! Just to showcase how good this removal is, here\u00a0 is a good technique from just one year ago.\u00a0\u00a0 Do you see it? This requires the\u00a0 shadows to be found manually,\u00a0\u00a0 so we have to work with that. And still, in the\u00a0 outputs, you can see the silhouette we mentioned.\u00a0\u00a0 And, how much better is the new method?\u00a0 Well, it finds the shadows automatically.\u00a0\u00a0 That is already mind blowing, and the outputs\u00a0 are\u2026yes, much cleaner. Not perfect, there is still\u00a0\u00a0 some silhouette action, but if I were not actively\u00a0 looking for it, I might not have noticed it. It can also remove people\u00a0 from this trampoline scene,\u00a0\u00a0 and not only the bodies, but it also removes\u00a0 their effect on the trampolines as well. Wow. And as this method can perform\u00a0 all this reliably, it opens up\u00a0\u00a0 the possibility for new, magical effects. For instance, we can duplicate this test subject,\u00a0\u00a0 and even fade it in and out. Note that it\u00a0 has found its shadows as well. Excellent! So, it can deal with finding not only\u00a0 the shape of the boy and the dog,\u00a0\u00a0 and it knows that it\u2019s not enough to just find\u00a0 their silhouettes, but it also has to find\u00a0\u00a0 additional effects they have on the footage.\u00a0 For instance, their shadows. That is wonderful,\u00a0\u00a0 and what is even more wonderful is that this\u00a0 was only one of the simpler things it could do. Shadows are not the only\u00a0 potential correlated effects,\u00a0\u00a0 look. A previous method was able to find the\u00a0 swan here, but that\u2019s not enough to remove it,\u00a0\u00a0 because it has additional effects on the scene.\u00a0 What are those? Well, look, it has reflections,\u00a0\u00a0 and it creates ripples too. This is so much\u00a0 more difficult than just finding shadows.\u00a0\u00a0 And now, let\u2019s see the new method..and! Whoa.\u00a0 It knows about the reflections and ripples,\u00a0\u00a0 finds both of them, and gives us this\u00a0 beautifully clean result. Nothing to see here. Also, look at this elephant. Removing\u00a0 just the silhouette of the elephant\u00a0\u00a0 is not enough, it also has to\u00a0 find all the dust around it,\u00a0\u00a0 and it gets worse, the dust is changing\u00a0 rapidly over time. And believe it or not\u2026wow,\u00a0\u00a0 it can find the dust too, and remove the\u00a0 elephant. Again, nothing to see here. And if you think that this dust was the new\u00a0 algorithm at its best, then have a look at this\u00a0\u00a0 drifting car. Previous method. Yes, that is the\u00a0 car, but you know what I want. I want the smoke\u00a0\u00a0 gone too. That\u2019s probably impossible, right?\u00a0 Well, let\u2019s have a look. Wow. I can\u2019t believe\u00a0\u00a0 it. It grabbed and removed the car and the smoke\u00a0 together\u2026and, once again, nothing to see here. So, what are those more magical things\u00a0 that this opens up? Watch carefully\u2026it\u00a0\u00a0 can make the colors pop here. And, remember, it\u00a0 can find the reflections of the flamingo, so, it\u00a0\u00a0 keeps not only the flamingo, but the reflection of\u00a0 the flamingo in color as well. Absolutely amazing. And, if we can find the background of a video,\u00a0 we can even change the background. This works\u00a0\u00a0 even in the presence of a moving\u00a0 camera, which is a challenging problem.\u00a0\u00a0 Now, of course, not even this technique is\u00a0 perfect - look here. The reflections are copied\u00a0\u00a0 off of the previous scene,\u00a0 and it shows on the new one. So what do you think? What would\u00a0 you use this technique for?\u00a0\u00a0 Let me know in the comments, or if you wish\u00a0 to discuss similar topics with other Fellow\u00a0\u00a0 Scholars in a warm and welcoming environment,\u00a0 make sure to join our Discord channel. Also,\u00a0\u00a0 I would like to send a big thank you to the mods\u00a0 and everyone who helps running this community.\u00a0\u00a0 The link to the server is available in\u00a0 the video description. You\u2019re invited. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=lCBSGOwV-_o",
        "paper_link": "https://omnimatte.github.io/",
        "paper_title": "Omnimatte: Associating Objects and Their Effects in Video"
    },
    {
        "video_id": "SsJ_AusntiU",
        "video_title": "This AI Learned Boxing\u2026With Serious Knockout Power! \ud83e\udd4a",
        "position_in_playlist": 552,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Control Strategies for Physically Simulated Characters Performing Two-player Competitive Sports\" is available here:\nhttps://research.fb.com/publications/control-strategies-for-physically-simulated-characters-performing-two-player-competitive-sports/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see an AI learn boxing and even mimic gorillas during this process. Now, in an earlier work, we saw a few examples of AI agents playing two-player sports, for instance, this is the \u201cYou Shall Not Pass\u201d game, where the red agent is trying to hold back the blue character and not let it cross the line. Here you see two regular AIs duking it out, sometimes the red wins, sometimes the blue is able to get through. Nothing too crazy here. Until\u2026this happens. Look. What is happening? It seems that this agent started to do nothing\u2026and still won. Not only that, but it suddenly started winning almost all the games. How is this even possible? Well, what the agent did is perhaps the AI equivalent of hypnotizing the opponent, if you will. The more rigorous term for this is that it induces off-distribution activations in its opponent. This adversarial agent is really doing nothing, but that\u2019s not enough - it is doing nothing in a way that reprograms its opponent to make mistakes and behave close to a completely randomly acting agent! Now, this new paper showcases AI agents that can learn boxing. The AI is asked to control these joint-actuated characters which are embedded in a physics simulation. Well, that is quite a challenge - look, for quite a while after 130 million steps of training, it cannot even hold it together. And, yes\u2026these folks collapse. But this is not the good kind of hypnotic adversarial collapsing. I am afraid, this is just passing out without any particular benefits. That was quite a bit of training, and all this for nearly nothing. Right? Well, maybe\u2026let\u2019s see what they did after 200 million training steps. Look! They can not only hold it together, but they have a little footwork going on, and can circle each other and try to take the middle of the ring. Improvements. Good. But this is not dancing practice, this is boxing. I would really like to see some boxing today and it doesn\u2019t seem to happen. Until we wait for a little longer\u2026which is 250 million training steps. Now, is this boxing? Not quite, this is more like two drunkards trying to duke it out, where neither of them knows how to throw a real punch\u2026but! Their gloves are starting to touch the opponent, and they start getting rewards for it. What does that mean for an intelligent agent? Well, it means that over time, it will learn to do that a little better. And hold on to your papers and see what they do after 420 million steps. Oh wow! Look at that! I am seeing some punches, and not only that, but I also see some body and head movement to evade the punches, very cool. And if we keep going for longer, whoa! These guys can fight! They now learned to perform feints, jabs, and have some proper knockout power too. And if you have been holding on to your papers, now, squeeze that paper, because all they looked at before starting the training was 90 seconds of motion capture data. This is a general framework that also works for fencing as well. Look! The agents learned to lunge, deflect, evade attacks, and more. Absolutely amazing. What a time to be alive! So, this was approximately a billion training steps, right. So how long did that take to compute? It took approximately a week. And, you know what\u2019s coming. Of course, we invoke the First Law Of Papers, which says that research is a process. Do not look at where we are, look at where we will be two more papers down the line. And two more papers down the line, I bet this will be possible in a matter of hours. This is the part with the gorillas. It is also interesting that even though there were plenty of reasons to, the researchers didn\u2019t quit after a 130 million steps. They just kept on going, and eventually, succeeded. Especially in the presence of not so trivial training curves where the blocking of the other player can worsen the performance, and it\u2019s often not as easy to tell where we are. That is a great life lesson right there. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=SsJ_AusntiU",
        "paper_link": "https://research.fb.com/publications/control-strategies-for-physically-simulated-characters-performing-two-player-competitive-sports/",
        "paper_title": "Control Strategies for Physically Simulated Characters Performing Two-player Competitive Sports"
    },
    {
        "video_id": "UrB-tqA8oeg",
        "video_title": "This AI Helps Making A Music Video! \ud83d\udc83",
        "position_in_playlist": 553,
        "description": "\u2764\ufe0f Train a neural network and track your experiments with Weights & Biases here: http://wandb.me/paperintro\n\n\ud83d\udcdd The paper Editable Free-Viewpoint Video using a Layered Neural Representation\"\" is available here:\nhttps://jiakai-zhang.github.io/st-nerf/\nhttps://github.com/DarlingHang/st-nerf\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-882940/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#aimusicvideo",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to make some crazy synthetic music videos. In machine learning research, view synthesis papers are on the rise these days. These techniques are also referred to as NERF variants, which is a learning-based algorithm tries that to reproduce real-world scenes from only a few views. It is very challenging, look! In go a bunch of photos of a scene, and the method has to be able to synthesize new photorealistic images between these photos. But this is not the only paper in this area, researchers are very aware of the potential here, and thus, a great number of NERF variants are appearing every month. For instance, here is a recent one that extends the original technique to handle shiny and reflective objects better. So, what else is there to do here? Well, look here. This new one demands not a bunch of photos from just one camera, but from 16 different cameras. That\u2019s a big ask. But, in return, the method now has tons of information about the geometry and the movement of these test subjects, so, is it intelligent enough to make something useful out of it? Now, believe it or not, this, in return, can not only help us look around in the scene, but even edit it in three new ways. For instance, one, we can change the scale of these subjects, add and remove them from the scene, and even copy-paste them. Excellent for creating music videos. Well, talking about music videos. Do you know what is even more excellent for those? Retiming movements\u2026that is also possible. This can, for instance, improve an okay dancing performance into an excellent one. And three, because now we are in charge of the final footage, if the original footage is shaky, well, we can choose to eliminate that camera shake. Game changer. Still, it\u2019s not quite the hardware requirement where you just whip out your smartphone and start nerfing and editing, but for what it can do, it really does not ask for a lot. Look, if we wish to, we can even remove some of those cameras and still expect reasonable results. We lose roughly a decibel of signal per camera. Here is what that looks like. Not too shabby! And all this progress just one more paper down the line. And I like the idea behind this paper a great deal because typically what we are looking for in a followup paper is trying to achieve similar results while asking for less data from the user. This paper goes into the exact other direction, and asks what amazing things could be done if we had more data instead. Loving it. And with that, not only neural view synthesis, but neural scene editing is also possible. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=UrB-tqA8oeg",
        "paper_link": "https://jiakai-zhang.github.io/st-nerf/",
        "paper_title": ""
    },
    {
        "video_id": "_9Bli4zCzZY",
        "video_title": "This AI Creates Virtual Fingers! \ud83e\udd1d",
        "position_in_playlist": 554,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation\" is available here:\nhttps://github.com/cghezhang/ManipNet\n\n\u2764\ufe0f Watch these videos in early access on our Patreon page or join us here on YouTube: \n- https://www.patreon.com/TwoMinutePapers\n- https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg/join\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-5859606/\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#vr",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see how we can use our hands, but not our fingers to mingle with objects in virtual worlds. The promise of virtual reality, VR is indeed truly incredible. If one day it comes to fruition, doctors could be trained to perform surgery in a virtual environment, expose astronauts to virtual zero-gravity simulations, work together with telepresence applications, you name it. The dream is getting closer and closer, but something is still missing! For instance, this previous work uses a learning-based algorithm to teach a head-mounted camera to tell the orientation of our hands at all times. One more paper down the line, this technique appeared that can deal with examples with challenging hand-hand interactions, deformations, lots of self-contact and self-occlusion. This was absolutely amazing, because these are not gloves. No-no. This is the reconstruction of the hand by the algorithm. Absolutely amazing. However, it is slow, and mingling with other objects is still, quite limited. So, what is missing? What is left to be done here? So, let\u2019s have a look at today\u2019s paper and find out together. This is its output\u2026yes, mingling that looks very natural. But, what is so interesting here? The interesting part is that it has realistic finger movements. Well, that means, that it just reads the data from sensors on the fingers, right? Now, hold on to your papers, and we\u2019ll find out once we look at the input\u2026oh my! Is this really true? No sensors on the fingers anywhere! What kind of black magic is this? And with that, we can now make the most important observation in the paper: it reads information from only the wrist and the objects in the hand. Look, the sensors are on these gloves, but none are on the fingers. Once again: the sensors have no idea what we are doing with our fingers, it only reads the movement of our wrist and the object, and all the finger movement is synthesized by it automatically. Whoa! And, with this, we can not only have a virtual version of our hand, but we can also manipulate virtual objects with very few sensor readings. The rest is up to the AI to synthesize. This means that we can have a drink with a friend online, use a virtual hammer to, depending on our mood, fix or destroy virtual objects. This is very challenging because the finger movements have to follow the geometry of the object. Look, here, the same hand is holding different objects, and the AI knows how to synthesize the appropriate finger movements for both of them. This is especially apparent when we change the scale of the object. You see, the small one requires small and precise finger movements to turn around, these are motions that need to be completely re-synthesized for the bigger objects. So cool. And now comes the key - so, does this only work on objects that it has been trained on? No, not at all! For instance, the method has not seen this kind of teapot before, and still, it knows how to use its handle, and now to hold it from the bottom too, even if both of these parts look different. Be careful though, who knows, maybe virtual teapots can get hot too! What\u2019s more, it also handles the independent movement of the left and right hands. Now, how fast is all this? Can we have coffee together in virtual reality? Yes, absolutely! All this runs in close to real time! There is a tiny bit of delay though, but, a result like this is already amazing, and this is typically the kind of thing that can be fixed one more paper down the line. However, not even this technique is perfect. It still might miss small features on an object. For instance, a very thin handle might confuse it. Or, if it has an inaccurate reading of the hand pose and distances, this might happen. But for now, having a virtual coffee together\u2026yes please, sign me up! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=_9Bli4zCzZY",
        "paper_link": "https://github.com/cghezhang/ManipNet",
        "paper_title": "ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation"
    },
    {
        "video_id": "6hkiTejoyms",
        "video_title": "Watch Tesla\u2019s Self-Driving Car Learn In a Simulation! \ud83d\ude98",
        "position_in_playlist": 555,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#Tesla #TeslaAIDay",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see how Tesla uses no less than a simulated game world to train their self-driving cars. And more. In their AI day presentation video, they really put up a clinic of recent AI research results and how they apply them to develop self-driving cars. And of course, there is plenty of coverage of the event, but, as always, we are going to look at it from different angle. We\u2019re doing it Papers style. Why? Because after nearly every Two Minute Papers episode where we showcase an amazing paper, I get a question saying something like \u201cokay, but when do I get to see or use this in the real world?\u201d. And rightfully so, that is a good question. And in this presentation, you will see that these papers that you see here get transferred into real-world products so fast, it really makes my head spin. Let\u2019s see this effect demonstrated by looking through their system. Now, first, their cars have many cameras, no depth information, just the pixels from these cameras, and one of their goals is to create this vector space view that you see here. That is almost like a map, or a video game version of the real roads and objects around us. That is a very difficult problem. Why is that? Because the car has many cameras. Is that a problem? Yes\u2026 kind of. I\u2019ll explain in a moment. You see, there is a bottom layer that processes the raw sensor data from the cameras mounted on the vehicle. So here, in go the raw pixels, and out comes more useful, high-level information that can be used to determine whether this clump is pixels is a car or a traffic light. Then, in the upper layers, this data can be used for more specific tasks, for instance, trying to estimate where the lanes and curbs are. So, what papers are used to accomplish this? Looking through the architecture diagrams, we see, transformer neural networks, BiFPNs, and Regnet. All papers from the last few years. For instance, RegNet is a neural network variant that is great at extracting spatio-temporal information from the raw sensor data. And that is a paper from 2020. From just one year ago. Already actively used in training self-driving cars. That is unreal. Now, we mentioned that having many cameras is a bit of a problem. Why is that? Isn\u2019t that supposed to be a good thing? Well, look! Each of the cameras only sees parts of the truck. So how do we know where exactly it is, and how long it is? We need to know all of this information to be able to accurately put the truck into the vector space view. What we need for this is a technique that can fuse information from many cameras together intelligently. Note that this is devilishly difficult due to each of the cameras having a different calibration, location, view directions, and other properties. So who is to tell that a point here corresponds to which point in a different camera view? And this is accomplished through, yes\u2026a transformer neural network. A paper from 2017. So, does this multi-camera technique work? Does this improve anything? Well, let\u2019s see! Oh yes, the yellow predictions here are from the previous single-camera network, and as you see, unfortunately, things flicker in and out of existence. Why is that? It is because a passing car is leaving the view of one of the cameras, and as it enters the view of the next one, they don\u2019t have this correspondence technique that would say where it is exactly. And, look! The blue objects show the prediction of the multi-camera network that can do that, and things aren\u2019t perfect, but they are significantly better the single-camera network. That is great, however, we are still not taking into consideration time. Why is that important? Let\u2019s have a look at two examples. One, if we are only looking at still images and not take into consideration how they change over time, how do we know if this car is stationary? Is it about to park somewhere? Or, is it speeding? Also, two, this car is now occluded but we saw it second ago, so we should know what it is up to. That sounds great. And what else can we do if our self-driving system has a concept of time? Much like humans do, we can make predictions. These predictions can take place both in terms of mapping what is likely to come, an intersection, a roundabout, and so on. But, perhaps even more importantly, we can also make predictions about vehicle behavior. Let\u2019s see how that works. The green lines show how far away the next vehicle is, and how fast it is going. This green line tells us the real, true information about it. Do you see the green? No? That\u2019s right, it is barely visible, because it is occluded by a blue line, which is the prediction of the new video network. That means that its predictions are barely off from the real velocities and distances, which is absolutely amazing. And, as you see with orange, the old network that was based on single images is off by quite a bit. So now, a single car can make a rough map of its environment wherever it drives, and they can also stitch the readings of multiple cars together into an even more accurate map. Putting this all together, these cars have a proper understanding of their environment and this makes navigation much easier. Look at those crisp, temporally stable labelings. It has very little flickering. Still, not perfect by any means, but this is remarkable progress in so little time. And we are at the point where predicting the behaviors of other vehicles and pedestrians can also lead to better decision making. But, we are still not done yet. Not even close. Look! The sad truth of driving is that unexpected things happen. For instance, this truck makes it very difficult for us to see, and the self-driving system does not have a lot of training data to deal with that. So, what is a possible solution to that? There are two solutions. One is fetching more training data. One car can submit an unexpected event and request that the entire Tesla fleet sends over if they have encountered something similar. Since there are so many of these cars on the streets, tens of thousands of similar examples can be fetched from them, and added to the training data to improve the entire fleet. That is mind blowing. One car encounters a difficult situation, and then, every car can learn from it. How cool is that? That sounds great. So what is the second solution? Not fetching more training data, but creating more training data. What, just make stuff up? Yes, that\u2019s exactly right. And if you think that is ridiculous, and are asking how could that possibly work? Well, hold on to your papers, because it does work\u2026 you are looking at it right now! Yes, this is a photorealistic simulation that teaches self-driving cars to handle difficult corner cases better. In the real world, we can learn from things that already happened, but in a simulation, we can make anything happen. This concept really works, and is one of my favorite examples is OpenAI\u2019s robot hand that we have showcased earlier in this series. This also learns the rotation techniques in a simulation, and it does it so well, that the software can be uploaded to a real robot hand, and it will work in real situations too. And now, the same concept for self-driving cars. Loving it. With these simulations, we can even teach these cars about cases that would otherwise be impossible or unsafe to test. For instance, in this system, the car can safely learn what it should do if it sees people and dogs running on the highway. A capable artist can also create miles and miles of these virtual locations within a day of work. This simulation technique is truly a treasure trove of data, because it can also be procedurally generated, and the moment the self-driving system makes an incorrect decision, a Tesla employee can immediately create an endless set of similar situations to teach it. Now, I don\u2019t know if you remember, we talked about a fantastic paper a couple months ago that looked at real-world videos, then, took video footage from a game, and improved it to look like the real world. Convert video games to reality if you will. This had an interesting limitation. For instance since the AI was trained on the beautiful lush hills of Germany and Austria, it hasn\u2019t really seen the dry hills of LA. So, what does it do with them? Look, it redrew the hills the only way it saw hills exist, which is, covered with trees. So, what does this have to do with Tesla\u2019s self-driving cars? Well, if you have been holding on to your papers so far, now, squeeze that paper, because they went the other way around! Yes, that\u2019s right! They take the video footage of a real, unexpected event where the self-driving system failed, use their automatic labeler used for the vector space view, and what do they make out of it? A video game version! Holy mother of papers. And, in this video game, it is suddenly much easier to teach the algorithm safely. You can also make it easier, harder, replace a car with a dog, or a pack of dogs, and make many similar examples so that the AI can learn from these \u201cwhat if\u201d situations as much as possible. So, there you go. Full tech transfer into a real AI system in just a year or two. So, yes, the papers you see here are for real. As real as it gets. And yes, the robot is not real, just a silly joke. For now. And two more things that make all this even more mind-blowing. One, remember, they don\u2019t showcase the latest and greatest that they have. Just imagine that everything that you heard today is old news compared to the tech they have now. And two, we have only looked at just one side of what is going on, for instance, we haven\u2019t even talked about their amazing Dojo chip. And if all this comes to fruition, we will be able to travel cheaper, more relaxed, and also, perhaps most importantly, safer. I can\u2019t wait. I really cannot wait. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=6hkiTejoyms"
    },
    {
        "video_id": "81rBzfbFLiE",
        "video_title": "OpenAI Codex: Your Robot Assistant! \ud83e\udd16",
        "position_in_playlist": 556,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Evaluating Large Language Models Trained on Code\" is available here:\nhttps://openai.com/blog/openai-codex/\n\nCodex tweet/application links:\nExplaining code: https://twitter.com/CristiVlad25/status/1432017112885833734\nPong game: https://twitter.com/slava__bobrov/status/1425904829013102602\nBlender Scripting: https://www.youtube.com/watch?v=MvHbrVfEuyk\n\nGPT-3 tweet/application links:\nWebsite layout: https://twitter.com/sharifshameem/status/1283322990625607681\nPlots: https://twitter.com/aquariusacquah/status/1285415144017797126?s=12\nTypesetting math: https://twitter.com/sh_reya/status/1284746918959239168\nPopulation data: https://twitter.com/pavtalk/status/1285410751092416513\nLegalese: https://twitter.com/f_j_j_/status/1283848393832333313\nNutrition labels: https://twitter.com/lawderpaul/status/1284972517749338112\nUser interface design: https://twitter.com/jsngr/status/1284511080715362304\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\n\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#openai #codex",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see if an AI can become a good software engineer. Spoiler alert, the answer is: yes, kind of. Let me explain. Just one year ago, scientists at OpenAI published a technique by the name GPT-3, and it is an AI that was unleashed to read the internet with the sole task of finishing your sentences. So, what happened then? Well, now we know that of course, it learned whatever it needed to learn to perform the sentence completion properly. And to do this, it would need to learn English by itself, and that\u2019s exactly what it did! It also learned about a lot of topics to be able to discuss them well. For instance, we gave it a try, and I was somewhat surprised when I saw that it was able to continue a Two Minute Papers script, even though it seems to have turned into a history lesson. It also learned how to generate properly formatted plots from a tiny prompt written in plain English. Not just one kind - many kinds! And remember, this happened just about a year ago, and this AI was pretty good at many things. But soon after, a newer work was published by the name Image-GPT. What did this do? Well, this was a GPT-variant that could not finish your sentences, but your images. Yes, really. The problem statement is simple: we give it an incomplete image, and we ask the AI to fill in the missing pixels. Have a look at this water droplet example. We humans, know that since we see the remnants of some ripples over there too, there must be a splash, but does the AI know? Oh yes, yes it does! Amazing! And this is the true image for reference. So, what did they come out with now? Well, the previous GPT-3 was pretty good at many things, and this new work, OpenAI Codex is a GPT language model that was fine-tuned to be excellent at one thing. And that is, writing computer programs, or, finishing your code. Sounds good! Let\u2019s give it a try. First, please write a program that says hello world five times. It can do that. And, we can also ask it to create a graphical user interface for it. No coding skills required. That\u2019s not bad by any means, but this is OpenAI we are talking about, so I am sure it can do even better. Let\u2019s try something a tiny bit more challenging. For instance, writing a simple space game. First, we get an image of a spaceship that we like, instruct the algorithm to resize and crop it. And here comes one of my favorites: start animating it. Look, it immediately wrote the appropriate code where it will travel with a prescribed speed, and yes, it should get flipped as soon as it hits the wall. Looks good. Will it work? Let\u2019s see. It does. And all this from a written English description. Outstanding. Of course, this is still not quite the physics simulation that you all see and love around here, but I\u2019ll take it. But this is still not a game, so please, add a moving asteroid, check for collisions, and infuse the game with a scoring system. There we go. So, how long did all this take? And now, hold on to your papers, because this game was written in approximately 9 minutes. No coding knowledge is required. Wow. What a time to be alive! Now, in this 9-ish minutes, most of the time was not spent by the AI thinking, but the human typing. So, still, the human is the bottleneck. But, today, with all the amazing voice recognition systems that we have, we don\u2019t even need to type these instructions. Just say what you want and it will be able to do it! So, what else can it do? For instance, it can also deal with similar requests to what software engineers are asked in interviews, and I have to say, the results indicate that this AI would get hired to some places. But that\u2019s not all, it can also nail a first-grade math test. An AI. Food for thought. Now, this OpenAI Codex work has been out there for a few days now, and I decided to not cover it immediately, but wait a little and see where the users take it. This is, of course, not great for views, but no matter, we are not maximizing views, we are maximizing meaning. In return, now, there are some examples out there in the wild. Let\u2019s look at three of them. One, it can be asked to explain a piece of code, even if it is written in assembly. Two, it can create a pong game in 30 seconds. Remember, this used to be a blockbuster Atari game, and now, an AI can write it in half a minute. And yes, again, most of the half minute is taken by waiting for the human for instructions. Wow. It can also create a plugin for Blender, an amazing free 3D modeler program. These things used to take several hours of work at the very least. And with that, I feel that what I said for GPT-3 rings even more true today. I am replacing GPT-3 with Codex, and quoting: \u201cThe main point is that working with Codex is a really peculiar process where we know that a vast body of knowledge lies within, but it only emerges if we can bring it out with properly written prompts. It almost feels like a new kind of programming that is open to everyone, even people without any programming or technical knowledge. If a computer is a bicycle for the mind, then Codex is a fighter jet.\u201d And all this progress, in just one year. I cannot wait to see where you Fellow Scholars will take it, and what OpenAI has in mind for just one more paper down the line. And until then, software coding might soon be a thing anyone can do. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=81rBzfbFLiE",
        "paper_link": "https://openai.com/blog/openai-codex/",
        "paper_title": "Evaluating Large Language Models Trained on Code"
    },
    {
        "video_id": "HnkVoOdTiSo",
        "video_title": "Can An AI Design A Good Game Level? \ud83e\udd16",
        "position_in_playlist": 557,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-With-W-B-CAM-and-GradCAM--Vmlldzo5MTIyNw\n\n\ud83d\udcdd The paper \"Adversarial Reinforcement Learning for Procedural Content Generation\" is available here:\nhttps://www.ea.com/seed/news/cog2021-adversarial-rl-content-generation\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Testing modern computer games by using an AI is getting more and more popular these days. This earlier work showcased how we can use an automated agent test the integrity of the game by finding spots where we can get stuck. And when we fixed the problem, we could easily ask the agent to check whether the fix really worked. In this case, it did! And this new work also uses learning algorithms to test our levels. Now this chap has been trained on a fixed level, mastered it, and let\u2019s see if it has managed to obtain general knowledge from it. How? Well, by testing how it performs on a different level. It is very confident, good\u2026but..uh-oh! As you see, it is confidently incorrect. So, is it possible to train an agent to be able to beat these levels more reliably? Well, how about creating a more elaborate curriculum for them to learn on. Yes, let\u2019s do that\u2026but, with a twist! In this work, the authors chose not to feed the AI a fixed set of levels\u2026no-no! They created another AI that builds the levels for the player AI. So, both the builder and the player are learning algorithms, who are tasked to succeed together in getting the agent to the finish line. They have to collaborate to succeed. Building the level means choosing the appropriate distance, height, angle and size for these blocks. Let\u2019s see them playing together on an easy level. Okay, so far so good, but let\u2019s not let them build a little cartel where only easy levels are being generated so they get a higher score. I want to see a challenge! To do that, let\u2019s force the builder AI to use a larger average distance between the blocks, thereby creating levels of a prescribed difficulty. And with that, let\u2019s ramp up the difficulty a little. Things get a little more interesting here, because\u2026 whoa! Do you see what I see here? Look! It even found a shortcut to the end of the level. And, let\u2019s see the harder levels together. While many of these chaps failed, some of them are still able to succeed. Very cool! Let\u2019s compare the performance of the new technique with the previous, fixed track agent. This is the chap that learned by mastering only a fixed track. And this one learned in the wilderness. Neither of them have seen these levels before. So, who is going to be scrappier? Of course, the wilderness guy described in the new technique. Excellent. So, all this sounds great, but I hear you asking the key question here: what do we use these for? Well, one, the player AI can test the levels that we are building for our game and give us feedback on whether it is possible to finish, is it too hard or too easy, and more. This can be a godsend when updating some levels, because the agent will almost immediately tell us whether it has gotten easier or harder, or if we have broken the level. No human testing is required. Now, hold on to your papers, because the thing runs so quickly that we can even refine a level in real time. Loving it. Or two, the builder can also be given to a human player who might enjoy a level being built in real time in front of them. And here comes the best part. The whole concept generalizes well for other kinds of games too. Look, the builder can build race tracks, and the player can try to drive through them. So, do these great results also generalize to the racing game? Let\u2019s see what the numbers say. The agent that trained on a fixed track can succeed on an easy level about 75% of the time, while the newly proposed agent can do it nearly with a 100% chance. A bit of an improvement, okay. Now, look at this. The fixed track agent can only beat a hard level about 2 times out of 10, while the new agent can do it about six times out of ten. That is quite a bit of an improvement. Now, note that in a research paper, choosing a proper baseline to compare to is always a crucial question. I would like to note that the baseline here is not the state of the art, and with that, it is a little easier to make the new solution pop. No matter, the solutions are still good, but I think this is worth a note. So, from now on, whenever we create a new level in a computer game, we can have hundreds of competent AI players testing it in real time. So good! What a time to  be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=HnkVoOdTiSo",
        "paper_link": "https://www.ea.com/seed/news/cog2021-adversarial-rl-content-generation",
        "paper_title": "Adversarial Reinforcement Learning for Procedural Content Generation"
    },
    {
        "video_id": "rawsSOLNYE0",
        "video_title": "This AI Makes Digital Copies of Humans! \ud83d\udc64",
        "position_in_playlist": 558,
        "description": "\u2764\ufe0f Check out the Gradient Dissent podcast by Weights & Biases: http://wandb.me/gd\u00a0\n\n\ud83d\udcdd The paper \"The Relightables: Volumetric Performance Capture of Humans with Realistic Relighting\" is available here:\nhttps://augmentedperception.github.io/therelightables/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#vr",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to look at a\u00a0 paper with two twists. You know what,\u00a0\u00a0 I\u2019ll give you twist number one right away. This\u00a0 human isn\u2019t here. This human isn\u2019t here either.\u00a0\u00a0 And, neither is this human here.\u00a0\u00a0 Now, you are probably asking K\u00e1roly,\u00a0 what are you even talking about? Now hold on to your papers,\u00a0 because I am talking about this.\u00a0\u00a0 Look. This is the geometry of this virtual\u00a0 human inside a virtual world. Whoa. Yes,\u00a0\u00a0 all of these people are in a synthetic video, and\u00a0 in a virtual environment that can be changed with\u00a0\u00a0 a simple click. And more importantly, as we change\u00a0 the lighting or the environment, it also simulates\u00a0\u00a0 the effect of that environment on the character,\u00a0 making it look like they are really there. So that sounds good, but how do we take\u00a0 a human and make a digital copy of them? Well, first, we place them in a capture\u00a0 system that contains hundreds of led lights\u00a0\u00a0 and an elaborate sensor for capturing\u00a0 depth information. Why do we need these?\u00a0\u00a0 Well, all this this gives the system plenty\u00a0 of data on how the skin, hair and the clothes\u00a0\u00a0 reflect light. And, at this point, we know\u00a0 everything we need to know and can now proceed\u00a0\u00a0 and place our virtual copy in a computer\u00a0 game or even a telepresence meeting. Now, this is already amazing, but two things\u00a0 really stick out here. One you will see\u00a0\u00a0 when you look at this previous competing work.\u00a0 This had a really smooth output geometry,\u00a0\u00a0 which means that only few high-frequency\u00a0 details were retained. This other work was\u00a0\u00a0 better at retaining the details, but, look, tons\u00a0 of artifacts appear when the model is moving. And, what does the new one look like?\u00a0 Is it any better? Let\u2019s have a look.\u00a0\u00a0 Oh my! We get tons of fine details, and\u00a0 the movements have improved significantly.\u00a0\u00a0 Not perfect by any means, look here,\u00a0 but still, an amazing leap forward. Two, the other remarkable thing here\u00a0 is that the results are so realistic\u00a0\u00a0 that objects the in virtual scene can cast a\u00a0 shadow on our model. What a time to be alive! And now, yes, you remember that I promised\u00a0 two twists. Where is twist number two? Well,\u00a0\u00a0 it has been here all along for the entirety of\u00a0 this video. Have you noticed? Look, all this\u00a0\u00a0 is from 2019. From two years ago. Two years is\u00a0 a long time in machine learning and computer\u00a0\u00a0 graphics research, and I cannot wait to see how it\u00a0 will be improved two more papers down the line. If\u00a0\u00a0 you are excited too, make sure to subscribe and\u00a0 hit the bell icon to not miss it when it appears. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=rawsSOLNYE0",
        "paper_link": "https://augmentedperception.github.io/therelightables/",
        "paper_title": "The Relightables: Volumetric Performance Capture of Humans with Realistic Relighting"
    },
    {
        "video_id": "CfJ074h9K8s",
        "video_title": "The Tale Of The Unscrewable Bolt! \ud83d\udd29",
        "position_in_playlist": 559,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/mathisfederico/wandb_features/reports/Visualizing-Confusion-Matrices-With-W-B--VmlldzoxMzE5ODk\n\n\ud83d\udcdd The paper \"Intersection-free Rigid Body Dynamics\" is available here:\nhttps://ipc-sim.github.io/rigid-ipc/\n\nScene credits:\n- Bolt - YSoft be3D\n- Expanding Lock Box - Angus Deveson \n- Bike Chain and Sprocket - Okan (bike chain), Hampus Andersson (sprocket)\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image source: https://pixabay.com/images/id-1924173/\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. I have to say we haven\u2019t had a simulation paper in\u00a0 a while so today\u2019s episode is going to be my way\u00a0\u00a0 of medicating myself. You are more\u00a0 than welcome to watch the process. And this paper is about performing collision\u00a0 detection. You see, when we write a simple\u00a0\u00a0 space game, detecting whether a collision has\u00a0 happened or not is a mostly trivial endeavor.\u00a0\u00a0 However, now, instead, let\u2019s look\u00a0 at the kind of simulation complexity\u00a0\u00a0 that you are expecting from a Two Minute Papers\u00a0 video\u2026first, let\u2019s try to screw this bolt in\u00a0\u00a0 using an industry standard simulation\u00a0 system, and it is\u2026stuck. Hm\u2026why? Because here, we would need to simulate in\u00a0 detail, not only whether two things collide,\u00a0\u00a0 they collide all the time, but, we need to check\u00a0 for and simulate friction too! Let\u2019s see what this\u00a0\u00a0 new simulation method does with the same scene.\u00a0 And\u2026oh yes! This one isn\u2019t screwing with us and\u00a0\u00a0 does the job perfectly. Excellent. However, this\u00a0 was not nearly the most complex thing it can do.\u00a0\u00a0 Let\u2019s try some crazy geometry, with\u00a0 crazy movements and tons of friction. There we go. This one will do. Welcome\u00a0 to the expanding lock box experiment. So,\u00a0\u00a0 what is this? Look, as we turn the key,\u00a0 the locking pins retract, and the bottom\u00a0\u00a0 is now allowed to fall. This scene contains\u00a0 tens to hundreds of thousands of contacts,\u00a0\u00a0 and yet, it still works perfectly. Beautiful.\u00a0 I love this one because with this simulation,\u00a0\u00a0 we can test intricate mechanisms for robotics and\u00a0 more before committing to manufacturing anything.\u00a0\u00a0 And, unlike with previous methods, we don\u2019t need\u00a0 to worry whether the simulation is correct or not,\u00a0\u00a0 and we can be sure that if we 3D print this,\u00a0 it will behave exactly this way. So good! Also, here come some of my favorite\u00a0 experiments from the paper.\u00a0\u00a0 For instance, it can also simulate a\u00a0 piston attached to a rotating disk, or,\u00a0\u00a0 smooth motion on one wheel, leading to\u00a0 intermittent motion on the other one.\u00a0\u00a0 And, if you feel the urge to build a\u00a0 virtual bike, do not worry for a second,\u00a0\u00a0 because your chain and sprocket mechanisms\u00a0 will work exactly as you expect to. Loving it. Now, interestingly, look here. The time step size\u00a0 used with the new technique is a hundred times\u00a0\u00a0 bigger, which is great, we can advance the time\u00a0 in bigger pieces when computing the simulation.\u00a0\u00a0 That is good news indeed. However, every time we\u00a0 do so, we still have to compute a great deal more.\u00a0\u00a0 The resulting computation time is still at least\u00a0 a hundred times slower than previous methods.\u00a0\u00a0 However, those methods don\u2019t count, at least\u00a0 not on these scenes, because they have produced\u00a0\u00a0 incorrect results. Look at it some other way -\u00a0 this is the fastest simulator that actually works. Still, it is not that slow. The one\u00a0 with the intermittent motion takes\u00a0\u00a0 less than a second per time step, which\u00a0 likely means a few seconds per frame,\u00a0\u00a0 while the bolt screwing scene is likely in the\u00a0 minutes per frame domain. Very impressive! And,\u00a0\u00a0 if you are a seasoned Fellow Scholar, you\u00a0 know what\u2019s coming, this is where we invoke\u00a0\u00a0 the First Law Of Papers, which says that research\u00a0 is a process. Do not look at where we are,\u00a0\u00a0 look at where we will be two more papers down the\u00a0 line. And two more papers down the line, I am sure\u00a0\u00a0 even the more complex simulations will be done\u00a0 in a matter of seconds. What a time to be alive! Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=CfJ074h9K8s",
        "paper_link": "https://ipc-sim.github.io/rigid-ipc/",
        "paper_title": "Intersection-free Rigid Body Dynamics"
    },
    {
        "video_id": "ogL-2IClOug",
        "video_title": "NVIDIA\u2019s New Technique: Beautiful Models For Less! \ud83c\udf32",
        "position_in_playlist": 560,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"Appearance-Driven Automatic 3D Model Simplification\" is available here:\nhttps://research.nvidia.com/publication/2021-04_Appearance-Driven-Automatic-3D\n\n\ud83d\udcdd The differentiable material synthesis paper is available here:\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-learning-and-synthesis/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1225988/\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#nvidia #gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute\u00a0 Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see how crazy good NVIDIA\u2019s\u00a0 new system is at simplifying virtual objects. These objects are used to create photorealistic\u00a0 footage for feature-length movies, virtual worlds,\u00a0\u00a0 and more. But, here comes the problem.\u00a0 Sometimes these geometries are so detailed\u00a0\u00a0 they are prohibitively expensive to store and\u00a0 render efficiently. Here are some examples from\u00a0\u00a0 one of our papers that were quite challenging to\u00a0 iterate on and render. This took several minutes\u00a0\u00a0 to render and always ate all the memory in my\u00a0 computer. So, what can we do if we would still\u00a0\u00a0 like to get crisp, high-quality geometry, but\u00a0 cheaper and quicker? I\u2019ll show you in a moment. This is part of a super complex scene. Get this,\u00a0 it is so complex that it takes nearly a hundred\u00a0\u00a0 gigabytes of storage space to render just one\u00a0 image of this and is typically for benchmarking\u00a0\u00a0 rendering algorithms. This is the Nurburgring\u00a0 of light transport algorithms if you will. Well, hold on to your papers because\u00a0 I said that I\u2019ll show you in a moment\u00a0\u00a0 what we can do to get all this\u00a0 at a more affordable cost, but,\u00a0\u00a0 in fact, you are looking at the\u00a0 results of the new method right now. Yes, parts of this image\u00a0 are the original geometry,\u00a0\u00a0 and other parts have already been simplified. So, which is which? Do you see the difference?\u00a0 Please stop the video and let me know in the\u00a0\u00a0 comments below. I\u2019ll wait. Thank you. So, let\u2019s see together\u2026yes,\u00a0\u00a0 this is the original geometry that requires over\u00a0 5 billion triangles. And, this is the simplified\u00a0\u00a0 one, which\u2026what? Can this really be? This uses\u00a0 less than 1 percent of the number of triangles\u00a0\u00a0 compared to this. In fact, it\u2019s less\u00a0 than half a percent. That is insanity.\u00a0\u00a0 This really means that about every 200\u00a0 triangles are replaced with just one triangle,\u00a0\u00a0 and it still looks mostly the same. That\u00a0 sounds flat out impossible to me. Wow. So, how does this witchcraft\u00a0 even work? Well, now, you see,\u00a0\u00a0 this is the power of differentiable rendering.\u00a0 The problem formulation is as follows. We tell the\u00a0\u00a0 algorithm that here are the results that you need\u00a0 to get, find the geometry and material properties\u00a0\u00a0 that will result in this. It runs all this by\u00a0 means of optimization, which means that it will\u00a0\u00a0 have a really crude initial guess that doesn\u2019t\u00a0 even seem to resemble the target geometry.\u00a0\u00a0 But then, over time, it starts refining it,\u00a0 and it gets closer and closer to the reference.\u00a0\u00a0 This process is truly a sight to behold.\u00a0 Look how beautifully it is approximating\u00a0\u00a0 the target geometry. This looks very close,\u00a0 and is much cheaper to store and render. I loved this example too. Previously, this differentiable rendering concept\u00a0 has been used to be able to take a photograph,\u00a0\u00a0 and find a photorealistic material model\u00a0 that we can put into our simulation program\u00a0\u00a0 that matches it. This work did very well with\u00a0 materials, but, it did not capture the geometry. This other work did something similar\u00a0 to this new paper, which means\u00a0\u00a0 that it jointly found the geometry and\u00a0 material properties. But, as you see\u00a0\u00a0 high-frequency details were not as good as with\u00a0 this one. You see here - these details are gone.\u00a0\u00a0 And, now, just two years and one paper later, we\u00a0 can get a piece of geometry that is so detailed\u00a0\u00a0 that it needs billions of triangles, and\u00a0 it can be simplified two hundred to one. Now if even that is not enough, admittedly, it is\u00a0 still a little rudimentary, but it even works for\u00a0\u00a0 animated characters. I wonder where we will\u00a0 be two more papers down the line from here?\u00a0\u00a0 Wow. Scientists at NVIDIA knocked\u00a0 it out of the park with this one.\u00a0\u00a0 Huge congratulations to the\u00a0 team! What a time to be alive! So there you go, this was quite a ride, and I hope\u00a0 you enjoyed it at least half as much as I did. And\u00a0\u00a0 if you enjoyed it at least as much as I did,\u00a0 and are thinking that this light transport thing\u00a0\u00a0 is pretty cool, and you would like to learn more\u00a0 about it, I held a Master-level course on this\u00a0\u00a0 topic at the Technical University of Vienna. Since\u00a0 I was always teaching it to a handful of motivated\u00a0\u00a0 students, I thought that the teachings shouldn\u2019t\u00a0 only be available for the privileged few who can\u00a0\u00a0 afford a college education, no-no. The teachings\u00a0 should be available for everyone. Free education\u00a0\u00a0 for everyone, that\u2019s what I want. So, the\u00a0 course is available free of charge for everyone,\u00a0\u00a0 no strings attached, so make sure to click the\u00a0 link in the video description to get started.\u00a0\u00a0 We write a full light simulation\u00a0 program from scratch there,\u00a0\u00a0 and learn about physics, the\u00a0 world around us, and more. Thanks for watching and for your generous\u00a0 support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ogL-2IClOug",
        "paper_link": "https://research.nvidia.com/publication/2021-04_Appearance-Driven-Automatic-3D",
        "paper_title": "Appearance-Driven Automatic 3D Model Simplification"
    },
    {
        "video_id": "ZDItmrqfxwI",
        "video_title": "This AI Stuntman Just Keeps Getting Better! \ud83c\udfc3",
        "position_in_playlist": 561,
        "description": "\u2764\ufe0f Train a neural network and track your experiments with Weights & Biases here: http://wandb.me/paperintro\n\n\ud83d\udcdd The paper \"Learning a family of motor skills from a single motion clip\" is available here:\nhttp://mrl.snu.ac.kr/research/ProjectParameterizedMotion/ParameterizedMotion.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see how an AI can learn crazy stunts\u2026from just one video clip. And if even that\u2019s not enough, it can even do more. This agent is embedded in a physics simulation, and first, it looks at a piece of reference motion, like this one. And then, after looking, it can reproduce it. That is already pretty cool, but it doesn\u2019t stop there. I think you know what\u2019s coming\u2026yes! Not only learning, but improving the original motion. Look, it can refine this motion a bit\u2026and then, a bit more\u2026and then, a bit more. And this just keeps on going, until\u2026wait a second. Hold on to your papers\u2026because this looks impossible! Are you trying to tell me that it\u2019s improved the move so much, that it can jump through this? Yes, yes it does. Here is the first reproduction of the jump motion, and the improved version side by side. Whoa. The difference speaks for itself. Absolutely amazing. We can also give it this reference clip to teach it to jump from one box to another. This isn\u2019t quite difficult. And now comes one of my favorites from the paper! And that is testing how much it can improve upon this technique. Let\u2019s give it a try! It also learned how to perform a shorter jump, a longer jump\u2026and now, oh yes, the final boss. Wow, it could even pull off this super long jump. It seems that this super bot can do absolutely anything! Well\u2026almost. And, it can not only learn these amazing moves, but it can also weave them together so well, that we can build a cool little playground, and it gets through it with ease\u2026 well, most of it anyway. So at this point, I was wondering how general the knowledge is that it learns from these example clips? A good sign of an intelligent actor is that things can change a little and it can adapt to that. Now, it clearly can deal with a changing environment, that is fantastic, but do you know what else it can deal with? And now, if you have been holding on to your papers, squeeze that paper, because it can also deal with changing body proportions. Yes, really. We can put it in a different body, and it will still work. This chap is cursed with this crazy configuration, and can still pull off a cartwheel. If you haven\u2019t been exercising lately, what\u2019s your excuse now? We can also ask it to perform the same task with more or less energy, or to even apply just a tiny bit of force for a punch, or to go full Mike Tyson on the opponent. So how is all this wizardry possible? Well, one of the key contributions of this work is that the authors devised a method to search this space of motions efficiently. Since it does it in a continuous reinforcement learning environment, this is super challenging. At the risk of simplifying the solution, their method solves this by running both an exploration phase to find new ways of pulling off a move, and, with blue you see that when it found something that seems to work, it also keeps refining it. Similar endeavors are also referred to as the exploration-exploitation problem, and the authors proposed a really cool new way of handling it. Now, there are plenty more contributions in the paper, so make sure to have a look at it in the video description. Especially given that this is a fantastic paper, and a presentation is second to none. I am sure that the authors could have worked half as much on this project and this paper would still have been accepted, but they still decided to put in that extra mile. And I am honored to be able to celebrate their amazing work together with you Fellow Scholars. And, for now, an AI agent can look at a single clip of a motion, and can not only perform it, but it can make it better, pull it off in different environments, and it can even be put in a different body and still do it well. What  a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=ZDItmrqfxwI",
        "paper_link": "http://mrl.snu.ac.kr/research/ProjectParameterizedMotion/ParameterizedMotion.html",
        "paper_title": "Learning a family of motor skills from a single motion clip"
    },
    {
        "video_id": "t33jvL7ftd4",
        "video_title": "This AI Learned Some Crazy Fighting Moves! \ud83e\udd4a",
        "position_in_playlist": 562,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Neural Animation Layering for Synthesizing Martial Arts Movements\" is available here:\nhttps://github.com/sebastianstarke/AI4Animation/blob/master/Media/SIGGRAPH_2021/Paper.pdf\nhttps://github.com/sebastianstarke/AI4Animation\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#gamedev",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to see if a virtual AI character can learn, or perhaps even invent these amazing signature moves. And this is a paper that was written by Sebastian Starke and his colleagues. He is a recurring scientist on this series, for instance, earlier he wrote this magnificent paper about Dribbling AI characters. Look. The key challenge here was that we were given only 3 hours of unstructured motion capture data. That is next to nothing, and from this next to nothing, it not only learned these motions really well, but it could weave them together, even when a specific movement combination was not present in this training data. But, as these motions are created by human animators, and may show at least three problems. One, the training data may contain poses that don\u2019t quite adhere to the physics of a real human character. Two, it is possible that the upper body does something that makes sense, the lower body also does something that makes sense, but the whole thing, put together, does not make too much sense anymore. Or, three, we may have these foot sliding artifacts that you see here. These are more common than you might first think, here is an example of it from a previous work, and, look, nearly all of the previous methods struggle with it. Now, this new work uses 20 hours of unstructured training data. Now, remember, the previous one only used 3, so we rightfully expect that by using more information, it can also learn more. But the previous work was already amazing, so, what more can we really expect this new one to do? Well, it can not only learn these motions, weave together these motions like previous works, but, hold on to your papers, because it can now also come up with novel moves as well! Wow. This includes new attacking sequences, and combining already existing attacks with novel footwork patterns. And it does all this spectacularly well. For instance, if we show it how to have its guard up, and how to throw a punch, what will it learn? Get this, it will keep its guard up while throwing that punch. And it not only does that in a realistic, fluid movement pattern, but it also found out about something that has strategic value. Same with evading an attack with some head movement and counterattacking. Loving it. But how easy is it to use this? Do we need to be an AI scientist to be able to invoke these amazing motions? Well, if you have been holding on to your papers so far, now, squeeze that paper, and look here. Wow. You don\u2019t need to be an AI scientist to play with this! Not at all! All you need is a controller to invoke these beautiful motions. And all this runs in real time. My goodness! For instance, you can crouch down and evade a potential attack by controlling the right stick, and launch a punch in the meantime. Remember, not only both halves have to make sense separately, the body motion has to make sense as a whole. And it really does, look at that. And here comes the best part, you can even assemble your own signature attacks, for instance, perform that surprise spinning backfist, an amazing spin kick, and, yes! You can even go full Karate Kid with that crane kick. And as a cherry on top, the characters can also react to the strikes, clinch, or even try a takedown. So, with that, there we go, we are again, one step closer to having access to super realistic motion techniques for virtual characters, and all we need for this is a controller. And remember, all this already runs in real time! Another amazing SIGGRAPH paper from Sebastian Starke. And get this, he is currently a 4th year PhD student and already made profound contributions to the industry. Huge congratulations on this amazing achievement. What a time  to be alive! Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=t33jvL7ftd4",
        "paper_link": "https://github.com/sebastianstarke/AI4Animation/blob/master/Media/SIGGRAPH_2021/Paper.pdf",
        "paper_title": "Neural Animation Layering for Synthesizing Martial Arts Movements"
    },
    {
        "video_id": "IXqj4HqNbPE",
        "video_title": "3D Modeling This Toaster Just Became Easier!",
        "position_in_playlist": 563,
        "description": "\u2764\ufe0f Check out Fully Connected by Weights & Biases: https://wandb.me/papers \n\n\ud83d\udcdd The paper \"DAG Amendment for Inverse Control of Parametric Shapes\" is available here:\nhttps://perso.telecom-paristech.fr/boubek/papers/DAG_Amendment/\n\nCheck out Yannic Kilcher's channel here:\nhttps://www.youtube.com/c/YannicKilcher/videos\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Dr. K\u00e1roly Zsolnai-Feh\u00e9r. Today we are going to build the best virtual toaster that you have ever seen. And it\u2019s going to be so easy that it hardly seems possible. All this technique asks for is our input geometry to be a collection of parametric shapes. I\u2019ll tell you in a moment what that is, but for now, let\u2019s see that toaster. Hmm, this looks fine, but what if we feel that it is not quite tall enough? Well, with this technique, look! It can change the height of it, which is fantastic, but something else also happens! Look, it also understands the body\u2019s relation to other objects that are connected to it. We can also change the location of the handle, the slits can be adjusted symmetrically, and when we move the toasts, it understands that it moves together with the handles. This is super useful, for instance, have a look at this train example, where if we change the wheels, it also understands that not only the wheels, but the wheel wells also have to change as well. This concept also works really well on this curtain. And all this means that we can not only dream up and execute these changes ourselves without having to ask a trained artist, but we can also do it super quickly and efficiently. Loving it. Just to demonstrate how profound and non-trivial this understanding of interrelations is, here is an example of a complex object. Without this technique, if we grab one thing, exactly that one thing moves, which is represented by one of these sliders changing here. However, if we would grab this contraption in the real world, not only one thing would move, nearly every part would move at the same time. So, does this new method know that? Oh wow, it does, look at that! And, at the same time, not one, but many sliders are dancing around beautifully. Now, I mentioned that the requirement was that the input object has to be a parametric shape, this is something that can be generated from intuitive parameters, for instance, we can generate a circle if we say what the radius of the circle should be. The radius would be the parameter here, and the resulting circle is hence a parametric shape. In many domains, this is standard procedure, for instance, many computer aided design systems work with parametric objects. But we are not done yet, not even close! It also understands how the brush sizes that we use relates to our thinking. Don\u2019t believe it? Let\u2019s have a look together! Right after we click, it detects that we have a small brush size, and therefore infers that we probably wish to do something with the handle, and, there we go! That is really cool. And now, let\u2019s increase the brush size and click nearby, and, bam! There we go! Now it knows that we wish to interact with the drawer. Same with the doors. And, hold on to your papers, because to demonstrate the utility of their technique, the authors also made a scene just for us. Look! Nice - this is no less than a Two Minute Papers branded chronometer! And here, we can change the proportions, the dial, the hands, whatever we wish, and it is so easy and showcases the utility of the new method so well. Now, I know what you\u2019re thinking, let\u2019s see it ticking\u2026and\u2026will it be two minutes? Well, close enough. Certainly, much closer to two minutes than the length of these videos, that is for sure. Thank you so much for the authors for taking time off their busy day just to make this. So, this truly is a wonderful tool because even a novice artist without 3D modeling expertise can apply meaningful changes to complex pieces of geometry. No trained artist is required! What a time to be alive! Now, this didn\u2019t quite fit anywhere in this video, but I really wanted to show you this heartwarming message from Mark Chen, research scientist at OpenAI. This really showcases one of the best parts of my job, and that is when the authors of the paper come in and enjoy the results with you Fellow Scholars. Loving it. Thank you so much again! Also make sure to check out Yannic\u2019s channel for cool in-depth videos on machine learning works. The link is available  in  the video description. Thanks for watching and for your generous support, and I'll see you next time!",
        "transcription_mode": "YouTube Transcript API",
        "source_link": "https://www.youtube.com/watch?v=IXqj4HqNbPE",
        "paper_link": "https://perso.telecom-paristech.fr/boubek/papers/DAG_Amendment/",
        "paper_title": "DAG Amendment for Inverse Control of Parametric Shapes"
    },
    {
        "video_id": "ia-VBSF4KXA",
        "video_title": "This New Method Can Simulate a Vast Ocean! \ud83c\udf0a",
        "position_in_playlist": 564,
        "description": "\u2764\ufe0f Check out Perceptilabs and sign up for a free demo here: https://www.perceptilabs.com/papers\n\n\ud83d\udcdd The paper \"Ships, Splashes, and Waves on a Vast Ocean\" is available here:\nhttp://computationalsciences.org/publications/huang-2021-vast-ocean.html\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "the fellow scholars this is too many papers with Dr Connors on anything yet today we are going to put a fluid simulation in another fluid simulation and with that create beautiful videos like this one  but how and more importantly why  well let's take to flee simulation techniques one will be the fluid implicit particle slip in short and the other will be the boundary element method B. E. M. cell why do we need to or perhaps even more methods why not just one well first if we wish to simulate turbulence and high frequency splashes near moving objects Philip is the answer it is great at exactly that however it is not great at simulating big volumes of water no matter because there are other methods to handle that for instance the boundary element method that we just mentioned B. M. in short it is great in these cases because the B. M. variant that's been used in this paper simulates only the surface of the liquid and for a large ocean the surface is much much smaller than the volume  let's have a look at an example here is a pure sleep simulation this should be great for small splashes yes that is indeed to black look the waves then disappear quickly  let's look at what B. M. does with the scene  yes the details are lacking but the waves are lovely  now we have a good feel of the limitations of these techniques small splashes flip  oceans BM  what here is the problem what if we have a scene where we have both which one should we use these new techniques as well use both  what is this insanity  now hold on to your papers and look  this is the result of fusing the two simulation techniques together now if you look carefully yes you get this right within the box there is a flip simulation and outside of the boxes there is the B. M. simulation  and the two are fused together in a way that the new method it really takes the best of both worlds just look at all that detail what a beautiful simulation my goodness  now this is not nearly as easy as slapping together to simulation domains  look there's plenty of work to be done in the transition zone and also how accurate is this here is the reference simulation for the water droplets scene from earlier the simulation will take forever for a big scene and he's here for us to know how it should look  and let's see how close the new method is to it  what %HESITATION  now we're talking  now worry not about the seams they are there for us to see the internal workings of the algorithm the final composition will look like this  however not even this technique is perfect as a potential limitation look here is the new method compared to the reference footage  the waves in the wake of the ship are simulated really well and so are the ways further away at the same time and that's amazing however what we don't get is look crisp details in the B. M. regions  those are gone  but just compare the results to this technique from a few years ago and get a feel of how far we have come just a couple of papers down the line  the pace of progress in computer graphics research is through the roof loving it what a time to be alive and let's see yes the first author is label hog  again if you are a seasoned fellow scholar you may remember our video on his first paper on ferro fluids and this is his third one this man writes nothing but masterpieces as a result this paper has been accepted to the C. graph Asia conference and being the first author there is perhaps the computer graphics equivalent of winning the Olympic gold medal it is also beautifully written so make sure to check it out in the video description huge congratulations on this amazing work I cannot believe that we are still progressing so quickly year after year  perceptor labs is a visual API for tensorflow carefully designed to make machine learning as intuitive as possible this gives you a faster way to build out models with more transparency into higher model is architected how it performs and how to debug it look it let you toggle between the visual modeler and the code editor even generates visualisations for older model variables and gives you recommendations both doing modeling and training and all this automatically I only wish I had a tool like this when I was working on my neural networks during my PhD years visit perceptive labs dot com slash beepers to easily install the free local version of their system today I thank the perceptive labs for their support and for helping us make better videos for you thanks for watching out for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=ia-VBSF4KXA",
        "paper_link": "http://computationalsciences.org/publications/huang-2021-vast-ocean.html",
        "paper_title": "Ships, Splashes, and Waves on a Vast Ocean"
    },
    {
        "video_id": "9L5NqNDZHjk",
        "video_title": "Finally, Beautiful Virtual Scenes\u2026For Less! \u2600\ufe0f",
        "position_in_playlist": 565,
        "description": "\u2764\ufe0f Check out Weights & Biases and sign up for a free demo here: https://wandb.com/papers \n\u2764\ufe0f Their mentioned post is available here: https://wandb.ai/latentspace/published-work/The-Science-of-Debugging-with-W-B-Reports--Vmlldzo4OTI3Ng\n\n\ud83d\udcdd The paper \"DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\" is available here:\nhttps://depthoraclenerf.github.io/\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Rajarshi Nigam, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-1477041/\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "dear fellow scholars this is too many papers with doctor cattle's on anything here today we are going to see the incredible piece of progress in research works on making our photos come to life how do we do that well of course through view synthesis  to be more exact we do that through this amazing technique that is referred to as a nurse variant which means that it is a learning based algorithm that tries to reproduce real art scenes from only a few vehicles even go a few photos of the scene and it has to be able to synthesize new photorealistic images in between these photos this is your synthesis in short as you see here it can be done quite well with the previous method so our question number one is way better publishing a new research paper on this  and question number two there are plenty of fuel synthesis papers sloshing around so why choose this paper  well this new technique is called the owner nerf via death oracles one of the key contributions here is that it is better at predicting how far things are from my camera and it also takes less time to evaluate than its predecessors  it still makes the instructors a little murky look at the tree here but otherwise the rest seems close to the reference  so is there value in this let's look at the results and see for ourselves we will compare to the original nerf technique first  and the results are  compatible what is going on is this not supposed to be better do these deaths oracle's help at all why is this just compatible to nerf  now hold on to your papers because here comes the key the output of the two techniques may be compatible not the input isn't what does that mean it means that the new technique was given fifty times less information what %HESITATION  fifty times less that's barely anything and when I read the paper this was the point where I immediately went from a little disappointed to stand fifty times less information and it can still create compatible videos  yes the answer is yes the death oracle's really work and it does not end there there's more it was also compared against local lightfield fusion which is from two years ago and the results are much cleaner  and it is also compared against what  this is the euro basis expansion technique next in short  we just showcased it approximately two months ago and not only an excellent follow up paper it appeared in the same year just a few months apart  but it already compares to the previous work and it out performs it handily  my goodness the pace of progress in machine learning research never disappoints and here comes the best part if you have been holding onto your paper so far now squeeze that paper because the new technique not only requires fifty times less information no no it is also nearly fifty times cheaper and faster to train the new neural network and to create the new images so let's pop the question is it time  yes %HESITATION this runs in real time  look if we wish to walk around in a photorealistic virtual scene normally we would have to write a light simulation program and compute every single image separately what each image will take several minutes to finish now we just need to shoot a few rays and neural network we try to understand the data and give us the rest instantly what a time to be alive and interestingly look the first author of this paper is Thomas Nash who wrote this variant on the earth  nomen est omen I guess congratulations and if you wish to discuss this paper make sure to drop by on our discord server the link is available in the video description this episode has been supported by weights and biases in this post they show you how to use that tool to check and visualize what your neural network is learning and even more importantly a case study on how to find bugs in your system and fix them during my PhD studies I trained a ton of neural networks which were used in our experiments however over time there was just too much data in our repositories and what I'm looking for is not data dot inside and that's exactly how weights and biases helps you by organizing your experiments it is used by more than two hundred companies and research institutions including OpenAI did I research get top and more and get this weights and biases is free for all individuals economics and open source project make sure to visit them through W. and B. E. dot com slash papers or just click the link in the video description and you can get a free demo today I thank the weights and biases for the long standing support and for helping us make better videos for you thanks for watching out for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=9L5NqNDZHjk",
        "paper_link": "https://depthoraclenerf.github.io/",
        "paper_title": "DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks"
    },
    {
        "video_id": "BS2la3C-TYc",
        "video_title": "This Image Is Fine. Completely Fine. \ud83e\udd16",
        "position_in_playlist": 566,
        "description": "\u2764\ufe0f Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n\ud83d\udcdd The paper \"The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning\" is available here:\nhttps://attentionneuron.github.io/\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAleksandr Mashrabov, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Benji Rabhan, Bryan Learn, Christian Ahlin, Eric Haddad, Eric Martel, Gordon Child, Ivo Galic, Jace O'Brien, Javier Bustamante, John Le, Jonas, Kenneth Davis, Klaus Busse, Lorin Atzberger, Lukas Biewald, Matthew Allen Fisher, Mark Oates, Michael Albrecht, Nikhil Velpanur, Owen Campbell-Moore, Owen Skarpness, Rajarshi Nigam, Ramsey Elbasheer, Steef, Taras Bobrovytsky, Thomas Krcmar, Timothy Sum Hon Mun, Torsten Reil, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nThumbnail background design: Fel\u00edcia Zsolnai-Feh\u00e9r - http://felicia.hu\n\nMeet and discuss your ideas with other Fellow Scholars on the Two Minute Papers Discord: https://discordapp.com/invite/hbcTJu2\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nInstagram: https://www.instagram.com/twominutepapers/\nTwitter: https://twitter.com/twominutepapers\nWeb: https://cg.tuwien.ac.at/~zsolnai/",
        "transcript": "the fellow scholars this is too many papers say Dr Connors on a thing here today we are going to find out whether a machine can really think like we humans think  the answer is yes and no let me try to explain obviously there are many differences between how we think but first let's try to argue for the similarities first this neural network looks at an image and tries to decide whether it depicts a dog or not what this does is that it slices of the image into small pieces and keeps a score on one it had seen in the snippets floppy ears  blacks not out for her okay we're good we can conclude that we have a dog over here we humans would also have a hard time identifying the dog without these landmarks plus one for thinking the same way  second this is DeepMind's deepening force meant learning algorithm it looks at the screen much like a human would and tries to learn what the controls do and what the game is about as the game is running and much like a human first it has no idea what is going on and loses all of its life almost immediately but over time it gets a bit of the feel of the game  improvements good night if we wait for longer it sharpens its skill set so much debt look it found out that the best way to beat the game is to dig a tunnel through the blocks and just kick back and enjoy the show  humanlike excellent again plus one for thinking the same way now let's look at this new term mutation invariant neural network and see what the name means what it can do and how it relates to our thinking experiment number one  since the permutation means shuffling things around and we will add shuffling into this carpool balancing experiment here the learning algorithm does not look at the pixels of the game but instead takes a look at numbers for instance angles velocity and position and as you see with does it learn to balance the pulled super quickly the permission part means that we shuffle disinformation every now and then that will surely confuse it so let's try that  there we go and  nice didn't even lose it shuffle again  it lost it due to a sudden change in the incoming data that look it recovered he rapidly and can it keep it upright  yes it can  so is this the plus one for a minus one is this human thinking or robot thinking well over time humans can get used to input the information switching around to  but not this quickly so this one is debatable however I guarantee that the next experiment will not be debatable at all now experiment number two re shuffling on steroids we already learned that some amount of reshuffling is okay so now let's have our little AI play pong but with a twist because this time the reshuffling is getting real  yes we now broke up the screen into small little blocks and have a shuffled it to the point that it is impossible to read but you know what let's make it even worse instead of just three shuffling we will reshuffle the reshuffling what does that mean we can rearrange these tires every few seconds the true nightmare situation for even an established act with them and especially when we are learning the game  okay this is nonsense right there is no way anyone can meaningfully play the game from this noise right and now hold on to your papers because the learning algorithm steel works fine just fine  not only on pong but on the racing game to  wall who big minus one for human thinking  but if it works fine you know exactly what needs to be done yes let's make it even harder experiment number three stolen blocks yes let's keep reshuffling change during shuffling over time and also steel seventy percent of the data and  wow it is still fine it only sees thirty percent of the game all jumbled up and it still plays just fine I cannot believe what I am seeing here another minus one this does not seem to think like a human would  so all that is absolutely amazing but what is it looking at  see the wide blocks it is looking at the sides of the road likely to know what the curvature is and how to drive it and look only occasionally eat peeps at the green patches to  so does this mean what I think it means experiment number four if you have been holding onto your paper so far now squeeze that paper shuffling and left shovel in some edition all use less complexity which would take the form of these background and  my goodness it still works just fine and to the minus ones just keep on coming  so this was quite a ride but what is the conclusion here well learning algorithms show some ways in which they think like we think that the answer is no do not think within your own network or reinforcement learner as a digital copy of the brain not even close now even better this is not just a fantastic thought experiment all this has utility for instance in his lecture one of the authors David Haugh notes that humans can also get upside down goggles or bicycles where the left and right directions are flipped and if they do it takes a great deal of time for the human to adapt for the neural network no issues whatsoever what a time to be alive this episode has been supported by lambda GPO clout if you're looking for inexpensive cloud GPUs for AI check out lambda GPO clout they recently launched quadro RT X. six thousand R. D. X. eight thousand and V. one hundred instances and hold on to your papers because lambda GPO clout can cost less than house of EWS and Asia plus they are the only cloud service with forty eight gigabyte R. D. X. eight thousands join researchers and organizations like apple MIT and Caltech in using lambda cloud instances workstations or servers make sure to go to lambda labs dot com slash papers to sign up for one of their amazing GPU instances today I thank the lambda offered a longstanding support and for helping us make better videos for you thanks for watching and for your generous support and I'll see you next time ",
        "transcription_mode": "IBM Watson",
        "source_link": "https://www.youtube.com/watch?v=BS2la3C-TYc",
        "paper_link": "https://attentionneuron.github.io/",
        "paper_title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"
    }
]