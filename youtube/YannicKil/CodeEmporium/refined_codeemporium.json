[
    {
        "sourceUrl": "https://www.youtube.com/watch?v=GcvGxXePI2g",
        "summary": "Dropout is a common method of regularization in neural networks. However, it doesn\u2019t work too well in Convolution Neural network Architectures. We are going to understand why this is this case, and offer an alternative approach to regularization: DropBlock.\n\nHit that SUBSCRIBE, ring that BELL, share this content, like and comment down below your video suggestions. I don\u2019t get many so I\u2019ll read them all.\n\nFOLLOW ME\nQuora: https://www.quora.com/profile/Ajay-Halthor\n\nREFERENCES\n\n[1] DropBlock (Main Paper): https://arxiv.org/pdf/1810.12890v1.pdf\n[2] Neural Network Playground: https://playground.tensorflow.org \n[3] AlexNet: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n[4] Pytorch Code for DropBlock: https://github.com/miguelvr/dropblock/blob/master/dropblock/dropblock.py",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1810.12890v1.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=xPCCyiw8M2U",
        "summary": "Reinforcement learning generally uses a carrot-and-stick approach. Good actions are rewarded and bad actions are punished. But what are the drawbacks of this simple approach? How can we use curiosity to overcome it? Let's find out in this video! \n\n\nREFERENCES\n[1]  Main Paper (Episodic Curiosity through Reachability) : https://arxiv.org/pdf/1810.02274.pdf\n[2] Blog: https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html\n[3] AI learns to walk: https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1810.02274.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=NyAosnNQv_U",
        "summary": "We talk about cycle consistent adversarial networks for unpaired image-image translation. Some image-image translation problems include:\n- Season Transfer\n- Object Transfiguration\n- Style transfer\n- Photo Enhancement\n\nIf you like the video, hit that like button. Wanna see content like this AI, Machine learning, Deep Learning Data Sciences, hit that SUBSCRIBE button. For instant notifications when I upload, RING that BELL.\n\nOTHER COOL VIDEOS\n- Generative Adversarial Networks: https://www.youtube.com/watch?v=O8LAi6ksC80\n- CNN Architectures: https://www.youtube.com/watch?v=m8pOnJxOcqY\n\nSOCIAL LINKS\nFollow me on Quora: https://www.quora.com/profile/Ajay-Halthor\nEmail: ask.ajhalthor@gmail.com\n\nREFERENCES\n\n[1] Main Paper: https://arxiv.org/pdf/1703.10593.pdf\n[2] Blog for architecture (Code too): https://hardikbansal.github.io/CycleGANBlog/\n[3] Architecture borrowed from here: https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf\n[5] ConvNets are PatchGANs: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39\n[6] Network Architecture is heavily based on Conditional Adversarial Nets: https://arxiv.org/pdf/1611.07004.pdf\n[7] pytorch implementation of CycleGAN: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1703.10593.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=BeYbQkbKox8",
        "summary": "In this video, we talk about \"Sketch-a-Classifier\"  released by researchers at the university of London.\n\nKEYWORDS\n1. Zero Shot Learning\n2. Model Regression Networks (MRN)\n3. Parametric Model\n4. Multilayer Perceptron (MLP)\n5. Fully Convolutional Network (FCN)\n6. Regression Loss\n7. Performance Loss\n\nREFERENCES\n1. Sketch-a-Classifier (main paper): https://arxiv.org/abs/1804.11182\n2. Sketchy: https://www.cc.gatech.edu/~hays/tmp/sketchy-database.pdf\n3. Imagenet: https://www-cs.stanford.edu/groups/vision/pdf/ImageNet_CVPR2009.pdf\n\nFollow me: https://www.quora.com/profile/Ajay-Halthor\n\nHit that SUBSCRIBE button and ring the bell for instant access to my videos on Machine Learning, Deep Learning, Data Sciences and Artificial Intelligence!",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.11182"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=gVehTbi6Ipc",
        "summary": "In this video, we take a look at a paper released by Baidu on Neural Voice Cloning with a few samples. The idea is to \u201cclone\u201d an unseen speaker\u2019s voice with only a few sound clips.\n\nIf you like the video, hit that like button. Ring the bell to stay notified of my videos on Machine Learning, Deep Learning, Data Sciences and AI.\n\nmain paper: https://arxiv.org/abs/1802.06006\nCheck out the audio demos: https://audiodemos.github.io/\n\nMY EQUIPMENT (on a $350 budget)\nCamera (GoPro Hero 5 Black + 32 GB Memory + Kit): https://goo.gl/V4542j\nMicrophone: https://goo.gl/BxBRcW\nPop filter: https://goo.gl/oQTQ8W\n\nFOLLOW ME\nhttps://www.quora.com/profile/Ajay-Halthor",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.06006"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Y2Tna77k1aI",
        "summary": "From the one that started it all \"LeNet\" (1998) to the deeper networks we see today like Xception (2017), here are some important CNN architectures you should know. If you like the video, show your support with a like, and SUBSCRIBE for more awesome content on Machine Learning, deep Learning, Data Science and AI\n\n\nMY EQUIPMENT (on a $350 budget)\nCamera (GoPro Hero 5 Black + 32 GB Memory + Kit): https://goo.gl/V4542j\nMicrophone: https://goo.gl/BxBRcW\nPop filter: https://goo.gl/oQTQ8W\n\nFOLLOW ME\nhttps://www.quora.com/profile/Ajay-Halthor\n\n\nREFERENCES\n[1] LeNet-5 (the start of it all): http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n[2] Nice Blog post: https://towardsdatascience.com/neural-network-architectures-156e5bad51ba\n[3] CNN Architectures: http://slazebni.cs.illinois.edu/spring17/lec01_cnn_architectures.pdf\n[4] ImageNet - The data that transformed AI research: https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\n[5]Imagenet (main paper): https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database\n[6] AlexNet: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n[7] Difference between saturating & non-saturating nonlinearities: https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean\n[8] Top-1 accuracy Vs Top-5 Accuracy. What do they mean? https://stats.stackexchange.com/questions/156471/imagenet-what-is-top-1-and-top-5-error-rate\n[9] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks: https://arxiv.org/abs/1312.6229\n[10] Network in Network (NiN) Architecture: https://arxiv.org/abs/1312.4400\n[11] GoogleNet: https://arxiv.org/pdf/1409.4842.pdf\n[12] R-CNN: https://arxiv.org/abs/1311.2524\n[13] ResNet: https://arxiv.org/abs/1512.03385\n[14] An Overview of ResNet and its Variants: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\n[15] Xception: Deep Learning with Depthwise Separable Convolutions: https://arxiv.org/abs/1610.02357",
        "sourceType": "blog",
        "linkToPaper": "https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=vpc35rBs_Bc",
        "summary": "We explore a neural network architecture that can solve multiple tasks: multimodal Neural Network. We discuss important components and concepts along the way.\n\nIf you like this video, hit that like button. If you really like this video, hit that SUBSCRIBE button. And if you just love me hit that BELL next to the subscribe button. \n\nRELATED VIDEOS\n[1] Convolution Neural Networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\n[2] Depthwise Separable Convolution (A Faster Convolution): https://www.youtube.com/watch?v=T7o3xvJLuHk&t=248s\n[3] Attention in Neural Networks: https://www.youtube.com/watch?v=W2rWgXJBZhU\n[4] Sound Play with Convolution Neural Networks: https://www.youtube.com/watch?v=GNza2ncnMfA\n\nREFERENCES\n[1]Main paper \"One Model to Learn them all\": https://arxiv.org/pdf/1706.05137v1.pdf\n[2] Separable Convolutions (and other types): https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n[3] Outrageously large neural networks: https://techburst.io/outrageously-large-neural-network-gated-mixture-of-experts-billions-of-parameter-same-d3e901f2fe05\n[4] Mobile Nets, Depthwise Separable Convolution: https://arxiv.org/pdf/1704.04861.pdf\n[5] Blog post on Depthwise Separable Convolution: https://arxiv.org/pdf/1610.02357.pdf\n[6] Attention Gan (Microsoft's AttnGAN): https://arxiv.org/abs/1711.10485\n[7] Show, attend and tell: https://arxiv.org/pdf/1502.03044.pdf \n\n\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1706.05137v1.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=T7o3xvJLuHk",
        "summary": "In this video, I talk about depthwise Separable Convolution - A faster method of convolution with less computation power & parameters. We mathematically prove how it is faster, and discuss applications where it is used in modern research. \n\nIf you liked that video, hit that like button. If you wanna stick around, hit that subscribe button. If you really wanna stick around, hit that bell icon next to the subscribe button to be notified of my uploads immediately. \n\nConvolution Neural Networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\n\nREFERENCES\n\nXception (main paper): https://arxiv.org/pdf/1610.02357.pdf\nMobile Nets (Efficient CNN for mobile vision applications) : https://arxiv.org/pdf/1704.04861.pdf\nOne model Learns all: https://arxiv.org/pdf/1706.05137v1.pdf\n\n\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1610.02357.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=4tkgOzQ9yyo",
        "summary": "In this video, we will take a look at new type of neural network architecture called \"Masked Region based Convolution Neural Networks\", Masked R-CNN for short. And in the process, highlight some key sub problems in computer vision.\n\nPlease SUBSCRIBE to the channel for more content on Machine Learning, Deep Learning, Data Science, and Artificial Intelligence. Hoping to build a community of AI geeks. You'll fit right in!\n\nREFERENCES\n\nMain paper: https://arxiv.org/pdf/1703.06870v3.pdf\nCode: https://github.com/facebookresearch/Detectron\n\nConvolution Neural networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\nSemantic segmentation in deep learning: http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review\nTop papers: http://www.arxiv-sanity.com/top?timefilter=alltime&vfilter=all\nRecurrent Instance Segmentation: http://www.robots.ox.ac.uk/~tvg/publications/2016/RIS7.pdf\nMask R-CNN Presentation by the Author: https://www.youtube.com/watch?v=g7z4mkfRjI4\nMark Jay's Video: https://www.youtube.com/watch?v=2TikTv6PWDw\nCOCO dataset: http://cocodataset.org/#home\nFully Convolutional Networks: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\nFaster R-CNN explained: https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8\nNotes/summary of Masked R-CNN: http://www.shortscience.org/paper?bibtexKey=journals/corr/HeGDG17#aleju\n\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1703.06870v3.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=sIoHFPGOY0I",
        "summary": "Let\u2019s take a look at how Google Translate\u2019s Neural Network works behind the scenes! Read these references below for the best understanding of Neural Machine Translation! \n\nREFERENCES\n\n[1] Landmark paper of LSTM (Hochreiter et al., 1997): https://www.bioinf.jku.at/publications/older/2604.pdf \n[2] Landmark paper of Neural Machine Translation NMT (Kalchbrenner et al., 2013): https://arxiv.org/abs/1306.3584 \n[3] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (Cho et al., 2014): https://arxiv.org/abs/1406.1078\n[4] Seq to Seq learning with neural networks (Sutskever et al., 2014): https://arxiv.org/abs/1409.3215)\n[5] The paper that introduced Bidirectional RNN : https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf\n[6] On the properties of NMP: Encoder-Decoder Approaches (Cho et al., 2014): https://arxiv.org/pdf/1409.1259.pdf Fig. 4 (a)\n[7] NMT by jointly learning to align & translate (Bahdanau et al., 2016): https://arxiv.org/pdf/1409.0473.pdf 5.2.2\n[8] Google Translate Main paper (Wu et al., 2016): https://ai.google/research/pubs/pub45610",
        "sourceType": "blog",
        "linkToPaper": "https://ai.google/research/pubs/pub45610"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=7IEEKvcudrA",
        "summary": "JukeboxAI can generate music in the voice of any artist with any style.\n\nPlease subscribe to keep me alive: https://www.youtube.com/c/CodeEmporium?sub_confirmation=1\n\nSPONSOR\nKite is a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\u2019re typing. I've been using Kite. Love it! https://www.kite.com/get-kite/?utm_medium=referral&utm_source=youtube&utm_campaign=codeemporium&utm_content=description-only\n\n\nREFERENCES\n[1] The blog for more info: https://openai.com/blog/jukebox/\n[2] The main paper: https://cdn.openai.com/papers/jukebox.pdf\n[3] Variational AutoEncoder Tutorial: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n[4] Vector Quantized Variational AutoEncoders (VQ-VAE) - the main paper: https://arxiv.org/abs/1711.00937\n[5] Hierarchical Quantized AutoEncoders: https://arxiv.org/abs/2002.08111\n[6] Generating High quality images with VQ-VAE-2: https://arxiv.org/pdf/1906.00446.pdf\n[7] Disadvantage of Variational AutoEncoders is \"Posterior Collapse\". Learn more here: https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon\n[8] More reasoning on why posterior collapse occurs: https://papers.nips.cc/paper/9138-dont-blame-the-elbo-a-linear-vae-perspective-on-posterior-collapse.pdf",
        "sourceType": "blog",
        "linkToPaper": "https://cdn.openai.com/papers/jukebox.pdf"
    }
]