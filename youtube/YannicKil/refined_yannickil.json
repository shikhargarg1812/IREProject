[
    {
        "sourceUrl": "https://www.youtube.com/watch?v=fvctpYph8Pc",
        "summary": "Has the world overfitted to ImageNet? What if we collect another dataset in exactly the same fashion? This paper gives a surprising answer!\n\nPaper: https://arxiv.org/abs/1902.10811\nData: https://github.com/modestyachts/ImageNetV2\n\nAbstract:\nWe build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets.\n\nAuthors: Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1902.10811"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=1aO-uHXbzmQ",
        "summary": "This generative model for music can make entire songs with remarkable quality and consistency. It can be conditioned on genre, artist, and even lyrics.\n\nBlog: https://openai.com/blog/jukebox/\nPaper: https://cdn.openai.com/papers/jukebox.pdf\nCode: https://github.com/openai/jukebox/\n\nAbstract:\nWe introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.\n\nAuthors: Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://cdn.openai.com/papers/jukebox.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=cIUtRNhY6Rw",
        "summary": "Answering complex questions about tabular information is hard. No two tables are alike and sometimes the answer you're looking for is not even in the table and needs to be computed from a subset of the cells. Surprisingly, this model can figure it all out by itself through some clever input encoding and loss engineering.\n\nPaper: https://arxiv.org/abs/2004.02349\nCode: https://github.com/google-research/tapas\n\nAbstract:\nAnswering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.\n\nAuthors: Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Martin Eisenschlos\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2004.02349"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=to7vCdkLi4s",
        "summary": "This ONE SIMPLE TRICK can take a vanilla RL algorithm to achieve state-of-the-art. What is it? Simply augment your training data before feeding it to the learner! This can be dropped into any RL pipeline and promises big improvements across the board.\n\nPaper: https://arxiv.org/abs/2004.14990\nCode: https://www.github.com/MishaLaskin/rad\n\nAbstract:\nLearning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at this https URL.\n\nAuthors: Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2004.14990"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=tjbEVY5XIk0",
        "summary": "When AI makes a plan it usually does so step by step, forward in time. But often it is beneficial to define intermediate goals to divide a large problem into easier sub-problems. This paper proposes a generalization of MCTS that searches not for the best next actions to take, but for the best way to sub-divide the problem recursively into problems so tiny that they can each be solved in a single step.\n\nPaper: https://arxiv.org/abs/2004.11410\nSite: https://sites.google.com/view/dc-mcts/home\n\nAbstract:\nStandard planners for sequential decision making (including Monte Carlo planning, tree search, dynamic programming, etc.) are constrained by an implicit sequential planning assumption: The order in which a plan is constructed is the same in which it is executed. We consider alternatives to this assumption for the class of goal-directed Reinforcement Learning (RL) problems. Instead of an environment transition model, we assume an imperfect, goal-directed policy. This low-level policy can be improved by a plan, consisting of an appropriate sequence of sub-goals that guide it from the start to the goal state. We propose a planning algorithm, Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS), for approximating the optimal plan by means of proposing intermediate sub-goals which hierarchically partition the initial tasks into simpler ones that are then solved independently and recursively. The algorithm critically makes use of a learned sub-goal proposal for finding appropriate partitions trees of new tasks based on prior experience. Different strategies for learning sub-goal proposals give rise to different planning strategies that strictly generalize sequential planning. We show that this algorithmic flexibility over planning order leads to improved results in navigation tasks in grid-worlds as well as in challenging continuous control environments.\n\nAuthors: Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B. Hamrick, Nicolas Heess, Alexander Neitz, Theophane Weber\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2004.11410"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=k1GOF2jmX7c",
        "summary": "One CNN to rule them all! BiT is a pre-trained ResNet that can be used as a starting point for any visual task. This paper explains what it takes to pre-train such a large model and details how fine-tuning on downstream tasks is done best.\n\nPaper: https://arxiv.org/abs/1912.11370\nCode & Models: TBA\n\nAbstract:\nTransfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.\n\nAuthors: Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1912.11370"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Cs_j-oNwGgg",
        "summary": "This is a hard paper! Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities.\n\nPaper: https://arxiv.org/abs/1811.02486\nBlog: https://openai.com/blog/learning-concepts-with-energy-functions/\nVideos: https://sites.google.com/site/energyconceptmodels/\n\nAbstract:\nMany hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at this http URL\n\nAuthors: Igor Mordatch\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1811.02486"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=l_3zj6HeWUE",
        "summary": "The dirty little secret of Batch Normalization is its intrinsic dependence on the training batch size. Group Normalization attempts to achieve the benefits of normalization without batch statistics and, most importantly, without sacrificing performance compared to Batch Normalization.\n\nhttps://arxiv.org/abs/1803.08494\n\nAbstract:\nBatch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.\n\nAuthors: Yuxin Wu, Kaiming He\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1803.08494"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=bFn2xcGi1TQ",
        "summary": "CPUs are often bottlenecks in Machine Learning pipelines. Data fetching, loading, preprocessing and augmentation can be slow to a point where the GPUs are mostly idle. Data Echoing is a technique to re-use data that is already in the pipeline to reclaim this idle time and keep the GPUs busy at all times.\n\nhttps://arxiv.org/abs/1907.05550\n\nAbstract:\nIn the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce \"data echoing,\" which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or \"echoes\") intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.\n\nAuthors: Dami Choi, Alexandre Passos, Christopher J. Shallue, George E. Dahl\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1907.05550"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=p-zOeQCoG9c",
        "summary": "It's common for neural networks to include data normalization such as BatchNorm or GroupNorm. This paper extends the normalization to also include the weights of the network. This surprisingly simple change leads to a boost in performance and - combined with GroupNorm - new state-of-the-art results.\n\nhttps://arxiv.org/abs/1903.10520\n\nAbstract:\nIn this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: this https URL.\n\nAuthors: Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1903.10520"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=l5he9JNJqHA",
        "summary": "Does self-supervision really need a lot of data? How low can you go? This paper shows that a single image is enough to learn the lower layers of a deep neural network. Interestingly, more data does not appear to help as long as enough data augmentation is applied.\n\nOUTLINE:\n0:00 - Overview\n1:40 - What is self-supervision\n4:20 - What does this paper do\n7:00 - Linear probes\n11:15 - Linear probe results\n17:10 - Results\n22:25 - Learned Features\n\nhttps://arxiv.org/abs/1904.13132\n\nAbstract:\nWe look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.\n\nAuthors: Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi\n\nThumbnail Image: https://commons.wikimedia.org/wiki/File:Golden_Gate_Bridge_during_blue_hour_(16_x_10).jpg\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1904.13132"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=u5BkO8XMS2I",
        "summary": "Gradient-based Meta-Learning requires full backpropagation through the inner optimization procedure, which is a computational nightmare. This paper is able to circumvent this and implicitly compute meta-gradients by the clever introduction of a quadratic regularizer.\n\nOUTLINE:\n0:00 - Intro\n0:15 - What is Meta-Learning?\n9:05 - MAML vs iMAML\n16:35 - Problem Formulation\n19:15 - Proximal Regularization\n26:10 - Derivation of the Implicit Gradient\n40:55 - Intuition why this works\n43:20 - Full Algorithm\n47:40 - Experiments\n\nPaper: https://arxiv.org/abs/1909.04630\nBlog Post: https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/\n\nAbstract:\nA core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.\n\nAuthors: Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1909.04630"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=a-VQfQqIMrE",
        "summary": "Neural Networks often draw hard boundaries in high-dimensional space, which makes them very brittle. Mixup is a technique that linearly interpolates between data and labels at training time and achieves much smoother and more regular class boundaries.\n\nOUTLINE:\n0:00 - Intro\n0:30 - The problem with ERM\n2:50 - Mixup\n6:40 - Code\n9:35 - Results\n\nhttps://arxiv.org/abs/1710.09412\n\nAbstract:\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\n\nAuthors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1710.09412"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=IiBFqnNu7A8",
        "summary": "What can an agent do without any reward? Explore the world! While many formulations of intrinsic rewards exist (Curiosity, Novelty, etc.), they all look back in time to learn. Plan2Explore is the first model that uses planning in a learned imaginary latent world model to seek out states where it is uncertain about what will happen.\n\nOUTLINE:\n0:00 - Intro & Problem Statement\n3:30 - Model\n5:10 - Intrinsic Motivation\n9:05 - Planning in Latent Space\n11:15 - Latent Disagreement\n16:30 - Maximizing Information Gain\n21:00 - More problems with the model\n26:45 - Experiments\n32:10 - Final Comments\n\nPaper: https://arxiv.org/abs/2005.05960\nWebsite: https://ramanans1.github.io/plan2explore/\nCode: https://github.com/ramanans1/plan2explore\n\nAbstract:\nReinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at this https URL\n\nAuthors: Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.05960"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=YrO1v7-KcXs",
        "summary": "Can you peek into people's brains? Reading human thoughts is a long-standing dream of the AI field. This paper reads fMRI signals from a person and then reconstructs what that person's eyes currently see. This is achieved by translating the fMRI signal to features of a Deep Neural Network and then iteratively optimizing the input of the network to match those features. The results are impressive.\n\nOUTLINE:\n0:00 - Overview\n1:35 - Pipeline\n4:00 - Training\n5:20 - Image Reconstruction\n7:00 - Deep Generator Network\n8:15 - Results\n\nPaper: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633\nMy Video on OpenAI Microscope (what I called Atlas): https://youtu.be/Ok44otx90D4\n\nAbstract:\nThe mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.\n\nAuthors: Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Nfry2b4RFI4",
        "summary": "Why are humans so good at video games? Maybe it's because a lot of games are designed with humans in mind. What happens if we change that? This paper removes the influence of human priors from a game and ends up with a pretty fun experience.\n\nPaper: https://arxiv.org/abs/1802.10217\nWebsite: https://rach0012.github.io/humanRL_website/\nCode: https://github.com/rach0012/humanRL_prior_games\n\nAbstract:\nWhat makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at this https URL\n\nAuthors: Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, Alexei A. Efros\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1802.10217"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=IIebBjbBevs",
        "summary": "BERT is a giant model. Turns out you can prune away many of its components and it still works. This paper analyzes BERT pruning in light of the Lottery Ticket Hypothesis and finds that even the \"bad\" lottery tickets can be fine-tuned to good accuracy.\n\nOUTLINE:\n0:00 - Overview\n1:20 - BERT\n3:20 - Lottery Ticket Hypothesis\n13:00 - Paper Abstract\n18:00 - Pruning BERT\n23:00 - Experiments\n50:00 - Conclusion\n\nhttps://arxiv.org/abs/2005.00561\n\nML Street Talk Channel: https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ\n\nAbstract:\nMuch of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the \"bad\" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the \"good\" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the \"good\" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.\n\nAuthors: Sai Prasanna, Anna Rogers, Anna Rumshisky\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.00561"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=UjJU13GdL94",
        "summary": "Can you plan with a learned model of the world? Yes, but there's a catch: The better your planning algorithm is, the more the errors of your world model will hurt you! This paper solves this problem by regularizing the planning algorithm to stay in high probability regions, given its experience.\n\nhttps://arxiv.org/abs/1903.11981\n\nInterview w/ Harri: https://youtu.be/HnZDmxYnpg4\n\nAbstract:\nTrajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.\n\nAuthors: Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1903.11981"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=T35ba_VXkMY",
        "summary": "Object detection in images is a notoriously hard task! Objects can be of a wide variety of classes, can be numerous or absent, they can occlude each other or be out of frame. All of this makes it even more surprising that the architecture in this paper is so simple. Thanks to a clever loss function, a single Transformer stacked on a CNN is enough to handle the entire task!\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:50 - Problem Formulation\n2:30 - Architecture Overview\n6:20 - Bipartite Match Loss Function\n15:55 - Architecture in Detail\n25:00 - Object Queries\n31:00 - Transformer Properties\n35:40 - Results\n\nERRATA:\nWhen I introduce bounding boxes, I say they consist of x and y, but you also need the width and height.\n\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\n\nPaper: https://arxiv.org/abs/2005.12872\nBlog: https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/\nCode: https://github.com/facebookresearch/detr\n\nAbstract:\nWe present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.\n\nAuthors: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.12872"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=SY5PvZrJhLE",
        "summary": "#gpt3 #openai #gpt-3\n\nHow far can you go with ONLY language modeling? Can a large enough language model perform NLP task out of the box? OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before and the results are astounding.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Language Models\n2:45 - Language Modeling Datasets\n3:20 - Model Size\n5:35 - Transformer Models\n7:25 - Fine Tuning\n10:15 - In-Context Learning\n17:15 - Start of Experimental Results\n19:10 - Question Answering\n23:10 - What I think is happening\n28:50 - Translation\n31:30 - Winograd Schemes\n33:00 - Commonsense Reasoning\n37:00 - Reading Comprehension\n37:30 - SuperGLUE\n40:40 - NLI\n41:40 - Arithmetic Expressions\n48:30 - Word Unscrambling\n50:30 - SAT Analogies\n52:10 - News Article Generation\n58:10 - Made-up Words\n1:01:10 - Training Set Contamination\n1:03:10 - Task Examples\n\nhttps://arxiv.org/abs/2005.14165\nhttps://github.com/openai/gpt-3\n\nAbstract:\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\nAuthors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.14165"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=q7QP_lfqnQM",
        "summary": "Do we really need dot-product attention? The attention mechanism is a central part of modern Transformers, mainly due to the dot-product attention mechanism. This paper changes the mechanism to remove the quadratic interaction terms and comes up with a new model, the Synthesizer. As it turns out, you can do pretty well like that!\n\nOUTLINE:\n0:00 - Intro & High Level Overview\n1:00 - Abstract\n2:30 - Attention Mechanism as Information Routing\n5:45 - Dot Product Attention\n8:05 - Dense Synthetic Attention\n15:00 - Random Synthetic Attention\n17:15 - Comparison to Feed-Forward Layers\n22:00 - Factorization & Mixtures\n23:10 - Number of Parameters\n25:35 - Machine Translation & Language Modeling Experiments\n36:15 - Summarization & Dialogue Generation Experiments\n37:15 - GLUE & SuperGLUE Experiments\n42:00 - Weight Sizes & Number of Head Ablations\n47:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2005.00743\nMy Video on Transformers (Attention Is All You Need): https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nThe dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \\textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).\n\nAuthors: Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.00743"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=HYEzHX6-fIA",
        "summary": "This RL framework can discover low-level skills all by itself without any reward. Even better, at test time it can compose its learned skills and reach a specified goal without any additional learning! Warning: Math-heavy!\n\nOUTLINE:\n0:00 - Motivation\n2:15 - High-Level Overview\n3:20 - Model-Based vs Model-Free Reinforcement Learning\n9:00 - Skills\n12:10 - Mutual Information Objective\n18:40 - Decomposition of the Objective\n27:10 - Unsupervised Skill Discovery Algorithm\n42:20 - Planning in Skill Space\n48:10 - Conclusion\n\nPaper: https://arxiv.org/abs/1907.01657\nWebsite: https://sites.google.com/view/dads-skill\nCode: https://github.com/google-research/dads\n\nAbstract:\nConventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.\n\nAuthors: Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1907.01657"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=3_qGrmD6iQY",
        "summary": "How does one measure the Intelligence of an AI? Is AlphaGo intelligent? How about GPT-3? In this landmark paper, Chollet proposes a solid measure of intelligence for AI that revolves around generalization, rather than skill.\n\nOUTLINE:\n0:00 - Intro\n1:15 - The need for a measure of intelligence\n3:35 - Intelligence as generalization ability\n5:45 - Nature vs nurture\n11:45 - Skill-based evaluation\n18:30 - Generalization based evaluation\n30:25 - Inspiration from psychometrics\n36:30 - Conclusion\n\nhttps://arxiv.org/abs/1911.01547\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nThumbnail: Photo by mohamed hassan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1911.01547"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=hQEnzdLkPj4",
        "summary": "How do you learn labels without labels? How do you classify images when you don't know what to classify them into? This paper investigates a new combination of representation learning, clustering, and self-labeling in order to group visually similar images together - and achieves surprisingly high accuracy on benchmark datasets.\n\nOUTLINE:\n0:00 - Intro & High-level Overview\n2:15 - Problem Statement\n4:50 - Why naive Clustering does not work\n9:25 - Representation Learning\n13:40 - Nearest-neighbor-based Clustering\n28:00 - Self-Labeling\n32:10 - Experiments\n38:20 - ImageNet Experiments\n41:00 - Overclustering\n\nPaper: https://arxiv.org/abs/2005.12320\nCode: https://github.com/wvangansbeke/Unsupervised-Classification\n\nAbstract:\nIs it possible to automatically classify images without the use of ground-truth annotations? Or when even the classes themselves, are not a priori known? These remain important, and open questions in computer vision. Several approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by huge margins, in particular +26.9% on CIFAR10, +21.5% on CIFAR100-20 and +11.7% on STL10 in terms of classification accuracy. Furthermore, results on ImageNet show that our approach is the first to scale well up to 200 randomly selected classes, obtaining 69.3% top-1 and 85.5% top-5 accuracy, and marking a difference of less than 7.5% with fully-supervised methods. Finally, we applied our approach to all 1000 classes on ImageNet, and found the results to be very encouraging. The code will be made publicly available.\n\nAuthors: Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.12320"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=CA8JPbJ75tY",
        "summary": "Many object detectors focus on locating the center of the object they want to find. However, this leaves them with the secondary problem of determining the specifications of the bounding box, leading to undesirable solutions like anchor boxes. This paper directly detects the top left and the bottom right corners of objects independently, along with descriptors that allows to match the two later and form a complete bounding box. For this, a new pooling method, called corner pooling, is introduced.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:40 - Object Detection\n2:40 - Pipeline I - Hourglass\n4:00 - Heatmap & Embedding Outputs\n8:40 - Heatmap Loss\n10:55 - Embedding Loss\n14:35 - Corner Pooling\n20:40 - Experiments\n\nPaper: https://arxiv.org/abs/1808.01244\nCode: https://github.com/princeton-vl/CornerNet\n\nAbstract:\nWe propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.\n\nAuthors: Hei Law, Jia Deng\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1808.01244"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=4GKCxJQSw-g",
        "summary": "Neural Architecture Search is usually prohibitively expensive in both time and resources to be useful. A search strategy has to keep evaluating new models, training them to convergence in an inner loop to find out if they are any good. This paper proposes to abstract the problem and extract the essential part of the architecture to be optimized into a smaller version and evaluates that version on specifically custom learned data points to predict its performance, which is much faster and cheaper than running the full model.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:00 - Neural Architecture Search\n4:30 - Predicting performance via architecture encoding\n7:50 - Synthetic Petri Dish\n12:50 - Motivating MNIST example\n18:15 - Entire Algorithm\n23:00 - Producing the synthetic data\n26:00 - Combination with architecture search\n27:30 - PTB RNN-Cell Experiment\n29:20 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2005.13092\nCode: https://github.com/uber-research/Synthetic-Petri-Dish\n\nAbstract:\nNeural Architecture Search (NAS) explores a large space of architectural motifs -- a compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands of domain-specific data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available. Our hope is that this work can inspire a new research direction in studying the performance of extracted components of models in an alternative controlled setting.\n\nAuthors: Aditya Rawal, Joel Lehman, Felipe Petroski Such, Jeff Clune, Kenneth O. Stanley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.13092"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=rl4nUngiR2k",
        "summary": "Proper evaluation of text generation models, such as machine translation systems, requires expensive and slow human assessment. As these models have gotten better in previous years, proxy-scores, like BLEU, are becoming less and less useful. This paper proposes to learn a proxy score and demonstrates that it correlates well with human raters, even as the data distribution shifts.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:00 - The Problem with Evaluating Machine Translation\n5:10 - Task Evaluation as a Learning Problem\n10:45 - Naive Fine-Tuning BERT\n13:25 - Pre-Training on Synthetic Data\n16:50 - Generating the Synthetic Data\n18:30 - Priming via Auxiliary Tasks\n23:35 - Experiments & Distribution Shifts\n27:00 - Concerns & Conclusion\n\nPaper: https://arxiv.org/abs/2004.04696\nCode: https://github.com/google-research/bleurt\n\nAbstract:\nText generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.\n\nAbstract: Thibault Sellam, Dipanjan Das, Ankur P. Parikh\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2004.04696"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=xTzFJIknh7E",
        "summary": "Code migration between languages is an expensive and laborious task. To translate from one language to the other, one needs to be an expert at both. Current automatic tools often produce illegible and complicated code. This paper applies unsupervised neural machine translation to source code of Python, C++, and Java and is able to translate between them, without ever being trained in a supervised fashion.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - The Transcompiling Problem\n5:55 - Neural Machine Translation\n8:45 - Unsupervised NMT\n12:55 - Shared Embeddings via Token Overlap\n20:45 - MLM Objective\n25:30 - Denoising Objective\n30:10 - Back-Translation Objective\n33:00 - Evaluation Dataset\n37:25 - Results\n41:45 - Tokenization\n42:40 - Shared Embeddings\n43:30 - Human-Aware Translation\n47:25 - Failure Cases\n48:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.03511\n\nAbstract:\nA transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.\n\nAuthors: Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.03511"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=WTB2p4bqtXU",
        "summary": "Text-to-speech engines are usually multi-stage pipelines that transform the signal into many intermediate representations and require supervision at each step. When trying to train TTS end-to-end, the alignment problem arises: Which text corresponds to which piece of sound? This paper uses an alignment module to tackle this problem and produces astonishingly good sound.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems with Text-to-Speech\n3:55 - Adversarial Training\n5:20 - End-to-End Training\n7:20 - Discriminator Architecture\n10:40 - Generator Architecture\n12:20 - The Alignment Problem\n14:40 - Aligner Architecture\n24:00 - Spectrogram Prediction Loss\n32:30 - Dynamic Time Warping\n38:30 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.03575\nWebsite: https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech\n\nAbstract:\nModern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable monotonic interpolation scheme to predict the duration of each input token. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.\n\nAuthors: Jeff Donahue, Sander Dieleman, Miko\u0142aj Bi\u0144kowski, Erich Elsen, Karen Simonyan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.03575"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-_2AF9Lhweo",
        "summary": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04768"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-_2AF9Lhweo",
        "summary": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04768"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-_2AF9Lhweo",
        "summary": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04768"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-_2AF9Lhweo",
        "summary": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04768"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-_2AF9Lhweo",
        "summary": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04768"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-_2AF9Lhweo",
        "summary": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04768"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=ZfDZRX3WiJg",
        "summary": "Pre-training a CNN backbone for visual transfer learning has recently seen a big push into the direction of incorporating more data, at the cost of less supervision. This paper investigates the opposite: Visual transfer learning by pre-training from very few, but very high-quality samples on an image captioning task.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pre-Training for Visual Tasks\n3:40 - Quality-Quantity Tradeoff\n5:50 - Image Captioning\n8:35 - VirTex Method\n14:30 - Linear Classification\n20:30 - Ablations\n22:05 - Fine-Tuning\n25:45 - Attention Visualization\n27:30 - Conclusion & Remarks\n\nPaper: https://arxiv.org/abs/2006.06666\nCode: https://github.com/kdexd/virtex\n\nAbstract:\nThe de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.\n\nAuthors: Karan Desai, Justin Johnson\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.06666"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=ZfDZRX3WiJg",
        "summary": "Pre-training a CNN backbone for visual transfer learning has recently seen a big push into the direction of incorporating more data, at the cost of less supervision. This paper investigates the opposite: Visual transfer learning by pre-training from very few, but very high-quality samples on an image captioning task.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pre-Training for Visual Tasks\n3:40 - Quality-Quantity Tradeoff\n5:50 - Image Captioning\n8:35 - VirTex Method\n14:30 - Linear Classification\n20:30 - Ablations\n22:05 - Fine-Tuning\n25:45 - Attention Visualization\n27:30 - Conclusion & Remarks\n\nPaper: https://arxiv.org/abs/2006.06666\nCode: https://github.com/kdexd/virtex\n\nAbstract:\nThe de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.\n\nAuthors: Karan Desai, Justin Johnson\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.06666"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=THcuTJbeD34",
        "summary": "In this part, we go much more in-depth into the relationship between intelligence, generality, skill, experience, and prior knowledge and take a close look at what priors are built into humans. This will form the basis for comparing the intelligence of humans and AI systems.\n\nOUTLINE:\n0:00 - Intro & Recap\n3:00 - Optimize for Generality\n5:45 - Buying Skill with Data and Priors\n12:40 - The Human Scope\n17:30 - Human Priors\n24:05 - Core Knowledge\n28:50 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/1911.01547\nTim Scarfe's Video: https://youtu.be/GpWLZUbPhr0\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1911.01547"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=O9kFX33nUcU",
        "summary": "In this part, we look at the ARC challenge as a proposed test of machine intelligence. The dataset features 1000 tasks that test rapid generalization based on human core knowledge priors, such as object-ness, symmetry, and navigation.\n\nOUTLINE:\n0:00 - Intro\n0:55 - What is ARC?\n6:30 - The Goals of ARC\n10:40 - Assumed Priors & Examples\n21:50 - An Imagined Solution\n28:15 - Consequences of a Solution\n31:00 - Weaknesses\n31:25 - My Comments & Ideas\n\nPaper: https://arxiv.org/abs/1911.01547\nARC: https://github.com/fchollet/ARC\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1911.01547"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=cuyM63ugsxI",
        "summary": "In this part, we go over the formal definition of the measure of intelligence. In order to do this, we have to frame and quantify the notions of generalization difficulty, priors, and experience in terms of algorithmic complexity.\n\nOUTLINE:\n0:00 - Intro & Recap\n2:50 - Concept Schema\n10:00 - Algorithmic Complexity\n13:00 - Definitions\n15:25 - Generalization Difficulty\n18:55 - Developer Aware Generalization Difficulty\n22:40 - Priors\n25:10 - Experience\n30:50 - The Measure Of Intelligence\n38:00 - An Ideal Intelligence Benchmark\n42:30 - Conclusion\n\nPaper: https://arxiv.org/abs/1911.01547\n\nPart 1: https://youtu.be/3_qGrmD6iQY\nPart 2: https://youtu.be/THcuTJbeD34\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1911.01547"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=ml3Y1ljVSQ8",
        "summary": "#ai #research #gaming\n\nDeep RL is usually used to solve games, but this paper turns the process on its head and applies RL to game level creation. Compared to traditional approaches, it frames level design as a sequential decision making progress and ends up with a fast and diverse level generator.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:30 - Level Design via Reinforcement Learning\n3:00 - Reinforcement Learning\n4:45 - Observation Space\n5:40 - Action Space\n15:40 - Change Percentage Limit\n20:50 - Quantitative Results\n22:10 - Conclusion & Outlook\n\nPaper: https://arxiv.org/abs/2001.09212\nCode: https://github.com/amidos2006/gym-pcgrl\n\nAbstract:\nWe investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.\n\nAuthors: Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius\n\nERRATA:\n- The reward is given after each step.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2001.09212"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=l12GXD0t_RE",
        "summary": "Determining the stability properties of differential systems is a challenging task that involves very advanced symbolic and numeric mathematical manipulations. This paper shows that given enough training data, a simple language model with no underlying knowledge of mathematics can learn to solve these problems with remarkably high accuracy.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Differential System Tasks\n11:30 - Datasets & Models\n15:15 - Experiments\n21:00 - Discussion & My Comments\n\nPaper: https://arxiv.org/abs/2006.06462\nMy Video on Deep Learning for Symbolic Mathematics: https://youtu.be/p3sAF3gVMMA\n\nAbstract:\nCan advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.\n\nAuthors: Fran\u00e7ois Charton, Amaury Hayat, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.06462"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=l12GXD0t_RE",
        "summary": "Determining the stability properties of differential systems is a challenging task that involves very advanced symbolic and numeric mathematical manipulations. This paper shows that given enough training data, a simple language model with no underlying knowledge of mathematics can learn to solve these problems with remarkably high accuracy.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Differential System Tasks\n11:30 - Datasets & Models\n15:15 - Experiments\n21:00 - Discussion & My Comments\n\nPaper: https://arxiv.org/abs/2006.06462\nMy Video on Deep Learning for Symbolic Mathematics: https://youtu.be/p3sAF3gVMMA\n\nAbstract:\nCan advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.\n\nAuthors: Fran\u00e7ois Charton, Amaury Hayat, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.06462"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=8l-TDqpoUQs",
        "summary": "The Lottery Ticket Hypothesis has shown that it's theoretically possible to prune a neural network at the beginning of training and still achieve good performance, if we only knew which weights to prune away. This paper does not only explain where other attempts at pruning fail, but provides an algorithm that provably reaches maximum compression capacity, all without looking at any data!\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pruning Neural Networks\n3:40 - Lottery Ticket Hypothesis\n6:00 - Paper Story Overview\n9:45 - Layer Collapse\n18:15 - Synaptic Saliency Conservation\n23:25 - Connecting Layer Collapse & Saliency Conservation\n28:30 - Iterative Pruning avoids Layer Collapse\n33:20 - The SynFlow Algorithm\n40:45 - Experiments\n43:35 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05467\nCode: https://github.com/ganguli-lab/Synaptic-Flow\nMy Video on the Lottery Ticket Hypothesis: https://youtu.be/ZVVnvZdUMUk\nStreet Talk about LTH: https://youtu.be/SfjJoevBbjU\n\nAbstract:\nPruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.\n\nAuthors: Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.05467"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=DLq1DUcMh1Q",
        "summary": "Even though LSTMs and GRUs solve the vanishing and exploding gradient problems, they have trouble learning to remember things over very long time spans. Inspired from bistability, a property of biological neurons, this paper constructs a recurrent cell with an inherent memory property, with only minimal modification to existing architectures.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Recurrent Neural Networks\n6:00 - Gated Recurrent Unit\n14:40 - Neuronal Bistability\n22:50 - Bistable Recurrent Cell\n31:00 - Neuromodulation\n32:50 - Copy First Benchmark\n37:35 - Denoising Benchmark\n48:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05252\nCode: https://github.com/nvecoven/BRC\n\nAbstract:\nRecurrent neural networks (RNNs) provide state-of-the-art performances in a wide variety of tasks that require memory. These performances can often be achieved thanks to gated recurrent cells such as gated recurrent units (GRU) and long short-term memory (LSTM). Standard gated cells share a layer internal state to store information at the network level, and long term memory is shaped by network-wide recurrent connection weights. Biological neurons on the other hand are capable of holding information at the cellular level for an arbitrary long amount of time through a process called bistability. Through bistability, cells can stabilize to different stable states depending on their own past state and inputs, which permits the durable storing of past information in neuron state. In this work, we take inspiration from biological neuron bistability to embed RNNs with long-lasting memory at the cellular level. This leads to the introduction of a new bistable biologically-inspired recurrent cell that is shown to strongly improves RNN performance on time-series which require very long memory, despite using only cellular connections (all recurrent connections are from neurons to themselves, i.e. a neuron state is not influenced by the state of other neurons). Furthermore, equipping this cell with recurrent neuromodulation permits to link them to standard GRU cells, taking a step towards the biological plausibility of GRU.\n\nAuthors: Nicolas Vecoven, Damien Ernst, Guillaume Drion\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.05252"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=sEG8hD64c_Q",
        "summary": "Image-to-Image translation usually requires corresponding samples or at least domain labels of the dataset. This paper removes that restriction and allows for fully unsupervised image translation of a source image to the style of one or many reference images. This is achieved by jointly training a guiding network that provides style information and pseudo-labels.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Unsupervised Image-to-Image Translation\n7:05 - Architecture Overview\n14:15 - Pseudo-Label Loss\n19:30 - Encoder Style Contrastive Loss\n25:30 - Adversarial Loss\n31:20 - Generator Style Contrastive Loss\n35:15 - Image Reconstruction Loss\n36:55 - Architecture Recap\n39:55 - Full Loss\n42:05 - Experiments\n\nPaper: https://arxiv.org/abs/2006.06500\nCode: https://github.com/clovaai/tunit\n\nAbstract:\nEvery recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, our model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at this https URL\n\nAuthors: Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.06500"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=sEG8hD64c_Q",
        "summary": "Image-to-Image translation usually requires corresponding samples or at least domain labels of the dataset. This paper removes that restriction and allows for fully unsupervised image translation of a source image to the style of one or many reference images. This is achieved by jointly training a guiding network that provides style information and pseudo-labels.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Unsupervised Image-to-Image Translation\n7:05 - Architecture Overview\n14:15 - Pseudo-Label Loss\n19:30 - Encoder Style Contrastive Loss\n25:30 - Adversarial Loss\n31:20 - Generator Style Contrastive Loss\n35:15 - Image Reconstruction Loss\n36:55 - Architecture Recap\n39:55 - Full Loss\n42:05 - Experiments\n\nPaper: https://arxiv.org/abs/2006.06500\nCode: https://github.com/clovaai/tunit\n\nAbstract:\nEvery recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, our model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at this https URL\n\nAuthors: Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.06500"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=YPfUiOMYOEE",
        "summary": "Self-supervised representation learning relies on negative samples to keep the encoder from collapsing to trivial solutions. However, this paper shows that negative samples, which are a nuisance to implement, are not necessary for learning good representation, and their algorithm BYOL is able to outperform other baselines using just positive samples.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Image Representation Learning\n3:55 - Self-Supervised Learning\n5:35 - Negative Samples\n10:50 - BYOL\n23:20 - Experiments\n30:10 - Conclusion & Broader Impact\n\nPaper: https://arxiv.org/abs/2006.07733\n\nAbstract:\nWe introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.\n\nAuthors: Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.07733"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=2lkUNDZld-4",
        "summary": "This paper proposes SimCLRv2 and shows that semi-supervised learning benefits a lot from self-supervised pre-training. And stunningly, that effect gets larger the fewer labels are available and the more parameters the model has.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Semi-Supervised Learning\n3:50 - Pre-Training via Self-Supervision\n5:45 - Contrastive Loss\n10:50 - Retaining Projection Heads\n13:10 - Supervised Fine-Tuning\n13:45 - Unsupervised Distillation & Self-Training\n18:45 - Architecture Recap\n22:25 - Experiments\n34:15 - Broader Impact\n\nPaper: https://arxiv.org/abs/2006.10029\nCode: https://github.com/google-research/simclr\n\nAbstract:\nOne paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\\% ImageNet top-1 accuracy with just 1\\% of the labels (\u226413 labeled images per class) using ResNet-50, a 10\u00d7 improvement in label efficiency over the previous state-of-the-art. With 10\\% of labels, ResNet-50 trained with our method achieves 77.5\\% top-1 accuracy, outperforming standard supervised training with all of the labels.\n\nAuthors: Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.10029"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Q5g3p9Zwjrk",
        "summary": "Implicit neural representations are created when a neural network is used to represent a signal as a function. SIRENs are a particular type of INR that can be applied to a variety of signals, such as images, sound, or 3D shapes. This is an interesting departure from regular machine learning and required me to think differently.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:15 - Implicit Neural Representations\n9:40 - Representing Images\n14:30 - SIRENs\n18:05 - Initialization\n20:15 - Derivatives of SIRENs\n23:05 - Poisson Image Reconstruction\n28:20 - Poisson Image Editing\n31:35 - Shapes with Signed Distance Functions\n45:55 - Paper Website\n48:55 - Other Applications\n50:45 - Hypernetworks over SIRENs\n54:30 - Broader Impact\n\nPaper: https://arxiv.org/abs/2006.09661\nWebsite: https://vsitzmann.github.io/siren/\n\nAbstract:\nImplicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.\n\nAuthors: Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.09661"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=qSArFEIoSbo",
        "summary": "Counting repeated actions in a video is one of the easiest tasks for humans, yet remains incredibly hard for machines. RepNet achieves state-of-the-art by creating an information bottleneck in the form of a temporal self-similarity matrix, relating video frames to each other in a way that forces the model to surface the information relevant for counting. Along with that, the authors produce a new dataset for evaluating counting models.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Problem Statement\n5:15 - Output & Loss\n6:25 - Per-Frame Embeddings\n11:20 - Temporal Self-Similarity Matrix\n19:00 - Periodicity Predictor\n25:50 - Architecture Recap\n27:00 - Synthetic Dataset\n30:15 - Countix Dataset\n31:10 - Experiments\n33:35 - Applications\n35:30 - Conclusion & Comments\n\nPaper Website: https://sites.google.com/view/repnet\nColab: https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\n\nAbstract:\nWe present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called RepNet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix (~90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos.\n\nAuthors: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://sites.google.com/view/repnet"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=LMb5tvW-UoQ",
        "summary": "Neural networks are very good at predicting systems' numerical outputs, but not very good at deriving the discrete symbolic equations that govern many physical systems. This paper combines Graph Networks with symbolic regression and shows that the strong inductive biases of these models can be used to derive accurate symbolic equations from observation data.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:10 - Problem Statement\n4:25 - Symbolic Regression\n6:40 - Graph Neural Networks\n12:05 - Inductive Biases for Physics\n15:15 - How Graph Networks compute outputs\n23:10 - Loss Backpropagation\n24:30 - Graph Network Recap\n26:10 - Analogies of GN to Newtonian Mechanics\n28:40 - From Graph Network to Equation\n33:50 - L1 Regularization of Edge Messages\n40:10 - Newtonian Dynamics Example\n43:10 - Cosmology Example\n44:45 - Conclusions & Appendix\n\nPaper: https://arxiv.org/abs/2006.11287\nCode: https://github.com/MilesCranmer/symbolic_deep_learning\n\nAbstract:\nWe develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.\n\nAuthors: Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.11287"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Hdo81GtLC_4",
        "summary": "Backpropagation is one of the central components of modern deep learning. However, it's not biologically plausible, which limits the applicability of deep learning to understand how the human brain works. Direct Feedback Alignment is a biologically plausible alternative and this paper shows that, contrary to previous research, it can be successfully applied to modern deep architectures and solve challenging tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Problem with Backpropagation\n10:25 - Direct Feedback Alignment\n21:00 - My Intuition why DFA works\n31:20 - Experiments\n\nPaper: https://arxiv.org/abs/2006.12878\nCode: https://github.com/lightonai/dfa-scales-to-modern-deep-learning\nReferenced Paper by Arild N\u00f8kland: https://arxiv.org/abs/1609.01596\n\nAbstract:\nDespite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.\n\nAuthors: Julien Launay, Iacopo Poli, Fran\u00e7ois Boniface, Florent Krzakala\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.12878"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=eI8xTdcZ6VY",
        "summary": "Object detection often does not occur in a vacuum. Static cameras, such as wildlife traps, collect lots of irregularly sampled data over a large time frame and often capture repeating or similar events. This model learns to dynamically incorporate other frames taken by the same camera into its object detection pipeline.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Problem Formulation\n2:10 - Static Camera Data\n6:45 - Architecture Overview\n10:00 - Short-Term Memory\n15:40 - Long-Term Memory\n20:10 - Quantitative Results\n22:30 - Qualitative Results\n30:10 - False Positives\n32:50 - Appendix & Conclusion\n\nPaper: https://arxiv.org/abs/1912.03538\n\nMy Video On Attention Is All You Need: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nIn static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame.\nWe apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.\n\nAuthors: Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1912.03538"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=eI8xTdcZ6VY",
        "summary": "Object detection often does not occur in a vacuum. Static cameras, such as wildlife traps, collect lots of irregularly sampled data over a large time frame and often capture repeating or similar events. This model learns to dynamically incorporate other frames taken by the same camera into its object detection pipeline.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Problem Formulation\n2:10 - Static Camera Data\n6:45 - Architecture Overview\n10:00 - Short-Term Memory\n15:40 - Long-Term Memory\n20:10 - Quantitative Results\n22:30 - Qualitative Results\n30:10 - False Positives\n32:50 - Appendix & Conclusion\n\nPaper: https://arxiv.org/abs/1912.03538\n\nMy Video On Attention Is All You Need: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nIn static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame.\nWe apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.\n\nAuthors: Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1912.03538"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=V79rRI05Lj4",
        "summary": "We've become very good at making generative models for images and classes of images, but not yet of sets of images, especially when the number of sets is unknown and can contain sets that have never been encountered during training. This paper builds a probabilistic framework and a practical implementation of a generative model for sets of images based on variational methods.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n8:05 - Architecture Overview\n20:05 - Probabilistic Model\n33:50 - Likelihood Function\n40:30 - Model Architectures\n44:20 - Loss Function & Optimization\n47:30 - Results\n58:45 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.10705\n\nAbstract:\nImages with shared characteristics naturally form sets. For example, in a face verification benchmark, images of the same identity form sets. For generative models, the standard way of dealing with sets is to represent each as a one hot vector, and learn a conditional generative model p(x|y). This representation assumes that the number of sets is limited and known, such that the distribution over sets reduces to a simple multinomial distribution. In contrast, we study a more generic problem where the number of sets is large and unknown. We introduce Set Distribution Networks (SDNs), a novel framework that learns to autoencode and freely generate sets. We achieve this by jointly learning a set encoder, set discriminator, set generator, and set prior. We show that SDNs are able to reconstruct image sets that preserve salient attributes of the inputs in our benchmark datasets, and are also able to generate novel objects/identities. We examine the sets generated by SDN with a pre-trained 3D reconstruction network and a face verification network, respectively, as a novel way to evaluate the quality of generated sets of images.\n\nAuthors: Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Carlos Guestrin, Josh M. Susskind\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.10705"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=V79rRI05Lj4",
        "summary": "We've become very good at making generative models for images and classes of images, but not yet of sets of images, especially when the number of sets is unknown and can contain sets that have never been encountered during training. This paper builds a probabilistic framework and a practical implementation of a generative model for sets of images based on variational methods.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n8:05 - Architecture Overview\n20:05 - Probabilistic Model\n33:50 - Likelihood Function\n40:30 - Model Architectures\n44:20 - Loss Function & Optimization\n47:30 - Results\n58:45 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.10705\n\nAbstract:\nImages with shared characteristics naturally form sets. For example, in a face verification benchmark, images of the same identity form sets. For generative models, the standard way of dealing with sets is to represent each as a one hot vector, and learn a conditional generative model p(x|y). This representation assumes that the number of sets is limited and known, such that the distribution over sets reduces to a simple multinomial distribution. In contrast, we study a more generic problem where the number of sets is large and unknown. We introduce Set Distribution Networks (SDNs), a novel framework that learns to autoencode and freely generate sets. We achieve this by jointly learning a set encoder, set discriminator, set generator, and set prior. We show that SDNs are able to reconstruct image sets that preserve salient attributes of the inputs in our benchmark datasets, and are also able to generate novel objects/identities. We examine the sets generated by SDN with a pre-trained 3D reconstruction network and a face verification network, respectively, as a novel way to evaluate the quality of generated sets of images.\n\nAuthors: Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Carlos Guestrin, Josh M. Susskind\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.10705"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=q7PjrmGNx5A",
        "summary": "The abundance of data on the internet is vast. Especially unlabeled images are plentiful and can be collected with ease. This model investigates a new method for incorporating unlabeled data into a supervised learning pipeline. First, a teacher model is trained in a supervised fashion. Then, that teacher is used to label the unlabeled data. Next, a larger student model is trained on the combination of all data and achieves better performance than the teacher by itself.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:05 - Semi-Supervised & Transfer Learning\n5:45 - Self-Training & Knowledge Distillation\n10:00 - Noisy Student Algorithm Overview\n20:20 - Noise Methods\n22:30 - Dataset Balancing\n25:20 - Results\n30:15 - Perturbation Robustness\n34:35 - Ablation Studies\n39:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1911.04252\nCode: https://github.com/google-research/noisystudent\nModels: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\n\nAbstract:\nWe present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.\nNoisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.\n\nAuthors: Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1911.04252"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=nxEr4VNgYOE",
        "summary": "Deep neural networks are large models and pruning has become an important part of ML product pipelines, making models small while keeping their performance high. However, the classic pruning method, Magnitude Pruning, is suboptimal in models that are obtained by transfer learning. This paper proposes a solution, called Movement Pruning and shows its superior performance.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:55 - Magnitude Pruning\n4:25 - Transfer Learning\n7:25 - The Problem with Magnitude Pruning in Transfer Learning\n9:20 - Movement Pruning\n22:20 - Experiments\n24:20 - Improvements via Distillation\n26:40 - Analysis of the Learned Weights\n\nPaper: https://arxiv.org/abs/2005.07683\nCode: https://github.com/huggingface/transformers/tree/master/examples/movement-pruning\n\nAbstract:\nMagnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.\n\nAuthors: Victor Sanh, Thomas Wolf, Alexander M. Rush\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2005.07683"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=DYBmD88vpiA",
        "summary": "Visual scenes are often comprised of sets of independent objects. Yet, current vision models make no assumptions about the nature of the pictures they look at. By imposing an objectness prior, this paper a module that is able to recognize permutation-invariant sets of objects from pixels in both supervised and unsupervised settings. It does so by introducing a slot attention module that combines an attention mechanism with dynamic routing.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Problem Formulation\n4:30 - Slot Attention Architecture\n13:30 - Slot Attention Algorithm\n21:30 - Iterative Routing Visualization\n29:15 - Experiments\n36:20 - Inference Time Flexibility\n38:35 - Broader Impact Statement\n42:05 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15055\n\nMy Video on Facebook's DETR: https://youtu.be/T35ba_VXkMY\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on Capsules: https://youtu.be/nXGHJTtFYRU\n\nAbstract:\nLearning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.\n\nAuthors: Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.15055"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=1VdEw_mGjFk",
        "summary": "Google builds a 600 billion parameter transformer to do massively multilingual, massive machine translation. Interestingly, the larger model scale does not come from increasing depth of the transformer, but from increasing width in the feedforward layers, combined with a hard routing to parallelize computations on up to 2048 TPUs. A very detailed engineering paper!\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Main Results\n5:10 - Mixture-of-Experts\n16:00 - Difference to Scaling Classic Transformers\n18:50 - Backpropagation in Mixture-of-Experts\n20:05 - MoE Routing Algorithm in GShard\n38:20 - GShard Einsum Examples\n47:40 - Massively Multilingual Translation\n56:00 - Results\n1:11:30 - Conclusion & Comments\n\nERRATA:\nI said the computation of MoE scales linearly, but actually, it's sub(!)-linear.\n\nPaper: https://arxiv.org/abs/2006.16668\n\nAbstract:\nNeural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.\n\nAuthors:\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.16668"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=q6Kyvy1zLwQ",
        "summary": "Proteins are the workhorses of almost all cellular functions and a core component of life. But despite their versatility, all proteins are built as sequences of the same 20 amino acids. These sequences can be analyzed with tools from NLP. This paper investigates the attention mechanism of a BERT model that has been trained on protein sequence data and discovers that the language model has implicitly learned non-trivial higher-order biological properties of proteins.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - From DNA to Proteins\n5:20 - BERT for Amino Acid Sequences\n8:50 - The Structure of Proteins\n12:40 - Investigating Biological Properties by Inspecting BERT\n17:45 - Amino Acid Substitution\n24:55 - Contact Maps\n30:15 - Binding Sites\n33:45 - Linear Probes\n35:25 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15222\nCode: https://github.com/salesforce/provis\n\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nTransformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at this https URL.\n\nAuthors: Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.15222"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=q6Kyvy1zLwQ",
        "summary": "Proteins are the workhorses of almost all cellular functions and a core component of life. But despite their versatility, all proteins are built as sequences of the same 20 amino acids. These sequences can be analyzed with tools from NLP. This paper investigates the attention mechanism of a BERT model that has been trained on protein sequence data and discovers that the language model has implicitly learned non-trivial higher-order biological properties of proteins.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - From DNA to Proteins\n5:20 - BERT for Amino Acid Sequences\n8:50 - The Structure of Proteins\n12:40 - Investigating Biological Properties by Inspecting BERT\n17:45 - Amino Acid Substitution\n24:55 - Contact Maps\n30:15 - Binding Sites\n33:45 - Linear Probes\n35:25 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15222\nCode: https://github.com/salesforce/provis\n\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nTransformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at this https URL.\n\nAuthors: Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.15222"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=qFRfnIRMNlk",
        "summary": "#machinelearning #ai #google\n\nThe high-level architecture of CNNs has not really changed over the years. We tend to build high-resolution low-dimensional layers first, followed by ever more coarse, but deep layers. This paper challenges this decades-old heuristic and uses neural architecture search to find an alternative, called SpineNet that employs multiple rounds of re-scaling and long-range skip connections.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Problem Statement\n2:30 - The Problem with Current Architectures\n8:20 - Scale-Permuted Networks\n11:40 - Neural Architecture Search\n14:00 - Up- and Downsampling\n19:10 - From ResNet to SpineNet\n24:20 - Ablations\n27:00 - My Idea: Attention Routing for CNNs\n29:55 - More Experiments\n34:45 - Conclusion & Comments\n\nPapers: https://arxiv.org/abs/1912.05027\nCode: https://github.com/tensorflow/tpu/tree/master/models/official/detection\n\nAbstract:\nConvolutional neural networks typically encode an input image into a series of intermediate features with decreasing resolutions. While this structure is suited to classification tasks, it does not perform well for tasks requiring simultaneous recognition and localization (e.g., object detection). The encoder-decoder architectures are proposed to resolve this by applying a decoder network onto a backbone model designed for classification tasks. In this paper, we argue encoder-decoder architecture is ineffective in generating strong multi-scale features because of the scale-decreased backbone. We propose SpineNet, a backbone with scale-permuted intermediate features and cross-scale connections that is learned on an object detection task by Neural Architecture Search. Using similar building blocks, SpineNet models outperform ResNet-FPN models by ~3% AP at various scales while using 10-20% fewer FLOPs. In particular, SpineNet-190 achieves 52.5% AP with a MaskR-CNN detector and achieves 52.1% AP with a RetinaNet detector on COCO for a single model without test-time augmentation, significantly outperforms prior art of detectors. SpineNet can transfer to classification tasks, achieving 5% top-1 accuracy improvement on a challenging iNaturalist fine-grained dataset. Code is at: this https URL.\n\nAuthors: Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, Xiaodan Song\n\nThumbnail art by Lucas Ferreira\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1912.05027"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=hAooAOFRsYc",
        "summary": "#ai #attention #transformer #deeplearning\n\nTransformers are famous for two things: Their superior performance and their insane requirements of compute and memory. This paper reformulates the attention mechanism in terms of kernel functions and obtains a linear formulation, which reduces these requirements. Surprisingly, this formulation also surfaces an interesting connection between autoregressive transformers and RNNs.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Softmax Attention & Transformers\n8:40 - Quadratic Complexity of Softmax Attention\n9:40 - Generalized Attention Mechanism\n13:45 - Kernels\n20:40 - Linear Attention\n25:20 - Experiments\n28:30 - Intuition on Linear Attention\n33:55 - Connecting Autoregressive Transformers and RNNs\n41:30 - Caveats with the RNN connection\n46:00 - More Results & Conclusion\n\nPaper: https://arxiv.org/abs/2006.16236\nWebsite: https://linear-transformers.com/\nCode: https://github.com/idiap/fast-transformers\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nTransformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \ue23b(N2) to \ue23b(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\nAuthors: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.16236"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=3jT1qJ8ETzk",
        "summary": "Supermasks are binary masks of a randomly initialized neural network that result in the masked network performing well on a particular task. This paper considers the problem of (sequential) Lifelong Learning and trains one Supermask per Task, while keeping the randomly initialized base network constant. By minimizing the output entropy, the system can automatically derive the Task ID of a data point at inference time and distinguish up to 2500 tasks automatically.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Catastrophic Forgetting\n5:20 - Supermasks\n9:35 - Lifelong Learning using Supermasks\n11:15 - Inference Time Task Discrimination by Entropy\n15:05 - Mask Superpositions\n24:20 - Proof-of-Concept, Task Given at Inference\n30:15 - Binary Maximum Entropy Search\n32:00 - Task Not Given at Inference\n37:15 - Task Not Given at Training\n41:35 - Ablations\n45:05 - Superfluous Neurons\n51:10 - Task Selection by Detecting Outliers\n57:40 - Encoding Masks in Hopfield Networks\n59:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.14769\nCode: https://github.com/RAIVNLab/supsup\n\nMy Video about Lottery Tickets: https://youtu.be/ZVVnvZdUMUk\nMy Video about Supermasks: https://youtu.be/jhCInVFE2sc\n\nAbstract:\nWe present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.\n\nAuthors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.14769"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Jqvb7jp4Nm8",
        "summary": "I take a closer look at \"Supermasks in Superposition\" after I've already done a video on it. Specifically, I look at: 1. The intuition and theoretical justification behind the G objective, 2. Whether Supermasks and Superposition can be viewed as two distinct ideas and 3. The Paper's Broader Impact Statement.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - SupSup Recap\n4:00 - In-Depth Analysis of the G Objective\n20:30 - Superposition without Supermasks\n25:40 - Broader Impact Statement\n36:40 - Conclusion\n37:20 - Live Coding\n\nPart 1 on SupSup: https://youtu.be/3jT1qJ8ETzk\nMy Code: https://colab.research.google.com/drive/1bEcppdN6qZRpEFplIiv41ZI3vDwDjcvC?usp=sharing\nPaper: https://arxiv.org/abs/2006.14769\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.14769"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Jqvb7jp4Nm8",
        "summary": "I take a closer look at \"Supermasks in Superposition\" after I've already done a video on it. Specifically, I look at: 1. The intuition and theoretical justification behind the G objective, 2. Whether Supermasks and Superposition can be viewed as two distinct ideas and 3. The Paper's Broader Impact Statement.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - SupSup Recap\n4:00 - In-Depth Analysis of the G Objective\n20:30 - Superposition without Supermasks\n25:40 - Broader Impact Statement\n36:40 - Conclusion\n37:20 - Live Coding\n\nPart 1 on SupSup: https://youtu.be/3jT1qJ8ETzk\nMy Code: https://colab.research.google.com/drive/1bEcppdN6qZRpEFplIiv41ZI3vDwDjcvC?usp=sharing\nPaper: https://arxiv.org/abs/2006.14769\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.14769"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=x6T1zMSE4Ts",
        "summary": "VAEs have been traditionally hard to train at high resolutions and unstable when going deep with many layers. In addition, VAE samples are often more blurry and less crisp than those from GANs. This paper details all the engineering choices necessary to successfully train a deep hierarchical VAE that exhibits global consistency and astounding sharpness at high resolutions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Variational Autoencoders\n8:25 - Hierarchical VAE Decoder\n12:45 - Output Samples\n15:00 - Hierarchical VAE Encoder\n17:20 - Engineering Decisions\n22:10 - KL from Deltas\n26:40 - Experimental Results\n28:40 - Appendix\n33:00 - Conclusion\n\nPaper: https://arxiv.org/abs/2007.03898\n\nAbstract:\nNormalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\u00d7256 pixels.\n\nAuthors: Arash Vahdat, Jan Kautz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.03898"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=x6T1zMSE4Ts",
        "summary": "VAEs have been traditionally hard to train at high resolutions and unstable when going deep with many layers. In addition, VAE samples are often more blurry and less crisp than those from GANs. This paper details all the engineering choices necessary to successfully train a deep hierarchical VAE that exhibits global consistency and astounding sharpness at high resolutions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Variational Autoencoders\n8:25 - Hierarchical VAE Decoder\n12:45 - Output Samples\n15:00 - Hierarchical VAE Encoder\n17:20 - Engineering Decisions\n22:10 - KL from Deltas\n26:40 - Experimental Results\n28:40 - Appendix\n33:00 - Conclusion\n\nPaper: https://arxiv.org/abs/2007.03898\n\nAbstract:\nNormalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\u00d7256 pixels.\n\nAuthors: Arash Vahdat, Jan Kautz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.03898"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=v-ZxzTSpmk4",
        "summary": "Neural networks for implicit representations, such as SIRENs, have been very successful at modeling natural signals. However, in the classical approach, each data point requires its own neural network to be fit. This paper extends implicit representations to an entire dataset by introducing latent vectors of data points to SIRENs. Interestingly, the paper shows that such latent vectors can be obtained without the need for an explicit encoder, by simply looking at the negative gradient of the zero-vector through the representation function.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:10 - Implicit Generative Models\n5:30 - Implicitly Represent a Dataset\n11:00 - Gradient Origin Networks\n23:55 - Relation to Gradient Descent\n28:05 - Messing with their Code\n37:40 - Implicit Encoders\n38:50 - Using GONs as classifiers\n40:55 - Experiments & Conclusion\n\nPaper: https://arxiv.org/abs/2007.02798\nCode: https://github.com/cwkx/GON\nProject Page: https://cwkx.github.io/data/GON/\n\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\n\nAbstract:\nThis paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.\n\nAuthors: Sam Bond-Taylor, Chris G. Willcocks\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.02798"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=v-ZxzTSpmk4",
        "summary": "Neural networks for implicit representations, such as SIRENs, have been very successful at modeling natural signals. However, in the classical approach, each data point requires its own neural network to be fit. This paper extends implicit representations to an entire dataset by introducing latent vectors of data points to SIRENs. Interestingly, the paper shows that such latent vectors can be obtained without the need for an explicit encoder, by simply looking at the negative gradient of the zero-vector through the representation function.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:10 - Implicit Generative Models\n5:30 - Implicitly Represent a Dataset\n11:00 - Gradient Origin Networks\n23:55 - Relation to Gradient Descent\n28:05 - Messing with their Code\n37:40 - Implicit Encoders\n38:50 - Using GONs as classifiers\n40:55 - Experiments & Conclusion\n\nPaper: https://arxiv.org/abs/2007.02798\nCode: https://github.com/cwkx/GON\nProject Page: https://cwkx.github.io/data/GON/\n\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\n\nAbstract:\nThis paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.\n\nAuthors: Sam Bond-Taylor, Chris G. Willcocks\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.02798"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=5IRlUVrEVL8",
        "summary": "#ai #research #optimization\n\nDeep Ensembles work surprisingly well for improving the generalization capabilities of deep neural networks. Surprisingly, they outperform Bayesian Networks, which are - in theory - doing the same thing. This paper investigates how Deep Ensembles are especially suited to capturing the non-convex loss landscape of neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Deep Ensembles\n4:15 - The Solution Space of Deep Networks\n7:30 - Bayesian Models\n9:00 - The Ensemble Effect\n10:25 - Experiment Setup\n11:30 - Solution Equality While Training\n19:40 - Tracking Multiple Trajectories\n21:20 - Similarity of Independent Solutions\n24:10 - Comparison to Baselines\n30:10 - Weight Space Cross-Sections\n35:55 - Diversity vs Accuracy\n41:00 - Comparing Ensembling Methods\n44:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1912.02757\n\nAbstract:\nDeep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.\n\nAuthors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1912.02757"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=5IRlUVrEVL8",
        "summary": "#ai #research #optimization\n\nDeep Ensembles work surprisingly well for improving the generalization capabilities of deep neural networks. Surprisingly, they outperform Bayesian Networks, which are - in theory - doing the same thing. This paper investigates how Deep Ensembles are especially suited to capturing the non-convex loss landscape of neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Deep Ensembles\n4:15 - The Solution Space of Deep Networks\n7:30 - Bayesian Models\n9:00 - The Ensemble Effect\n10:25 - Experiment Setup\n11:30 - Solution Equality While Training\n19:40 - Tracking Multiple Trajectories\n21:20 - Similarity of Independent Solutions\n24:10 - Comparison to Baselines\n30:10 - Weight Space Cross-Sections\n35:55 - Diversity vs Accuracy\n41:00 - Comparing Ensembling Methods\n44:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1912.02757\n\nAbstract:\nDeep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.\n\nAuthors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1912.02757"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=rFwQDDbYTm4",
        "summary": "#ai #dqn #deepmind\n\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Arcade Learning Environment\n4:25 - Deep Reinforcement Learning\n9:20 - Deep Q-Learning\n26:30 - Experience Replay\n32:25 - Network Architecture\n33:50 - Experiments\n37:45 - Conclusion\n\nPaper: https://arxiv.org/abs/1312.5602\n\nAbstract:\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1312.5602"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=rFwQDDbYTm4",
        "summary": "#ai #dqn #deepmind\n\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Arcade Learning Environment\n4:25 - Deep Reinforcement Learning\n9:20 - Deep Q-Learning\n26:30 - Experience Replay\n32:25 - Network Architecture\n33:50 - Experiments\n37:45 - Conclusion\n\nPaper: https://arxiv.org/abs/1312.5602\n\nAbstract:\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1312.5602"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Nq3auVtvd9Q",
        "summary": "#ai #research #alexnet\n\nAlexNet was the start of the deep learning revolution. Up until 2012, the best computer vision systems relied on hand-crafted features and highly specialized algorithms to perform object classification. This paper was the first to successfully train a deep convolutional neural network on not one, but two GPUs and managed to outperform the competition on ImageNet by an order of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - The necessity of larger models\n6:20 - Why CNNs?\n11:05 - ImageNet\n12:05 - Model Architecture Overview\n14:35 - ReLU Nonlinearities\n18:45 - Multi-GPU training\n21:30 - Classification Results\n24:30 - Local Response Normalization\n28:05 - Overlapping Pooling\n32:25 - Data Augmentation\n38:30 - Dropout\n40:30 - More Results\n43:50 - Conclusion\n\nPaper: http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\n\nAbstract:\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n\nAuthors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=eyxmSmjmNS0",
        "summary": "#ai #deeplearning #gan\n\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:50 - Motivation\n8:40 - Minimax Loss Function\n13:20 - Intuition Behind the Loss\n19:30 - GAN Algorithm\n22:05 - Theoretical Analysis\n27:00 - Experiments\n33:10 - Advantages & Disadvantages\n35:00 - Conclusion\n\nPaper: https://arxiv.org/abs/1406.2661\n\nAbstract:\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1406.2661"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=eyxmSmjmNS0",
        "summary": "#ai #deeplearning #gan\n\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:50 - Motivation\n8:40 - Minimax Loss Function\n13:20 - Intuition Behind the Loss\n19:30 - GAN Algorithm\n22:05 - Theoretical Analysis\n27:00 - Experiments\n33:10 - Advantages & Disadvantages\n35:00 - Conclusion\n\nPaper: https://arxiv.org/abs/1406.2661\n\nAbstract:\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1406.2661"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=yexR53My2O4",
        "summary": "#ai #research #word2vec\n\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:50 - Distributed Word Representations\n5:40 - Skip-Gram Model\n12:00 - Hierarchical Softmax\n14:55 - Negative Sampling\n22:30 - Mysterious 3/4 Power\n25:50 - Frequent Words Subsampling\n28:15 - Empirical Results\n29:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1310.4546\nCode: https://code.google.com/archive/p/word2vec/\n\nAbstract:\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\nAuthors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1310.4546"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=yexR53My2O4",
        "summary": "#ai #research #word2vec\n\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:50 - Distributed Word Representations\n5:40 - Skip-Gram Model\n12:00 - Hierarchical Softmax\n14:55 - Negative Sampling\n22:30 - Mysterious 3/4 Power\n25:50 - Frequent Words Subsampling\n28:15 - Empirical Results\n29:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1310.4546\nCode: https://code.google.com/archive/p/word2vec/\n\nAbstract:\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\nAuthors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1310.4546"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=GWt6Fu05voI",
        "summary": "#ai #research #resnet\n\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper's invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:45 - The Problem with Depth\n3:15 - VGG-Style Networks\n6:00 - Overfitting is Not the Problem\n7:25 - Motivation for Residual Connections\n10:25 - Residual Blocks\n12:10 - From VGG to ResNet\n18:50 - Experimental Results\n23:30 - Bottleneck Blocks\n24:40 - Deeper ResNets\n28:15 - More Results\n29:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1512.03385\n\nAbstract:\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=GWt6Fu05voI",
        "summary": "#ai #research #resnet\n\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper's invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:45 - The Problem with Depth\n3:15 - VGG-Style Networks\n6:00 - Overfitting is Not the Problem\n7:25 - Motivation for Residual Connections\n10:25 - Residual Blocks\n12:10 - From VGG to ResNet\n18:50 - Experimental Results\n23:30 - Bottleneck Blocks\n24:40 - Deeper ResNets\n28:15 - More Results\n29:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1512.03385\n\nAbstract:\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=a6v92P0EbJc",
        "summary": "#ai #research #machinelearning\n\nNeural Architecture Search is typically very slow and resource-intensive. A meta-controller has to train many hundreds or thousands of different models to find a suitable building plan. This paper proposes to use statistics of the Jacobian around data points to estimate the performance of proposed architectures at initialization. This method does not require training and speeds up NAS by orders of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n0:50 - Neural Architecture Search\n4:15 - Controller-based NAS\n7:35 - Architecture Search Without Training\n9:30 - Linearization Around Datapoints\n14:10 - Linearization Statistics\n19:00 - NAS-201 Benchmark\n20:15 - Experiments\n34:15 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04647\nCode: https://github.com/BayesWatch/nas-without-training\n\nAbstract:\nThe time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the NAS-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU. Code to reproduce our experiments is available at this https URL.\n\nAuthors: Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04647"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=a4VvcmqnkhY",
        "summary": "#ai #research #machinelearning\n\nOnline Reinforcement Learning is a flourishing field with countless methods for practitioners to choose from. However, each of those methods comes with a plethora of hyperparameter choices. This paper builds a unified framework for five continuous control tasks and investigates in a large-scale study the effects of these choices. As a result, they come up with a set of recommendations for future research and applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:55 - Parameterized Agents\n7:00 - Unified Online RL and Parameter Choices\n14:10 - Policy Loss\n16:40 - Network Architecture\n20:25 - Initial Policy\n24:20 - Normalization & Clipping\n26:30 - Advantage Estimation\n28:55 - Training Setup\n33:05 - Timestep Handling\n34:10 - Optimizers\n35:05 - Regularization\n36:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05990\n\nAbstract:\nIn recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress (Engstrom'20). As a step towards filling that gap, we implement over 50 such \"choices\" in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.\n\nAuthors: Marcin Andrychowicz, Anton Raichuk, Piotr Sta\u0144czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.05990"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=a4VvcmqnkhY",
        "summary": "#ai #research #machinelearning\n\nOnline Reinforcement Learning is a flourishing field with countless methods for practitioners to choose from. However, each of those methods comes with a plethora of hyperparameter choices. This paper builds a unified framework for five continuous control tasks and investigates in a large-scale study the effects of these choices. As a result, they come up with a set of recommendations for future research and applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:55 - Parameterized Agents\n7:00 - Unified Online RL and Parameter Choices\n14:10 - Policy Loss\n16:40 - Network Architecture\n20:25 - Initial Policy\n24:20 - Normalization & Clipping\n26:30 - Advantage Estimation\n28:55 - Training Setup\n33:05 - Timestep Handling\n34:10 - Optimizers\n35:05 - Regularization\n36:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05990\n\nAbstract:\nIn recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress (Engstrom'20). As a step towards filling that gap, we implement over 50 such \"choices\" in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.\n\nAuthors: Marcin Andrychowicz, Anton Raichuk, Piotr Sta\u0144czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.05990"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=nv6oFDp6rNQ",
        "summary": "#ai #transformer #attention\n\nHopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Binary Hopfield Networks\n5:55 - Continuous Hopfield Networks\n8:15 - Update Rules & Energy Functions\n13:30 - Connection to Transformers\n14:35 - Hopfield Attention Layers\n26:45 - Theoretical Analysis\n48:10 - Investigating BERT\n1:02:30 - Immune Repertoire Classification\n\nPaper: https://arxiv.org/abs/2008.02217\nCode: https://github.com/ml-jku/hopfield-layers\nImmune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nWe show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL\n\nAuthors: Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2008.02217"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=v2GRWzIhaqQ",
        "summary": "#ai #neuroscience #rl\n\nReinforcement Learning is a powerful tool, but it lacks biological plausibility because it learns a fixed policy network. Animals use neuroplasticity to reconfigure their policies on the fly and quickly adapt to new situations. This paper uses Hebbian Learning, a biologically inspired technique, to have agents adapt random networks to high-performing solutions as an episode is progressing, leading to agents that can reconfigure themselves in response to new observations.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Reinforcement Learning vs Hebbian Plasticity\n9:00 - Episodes in Hebbian Learning\n10:00 - Hebbian Plasticity Rules\n18:10 - Quadruped Experiment Results\n21:20 - Evolutionary Learning of Hebbian Plasticity\n29:10 - More Experimental Results\n34:50 - Conclusions\n35:30 - Broader Impact Statement\n\nVideos: https://twitter.com/risi1979/status/1280544779630186499\nPaper: https://arxiv.org/abs/2007.02686\n\nAbstract:\nLifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to different morphological damage in the absence of any explicit reward or error signal.\n\nAuthors: Elias Najarro, Sebastian Risi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.02686"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=lj-LGrnh1oU",
        "summary": "#ai #tech #science\n\nOpen Domain Question Answering is one of the most challenging tasks in NLP. When answering a question, the model is able to retrieve arbitrary documents from an indexed corpus to gather more information. REALM shows how Masked Language Modeling (MLM) pretraining can be used to train a retriever for relevant documents in an end-to-end fashion and improves over state-of-the-art by a significant margin.\n\nOUTLINE:\n0:00 - Introduction & Overview\n4:30 - World Knowledge in Language Models\n8:15 - Masked Language Modeling for Latent Document Retrieval\n14:50 - Problem Formulation\n17:30 - Knowledge Retriever Model using MIPS\n23:50 - Question Answering Model\n27:50 - Architecture Recap\n29:55 - Analysis of the Loss Gradient\n34:15 - Initialization using the Inverse Cloze Task\n41:40 - Prohibiting Trivial Retrievals\n44:05 - Null Document\n45:00 - Salient Span Masking\n50:15 - My Idea on Salient Span Masking\n51:50 - Experimental Results and Ablations\n57:30 - Concrete Example from the Model\n\nPaper: https://arxiv.org/abs/2002.08909\nCode: https://github.com/google-research/language/tree/master/language/realm\n\nMy Video on GPT-3: https://www.youtube.com/watch?v=SY5PvZrJhLE\nMy Video on BERT: https://www.youtube.com/watch?v=-9evrZnBorM\nMy Video on Word2Vec: https://www.youtube.com/watch?v=yexR53My2O4\n\nAbstract:\nLanguage model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\n\nAuthors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2002.08909"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=9-o2aAoN0rY",
        "summary": "#ai #research #reinforcementlearning\n\nReinforcement Learning is a powerful tool, but it is also incredibly data-hungry. Given a new task, an RL agent has to learn a good policy entirely from scratch. This paper proposes a new framework that allows an agent to carry over knowledge from previous tasks into solving new tasks, even deriving zero-shot policies that perform well on completely new reward functions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n6:25 - Q-Learning Primer\n11:40 - Multiple Rewards, Multiple Policies\n14:25 - Example Environment\n17:35 - Tasks as Linear Mixtures of Features\n24:15 - Successor Features\n28:00 - Zero-Shot Policy for New Tasks\n35:30 - Results on New Task W3\n37:00 - Inferring the Task via Regression\n39:20 - The Influence of the Given Policies\n48:40 - Learning the Feature Functions\n50:30 - More Complicated Tasks\n51:40 - Life-Long Learning, Comments & Conclusion\n\nPaper: https://www.pnas.org/content/early/2020/08/13/1907370117\n\nMy Video on Successor Features: https://youtu.be/KXEEqcwXn8w\n\nAbstract:\nThe combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.\n\nAuthors:\nAndr\u00e9 Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://www.pnas.org/content/early/2020/08/13/1907370117"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=G2sr1g6rLdE",
        "summary": "#ai #research #privacy\n\nData is the modern gold. Neural classifiers can improve their performance by training on more data, but given a trained classifier, it's difficult to tell what data it was trained on. This is especially relevant if you have proprietary or personal data and you want to make sure that other people don't use it to train their models. This paper introduces a method to mark a dataset with a hidden \"radioactive\" tag, such that any resulting classifier will clearly exhibit this tag, which can be detected.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - How Neural Classifiers Work\n5:45 - Radioactive Marking via Adding Features\n13:55 - Random Vectors in High-Dimensional Spaces\n18:05 - Backpropagation of the Fake Features\n21:00 - Re-Aligning Feature Spaces\n25:00 - Experimental Results\n28:55 - Black-Box Test\n32:00 - Conclusion & My Thoughts\n\nPaper: https://arxiv.org/abs/2002.00937\n\nAbstract:\nWe want to detect whether a particular image dataset has been used to train a model. We propose a new technique, \\emph{radioactive data}, that makes imperceptible changes to this dataset such that any model trained on it will bear an identifiable mark. The mark is robust to strong variations such as different architectures or optimization methods. Given a trained model, our technique detects the use of radioactive data and provides a level of confidence (p-value). Our experiments on large-scale benchmarks (Imagenet), using standard architectures (Resnet-18, VGG-16, Densenet-121) and training procedures, show that we can detect usage of radioactive data with high confidence (p &lt; 10^-4) even when only 1% of the data used to trained our model is radioactive. Our method is robust to data augmentation and the stochasticity of deep network optimization. As a result, it offers a much higher signal-to-noise ratio than data poisoning and backdoor methods.\n\nAuthors: Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\u00e9 J\u00e9gou\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2002.00937"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=hv3UO3G0Ofo",
        "summary": "#ai #machinelearning #attention\n\nConvolutional Neural Networks have dominated image processing for the last decade, but transformers are quickly replacing traditional models. This paper proposes a fully attentional model for images by combining learned Positional Embeddings with Axial Attention. This new model can compete with CNNs on image classification and achieve state-of-the-art in various image segmentation tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - This Paper's Contributions\n6:20 - From Convolution to Self-Attention for Images\n16:30 - Learned Positional Embeddings\n24:20 - Propagating Positional Embeddings through Layers\n27:00 - Traditional vs Position-Augmented Attention\n31:10 - Axial Attention\n44:25 - Replacing Convolutions in ResNet\n46:10 - Experimental Results & Examples\n\nPaper: https://arxiv.org/abs/2003.07853\nCode: https://github.com/csrhddlam/axial-deeplab\n\nMy Video on BigBird: https://youtu.be/WVPE62Gk3EM\nMy Video on ResNet: https://youtu.be/GWt6Fu05voI\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nConvolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.\n\nAuthors: Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2003.07853"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=EbHUU-gLyRA",
        "summary": "#ai #biology #machinelearning\n\nNeural Cellular Automata are models for how living creatures can use local message passing to reach global consensus without a central authority. This paper teaches pixels of an image to communicate with each other and figure out as a group which digit they represent. On the way, the authors have to deal with pesky side-effects that come from applying the Cross-Entropy Loss in combination with a Softmax layer, but ultimately achieve a self-sustaining, stable and continuous algorithm that models living systems.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Neural Cellular Automata\n7:30 - Global Agreement via Message-Passing\n11:05 - Neural CAs as Recurrent Convolutions\n14:30 - Training Continuously Alive Systems\n17:30 - Problems with Cross-Entropy\n26:10 - Out-of-Distribution Robustness\n27:10 - Chimeric Digits\n27:45 - Visualizing Latent State Dimensions\n29:05 - Conclusion & Comments\n\nPaper: https://distill.pub/2020/selforg/mnist/\n\nMy Video on Neural CAs: https://youtu.be/9Kec_7WFyp0\n\nAbstract:\nGrowing Neural Cellular Automata [1] demonstrated how simple cellular automata (CAs) can learn to self-organise into complex shapes while being resistant to perturbations. Such a computational model approximates a solution to an open question in biology, namely, how do cells cooperate to create a complex multicellular anatomy and work to regenerate it upon damage? The model parameterizing the cells\u2019 rules is parameter-efficient, end-to-end differentiable, and illustrates a new approach to modeling the regulation of anatomical homeostasis. In this work, we use a version of this model to show how CAs can be applied to a common task in machine learning: classification. We pose the question: can CAs use local message passing to achieve global agreement on what digit they compose?\n\nAuthors: Ettore Randazzo, Alexander Mordvintsev, Eyvind Niklasson, Michael Levin, Sam Greydanus\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://distill.pub/2020/selforg/mnist/"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=vLTmnaMpQCs",
        "summary": "#summarization #gpt3 #openai\n\nText Summarization is a hard task, both in training and evaluation. Training is usually done maximizing the log-likelihood of a human-generated reference summary, while evaluation is performed using overlap-based metrics like ROUGE. Both significantly undervalue the breadth and intricacies of language and the nature of the information contained in text summaries. This paper by OpenAI includes direct human feedback both in evaluation and - via reward model proxies - in training. The final model even outperforms single humans when judged by other humans and is an interesting application of using reinforcement learning together with humans in the loop.\n\nOUTLINE:\n0:00 - Intro & Overview\n5:35 - Summarization as a Task\n7:30 - Problems with the ROUGE Metric\n10:10 - Training Supervised Models\n12:30 - Main Results\n16:40 - Including Human Feedback with Reward Models & RL\n26:05 - The Unknown Effect of Better Data\n28:30 - KL Constraint & Connection to Adversarial Examples\n37:15 - More Results\n39:30 - Understanding the Reward Model\n41:50 - Limitations & Broader Impact\n\nPaper: https://arxiv.org/abs/2009.01325\nBlog: https://openai.com/blog/learning-to-summarize-with-human-feedback/\nCode: https://github.com/openai/summarize-from-feedback\nSamples: https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/\n\nMy Video on GPT-3: https://youtu.be/SY5PvZrJhLE\nMy Video on GPT-2: https://youtu.be/u1_qMdb0kYU\n\nAbstract:\nAs language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\n\nAuthors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2009.01325"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=MQ89be_685o",
        "summary": "#ai #research #hardware\n\nWe like to think that ideas in research succeed because of their merit, but this story is likely incomplete. The term \"hardware lottery\" describes the fact that certain algorithmic ideas are successful because they happen to be suited well to the prevalent hardware, whereas other ideas, which would be equally viable, are left behind because no accelerators for them exists. This paper is part history, part opinion and gives lots of inputs to think about.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - The Hardware Lottery\n8:30 - Sections Overview\n11:30 - Why ML researchers are disconnected from hardware\n16:50 - Historic Examples of Hardware Lotteries\n29:05 - Are we in a Hardware Lottery right now?\n39:55 - GPT-3 as an Example\n43:40 - Comparing Scaling Neural Networks to Human Brains\n46:00 - The Way Forward\n49:25 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2009.06489\nWebsite: https://hardwarelottery.github.io/\n\nAbstract:\nHardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which makes it increasingly costly to stray off of the beaten path of research ideas.\n\nAuthors: Sara Hooker\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2009.06489"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=3baFTP0uYOc",
        "summary": "#ai #research #optimization\n\nOptimization is still the domain of hand-crafted, simple algorithms. An ML engineer not only has to pick a suitable one for their problem but also often do grid-search over various hyper-parameters. This paper proposes to learn a single, unified optimization algorithm, given not by an equation, but by an LSTM-based neural network, to act as an optimizer for any deep learning problem, and ultimately to optimize itself.\n\nOUTLINE:\n0:00 - Intro & Outline\n2:20 - From Hand-Crafted to Learned Features\n4:25 - Current Optimization Algorithm\n9:40 - Learned Optimization\n15:50 - Optimizer Architecture\n22:50 - Optimizing the Optimizer using Evolution Strategies\n30:30 - Task Dataset\n34:00 - Main Results\n36:50 - Implicit Regularization in the Learned Optimizer\n41:05 - Generalization across Tasks\n41:40 - Scaling Up\n45:30 - The Learned Optimizer Trains Itself\n47:20 - Pseudocode\n49:45 - Broader Impact Statement\n52:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2009.11243\n\nAbstract:\nMuch as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.\n\nAuthors: Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, Jascha Sohl-Dickstein\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2009.11243"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=TrdevFK_am4",
        "summary": "#ai #research #transformers\n\nTransformers are Ruining Convolutions. This paper, under review at ICLR, shows that given enough data, a standard Transformer can outperform Convolutional Neural Networks in image recognition tasks, which are classically tasks where CNNs excel. In this Video, I explain the architecture of the Vision Transformer (ViT), the reason why it works better and rant about why double-bline peer review is broken.\n\nOUTLINE:\n0:00 - Introduction\n0:30 - Double-Blind Review is Broken\n5:20 - Overview\n6:55 - Transformers for Images\n10:40 - Vision Transformer Architecture\n16:30 - Experimental Results\n18:45 - What does the Model Learn?\n21:00 - Why Transformers are Ruining Everything\n27:45 - Inductive Biases in Transformers\n29:05 - Conclusion & Comments\n\nPaper (Under Review): https://openreview.net/forum?id=YicbFdNTTy\nArxiv version: https://arxiv.org/abs/2010.11929\n\nBiT Paper: https://arxiv.org/pdf/1912.11370.pdf\nImageNet-ReaL Paper: https://arxiv.org/abs/2006.07159\n\nMy Video on BiT (Big Transfer): https://youtu.be/k1GOF2jmX7c\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on ResNets: https://youtu.be/GWt6Fu05voI\n\n\nAbstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to sequences of image patches. When pre-trained on large amounts of data and transferred to multiple recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc), Vision Transformer attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\nAuthors: Anonymous / Under Review\n\nErrata:\n- Patches are not flattened, but vectorized\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2010.11929"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=DiNzQP7kK-s",
        "summary": "#ai #research #optimization\n\nDeep Learning famously gives rise to very complex, non-linear optimization problems that cannot be solved analytically. Therefore, the choice of a suitable optimization algorithm can often make or break the training of a Deep Neural Network. Yet, the literature is full with hundreds of different algorithms, each claiming to be superior and selecting one of them is mostly done based on popular opinion or anecdotes. This paper investigates 14 of the most popular optimizers in a standardized benchmark and even though there is no clear winner, it can give some recommendations as a result.\n\nOUTLINE:\n0:00 - Introduction & Overview\n2:15 - The Overwhelming Amount of Optimizers\n5:50 - Compared Optimizers\n6:50 - Default Parameters & Tuning Distribution\n13:10 - Deep Learning Problems Considered\n16:45 - Tuning on Single Seeds\n23:15 - Results & Interpretation\n34:00 - Learning Rate Schedules & Noise\n36:10 - Conclusions & Comments\n\nPaper: https://arxiv.org/abs/2007.01547\nRaw Results: https://github.com/SirRob1997/Crowded-Valley---Results\n\nAbstract:\nChoosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.\n\nAuthors: Robin M. Schmidt, Frank Schneider, Philipp Hennig\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.01547"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=3qxJ2WD8p4w",
        "summary": "#ai #research #attention\n\nTransformers, having already captured NLP, have recently started to take over the field of Computer Vision. So far, the size of images as input has been challenging, as the Transformers' Attention Mechanism's memory requirements grows quadratic in its input size. LambdaNetworks offer a way around this requirement and capture long-range interactions without the need to build expensive attention maps. They reach a new state-of-the-art in ImageNet and compare favorably to both Transformers and CNNs in terms of efficiency.\n\nOUTLINE:\n0:00 - Introduction & Overview\n6:25 - Attention Mechanism Memory Requirements\n9:30 - Lambda Layers vs Attention Layers\n17:10 - How Lambda Layers Work\n31:50 - Attention Re-Appears in Lambda Layers\n40:20 - Positional Encodings\n51:30 - Extensions and Experimental Comparisons\n58:00 - Code\n\nPaper: https://openreview.net/forum?id=xTJEN-ggl1b\nLucidrains' Code: https://github.com/lucidrains/lambda-networks\n\nAbstract:\nWe present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda layer, captures such interactions by transforming available contexts into linear functions,  termed lambdas,  and applying these linear functions to each input separately.  Lambda layers are versatile and may be implemented to model content and position-based interactions in global, local or masked contexts.  As they bypass the need for expensive attention maps, lambda layers can routinely be applied to inputs of length in the thousands, en-abling their applications to long sequences or high-resolution images. The resulting neural network architectures, LambdaNetworks, are computationally efficient and simple to implement using direct calls to operations available in modern neural network libraries.  Experiments on ImageNet classification and COCO object detection  and  instance  segmentation  demonstrate  that  LambdaNetworks  significantly  outperform  their  convolutional  and  attentional  counterparts  while  being more computationally efficient. Finally, we introduce LambdaResNets, a family of LambdaNetworks, that considerably improve the speed-accuracy tradeoff of image classification models. LambdaResNets reach state-of-the-art accuracies on ImageNet while being \u223c4.5x faster than the popular EfficientNets on modern machine learning accelerators.\n\nAuthors: Anonymous\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://openreview.net/forum?id=xTJEN-ggl1b"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=xJrKIPwVwGM",
        "summary": "#ai #research #attention\n\nTransformers have huge memory and compute requirements because they construct an Attention matrix, which grows quadratically in the size of the input. The Performer is a model that uses random positive orthogonal features to construct an unbiased estimator to the Attention matrix and obtains an arbitrarily good approximation in linear time! The method generalizes beyond attention and opens the door to the next generation of deep learning architectures.\n\nOUTLINE:\n0:00 - Intro & Outline\n6:15 - Quadratic Bottleneck in Attention Mechanisms\n10:00 - Decomposing the Attention Matrix\n15:30 - Approximating the Softmax Kernel\n24:45 - Different Choices, Different Kernels\n28:00 - Why the Naive Approach does not work!\n31:30 - Better Approximation via Positive Features\n36:55 - Positive Features are Infinitely Better\n40:10 - Orthogonal Features are Even Better\n43:25 - Experiments\n49:20 - Broader Impact Statement\n50:00 - Causal Attention via Prefix Sums\n52:10 - Code\n53:50 - Final Remarks & Conclusion\n\nPaper: https://arxiv.org/abs/2009.14794\nCode: https://github.com/google-research/google-research/tree/master/performer\nBlog: https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html\n\nKernels on ML Street Talk: https://www.youtube.com/watch?v=y_RjsDHl5Y4\nMy Video on Linformer: https://www.youtube.com/watch?v=-_2AF9Lhweo\nMy Video on Reformer: https://www.youtube.com/watch?v=i4H0kjxrias\nMy Video on Attention: https://www.youtube.com/watch?v=iDulhoQ2pro\n\nAbstract:\nWe introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\nAuthors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2009.14794"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=NAJOZTNkhlI",
        "summary": "#ai #research #nlp\n\nKnowledge Graphs are structured databases that capture real-world entities and their relations to each other. KGs are usually built by human experts, which costs considerable amounts of time and money. This paper hypothesizes that language models, which have increased their performance dramatically in the last few years, contain enough knowledge to use them to construct a knowledge graph from a given corpus, without any fine-tuning of the language model itself. The resulting system can uncover new, unknown relations and outperforms all baselines in automated KG construction, even trained ones!\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - TabNine Promotion\n4:20 - Title Misnomer\n6:45 - From Corpus To Knowledge Graph\n13:40 - Paper Contributions\n15:50 - Candidate Fact Finding Algorithm\n25:50 - Causal Attention Confusion\n31:25 - More Constraints\n35:00 - Mapping Facts To Schemas\n38:40 - Example Constructed Knowledge Graph\n40:10 - Experimental Results\n47:25 - Example Discovered Facts\n50:40 - Conclusion & My Comments\n\nPaper: https://arxiv.org/abs/2010.11967\n\nAbstract:\nThis paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.\n\nAuthors: Chenguang Wang, Xiao Liu, Dawn Song\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2010.11967"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=gch94ttuy5s",
        "summary": "#ai #research #machinelearning\n\nDeep Learning models are often overparameterized and have many degrees of freedom, which leads to many local minima that all perform equally well on the test set. But it turns out that even though they all generalize in-distribution, the performance of these models can be drastically different when tested out-of-distribution. Notably, in many cases, a good model can actually be found among all these candidates, but it seems impossible to select it. This paper describes this problem, which it calls underspecification, and gives several theoretical and practical examples.\n\nOUTLINE:\n0:00 - Into & Overview\n2:00 - Underspecification of ML Pipelines\n11:15 - Stress Tests\n12:40 - Epidemiological Example\n20:45 - Theoretical Model\n26:55 - Example from Medical Genomics\n34:00 - ImageNet-C Example\n36:50 - BERT Models\n56:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2011.03395\n\nAbstract:\nML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.\n\nAuthors: Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2011.03395"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=IaS72aHrJKE",
        "summary": "#ai #research #engineering\n\nNumerical solvers for Partial Differential Equations are notoriously slow. They need to evolve their state by tiny steps in order to stay accurate, and they need to repeat this for each new problem. Neural Fourier Operators, the architecture proposed in this paper, can evolve a PDE in time by a single forward pass, and do so for an entire family of PDEs, as long as the training set covers them well. By performing crucial operations only in Fourier Space, this new architecture is also independent of the discretization or sampling of the underlying signal and has the potential to speed up many scientific applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:15 - Navier Stokes Problem Statement\n11:00 - Formal Problem Definition\n15:00 - Neural Operator\n31:30 - Fourier Neural Operator\n48:15 - Experimental Examples\n50:35 - Code Walkthrough\n1:01:00 - Summary & Conclusion\n\nPaper: https://arxiv.org/abs/2010.08895\nBlog: https://zongyi-li.github.io/blog/2020/fourier-pde/\nCode: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_3d.py\nMIT Technology Review: https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/\n\nAbstract:\nThe classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.\n\nAuthors: Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2010.08895"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=LB4B5FYvtdI",
        "summary": "#ai #biology #neuroscience\n\nBackpropagation is the workhorse of modern deep learning and a core component of most frameworks, but it has long been known that it is not biologically plausible, driving a divide between neuroscience and machine learning. This paper shows that Predictive Coding, a much more biologically plausible algorithm, can approximate Backpropagation for any computation graph, which they verify experimentally by building and training CNNs and LSTMs using Predictive Coding. This suggests that the brain and deep neural networks could be much more similar than previously believed.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Backpropagation & Biology\n7:40 - Experimental Results\n8:40 - Predictive Coding\n29:00 - Pseudocode\n32:10 - Predictive Coding approximates Backprop\n35:00 - Hebbian Updates\n36:35 - Code Walkthrough\n46:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04182\nCode: https://github.com/BerenMillidge/PredictiveCodingBackprop\n\nAbstract:\nBackpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.\n\nAuthors: Beren Millidge, Alexander Tschantz, Christopher L. Buckley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.04182"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=B9PL__gVxLI",
        "summary": "#deepmind #biology #ai\n\nThis is Biology's AlexNet moment! DeepMind solves a 50-year old problem in Protein Folding Prediction. AlphaFold 2 improves over DeepMind's 2018 AlphaFold system with a new architecture and massively outperforms all competition. In this Video, we take a look at how AlphaFold 1 works and what we can gather about AlphaFold 2 from the little information that's out there.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Proteins & Protein Folding\n14:20 - AlphaFold 1 Overview\n18:20 - Optimizing a differentiable geometric model at inference\n25:40 - Learning the Spatial Graph Distance Matrix\n31:20 - Multiple Sequence Alignment of Evolutionarily Similar Sequences\n39:40 - Distance Matrix Output Results\n43:45 - Guessing AlphaFold 2 (it's Transformers)\n53:30 - Conclusion & Comments\n\nAlphaFold 2 Blog: https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\nAlphaFold 1 Blog: https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery\nAlphaFold 1 Paper: https://www.nature.com/articles/s41586-019-1923-7\nMSA Reference: https://arxiv.org/abs/1211.1281\nCASP14 Challenge: https://predictioncenter.org/casp14/index.cgi\nCASP14 Result Bar Chart: https://www.predictioncenter.org/casp14/zscores_final.cgi\n\nPaper Title: High Accuracy Protein Structure Prediction Using Deep Learning\n\nAbstract:\nProteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and what a protein does largely depends on its unique 3D structure. Figuring out what shapes proteins fold into is known as the \u201cprotein folding problem\u201d, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.\n\nAuthors: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \u017d\u00eddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/1211.1281"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=BhUWvQmLzSk",
        "summary": "#ai #technology #poker\n\nThis paper does for Poker what AlphaZero has done for Chess & Go. The combination of Self-Play Reinforcement Learning and Tree Search has had tremendous success in perfect-information games, but transferring such techniques to imperfect information games is a hard problem. Not only does ReBeL solve this problem, but it provably converges to a Nash Equilibrium and delivers a superhuman Heads Up No-Limit Hold'em bot with very little domain knowledge.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:20 - Rock, Paper, and Double Scissor\n10:00 - AlphaZero Tree Search\n18:30 - Notation Setup: Infostates & Nash Equilibria\n31:45 - One Card Poker: Introducing Belief Representations\n45:00 - Solving Games in Belief Representation\n55:20 - The ReBeL Algorithm\n1:04:00 - Theory & Experiment Results\n1:07:00 - Broader Impact\n1:10:20 - High-Level Summary\n\nPaper: https://arxiv.org/abs/2007.13544\nCode: https://github.com/facebookresearch/rebel\nBlog: https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/\n\nERRATA: As someone last video pointed out: This is not the best Poker algorithm, but the best one that uses very little expert knowledge.\n\nAbstract:\nThe combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.\n\nAuthors: Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2007.13544"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=plK2WVdLTOY",
        "summary": "#ai #privacy #tech\n\nThis paper demonstrates a method to extract verbatim pieces of the training data from a trained language model. Moreover, some of the extracted pieces only appear a handful of times in the dataset. This points to serious security and privacy implications for models like GPT-3. The authors discuss the risks and propose mitigation strategies.\n\nOUTLINE:\n0:00 - Intro & Overview\n9:15 - Personal Data Example\n12:30 - Eidetic Memorization & Language Models\n19:50 - Adversary's Objective & Outlier Data\n24:45 - Ethical Hedging\n26:55 - Two-Step Method Overview\n28:20 - Perplexity Baseline\n30:30 - Improvement via Perplexity Ratios\n37:25 - Weights for Patterns & Weights for Memorization\n43:40 - Analysis of Main Results\n1:00:30 - Mitigation Strategies\n1:01:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2012.07805\n\nAbstract:\nIt has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\n\nAuthors: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2012.07805"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=T9XSU0pKX2E",
        "summary": "#ai #openai #technology\n\nPaper Title: Learning Transferable Visual Models From Natural Language Supervision\nCLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new image & text tasks.\n\nOUTLINE:\n0:00 - Introduction\n3:15 - Overview\n4:40 - Connecting Images & Text\n9:00 - Building Zero-Shot Classifiers\n14:40 - CLIP Contrastive Training Objective\n22:25 - Encoder Choices\n25:00 - Zero-Shot CLIP vs Linear ResNet-50\n31:50 - Zero-Shot vs Few-Shot\n35:35 - Scaling Properties\n36:35 - Comparison on different tasks\n37:40 - Robustness to Data Shift\n44:20 - Broader Impact Section\n47:00 - Conclusion & Comments\n\nPaper: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf\nBlog: https://openai.com/blog/clip/\nCode: https://github.com/openai/CLIP\n\nAbstract:\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "Learning Transferable Visual Models From Natural Language Supervision"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=iAR8LkkMMIM",
        "summary": "#ai #technology #switchtransformer\n\nScale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model's parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:30 - Performance Gains from Scale\n8:30 - Switch Transformer Architecture\n17:00 - Model-, Data- and Expert-Parallelism\n25:30 - Experimental Results\n29:00 - Stabilizing Training\n32:20 - Distillation into Dense Models\n33:30 - Final Comments\n\nPaper: https://arxiv.org/abs/2101.03961\nCodebase T5: https://github.com/google-research/text-to-text-transfer-transformer\n\nAbstract:\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n\nAuthors: William Fedus, Barret Zoph, Noam Shazeer\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2101.03961"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=yFAuXmcGk2Y",
        "summary": "#ai #research #blockchain\n\nBig Tech is currently dominating the pursuit of ever more capable AI. This happens behind closed doors and results in a monopoly of power. SingularityNET is an open, decentralized network where anyone can offer and consume AI services, and where AI agents can interlink with each other to provide ever more sophisticated AI, with the goal to create a singularity that's beneficial for humanity. This video takes a look at the basics behind SingularityNET and some of its core components.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:55 - Document Summarization Example Workflow\n5:50 - Why AI needs a Marketplace?\n9:20 - A network of APIs\n12:30 - AI Evaluators & Matchmakers\n15:00 - My criticisms of the Marketplace\n17:45 - What is on the Blockchain?\n20:45 - AI Marketplace Demo\n22:00 - The AGI Token & Inflation\n26:30 - Reputation System & other features\n30:00 - Democratic Governance\n33:00 - Benefit Tasks\n36:15 - My general thoughts on the application examples\n38:05 - Measuring Intelligence on SingularityNET\n45:15 - OfferNet Economy\n50:00 - Summary & Comments\n\nWhitepaper: https://public.singularitynet.io/whitepaper.pdf\nWebsite: https://singularitynet.io/\nAI Marketplace: https://beta.singularitynet.io/aimarketplace\n\nReferences:\nhttps://www.hansonrobotics.com/wp-content/uploads/2018/12/Using-Tononi-Phi-to-Measure-Consciousness-of-a-Cognitive-System-While-Reading-and-Conversing.pdf\nhttps://arxiv.org/pdf/1601.02626.pdf\nhttps://blog.singularitynet.io/singularitynet-the-past-the-present-and-the-future-7bacb2b8e7f0\nhttps://blog.singularitynet.io/singularitynet-supervisory-council-e7c513fd3ea6\nhttps://blog.singularitynet.io/singularitynet-phase-two-massive-token-utilization-toward-decentralized-beneficial-agi-6e3ac5a5b44a\n\nADDENDUM:\nI forgot to mention one important example for the utility of dynamic matchmaking: If I have a German text to summarize, and there is a German summarizer, but there is also a better English one, a clever AI could figure out for me whether to use the German one or whether to use a translator to English, then the English summarizer, then a backtranslator. And it could even do so depending on the input text.\n\nAbstract:\n[...] Most AI research today is controlled by a handful of corporations\u2014those with\nthe resources to fund development. Independent developers of AI tools have no\nreadily available way to monetize their creations. Usually, their most lucrative\noption is to sell their tool to one of the big tech companies, leading to control of\nthe technology becoming even more concentrated. SingularityNET\u2019s open-source\nprotocol and collection of smart contracts are designed to address these problems.\nDevelopers can launch their AI tools on the network, where they can interoperate\nwith other AIs and with paying users.\nNot only does the SingularityNET platform give developers a commercial\nlaunchpad (much like app stores give mobile app developers an easy path to\nmarket), it also allows the AIs to interoperate, creating a more synergistic, broadly\ncapable intelligence. For example, if a text-to-speech AI and an Italian-to-English\ntranslation AI were both on the network, then the network as a whole would be\ncapable of using Italian text to produce English speech.\nWithin this framework, AI transforms from a corporate asset to a global\ncommons; anyone can access AI tech or become a stakeholder in its development.\nAlso, anyone can add an AI/machine learning service to SingularityNET for use\nby the network and receive network payment tokens in exchange. [...]\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/pdf/1601.02626"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=zdb8MM94A5c",
        "summary": "#ai #science #transformers\n\nAutoregressive Transformers have taken over the world of Language Modeling (GPT-3). However, in order to train them, people use causal masking and sample parallelism, which means computation only happens in a feedforward manner. This results in higher layer information, which would be available, to not be used in the lower layers of subsequent tokens, and leads to a loss in the computational capabilities of the overall model. Feedback Transformers trade-off training speed for access to these representations and demonstrate remarkable improvements in complex reasoning and long-range dependency tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems of Autoregressive Processing\n3:30 - Information Flow in Recurrent Neural Networks\n7:15 - Information Flow in Transformers\n9:10 - Solving Complex Computations with Neural Networks\n16:45 - Causal Masking in Transformers\n19:00 - Missing Higher Layer Information Flow\n26:10 - Feedback Transformer Architecture\n30:00 - Connection to Attention-RNNs\n36:00 - Formal Definition\n37:05 - Experimental Results\n43:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2002.09402\n\nMy video on Attention: https://youtu.be/iDulhoQ2pro\n\nERRATA: Sometimes I say \"Switch Transformer\" instead of \"Feedback Transformer\". Forgive me :)\n\nAbstract:\nTransformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\n\nAuthors: Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2002.09402"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=ahRPdiCop3E",
        "summary": "#deeplearning #kernels #neuralnetworks\n\nFull Title: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine\n\nDeep Neural Networks are often said to discover useful representations of the data. However, this paper challenges this prevailing view and suggest that rather than representing the data, deep neural networks store superpositions of the training data in their weights and act as kernel machines at inference time. This is a theoretical paper with a main theorem and an understandable proof and the result leads to many interesting implications for the field.\n\nOUTLINE:\n0:00 - Intro & Outline\n4:50 - What is a Kernel Machine?\n10:25 - Kernel Machines vs Gradient Descent\n12:40 - Tangent Kernels\n22:45 - Path Kernels\n25:00 - Main Theorem\n28:50 - Proof of the Main Theorem\n39:10 - Implications & My Comments\n\nPaper: https://arxiv.org/abs/2012.00152\nStreet Talk about Kernels: https://youtu.be/y_RjsDHl5Y4\n\nERRATA: I simplify a bit too much when I pit kernel methods against gradient descent. Of course, you can even learn kernel machines using GD, they're not mutually exclusive. And it's also not true that you \"don't need a model\" in kernel machines, as it usually still contains learned parameters.\n\nAbstract:\nDeep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.\n\nAuthors: Pedro Domingos\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2012.00152"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=m-zrcmRd7E4",
        "summary": "#transformer #nystromer #nystromformer\n\nThe Nystr\u00f6mformer (or Nystromformer, Nystr\u00f6mer, Nystromer), is a new drop-in replacement for approximating the Self-Attention matrix in Transformers with linear memory and time requirements. Most importantly, it uses the Nystrom-Method to subselect (or segment mean) queries and keys as so-called landmarks and uses those to reconstruct the inherently low-rank attention matrix. This is relevant for many areas of Machine Learning, especially Natural Language processing, where it enables longer sequences of text to be processed at once.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - The Quadratic Memory Bottleneck in Self-Attention\n7:20 - The Softmax Operation in Attention\n11:15 - Nystr\u00f6m-Approximation\n14:00 - Getting Around the Softmax Problem\n18:05 - Intuition for Landmark Method\n28:05 - Full Algorithm\n30:20 - Theoretical Guarantees\n35:55 - Avoiding the Large Attention Matrix\n36:55 - Subsampling Keys vs Negative Sampling\n43:15 - Experimental Results\n47:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.03902\nCode: https://github.com/mlpen/Nystromformer\nAppendix: https://github.com/mlpen/Nystromformer/blob/main/doc/Nystromformer_Supplement.pdf\nLRA Results: https://twitter.com/tanmingxing/status/1359301186734620675\nTwitter lucidrains w/ author: https://twitter.com/lucidrains/status/1359597104075661312\nTwitter lucidrains w/ _clashluke: https://twitter.com/_clashluke/status/1359483460851802115\n\nAbstract:\nTransformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -- a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard Transformer. Our code is at this https URL.\n\nAuthors: Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2102.03902"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=rNkHjZtH0RQ",
        "summary": "#nfnets #deepmind #machinelearning\n\nBatch Normalization is a core component of modern deep learning. It enables training at higher batch sizes, prevents mean shift, provides implicit regularization, and allows networks to reach higher performance than without. However, BatchNorm also has disadvantages, such as its dependence on batch size and its computational overhead, especially in distributed settings. Normalizer-Free Networks, developed at Google DeepMind, are a class of CNNs that achieve state-of-the-art classification accuracy on ImageNet without batch normalization. This is achieved by using adaptive gradient clipping (AGC), combined with a number of improvements in general network architecture. The resulting networks train faster, are more accurate, and provide better transfer learning performance. Code is provided in Jax.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:40 - What's the problem with BatchNorm?\n11:00 - Paper contribution Overview\n13:30 - Beneficial properties of BatchNorm\n15:30 - Previous work: NF-ResNets\n18:15 - Adaptive Gradient Clipping\n21:40 - AGC and large batch size\n23:30 - AGC induces implicit dependence between training samples\n28:30 - Are BatchNorm's problems solved?\n30:00 - Network architecture improvements\n31:10 - Comparison to EfficientNet\n33:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.06171\nCode: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nMy Video on BatchNorm: https://www.youtube.com/watch?v=OioFONrSETc\nMy Video on ResNets: https://www.youtube.com/watch?v=GWt6Fu05voI\n\nERRATA (from Lucas Beyer): \"I believe you missed the main concern with \"batch cheating\". It's for losses that act on the full batch, as opposed to on each sample individually.\nFor example, triplet in FaceNet or n-pairs in CLIP. BN allows for \"shortcut\" solution to loss. See also BatchReNorm paper.\"\n\nAbstract:\nBatch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. Our code is available at this https URL deepmind-research/tree/master/nfnets\n\nAuthors: Andrew Brock, Soham De, Samuel L. Smith, Karen Simonyan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2102.06171"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=R5DiLFOMZrc",
        "summary": "#transformer #gan #machinelearning\n\nGenerative Adversarial Networks (GANs) hold the state-of-the-art when it comes to image generation. However, while the rest of computer vision is slowly taken over by transformers or other attention-based architectures, all working GANs to date contain some form of convolutional layers. This paper changes that and builds TransGAN, the first GAN where both the generator and the discriminator are transformers. The discriminator is taken over from ViT (an image is worth 16x16 words), and the generator uses pixelshuffle to successfully up-sample the generated resolution. Three tricks make training work: Data augmentations using DiffAug, an auxiliary superresolution task, and a localized initialization of self-attention. Their largest model reaches competitive performance with the best convolutional GANs on CIFAR10, STL-10, and CelebA.\n\nOUTLINE:\n0:00 - Introduction & Overview\n3:05 - Discriminator Architecture\n5:25 - Generator Architecture\n11:20 - Upsampling with PixelShuffle\n15:05 - Architecture Recap\n16:00 - Vanilla TransGAN Results\n16:40 - Trick 1: Data Augmentation with DiffAugment\n19:10 - Trick 2: Super-Resolution Co-Training\n22:20 - Trick 3: Locality-Aware Initialization for Self-Attention\n27:30 - Scaling Up & Experimental Results\n28:45 - Recap & Conclusion\n\nPaper: https://arxiv.org/abs/2102.07074\nCode: https://github.com/VITA-Group/TransGAN\nMy Video on ViT: https://youtu.be/TrdevFK_am4\n\nAbstract:\nThe recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \\textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \\textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \\textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA 64\u00d764, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at \\url{this https URL}.\n\nAuthors: Yifan Jiang, Shiyu Chang, Zhangyang Wang\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2102.07074"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=o75ybZ-6Uu8",
        "summary": "#dreamer #deeprl #reinforcementlearning\n\nModel-Based Reinforcement Learning has been lagging behind Model-Free RL on Atari, especially among single-GPU algorithms. This collaboration between Google AI, DeepMind, and the University of Toronto (UofT) pushes world models to the next level. The main contribution is a learned latent state consisting of one discrete part and one stochastic part, whereby the stochastic part is a set of 32 categorical variables, each with 32 possible values. The world model can freely decide how it wants to use these variables to represent the input, but is tasked with the prediction of future observations and rewards. This procedure gives rise to an informative latent representation and in a second step, reinforcement learning (A2C Actor-Critic) can be done purely - and very efficiently - on the basis of the world-model's latent states. No observations needed! This paper combines this with straight-through estimators, KL balancing, and many other tricks to achieve state-of-the-art single-GPU performance in Atari.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:50 - Short Recap of Reinforcement Learning\n6:05 - Problems with Model-Free Reinforcement Learning\n10:40 - How World Models Help\n12:05 - World Model Learner Architecture\n16:50 - Deterministic & Stochastic Hidden States\n18:50 - Latent Categorical Variables\n22:00 - Categorical Variables and Multi-Modality\n23:20 - Sampling & Stochastic State Prediction\n30:55 - Actor-Critic Learning in Dream Space\n32:05 - The Incompleteness of Learned World Models\n34:15 - How General is this Algorithm?\n37:25 - World Model Loss Function\n39:20 - KL Balancing\n40:35 - Actor-Critic Loss Function\n41:45 - Straight-Through Estimators for Sampling Backpropagation\n46:25 - Experimental Results\n52:00 - Where Does It Fail?\n54:25 - Conclusion\n\nPaper: https://arxiv.org/abs/2010.02193\nCode: https://github.com/danijar/dreamerv2\nAuthor Blog: https://danijar.com/project/dreamerv2/\nGoogle AI Blog: https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html\n\nERRATA (from the authors): \n- KL balancing (prior vs posterior within the KL) is different from beta VAEs (reconstruction vs KL)\n- The vectors of categoricals can in theory represent 32^32 different images so their capacity is quite large\n\nAbstract:\nIntelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.\n\nAuthors: Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2010.02193"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=_c6A33Fg5Ns",
        "summary": "#deberta #bert #huggingface\n\nDeBERTa by Microsoft is the next iteration of BERT-style Self-Attention Transformer models, surpassing RoBERTa in State-of-the-art in multiple NLP tasks. DeBERTa brings two key improvements: First, they treat content and position information separately in a new form of disentangled attention mechanism. Second, they resort to relative positional encodings throughout the base of the transformer, and provide absolute positional encodings only at the very end. The resulting model is both more accurate on downstream tasks and needs less pretraining steps to reach good accuracy. Models are also available in Huggingface and on Github.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:15 - Position Encodings in Transformer's Attention Mechanism\n9:55 - Disentangling Content & Position Information in Attention\n21:35 - Disentangled Query & Key construction in the Attention Formula\n25:50 - Efficient Relative Position Encodings\n28:40 - Enhanced Mask Decoder using Absolute Position Encodings\n35:30 - My Criticism of EMD\n38:05 - Experimental Results\n40:30 - Scaling up to 1.5 Billion Parameters\n44:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.03654\nCode: https://github.com/microsoft/DeBERTa\nHuggingface models: https://huggingface.co/models?search=deberta\n\nAbstract:\nRecent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).\n\nAuthors: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.03654"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=RSSVWpBak6s",
        "summary": "#fastweights #deeplearning #transformers\n\nTransformers are dominating Deep Learning, but their quadratic memory and compute requirements make them expensive to train and hard to use. Many papers have attempted to linearize the core module: the attention mechanism, using kernels - for example, the Performer. However, such methods are either not satisfactory or have other downsides, such as a reliance on random features. This paper establishes an intrinsic connection between linearized (kernel) attention and the much older Fast Weight Memory Systems, in part popularized by J\u00fcrgen Schmidhuber in the 90s. It shows the fundamental limitations of these algorithms and suggests new update rules and new kernels in order to fix these problems. The resulting model compares favorably to Performers on key synthetic experiments and real-world tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Fast Weight Systems\n7:00 - Distributed Storage of Symbolic Values\n12:30 - Autoregressive Attention Mechanisms\n18:50 - Connecting Fast Weights to Attention Mechanism\n22:00 - Softmax as a Kernel Method (Performer)\n25:45 - Linear Attention as Fast Weights\n27:50 - Capacity Limitations of Linear Attention\n29:45 - Synthetic Data Experimental Setup\n31:50 - Improving the Update Rule\n37:30 - Deterministic Parameter-Free Projection (DPFP) Kernel\n46:15 - Experimental Results\n50:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.11174\nCode: https://github.com/ischlag/fast-weight-transformers\nMachine Learning Street Talk on Kernels: https://youtu.be/y_RjsDHl5Y4\n\nAbstract:\nWe show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early '90s. From this observation we infer a memory capacity limitation of recent linearised softmax attention variants. With finite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.\n\nAuthors: Imanol Schlag, Kazuki Irie, J\u00fcrgen Schmidhuber\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2102.11174"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=cllFzkvrYmE",
        "summary": "#glom #hinton #capsules\n\nGeoffrey Hinton describes GLOM, a Computer Vision model that combines transformers, neural fields, contrastive learning, capsule networks, denoising autoencoders and RNNs. GLOM decomposes an image into a parse tree of objects and their parts. However, unlike previous systems, the parse tree is constructed dynamically and differently for each input, without changing the underlying neural network. This is done by a multi-step consensus algorithm that runs over different levels of abstraction at each location of an image simultaneously. GLOM is just an idea for now but suggests a radically new approach to AI visual scene understanding.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Object Recognition as Parse Trees\n5:40 - Capsule Networks\n8:00 - GLOM Architecture Overview\n13:10 - Top-Down and Bottom-Up communication\n18:30 - Emergence of Islands\n22:00 - Cross-Column Attention Mechanism\n27:10 - My Improvements for the Attention Mechanism\n35:25 - Some Design Decisions\n43:25 - Training GLOM as a Denoising Autoencoder & Contrastive Learning\n52:20 - Coordinate Transformations & Representing Uncertainty\n57:05 - How GLOM handles Video\n1:01:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.12627\n\nAbstract:\nThis paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language\n\nAuthors: Geoffrey Hinton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2102.12627"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Z_kWZpgEZ7w",
        "summary": "#openai #clip #microscope\n\nOpenAI does a huge investigation into the inner workings of their recent CLIP model via faceted feature visualization and finds amazing things: Some neurons in the last layer respond to distinct concepts across multiple modalities, meaning they fire for photographs, drawings, and signs depicting the same concept, even when the images are vastly distinct. Through manual examination, they identify and investigate neurons corresponding to persons, geographical regions, religions, emotions, and much more. In this video, I go through the publication and then I present my own findings from digging around in the OpenAI Microscope.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:35 - OpenAI Microscope\n7:10 - Categories of found neurons\n11:10 - Person Neurons\n13:00 - Donald Trump Neuron\n17:15 - Emotion Neurons\n22:45 - Region Neurons\n26:40 - Sparse Mixture of Emotions\n28:05 - Emotion Atlas\n29:45 - Adversarial Typographic Attacks\n31:55 - Stroop Test\n33:10 - My Findings in OpenAI Microscope\n33:30 - Superman Neuron\n33:50 - Resting B*tchface Neuron\n34:10 - Trash Bag Neuron\n35:25 - God Weightlifting Neuron\n36:40 - Organ Neuron\n38:35 - Film Spool Neuron\n39:05 - Feather Neuron\n39:20 - Spartan Neuron\n40:25 - Letter E Neuron\n40:35 - Cleanin Neuron\n40:45 - Frown Neuron\n40:55 - Lion Neuron\n41:05 - Fashion Model Neuron\n41:20 - Baseball Neuron\n41:50 - Bride Neuron\n42:00 - Navy Neuron\n42:30 - Hemp Neuron\n43:25 - Staircase Neuron\n43:45 - Disney Neuron\n44:15 - Hillary Clinton Neuron\n44:50 - God Neuron\n45:15 - Blurry Neuron\n45:35 - Arrow Neuron\n45:55 - Trophy Presentation Neuron\n46:10 - Receding Hairline Neuron\n46:30 - Traffic Neuron\n46:40 - Raised Hand Neuron\n46:50 - Google Maps Neuron\n47:15 - Nervous Smile Neuron\n47:30 - Elvis Neuron\n47:55 - The Flash Neuron\n48:05 - Beard Neuron\n48:15 - Kilt Neuron\n48:25 - Rainy Neuron\n48:35 - Electricity Neuron\n48:50 - Droplets Neuron\n49:00 - Escape Neuron\n49:25 - King Neuron\n49:35 - Country Neuron\n49:45 - Overweight Men Neuron\n49:55 - Wedding\n50:05 - Australia Neuron\n50:15 - Yawn Neuron\n50:30 - Bees & Simpsons Neuron\n50:40 - Mussles Neuron\n50:50 - Spice Neuron\n51:00 - Conclusion\n\nPaper: https://distill.pub/2021/multimodal-neurons/\nMy Findings: https://www.notion.so/CLIP-OpenAI-Microscope-Findings-27465eac373c451d8083428443e0837c\nMy Video on CLIP: https://youtu.be/T9XSU0pKX2E\nMy Video on Feature Visualizations & The OpenAI Microscope: https://youtu.be/Ok44otx90D4\n\nAbstract:\nIn 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry. The exciting thing wasn\u2019t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person\u2019s name. The neurons were multimodal. As the lead author would put it: \"You are looking at the far end of the transformation from metric, visual shapes to conceptual... information.\" We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. Like the biological multimodal neurons, these artificial neurons respond to the same subject in photographs, drawings, and images of their name.\n\nAuthors: Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://distill.pub/2021/multimodal-neurons/"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Ag1bw8MfHGQ",
        "summary": "#selfsupervisedlearning #yannlecun #facebookai\n\nDeep Learning systems can achieve remarkable, even super-human performance through supervised learning on large, labeled datasets. However, there are two problems: First, collecting ever more labeled data is expensive in both time and money. Second, these deep neural networks will be high performers on their task, but cannot easily generalize to other, related tasks, or they need large amounts of data to do so. In this blog post, Yann LeCun and Ishan Misra of Facebook AI Research (FAIR) describe the current state of Self-Supervised Learning (SSL) and argue that it is the next step in the development of AI that uses fewer labels and can transfer knowledge faster than current systems. They suggest as a promising direction to build non-contrastive latent-variable predictive models, like VAEs, but ones that also provide high-quality latent representations for downstream tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - Supervised Learning, Self-Supervised Learning, and Common Sense\n7:35 - Predicting Hidden Parts from Observed Parts\n17:50 - Self-Supervised Learning for Language vs Vision\n26:50 - Energy-Based Models\n30:15 - Joint-Embedding Models\n35:45 - Contrastive Methods\n43:45 - Latent-Variable Predictive Models and GANs\n55:00 - Summary & Conclusion\n\nPaper (Blog Post): https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence\nMy Video on BYOL: https://www.youtube.com/watch?v=YPfUiOMYOEE\n\nERRATA:\n- The difference between loss and energy: Energy is for inference, loss is for training.\n- The R(z) term is a regularizer that restricts the capacity of the latent variable. I think I said both of those things, but never together.\n- The way I explain why BERT is contrastive is wrong. I haven't figured out why just yet, though :)\n\nVideo approved by Antonio.\n\nAbstract:\nWe believe that self-supervised learning (SSL) is one of the most promising ways to build such background knowledge and approximate a form of common sense in AI systems.\n\nAuthors: Yann LeCun, Ishan Misra\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Elxn8rS88bI",
        "summary": "#universalcomputation #pretrainedtransformers #finetuning\n\nLarge-scale pre-training and subsequent fine-tuning is a common recipe for success with transformer models in machine learning. However, most such transfer learning is done when a model is pre-trained on the same or a very similar modality to the final task to be solved. This paper demonstrates that transformers can be fine-tuned to completely different modalities, such as from language to vision. Moreover, they demonstrate that this can be done by freezing all attention layers, tuning less than .1% of all parameters. The paper further claims that language modeling is a superior pre-training task for such cross-domain transfer. The paper goes through various ablation studies to make its point.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - Frozen Pretrained Transformers\n4:50 - Evaluated Tasks\n10:05 - The Importance of Training LayerNorm\n17:10 - Modality Transfer\n25:10 - Network Architecture Ablation\n26:10 - Evaluation of the Attention Mask\n27:20 - Are FPTs Overfitting or Underfitting?\n28:20 - Model Size Ablation\n28:50 - Is Initialization All You Need?\n31:40 - Full Model Training Overfits\n32:15 - Again the Importance of Training LayerNorm\n33:10 - Conclusions & Comments\n\nPaper: https://arxiv.org/abs/2103.05247\nCode: https://github.com/kzl/universal-computation\n\nAbstract:\nWe investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.\n\nAuthors: Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2103.05247"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=P_xeshTnPZg",
        "summary": "#perceiver #deepmind #transformer\n\nInspired by the fact that biological creatures attend to multiple modalities at the same time, DeepMind releases its new Perceiver model. Based on the Transformer architecture, the Perceiver makes no assumptions on the modality of the input data and also solves the long-standing quadratic bottleneck problem. This is achieved by having a latent low-dimensional Transformer, where the input data is fed multiple times via cross-attention. The Perceiver's weights can also be shared across layers, making it very similar to an RNN. Perceivers achieve competitive performance on ImageNet and state-of-the-art on other modalities, all while making no architectural adjustments to input data.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:20 - Built-In assumptions of Computer Vision Models\n5:10 -  The Quadratic Bottleneck of Transformers\n8:00 - Cross-Attention in Transformers\n10:45 - The Perceiver Model Architecture & Learned Queries\n20:05 - Positional Encodings via Fourier Features\n23:25 - Experimental Results & Attention Maps\n29:05 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2103.03206\n\nMy Video on Transformers (Attention is All You Need): https://youtu.be/iDulhoQ2pro\n\nAbstract:\nBiological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.\n\nAuthors: Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2103.03206"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=qtu0aSTDE2I",
        "summary": "#dreamcoder #programsynthesis #symbolicreasoning\n\nClassic Machine Learning struggles with few-shot generalization for tasks where humans can easily generalize from just a handful of examples, for example sorting a list of numbers. Humans do this by coming up with a short program, or algorithm, that explains the few data points in a compact way. DreamCoder emulates this by using neural guided search over a language of primitives, a library, that it builds up over time. By doing this, it can iteratively construct more and more complex programs by building on its own abstractions and therefore solve more and more difficult tasks in a few-shot manner by generating very short programs that solve the few given datapoints. The resulting system can not only generalize quickly but also delivers an explainable solution to its problems in form of a modular and hierarchical learned library. Combining this with classic Deep Learning for low-level perception is a very promising future direction.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:55 - DreamCoder System Architecture\n9:00 - Wake Phase: Neural Guided Search\n19:15 - Abstraction Phase: Extending the Internal Library\n24:30 - Dreaming Phase: Training Neural Search on Fictional Programs and Replays\n30:55 - Abstraction by Compressing Program Refactorings\n32:40 - Experimental Results on LOGO Drawings\n39:00 - Ablation Studies\n39:50 - Re-Discovering Physical Laws\n42:25 - Discovering Recursive Programming Algorithms\n44:20 - Conclusions & Discussion\n\nPaper: https://arxiv.org/abs/2006.08381\nCode: https://github.com/ellisk42/ec\n\nAbstract:\nExpert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.\n\nAuthors: Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, Joshua B. Tenenbaum\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2006.08381"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=CRlN-cYFxTk",
        "summary": "#nerf #neuralrendering #deeplearning\n\nView Synthesis is a tricky problem, especially when only given a sparse set of images as an input. NeRF embeds an entire scene into the weights of a feedforward neural network, trained by backpropagation through a differential volume rendering procedure, and achieves state-of-the-art view synthesis. It includes directional dependence and is able to capture fine structural details, as well as reflection effects and transparency.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:50 - View Synthesis Task Description\n5:50 - The fundamental difference to classic Deep Learning\n7:00 - NeRF Core Concept\n15:30 - Training the NeRF from sparse views\n20:50 - Radiance Field Volume Rendering\n23:20 - Resulting View Dependence\n24:00 - Positional Encoding\n28:00 - Hierarchical Volume Sampling\n30:15 - Experimental Results\n33:30 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2003.08934\nWebsite & Code: https://www.matthewtancik.com/nerf\n\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\n\nAbstract:\nWe present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (\u03b8,\u03d5)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.\n\nAuthors: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2003.08934"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=uwfVxckuq50",
        "summary": "#aiwinter #agi #embodiedcognition\n\nThe AI community has gone through regular cycles of AI Springs, where rapid progress gave rise to massive overconfidence, high funding, and overpromise, followed by these promises being unfulfilled, subsequently diving into periods of disenfranchisement and underfunding, called AI Winters. This paper examines the reasons for the repeated periods of overconfidence and identifies four fallacies that people make when they see rapid progress in AI.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:10 - AI Springs & AI Winters\n5:40 - Is the current AI boom overhyped?\n15:35 - Fallacy 1: Narrow Intelligence vs General Intelligence\n19:40 - Fallacy 2: Hard for humans doesn't mean hard for computers\n21:45 - Fallacy 3: How we call things matters\n28:15 - Fallacy 4: Embodied Cognition\n35:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.12871\n\nMy Video on Shortcut Learning: https://youtu.be/D-eg7k8YSfs\n\nAbstract:\nSince its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (\"AI spring\") and periods of disappointment, loss of confidence, and reduced funding (\"AI winter\"). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.\n\nAuthors: Melanie Mitchell\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2104.12871"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=h3ij3F3cPIk",
        "summary": "#dino #facebook #selfsupervised\n\nSelf-Supervised Learning is the final frontier in Representation Learning: Getting useful features without any labels. Facebook AI's new system, DINO, combines advances in Self-Supervised Learning for Computer Vision with the new Vision Transformer (ViT) architecture and achieves impressive results without any labels. Attention maps can be directly interpreted as segmentation maps, and the obtained representations can be used for image retrieval and zero-shot k-nearest neighbor classifiers (KNNs).\n\nOUTLINE:\n0:00 - Intro & Overview\n6:20 - Vision Transformers\n9:20 - Self-Supervised Learning for Images\n13:30 - Self-Distillation\n15:20 - Building the teacher from the student by moving average\n16:45 - DINO Pseudocode\n23:10 - Why Cross-Entropy Loss?\n28:20 - Experimental Results\n33:40 - My Hypothesis why this works\n38:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.14294\nBlog: https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training\nCode: https://github.com/facebookresearch/dino\n\nMy Video on ViT: https://youtu.be/TrdevFK_am4\nMy Video on BYOL: https://youtu.be/YPfUiOMYOEE\n\nAbstract:\nIn this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\n\nAuthors: Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2104.14294"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=pH2jZun8MoY",
        "summary": "#involution #computervision #attention\n\nConvolutional Neural Networks (CNNs) have dominated computer vision for almost a decade by applying two fundamental principles: Spatial agnosticism and channel-specific computations. Involution aims to invert these principles and presents a spatial-specific computation, which is also channel-agnostic. The resulting Involution Operator and RedNet architecture are a compromise between classic Convolutions and the newer Local Self-Attention architectures and perform favorably in terms of computation accuracy tradeoff when compared to either.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Principles of Convolution\n10:50 - Towards spatial-specific computations\n17:00 - The Involution Operator\n20:00 - Comparison to Self-Attention\n25:15 - Experimental Results\n30:30 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2103.06255\nCode: https://github.com/d-li14/involution\n\nAbstract:\nConvolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL.\n\nAuthors: Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2103.06255"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=W-O7AZNzbzQ",
        "summary": "#ddpm #diffusionmodels #openai\n\nGANs have dominated the image generation space for the majority of the last decade. This paper shows for the first time, how a non-GAN model, a DDPM, can be improved to overtake GANs at standard evaluation metrics for image generation. The produced samples look amazing and other than GANs, the new model has a formal probabilistic foundation. Is there a future for GANs or are Diffusion Models going to overtake them for good?\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Denoising Diffusion Probabilistic Models\n11:30 - Formal derivation of the training loss\n23:00 - Training in practice\n27:55 - Learning the covariance\n31:25 - Improving the noise schedule\n33:35 - Reducing the loss gradient noise\n40:35 - Classifier guidance\n52:50 - Experimental Results\n\nPaper (this): https://arxiv.org/abs/2105.05233\nPaper (previous): https://arxiv.org/abs/2102.09672\nCode: https://github.com/openai/guided-diffusion\n\nAbstract:\nWe show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for sample quality using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\u00d7128, 4.59 on ImageNet 256\u00d7256, and 7.72 on ImageNet 512\u00d7512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.85 on ImageNet 512\u00d7512. We release our code at this https URL\n\nAuthors: Alex Nichol, Prafulla Dhariwal\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2105.05233"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=2PYLNHqxd5A",
        "summary": "#expirespan #nlp #facebookai\n\nFacebook AI (FAIR) researchers present Expire-Span, a variant of Transformer XL that dynamically assigns expiration dates to previously encountered signals. Because of this, Expire-Span can handle sequences of many thousand tokens, while keeping the memory and compute requirements at a manageable level. It severely matches or outperforms baseline systems, while consuming much less resources. We discuss its architecture, advantages, and shortcomings.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Remembering the past in sequence models\n5:45 - Learning to expire past memories\n8:30 - Difference to local attention\n10:00 - Architecture overview\n13:45 - Comparison to Transformer XL\n18:50 - Predicting expiration masks\n32:30 - Experimental Results\n40:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2105.06548\nCode: https://github.com/facebookresearch/transformer-sequential\n\nADDENDUM: I mention several times that the gradient signal of the e quantity only occurs inside the R ramp. By that, I mean the gradient stemming from the model loss. The regularization loss acts also outside the R ramp.\n\nAbstract:\nAttention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.\n\nAuthors: Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, Angela Fan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2105.06548"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=kU-tWy_wr78",
        "summary": "#metarim #deeprl #catastrophicforgetting\n\nReinforcement Learning is very tricky in environments where the objective shifts over time. This paper explores agents in multi-task environments that are usually subject to catastrophic forgetting. Building on the concept of Recurrent Independent Mechanisms (RIM), the authors propose to separate the learning procedures for the mechanism parameters (fast) and the attention parameters (slow) and achieve superior results and more stability, and even better zero-shot transfer performance.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:30 - Recombining pieces of knowledge\n11:30 - Controllers as recurrent neural networks\n14:20 - Recurrent Independent Mechanisms\n21:20 - Learning at different time scales\n28:40 - Experimental Results & My Criticism\n44:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2105.08710\nRIM Paper: https://arxiv.org/abs/1909.10893\n\nAbstract:\nDecomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.\n\nAuthors: Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Sch\u00f6lkopf, Yoshua Bengio\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2105.08710"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=dmH1ZpcROMk",
        "summary": "#reinforcementlearning #deepmind #agi\n\nWhat's the most promising path to creating Artificial General Intelligence (AGI)? This paper makes the bold claim that a learning agent maximizing its reward in a sufficiently complex environment will necessarily develop intelligence as a by-product, and that Reward Maximization is the best way to move the creation of AGI forward. The paper is a mix of philosophy, engineering, and futurism, and raises many points of discussion.\n\nOUTLINE:\n0:00 - Intro & Outline\n4:10 - Reward Maximization\n10:10 - The Reward-is-Enough Hypothesis\n13:15 - Abilities associated with intelligence\n16:40 - My Criticism\n26:15 - Reward Maximization through Reinforcement Learning\n31:30 - Discussion, Conclusion & My Comments\n\nPaper: https://www.sciencedirect.com/science/article/pii/S0004370221000862\n\nAbstract:\nIn this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.\n\nAuthors: David Silver, Satinder Singh, Doina Precup, Richard S. Sutton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://www.sciencedirect.com/science/article/pii/S0004370221000862"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-buULmf7dec",
        "summary": "#decisiontransformer #reinforcementlearning #transformer\n\nProper credit assignment over long timespans is a fundamental problem in reinforcement learning. Even methods designed to combat this problem, such as TD-learning, quickly reach their limits when rewards are sparse or noisy. This paper reframes offline reinforcement learning as a pure sequence modeling problem, with the actions being sampled conditioned on the given history and desired future rewards. This allows the authors to use recent advances in sequence modeling using Transformers and achieve competitive results in Offline RL benchmarks.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:15 - Offline Reinforcement Learning\n10:10 - Transformers in RL\n14:25 - Value Functions and Temporal Difference Learning\n20:25 - Sequence Modeling and Reward-to-go\n27:20 - Why this is ideal for offline RL\n31:30 - The context length problem\n34:35 - Toy example: Shortest path from random walks\n41:00 - Discount factors\n45:50 - Experimental Results\n49:25 - Do you need to know the best possible reward?\n52:15 - Key-to-door toy experiment\n56:00 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2106.01345\nWebsite: https://sites.google.com/berkeley.edu/decision-transformer\nCode: https://github.com/kzl/decision-transformer\n\nTrajectory Transformer: https://trajectory-transformer.github.io/\nUpside-Down RL: https://arxiv.org/abs/1912.02875\n\nAbstract:\nWe present a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\n\nAuthors: Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2106.01345"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=8Oy7o3Yu-Xo",
        "summary": "#implicitfunction #jax #autodiff\n\nMany problems in Machine Learning involve loops of inner and outer optimization. Finding update steps for the outer loop is usually difficult, because of the.need to differentiate through the inner loop's procedure over multiple steps. Such loop unrolling is very limited and constrained to very few steps. Other papers have found solutions around unrolling in very specific, individual problems. This paper proposes a unified framework for implicit differentiation of inner optimization procedures without unrolling and provides implementations that integrate seamlessly into JAX.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Automatic Differentiation of Inner Optimizations\n4:30 - Example: Meta-Learning\n7:45 - Unrolling Optimization\n13:00 - Unified Framework Overview & Pseudocode\n21:10 - Implicit Function Theorem\n25:45 - More Technicalities\n28:45 - Experiments\n\nERRATA:\n- Dataset Distillation is done with respect to the training set, not the validation or test set.\n\nPaper: https://arxiv.org/abs/2105.15183\nCode coming soon\n\nAbstract:\nAutomatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.\n\nAuthors: Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-L\u00f3pez, Fabian Pedregosa, Jean-Philippe Vert\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2105.15183"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=P38FZrbNHV4",
        "summary": "#reiforcementlearning #gan #imitationlearning\n\nLearning from demonstrations is a fascinating topic, but what if the demonstrations are not exactly the behaviors we want to learn? Can we adhere to a dataset of demonstrations and still achieve a specified goal? This paper uses GANs to combine goal-achieving reinforcement learning with imitation learning and learns to perform well at a given task while doing so in the style of a given presented dataset. The resulting behaviors include many realistic-looking transitions between the demonstrated movements.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n6:10 - Reward Signals\n8:15 - Motion Prior from GAN\n14:10 - Algorithm Overview\n20:15 - Reward Engineering & Experimental Results\n30:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.02180\nMain Video: https://www.youtube.com/watch?v=wySUxZN_KbM\nSupplementary Video: https://www.youtube.com/watch?v=O6fBSMxThR4\n\nAbstract:\nSynthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character's behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.\n\nAuthors: Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, Angjoo Kanazawa\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2104.02180"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=g08NkNWmZTA",
        "summary": "#xcit #transformer #attentionmechanism\n\nAfter dominating Natural Language Processing, Transformers have taken over Computer Vision recently with the advent of Vision Transformers. However, the attention mechanism's quadratic complexity in the number of tokens means that Transformers do not scale well to high-resolution images. XCiT is a new Transformer architecture, containing XCA, a transposed version of attention, reducing the complexity from quadratic to linear, and at least on image data, it appears to perform on par with other models. What does this mean for the field? Is this even a transformer? What really matters in deep learning?\n\nOUTLINE:\n0:00 - Intro & Overview\n3:45 - Self-Attention vs Cross-Covariance Attention (XCA)\n19:55 - Cross-Covariance Image Transformer (XCiT) Architecture\n26:00 - Theoretical & Engineering considerations\n30:40 - Experimental Results\n33:20 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2106.09681\nCode: https://github.com/facebookresearch/xcit\n\nAbstract:\nFollowing their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a \"transposed\" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.\n\nAuthors: Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, Herv\u00e9 Jegou\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2106.09681"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=k_hUdZJNzkU",
        "summary": "#adversarialexamples #dimpledmanifold #security\n\nAdversarial Examples have long been a fascinating topic for many Machine Learning researchers. How can a tiny perturbation cause the neural network to change its output by so much? While many explanations have been proposed over the years, they all appear to fall short. This paper attempts to comprehensively explain the existence of adversarial examples by proposing a view of the classification landscape, which they call the Dimpled Manifold Model, which says that any classifier will adjust its decision boundary to align with the low-dimensional data manifold, and only slightly bend around the data. This potentially explains many phenomena around adversarial examples. Warning: In this video, I disagree. Remember that I'm not an authority, but simply give my own opinions.\n\nOUTLINE:\n0:00 - Intro & Overview\n7:30 - The old mental image of Adversarial Examples\n11:25 - The new Dimpled Manifold Hypothesis\n22:55 - The Stretchy Feature Model\n29:05 - Why do DNNs create Dimpled Manifolds?\n38:30 - What can be explained with the new model?\n1:00:40 - Experimental evidence for the Dimpled Manifold Model\n1:10:25 - Is Goodfellow's claim debunked?\n1:13:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2106.10151\nMy replication code: https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64\nGoodfellow's Talk: https://youtu.be/CIfsB_EYsVI?t=4280\n\nAbstract:\nThe extreme fragility of deep neural networks when presented with tiny perturbations in their inputs was independently discovered by several research groups in 2013, but in spite of enormous effort these adversarial examples remained a baffling phenomenon with no clear explanation. In this paper we introduce a new conceptual framework (which we call the Dimpled Manifold Model) which provides a simple explanation for why adversarial examples exist, why their perturbations have such tiny norms, why these perturbations look like random noise, and why a network which was adversarially trained with incorrectly labeled images can still correctly classify test images. In the last part of the paper we describe the results of numerous experiments which strongly support this new model, and in particular our assertion that adversarial perturbations are roughly perpendicular to the low dimensional manifold which contains all the training examples.\n\nAbstract: Adi Shamir, Odelia Melamed, Oriel BenShmuel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2106.10151"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=z15JLtAuwVI",
        "summary": "#apple #icloud #privacy\n\nApple recently announced scanning all images uploaded to iCloud for CSAM (child abuse material), and that this scan would happen locally on users' phones. We take a look at the technical report and explore how the system works in detail, how it is designed to preserve user privacy, and what weak points it still has.\n\nOUTLINE:\n0:00 - Introduction\n3:05 - System Requirements\n9:15 - System Overview\n14:00 - NeuralHash\n20:45 - Private Set Intersection\n31:15 - Threshold Secret Sharing\n35:25 - Synthetic Match Vouchers\n38:20 - Problem 1: Who controls the database?\n42:40 - Problem 2: Adversarial Attacks\n49:40 - Comments & Conclusion\n\nPaper: https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf\nML News Episode about CSAM: https://youtu.be/gFkBqD2hbnU\n\nAbstract:\nCSAM Detection enables Apple to accurately identify and report iCloud users who store known Child Sexual Abuse Material (CSAM) in their iCloud Photos accounts. Apple servers flag accounts exceeding a threshold number of images that match a known database of CSAM image hashes so that Apple can provide relevant information to the National Center for Missing and Exploited Children (NCMEC). This process is secure, and is expressly designed to preserve user privacy.\nCSAM Detection provides these privacy and security assurances:\n\u2022 Apple does not learn anything about images that do not match the known CSAM database.\n\u2022 Apple can\u2019t access metadata or visual derivatives for matched CSAM images until a threshold of matches is exceeded for an iCloud Photos account.\n\u2022 The risk of the system incorrectly flagging an account is extremely low. In addition, Apple manually reviews all reports made to NCMEC to ensure reporting accuracy.\n\u2022 Users can\u2019t access or view the database of known CSAM images.\n\u2022 Users can\u2019t identify which images were flagged as CSAM by the system.\nFor detailed information about the cryptographic protocol and security proofs that the CSAM Detection process uses, see The Apple PSI System.\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=nQDZmf2Yb9k",
        "summary": "#pondernet #deepmind #machinelearning\n\nHumans don't spend the same amount of mental effort on all problems equally. Instead, we respond quickly to easy tasks, and we take our time to deliberate hard tasks. DeepMind's PonderNet attempts to achieve the same by dynamically deciding how many computation steps to allocate to any single input sample. This is done via a recurrent architecture and a trainable function that computes a halting probability. The resulting model performs well in dynamic computation tasks and is surprisingly robust to different hyperparameter settings.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Problem Statement\n8:00 - Probabilistic formulation of dynamic halting\n14:40 - Training via unrolling\n22:30 - Loss function and regularization of the halting distribution\n27:35 - Experimental Results\n37:10 - Sensitivity to hyperparameter choice\n41:15 - Discussion, Conclusion, Broader Impact\n\nPaper: https://arxiv.org/abs/2107.05407\n\nAbstract:\nIn standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1\n\nAuthors: Andrea Banino, Jan Balaguer, Charles Blundell\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2107.05407"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=qgUegkefocg",
        "summary": "#attention #transformer #fastformer\n\nTransformers have become the dominant model class in the last few years for large data, but their quadratic complexity in terms of sequence length has plagued them until now. Fastformer claims to be the fastest and most performant linear attention variant, able to consume long contexts at once. This is achieved by a combination of additive attention and elementwise products. While initial results look promising, I have my reservations...\n\nOUTLINE:\n0:00 - Intro & Outline\n2:15 - Fastformer description\n5:20 - Baseline: Classic Attention\n10:00 - Fastformer architecture\n12:50 - Additive Attention\n18:05 - Query-Key element-wise multiplication\n21:35 - Redundant modules in Fastformer\n25:00 - Problems with the architecture\n27:30 - Is this even attention?\n32:20 - Experimental Results\n34:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2108.09084\n\nAbstract:\nTransformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.\n\nAuthors: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2108.09084"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=-Kgxv64aG3o",
        "summary": "#alibi #transformers #attention\n\nTransformers are essentially set models that need additional inputs to make sense of sequence data. The most widespread additional inputs are position encodings or position embeddings, which add sequence index information in various forms. However, this has put a limit on the resulting model, which cannot run inference on sequences longer than it has been trained on, as it would encounter unfamiliar position encodings. ALiBi solves this by proposing simple linear fixed biases as position information, adding negligible overhead in time and memory, but surprisingly, the resulting model is able to handle inference on sequences many times as long as its training sequences.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Position Encodings in Transformers\n4:55 - Sinusoidial Position Encodings\n11:50 - ALiBi Position Encodings\n20:50 - How to choose the slope parameter\n23:55 - Experimental Results\n29:10 - Comments & Conclusion\n\nPaper: https://ofir.io/train_short_test_long.pdf\nCode: https://github.com/ofirpress/attention_with_linear_biases\n\nAbstract:\nSince the introduction of the transformer model by Vaswani et al. (2017), a fundamental question remains open: how to achieve extrapolation at inference time to longer sequences than seen during training? We first show that extrapolation can be improved by changing the position representation method, though we find that existing proposals do not allow efficient extrapolation. We introduce a simple and efficient method, Attention with Linear Biases (ALiBi), that allows for extrapolation. ALiBi does not add positional embeddings to the word embeddings; instead, it biases the query-key attention scores with a term that is proportional to their distance. We show that this method allows training a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048, 11% faster and using 11% less memory. ALiBi\u2019s inductive bias towards recency allows it to outperform multiple strong position methods on the WikiText-103 benchmark. Finally, we provide analysis of ALiBi to understand why it leads to better performance.\n\nAuthors: Ofir Press, Noah A. Smith, Mike Lewis\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://ofir.io/train_short_test_long.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=0JlB9gufTw8",
        "summary": "#inftyformer #infinityformer #transformer\n\nVanilla Transformers are excellent sequence models, but suffer from very harsch constraints on the length of the sequences they can process. Several attempts have been made to extend the Transformer's sequence length, but few have successfully gone beyond a constant factor improvement. This paper presents a method, based on continuous attention mechanisms, to attend to an unbounded past sequence by representing the past as a continuous signal, rather than a sequence. This enables the Infty-Former to effectively enrich the current context with global information, which increases performance on long-range dependencies in sequence tasks. Further, the paper presents the concept of sticky memories, which highlight past events that are of particular importance and elevates their representation in the long-term memory.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Sponsor Spot: Weights & Biases\n3:35 - Problem Statement\n8:00 - Continuous Attention Mechanism\n16:25 - Unbounded Memory via concatenation & contraction\n18:05 - Does this make sense?\n20:25 - How the Long-Term Memory is used in an attention layer\n27:40 - Entire Architecture Recap\n29:30 - Sticky Memories by Importance Sampling\n31:25 - Commentary: Pros and cons of using heuristics\n32:30 - Experiments & Results\n\nPaper: https://arxiv.org/abs/2109.00301\n\nSponsor: Weights & Biases\nhttps://wandb.me/start\n\nAbstract:\nTransformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the \u221e-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \u221e-former's attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain \"sticky memories\" while keeping a fixed computation budget. Experiments on a synthetic sorting task demonstrate the ability of the \u221e-former to retain information from long sequences. We also perform experiments on language modeling, by training a model from scratch and by fine-tuning a pre-trained language model, which show benefits of unbounded long-term memories.\n\nAuthors: Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2109.00301"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=pBau7umFhjQ",
        "summary": "#tvae #topographic #equivariant\n\nVariational Autoencoders model the latent space as a set of independent Gaussian random variables, which the decoder maps to a data distribution. However, this independence is not always desired, for example when dealing with video sequences, we know that successive frames are heavily correlated. Thus, any latent space dealing with such data should reflect this in its structure. Topographic VAEs are a framework for defining correlation structures among the latent variables and induce equivariance within the resulting model. This paper shows how such correlation structures can be built by correctly arranging higher-level variables, which are themselves independent Gaussians.\n\nOUTLINE:\n0:00 - Intro\n1:40 - Architecture Overview\n6:30 - Comparison to regular VAEs\n8:35 - Generative Mechanism Formulation\n11:45 - Non-Gaussian Latent Space\n17:30 - Topographic Product of Student-t\n21:15 - Introducing Temporal Coherence\n24:50 - Topographic VAE\n27:50 - Experimental Results\n31:15 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2109.01394\nCode: https://github.com/akandykeller/topographicvae\n\nAbstract:\nIn this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. \"capsules\") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.\n\nAuthors: T. Anderson Keller, Max Welling\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2109.01394"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=aX8phGhG8VQ",
        "summary": "#gpt-3 #truth #conspiracy\n\nA new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people's criticisms of large language models, but is it really the correct conclusion?\n\nOUTLINE:\n0:00 - Intro\n0:30 - Twitter Paper Announcement\n4:10 - Large Language Models are to blame!\n5:50 - How was the dataset constructed?\n9:25 - The questions are adversarial\n12:30 - Are you surprised?!\n\nPaper: https://arxiv.org/abs/2109.07958\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2109.07958"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=19Q-vMd9bYg",
        "summary": "#neurips #peerreview #nips\n\nThe peer-review system at Machine Learning conferences has come under much criticism over the last years. One major driver was the infamous 2014 NeurIPS experiment, where a subset of papers were given to two different sets of reviewers. This experiment showed that only about half of all accepted papers were consistently accepted by both committees and demonstrated significant influence of subjectivity. This paper revisits the data from the 2014 experiment and traces the fate of accepted and rejected papers during the 7 years since, and analyzes how well reviewers can assess future impact, among other things.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Recap: The 2014 NeurIPS Experiment\n5:40 - How much of reviewing is subjective?\n11:00 - Validation via simulation\n15:45 - Can reviewers predict future impact?\n23:10 - Discussion & Comments\n\nPaper: https://arxiv.org/abs/2109.09774\nCode: https://github.com/lawrennd/neurips2014/\n\nAbstract:\nIn this paper we revisit the 2014 NeurIPS experiment that examined inconsistency in conference peer review. We determine that 50% of the variation in reviewer quality scores was subjective in origin. Further, with seven years passing since the experiment we find that for accepted papers, there is no correlation between quality scores and impact of the paper as measured as a function of citation count. We trace the fate of rejected papers, recovering where these papers were eventually published. For these papers we find a correlation between quality scores and impact. We conclude that the reviewing process for the 2014 conference was good for identifying poor papers, but poor for identifying good papers. We give some suggestions for improving the reviewing process but also warn against removing the subjective element. Finally, we suggest that the real conclusion of the experiment is that the community should place less onus on the notion of top-tier conference publications when assessing the quality of individual researchers. For NeurIPS 2021, the PCs are repeating the experiment, as well as conducting new ones.\n\nAuthors: Corinna Cortes, Neil D. Lawrence\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2109.09774"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=wTzvKB6D_34",
        "summary": "#deeplearning #co2 #cost\n\nDeep Learning has achieved impressive results in the last years, not least due to the massive increases in computational power and data that has gone into these models. Scaling up currently promises to be a reliable way to create more performant systems, but how far can we go? This article explores the limits of exponential scaling in AI, and what people are doing to get around this problem\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Deep Learning at its limits\n3:10 - The cost of overparameterization\n5:40 - Extrapolating power usage and CO2 emissions\n10:45 - We cannot just continue scaling up\n13:25 - Current solution attempts\n15:25 - Aside: ImageNet V2\n17:50 - Are symbolic methods the way out?\n\nPaper: https://spectrum.ieee.org/deep-learning-computational-cost\n\nImage by Ralf Vetterle from Pixabay: https://pixabay.com/images/id-1752876/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://spectrum.ieee.org/deep-learning-computational-cost"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=dND-7llwrpw",
        "summary": "#grokking #openai #deeplearning\n\nGrokking is a phenomenon when a neural network suddenly learns a pattern in the dataset and jumps from random chance generalization to perfect generalization very suddenly. This paper demonstrates grokking on small algorithmic datasets where a network has to fill in binary tables. Interestingly, the learned latent spaces show an emergence of the underlying binary operations that the data were created with.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Grokking Phenomenon\n3:50 - Related: Double Descent\n7:50 - Binary Operations Datasets\n11:45 - What quantities influence grokking?\n15:40 - Learned Emerging Structure\n17:35 - The role of smoothness\n21:30 - Simple explanations win\n24:30 - Why does weight decay encourage simplicity?\n26:40 - Appendix\n28:55 - Conclusion & Comments\n\nPaper: https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf\n\nAbstract:\nIn this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \u201cgrokking\u201d a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.\n\nAuthors: Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin & Vedant Misra\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=kP-dXK9JEhY",
        "summary": "#gpt3 #knowledge #symbolic\n\nSymbolic knowledge models are usually trained on human-generated corpora that are cumbersome and expensive to create. Such corpora consist of structured triples of symbolic knowledge. This paper takes a different approach and attempts to generate such a corpus by prompting GPT-3. Results show that clever prompting, combined with targeted small critic models trained on human ratings can outperform both human-generated data, as well as the teacher model (GPT-3) itself. The results of this paper give a general recipe for automatically building corpora for various NLP tasks by extracting samples from large language models.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Sponsor: Weights & Biases\n4:15 - Commonsense Knowledge Graphs\n7:50 - ATOMIC dataset\n10:00 - Generating the corpus from a model\n13:00 - Prompting GPT-3\n15:30 - Generating Events\n18:40 - Generating Inferences\n23:00 - Evaluating the created dataset\n26:45 - Introducing the critic\n31:25 - Using the critic to filter the data\n36:30 - Training a student on the generated data\n41:00 - Key Findings\n44:45 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2110.07178\nCode & Corpus: https://github.com/peterwestai2/symbolic-knowledge-distillation\n\nSponsor: Weights & Biases\nhttps://wandb.com\nhttps://community.wandb.ai/\n\nAbstract:\nThe common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.\n\nAuthors: Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2110.07178"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=NJCLUzkn-sA",
        "summary": "#efficientzero #muzero #atari\n\nReinforcement Learning methods are notoriously data-hungry. Notably, MuZero learns a latent world model just from scalar feedback of reward- and policy-predictions, and therefore relies on scale to perform well. However, most RL algorithms fail when presented with very little data. EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low-sample setting. This could be a staple algorithm for future RL research.\n\nOUTLINE:\n0:00 - Intro & Outline\n2:30 - MuZero Recap\n10:50 - EfficientZero improvements\n14:15 - Self-Supervised consistency loss\n17:50 - End-to-end prediction of the value prefix\n20:40 - Model-based off-policy correction\n25:45 - Experimental Results & Conclusion\n\nPaper: https://arxiv.org/abs/2111.00210\nCode: https://github.com/YeWR/EfficientZero\nNote: code not there yet as of release of this video\n\nAbstract:\nReinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 190.4% mean human performance and 116.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.\n\nAuthors: Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2111.00210"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=2h4tRsQzipQ",
        "summary": "#machinelearning #ardm #generativemodels\n\nDiffusion models have made large advances in recent months as a new type of generative models. This paper introduces Autoregressive Diffusion Models (ARDMs), which are a mix between autoregressive generative models and diffusion models. ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time. This paper applies ARDMs to both text and image data, and as an extension, the models can also be used to perform lossless compression.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Decoding Order in Autoregressive Models\n6:15 - Autoregressive Diffusion Models\n8:35 - Dependent and Independent Sampling\n14:25 - Application to Character-Level Language Models\n18:15 - How Sampling & Training Works\n26:05 - Extension 1: Parallel Sampling\n29:20 - Extension 2: Depth Upscaling\n33:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2110.02037\n\nAbstract:\nWe introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.\n\nAuthors: Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2110.02037"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=EeMhj0sPrhE",
        "summary": "#deeplearning #backpropagation #simulation\n\nMore and more systems are made differentiable, which means that accurate gradients of these systems' dynamics can be computed exactly. While this development has led to a lot of advances, there are also distinct situations where backpropagation can be a very bad idea. This paper characterizes a few such systems in the domain of iterated dynamical systems, often including some source of stochasticity, resulting in chaotic behavior. In these systems, it is often better to use black-box estimators for gradients than computing them exactly.\n\nOUTLINE:\n0:00 - Foreword\n1:15 - Intro & Overview\n3:40 - Backpropagation through iterated systems\n12:10 - Connection to the spectrum of the Jacobian\n15:35 - The Reparameterization Trick\n21:30 - Problems of reparameterization\n26:35 - Example 1: Policy Learning in Simulation\n33:05 - Example 2: Meta-Learning Optimizers\n36:15 - Example 3: Disk packing\n37:45 - Analysis of Jacobians\n40:20 - What can be done?\n45:40 - Just use Black-Box methods\n\nPaper: https://arxiv.org/abs/2111.05803\n\nAbstract:\nDifferentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.\n\nAuthors: Luke Metz, C. Daniel Freeman, Samuel S. Schoenholz, Tal Kachman\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2111.05803"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=vVRC-0VKPrg",
        "summary": "#grafting #adam #sgd\n\nThe last years in deep learning research have given rise to a plethora of different optimization algorithms, such as SGD, AdaGrad, Adam, LARS, LAMB, etc. which all claim to have their special peculiarities and advantages. In general, all algorithms modify two major things: The (implicit) learning rate schedule, and a correction to the gradient direction. This paper introduces grafting, which allows to transfer the induced learning rate schedule of one optimizer to another one. In that, the paper shows that much of the benefits of adaptive methods (e.g. Adam) are actually due to this schedule, and not necessarily to the gradient direction correction. Grafting allows for more fundamental research into differences and commonalities between optimizers, and a derived version of it makes it possible to computes static learning rate corrections for SGD, which potentially allows for large savings of GPU memory.\n\nOUTLINE\n0:00 - Rant about Reviewer #2\n6:25 - Intro & Overview\n12:25 - Adaptive Optimization Methods\n20:15 - Grafting Algorithm\n26:45 - Experimental Results\n31:35 - Static Transfer of Learning Rate Ratios\n35:25 - Conclusion & Discussion\n\nPaper (OpenReview): https://openreview.net/forum?id=FpKgG31Z_i9\nOld Paper (Arxiv): https://arxiv.org/abs/2002.11803\n\nOur Discord: https://discord.gg/4H8xxDF\n\nAbstract:\nIn the empirical science of training large neural networks, the learning rate schedule is a notoriously challenging-to-tune hyperparameter, which can depend on all other properties (architecture, optimizer, batch size, dataset, regularization, ...) of the problem. In this work, we probe the entanglements between the optimizer and the learning rate schedule. We propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Using grafting, we discover a non-adaptive learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance. Besides providing a resource-saving tool for practitioners, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning.\n\nAuthors: Anonymous (Under Review)\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n",
        "sourceType": "youtube",
        "linkToPaper": "https://arxiv.org/abs/2002.11803"
    }
]